<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Report - Multi-Source Literature Review</title>
    <style>
        * { box-sizing: border-box; margin: 0; padding: 0; }
        :root {
            --accent: #00d4ff;
            --accent-secondary: #667eea;
            --bg-dark: #0a0a1a;
            --bg-card: #12122a;
            --bg-hover: #1a1a3a;
            --text-primary: #e8e8f0;
            --text-secondary: #888899;
            --border: #2a2a4a;
            --success: #10b981;
            --warning: #f59e0b;
            --highlight: #ec4899;
        }
        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: var(--bg-dark);
            color: var(--text-primary);
            min-height: 100vh;
            line-height: 1.6;
        }
        .container { max-width: 1600px; margin: 0 auto; padding: 30px; }

        /* Header */
        header {
            text-align: center;
            padding: 60px 40px;
            margin-bottom: 40px;
            background: linear-gradient(180deg, #1a1a3a 0%, var(--bg-dark) 100%);
            border-bottom: 1px solid var(--border);
        }
        h1 {
            color: var(--text-primary);
            font-size: 2.8rem;
            font-weight: 700;
            margin-bottom: 10px;
            letter-spacing: -0.5px;
        }
        .subtitle {
            color: var(--text-secondary);
            font-size: 1.1rem;
            margin-bottom: 15px;
        }
        .timeline-link {
            display: inline-block;
            padding: 12px 28px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: #fff;
            text-decoration: none;
            border-radius: 25px;
            font-weight: 600;
            font-size: 0.95rem;
            margin-bottom: 20px;
            transition: all 0.3s;
            box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4);
        }
        .timeline-link:hover {
            transform: translateY(-3px);
            box-shadow: 0 8px 25px rgba(102, 126, 234, 0.6);
        }
        .stats {
            display: flex;
            justify-content: center;
            gap: 40px;
            margin-top: 30px;
        }
        .stat {
            text-align: center;
        }
        .stat-value {
            font-size: 2.5rem;
            font-weight: 700;
            color: var(--accent);
            display: block;
        }
        .stat-label {
            font-size: 0.9rem;
            color: var(--text-secondary);
            text-transform: uppercase;
            letter-spacing: 1px;
        }

        /* Filters */
        .filters {
            background: var(--bg-card);
            border: 1px solid var(--border);
            border-radius: 12px;
            padding: 20px 25px;
            margin-bottom: 30px;
            display: flex;
            flex-wrap: wrap;
            gap: 12px;
            align-items: center;
        }
        .filter-label {
            color: var(--text-secondary);
            font-size: 0.85rem;
            font-weight: 500;
            margin-right: 5px;
        }
        .filter-btn {
            padding: 8px 16px;
            border-radius: 20px;
            border: 1px solid var(--border);
            background: transparent;
            color: var(--text-secondary);
            cursor: pointer;
            font-size: 0.85rem;
            transition: all 0.2s;
        }
        .filter-btn:hover, .filter-btn.active {
            background: var(--accent);
            border-color: var(--accent);
            color: var(--bg-dark);
        }
        .search-box {
            padding: 10px 16px;
            border-radius: 8px;
            border: 1px solid var(--border);
            background: var(--bg-dark);
            color: var(--text-primary);
            font-size: 0.9rem;
            width: 280px;
            margin-left: auto;
        }
        .search-box:focus {
            outline: none;
            border-color: var(--accent);
        }
        .filter-divider {
            width: 1px;
            height: 30px;
            background: var(--border);
            margin: 0 10px;
        }

        /* Year sections */
        .year-section {
            margin-bottom: 50px;
        }
        .year-header {
            font-size: 1.8rem;
            font-weight: 700;
            color: var(--text-primary);
            margin-bottom: 25px;
            padding-bottom: 15px;
            border-bottom: 2px solid var(--accent);
            display: inline-block;
        }

        /* Paper grid - PSI Lab style */
        .papers-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(450px, 1fr));
            gap: 25px;
        }

        /* Paper card */
        .paper-card {
            background: var(--bg-card);
            border: 1px solid var(--border);
            border-radius: 12px;
            overflow: hidden;
            transition: all 0.3s ease;
            display: flex;
            flex-direction: column;
        }
        .paper-card:hover {
            transform: translateY(-4px);
            border-color: var(--accent);
            box-shadow: 0 20px 40px rgba(0, 212, 255, 0.1);
        }

        /* Card thumbnail area */
        .card-visual {
            height: 180px;
            background: linear-gradient(135deg, #1a1a3a 0%, #2a2a5a 100%);
            display: flex;
            align-items: center;
            justify-content: center;
            position: relative;
            overflow: hidden;
        }
        .card-visual-icon {
            font-size: 4rem;
            opacity: 0.3;
        }
        .card-badges {
            position: absolute;
            top: 12px;
            right: 12px;
            display: flex;
            gap: 6px;
        }
        .badge {
            padding: 4px 10px;
            border-radius: 12px;
            font-size: 0.7rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }
        .badge-citations {
            background: var(--warning);
            color: #000;
        }
        .badge-new {
            background: var(--success);
            color: #000;
        }
        .badge-highlight {
            background: var(--highlight);
            color: #fff;
        }
        /* Research theme badges */
        .theme-badge {
            padding: 3px 8px;
            border-radius: 4px;
            font-size: 0.65rem;
            font-weight: 600;
            text-transform: uppercase;
        }
        .theme-L { background: #3b82f6; color: #fff; }  /* Locomotion - Blue */
        .theme-I { background: #8b5cf6; color: #fff; }  /* Imitation - Purple */
        .theme-M { background: #f59e0b; color: #000; }  /* Manipulation - Orange */
        .theme-E { background: #10b981; color: #000; }  /* Embodied AI - Green */
        .theme-R { background: #ef4444; color: #fff; }  /* Multi-Robot - Red */
        .theme-S { background: #6366f1; color: #fff; }  /* Safety - Indigo */
        /* Read rating badges */
        .rating-must_read { background: linear-gradient(135deg, #fbbf24, #f59e0b); color: #000; }
        .rating-optional { background: rgba(255,255,255,0.2); color: var(--text-secondary); border: 1px solid var(--border); }
        .rating-skip { background: rgba(100,100,100,0.3); color: var(--text-secondary); }
        /* Summary info block */
        .summary-block {
            background: rgba(0, 212, 255, 0.05);
            border-left: 3px solid var(--accent);
            padding: 12px 15px;
            margin: 10px 0;
            border-radius: 0 8px 8px 0;
        }
        .summary-label {
            font-size: 0.7rem;
            color: var(--accent);
            text-transform: uppercase;
            letter-spacing: 0.5px;
            margin-bottom: 4px;
        }
        .summary-text {
            font-size: 0.85rem;
            color: var(--text-secondary);
            line-height: 1.5;
        }
        .pros-cons {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 10px;
            margin-top: 10px;
        }
        .pros { border-left-color: var(--success); }
        .cons { border-left-color: var(--highlight); }
        .theme-tags {
            display: flex;
            gap: 4px;
            flex-wrap: wrap;
            margin-top: 8px;
        }
        .card-source {
            position: absolute;
            bottom: 12px;
            left: 12px;
            padding: 4px 10px;
            background: rgba(0,0,0,0.6);
            border-radius: 6px;
            font-size: 0.75rem;
            color: var(--text-secondary);
        }
        .card-tech {
            position: absolute;
            bottom: 12px;
            right: 12px;
            display: flex;
            gap: 4px;
        }
        .tech-chip {
            padding: 3px 8px;
            background: rgba(102, 126, 234, 0.3);
            border-radius: 4px;
            font-size: 0.65rem;
            color: var(--accent-secondary);
        }

        /* Card content */
        .card-content {
            padding: 20px;
            flex: 1;
            display: flex;
            flex-direction: column;
        }
        .card-title {
            font-size: 1.05rem;
            font-weight: 600;
            color: var(--text-primary);
            margin-bottom: 10px;
            line-height: 1.4;
            display: -webkit-box;
            -webkit-line-clamp: 2;
            -webkit-box-orient: vertical;
            overflow: hidden;
        }
        .card-title a {
            color: inherit;
            text-decoration: none;
        }
        .card-title a:hover {
            color: var(--accent);
        }
        .card-authors {
            font-size: 0.85rem;
            color: var(--text-secondary);
            margin-bottom: 12px;
            display: -webkit-box;
            -webkit-line-clamp: 1;
            -webkit-box-orient: vertical;
            overflow: hidden;
        }
        .card-keywords {
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            margin-bottom: 15px;
        }
        .keyword-tag {
            padding: 3px 10px;
            background: rgba(0, 212, 255, 0.1);
            border: 1px solid rgba(0, 212, 255, 0.3);
            border-radius: 12px;
            font-size: 0.7rem;
            color: var(--accent);
        }

        /* Card details (expandable) */
        .card-details {
            display: none;
            margin-top: 15px;
            padding-top: 15px;
            border-top: 1px solid var(--border);
        }
        .card-details.show {
            display: block;
        }
        .detail-block {
            margin-bottom: 15px;
        }
        .detail-label {
            font-size: 0.75rem;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            color: var(--accent);
            margin-bottom: 6px;
            font-weight: 600;
        }
        .detail-text {
            font-size: 0.85rem;
            color: var(--text-secondary);
            line-height: 1.6;
        }
        .abstract-text {
            max-height: 150px;
            overflow-y: auto;
        }

        /* Card footer */
        .card-footer {
            padding: 15px 20px;
            background: rgba(0,0,0,0.2);
            border-top: 1px solid var(--border);
            display: flex;
            gap: 8px;
            align-items: center;
        }
        .card-link {
            padding: 6px 14px;
            border-radius: 6px;
            font-size: 0.8rem;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s;
            display: inline-flex;
            align-items: center;
            gap: 5px;
        }
        .link-paper {
            background: var(--accent);
            color: var(--bg-dark);
        }
        .link-pdf {
            background: var(--success);
            color: #000;
        }
        .link-doi {
            background: var(--warning);
            color: #000;
        }
        .card-link:hover {
            transform: scale(1.05);
        }
        .expand-toggle {
            margin-left: auto;
            background: transparent;
            border: 1px solid var(--border);
            color: var(--text-secondary);
            padding: 6px 12px;
            border-radius: 6px;
            cursor: pointer;
            font-size: 0.8rem;
            transition: all 0.2s;
        }
        .expand-toggle:hover {
            border-color: var(--accent);
            color: var(--accent);
        }
        .cite-btn {
            background: linear-gradient(135deg, #764ba2, #667eea);
            border: none;
            color: #fff;
            padding: 6px 14px;
            border-radius: 6px;
            cursor: pointer;
            font-size: 0.8rem;
            font-weight: 500;
            transition: all 0.2s;
            display: inline-flex;
            align-items: center;
            gap: 5px;
        }
        .cite-btn:hover {
            transform: scale(1.05);
            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4);
        }
        .cite-btn.copied {
            background: var(--success);
        }
        .cite-tooltip {
            position: fixed;
            bottom: 30px;
            left: 50%;
            transform: translateX(-50%);
            background: var(--success);
            color: #000;
            padding: 12px 24px;
            border-radius: 8px;
            font-size: 0.9rem;
            font-weight: 600;
            z-index: 1000;
            opacity: 0;
            transition: opacity 0.3s;
            pointer-events: none;
        }
        .cite-tooltip.show {
            opacity: 1;
        }

        /* View Toggle */
        .view-toggle {
            display: flex;
            align-items: center;
            gap: 12px;
            margin-bottom: 20px;
        }
        .toggle-btn {
            padding: 12px 24px;
            border-radius: 8px;
            border: 2px solid var(--border);
            background: transparent;
            color: var(--text-secondary);
            cursor: pointer;
            font-size: 0.95rem;
            font-weight: 600;
            transition: all 0.2s;
        }
        .toggle-btn:hover {
            border-color: var(--accent);
            color: var(--accent);
        }
        .toggle-btn.active {
            background: var(--accent);
            border-color: var(--accent);
            color: var(--bg-dark);
        }

        /* Section headers */
        .section-header {
            font-size: 1.6rem;
            font-weight: 700;
            color: var(--text-primary);
            margin-bottom: 25px;
            padding-bottom: 15px;
            border-bottom: 2px solid var(--accent);
            display: flex;
            align-items: center;
            gap: 10px;
        }
        .section-count {
            font-size: 1rem;
            font-weight: 400;
            color: var(--text-secondary);
        }
        .group-section {
            margin-bottom: 50px;
        }

        /* Utility */
        .hidden { display: none !important; }

        /* Responsive */
        @media (max-width: 768px) {
            .container { padding: 15px; }
            h1 { font-size: 1.8rem; }
            .papers-grid { grid-template-columns: 1fr; }
            .stats { flex-direction: column; gap: 20px; }
            .search-box { width: 100%; margin: 10px 0; }
            .filters { flex-direction: column; align-items: stretch; }
            .view-toggle { flex-wrap: wrap; }
        }
    </style>
</head>
<body>
    <header>
        <h1>Research Literature Review</h1>
        <p class="subtitle">Multi-Source Academic Paper Analysis | summary, gaps, semantic, arxiv, googlescholar, scopusWOS</p>
        <a href="research_timeline.html" class="timeline-link">üó∫Ô∏è Interactive Timeline & Mindmap ‚Üí</a>
        <div class="stats">
            <div class="stat">
                <span class="stat-value">411</span>
                <span class="stat-label">Papers</span>
            </div>
            <div class="stat">
                <span class="stat-value">2</span>
                <span class="stat-label">Sources</span>
            </div>
            <div class="stat">
                <span class="stat-value">17</span>
                <span class="stat-label">Categories</span>
            </div>
            <div class="stat">
                <span class="stat-value">6</span>
                <span class="stat-label">Years</span>
            </div>
        </div>
    </header>

    <div class="container">
        <!-- View Toggle -->
        <div class="view-toggle">
            <span class="filter-label">Group by:</span>
            <button class="toggle-btn active" onclick="setView('year')" id="btn-year">üìÖ Year</button>
            <button class="toggle-btn" onclick="setView('category')" id="btn-category">üè∑Ô∏è Category</button>
        </div>

        <div class="filters">
            <span class="filter-label">Source:</span>
            <button class="filter-btn source-btn active" onclick="filterBySource('all')">All</button>
            <button class="filter-btn source-btn" onclick="filterBySource('arXiv')">arXiv</button><button class="filter-btn source-btn" onclick="filterBySource('Scopus')">Scopus</button>
            <div class="filter-divider"></div>
            <span class="filter-label">Year:</span>
            <button class="filter-btn year-btn active" onclick="filterByYear('all')">All</button>
            <button class="filter-btn year-btn" onclick="filterByYear(2026)">2026</button><button class="filter-btn year-btn" onclick="filterByYear(2025)">2025</button><button class="filter-btn year-btn" onclick="filterByYear(2024)">2024</button><button class="filter-btn year-btn" onclick="filterByYear(2023)">2023</button><button class="filter-btn year-btn" onclick="filterByYear(2022)">2022</button><button class="filter-btn year-btn" onclick="filterByYear(2021)">2021</button>
            <div class="filter-divider"></div>
            <span class="filter-label">Rating:</span>
            <button class="filter-btn rating-btn active" onclick="filterByRating('all')">All</button>
            <button class="filter-btn rating-btn" onclick="filterByRating('must_read')">‚≠ê Must Read</button>
            <button class="filter-btn rating-btn" onclick="filterByRating('optional')">üìñ Optional</button>
            <button class="filter-btn rating-btn" onclick="filterByRating('skip')">‚è≠Ô∏è Skip</button>
            <input type="text" class="search-box" placeholder="Search papers & keywords..." oninput="searchPapers(this.value)">
        </div>

        <div class="filters" id="category-filters">
            <span class="filter-label">Category:</span>
            <button class="filter-btn cat-btn active" onclick="filterByCategory('all')">All</button>
            <button class="filter-btn cat-btn" onclick="filterByCategory('other')">OTHER (142)</button><button class="filter-btn cat-btn" onclick="filterByCategory('cs.ro')">Robotics (119)</button><button class="filter-btn cat-btn" onclick="filterByCategory('cs.cl')">Computational Linguistics (95)</button><button class="filter-btn cat-btn" onclick="filterByCategory('cs.lg')">Machine Learning (12)</button><button class="filter-btn cat-btn" onclick="filterByCategory('cs.cv')">Computer Vision (11)</button><button class="filter-btn cat-btn" onclick="filterByCategory('cs.ai')">Artificial Intelligence (10)</button><button class="filter-btn cat-btn" onclick="filterByCategory('cs.cr')">CS.CR (6)</button><button class="filter-btn cat-btn" onclick="filterByCategory('cs.hc')">Human-Computer Interaction (4)</button><button class="filter-btn cat-btn" onclick="filterByCategory('cs.se')">CS.SE (3)</button><button class="filter-btn cat-btn" onclick="filterByCategory('eess.as')">EESS.AS (2)</button>
        </div>

        <!-- Year View -->
        <div id="year-view">

            <div class="group-section year-section" data-year="2026">
                <h2 class="section-header">üìÖ 2026 <span class="section-count">(4 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2026" data-category="cs.cl" data-title="quantifying non deterministic drift in large language models" data-keywords="gpt control ros language model cs.cl cs.ai" data-themes="S I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-new">New</span></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2601.19934v1" target="_blank">Quantifying non deterministic drift in large language models</a>
                            </h3>
                            <p class="card-authors">Claire Nicholson</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Control</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4398405456">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) are widely used for tasks ranging from summarisation to decision support. In practice, identical prompts do not always produce identical outputs, even when temperature and other decoding parameters are fixed. In this work, we conduct repeated-run experiments to empirically quantify baseline behavioural drift, defined as output variability observed when the same prompt is issued multiple times under operator-free conditions. We evaluate two publicly accessible models, gpt-4o-mini and llama3.1-8b, across five prompt categories using exact repeats, perturbed inputs, a...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">In this work, we conduct repeated-run experiments to empirically quantify baseline behavioural drift, defined as output variability observed when the same prompt is issued multiple times under operator-free conditions. We situate these findings within existing work on concept drift, behavioural drift, and infrastructure-induced nondeterminism, discuss the limitations of lexical metrics, and highlight emerging semantic approaches</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large language models (LLMs) are widely used for tasks ranging from summarisation to decision support</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2601.19934v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2601.19934v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Nicholson, &quot;Quantifying non deterministic drift in large language models,&quot; arXiv, 2026. [Online]. Available: http://arxiv.org/abs/2601.19934v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398405456')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2026" data-category="cs.cl" data-title="sharp: social harm analysis via risk profiles for measuring inequities in large language models" data-keywords="ros language model cs.cl cs.ai" data-themes="E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-new">New</span></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2601.21235v1" target="_blank">SHARP: Social Harm Analysis via Risk Profiles for Measuring Inequities in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Alok Abhishek, Tushar Bandopadhyay, Lisa Erickson</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4398402048">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) are increasingly deployed in high-stakes domains, where rare but severe failures can result in irreversible harm. However, prevailing evaluation benchmarks often reduce complex social risk to mean-centered scalar scores, thereby obscuring distributional structure, cross-dimensional interactions, and worst-case behavior. This paper introduces Social Harm Analysis via Risk Profiles (SHARP), a framework for multidimensional, distribution-aware evaluation of social harm. SHARP models harm as a multivariate random variable and integrates explicit decomposition into bias...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large language models (LLMs) are increasingly deployed in high-stakes domains, where rare but severe failures can result in irreversible harm</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2601.21235v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2601.21235v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Abhishek, T. Bandopadhyay, and L. Erickson, &quot;SHARP: Social Harm Analysis via Risk Profiles for Measuring Inequities in Large Language Models,&quot; arXiv, 2026. [Online]. Available: http://arxiv.org/abs/2601.21235v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398402048')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2026" data-category="cs.ro" data-title="locomotion beyond feet" data-keywords="reinforcement learning robot humanoid locomotion planning simulation ros imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-new">New</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2601.03607v1" target="_blank">Locomotion Beyond Feet</a>
                            </h3>
                            <p class="card-authors">Tae Hoon Yang, Haochen Shi, Jiacheng Hu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399347648">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Most locomotion methods for humanoid robots focus on leg-based gaits, yet natural bipeds frequently rely on hands, knees, and elbows to establish additional contacts for stability and support in complex environments. This paper introduces Locomotion Beyond Feet, a comprehensive system for whole-body humanoid locomotion across extremely challenging terrains, including low-clearance spaces under chairs, knee-high walls, knee-high platforms, and steep ascending and descending stairs. Our approach addresses two key challenges: contact-rich motion planning and generalization across diverse terrains...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Our approach addresses two key challenges: contact-rich motion planning and generalization across diverse terrains</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Our approach addresses two key challenges: contact-rich motion planning and generalization across diverse terrains</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Most locomotion methods for humanoid robots focus on leg-based gaits, yet natural bipeds frequently rely on hands, knees, and elbows to establish additional contacts for stability and support in complex environments</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2601.03607v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2601.03607v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. H. Yang et al., &quot;Locomotion Beyond Feet,&quot; arXiv, 2026. [Online]. Available: http://arxiv.org/abs/2601.03607v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399347648')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2026" data-category="cs.ro" data-title="walk the planc: physics-guided rl for agile humanoid locomotion on constrained footholds" data-keywords="reinforcement learning robot humanoid bipedal locomotion perception planning control optimization cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-new">New</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2601.06286v1" target="_blank">Walk the PLANC: Physics-Guided RL for Agile Humanoid Locomotion on Constrained Footholds</a>
                            </h3>
                            <p class="card-authors">Min Dai, William D. Compton, Junheng Li et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span></div>

                            <div class="card-details" id="details-4399347744">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Bipedal humanoid robots must precisely coordinate balance, timing, and contact decisions when locomoting on constrained footholds such as stepping stones, beams, and planks -- even minor errors can lead to catastrophic failure. Classical optimization and control pipelines handle these constraints well but depend on highly accurate mathematical representations of terrain geometry, making them prone to error when perception is noisy or incomplete. Meanwhile, reinforcement learning has shown strong resilience to disturbances and modeling errors, yet end-to-end policies rarely discover the precise...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we introduce a locomotion framework in which a reduced-order stepping planner supplies dynamically consistent motion targets that steer the RL training process via Control Lyapunov Function (CLF) rewards</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">These contrasting limitations motivate approaches that guide learning with physics-based structure rather than relying purely on reward shaping</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Meanwhile, reinforcement learning has shown strong resilience to disturbances and modeling errors, yet end-to-end policies rarely discover the precise foothold placement and step sequencing required for discontinuous terrain</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2601.06286v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2601.06286v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Dai, W. D. Compton, J. Li, L. Yang, and A. D. Ames, &quot;Walk the PLANC: Physics-Guided RL for Agile Humanoid Locomotion on Constrained Footholds,&quot; arXiv, 2026. [Online]. Available: http://arxiv.org/abs/2601.06286v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399347744')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section year-section" data-year="2025">
                <h2 class="section-header">üìÖ 2025 <span class="section-count">(105 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="classifying german language proficiency levels using large language models" data-keywords="language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2512.06483v1" target="_blank">Classifying German Language Proficiency Levels Using Large Language Models</a>
                            </h3>
                            <p class="card-authors">Elias-Leander Ahlers, Witold Brunsmann, Malte Schilling</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4397313488">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Assessing language proficiency is essential for education, as it enables instruction tailored to learners needs. This paper investigates the use of Large Language Models (LLMs) for automatically classifying German texts according to the Common European Framework of Reference for Languages (CEFR) into different proficiency levels. To support robust training and evaluation, we construct a diverse dataset by combining multiple existing CEFR-annotated corpora with synthetic data. We then evaluate prompt-engineering strategies, fine-tuning of a LLaMA-3-8B-Instruct model and a probing-based approach...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper investigates the use of Large Language Models (LLMs) for automatically classifying German texts according to the Common European Framework of Reference for Languages (CEFR) into different proficiency levels</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2512.06483v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2512.06483v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'E. Ahlers, W. Brunsmann, and M. Schilling, &quot;Classifying German Language Proficiency Levels Using Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2512.06483v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397313488')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="reinforcement learning meets large language models: a survey of advancements and applications across the llm lifecycle" data-keywords="reinforcement learning ros language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.16679v1" target="_blank">Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle</a>
                            </h3>
                            <p class="card-authors">Keliang Liu, Dingkang Yang, Ziyun Qian et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4397308784">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In recent years, training methods centered on Reinforcement Learning (RL) have markedly enhanced the reasoning and alignment performance of Large Language Models (LLMs), particularly in understanding human intents, following user instructions, and bolstering inferential strength. Although existing surveys offer overviews of RL augmented LLMs, their scope is often limited, failing to provide a comprehensive summary of how RL operates across the full lifecycle of LLMs. We systematically review the theoretical and practical advancements whereby RL empowers LLMs, especially Reinforcement Learning ...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Finally, we analyse the future challenges and trends in the field of RL-enhanced LLMs</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In recent years, training methods centered on Reinforcement Learning (RL) have markedly enhanced the reasoning and alignment performance of Large Language Models (LLMs), particularly in understanding human intents, following user instructions, and bolstering inferential strength</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.16679v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.16679v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Liu et al., &quot;Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.16679v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397308784')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="emissions and performance trade-off between small and large language models" data-keywords="ros language model cs.cl cs.ai cs.cy" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2601.08844v1" target="_blank">Emissions and Performance Trade-off Between Small and Large Language Models</a>
                            </h3>
                            <p class="card-authors">Anandita Garg, Uma Gaba, Deepan Muthirayan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4397306912">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The advent of Large Language Models (LLMs) has raised concerns about their enormous carbon footprint, starting with energy-intensive training and continuing through repeated inference. This study investigates the potential of using fine-tuned Small Language Models (SLMs) as a sustainable alternative for predefined tasks. Here, we present a comparative analysis of the performance-emissions trade-off between LLMs and fine-tuned SLMs across selected tasks under Natural Language Processing, Reasoning and Programming. Our results show that in four out of the six selected tasks, SLMs maintained comp...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Here, we present a comparative analysis of the performance-emissions trade-off between LLMs and fine-tuned SLMs across selected tasks under Natural Language Processing, Reasoning and Programming</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The advent of Large Language Models (LLMs) has raised concerns about their enormous carbon footprint, starting with energy-intensive training and continuing through repeated inference</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2601.08844v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2601.08844v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Garg, U. Gaba, D. Muthirayan, and A. R. Chowdhury, &quot;Emissions and Performance Trade-off Between Small and Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2601.08844v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397306912')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cond-mat.mtrl-sci" data-title="hierarchical multi-agent large language model reasoning for autonomous functional materials discovery" data-keywords="multi-agent simulation ros imu language model cond-mat.mtrl-sci cs.ai cs.cl" data-themes="I E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ COND-MAT.MTRL-SCI</span>
                            <div class="card-tech"><span class="tech-chip">Multi-Agent Systems</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2512.13930v1" target="_blank">Hierarchical Multi-agent Large Language Model Reasoning for Autonomous Functional Materials Discovery</a>
                            </h3>
                            <p class="card-authors">Samuel Rothfarb, Megan C. Davis, Ivana Matanovic et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Multi-Agent</span><span class="keyword-tag">Simulation</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Imu</span></div>

                            <div class="card-details" id="details-4397308448">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Artificial intelligence is reshaping scientific exploration, but most methods automate procedural tasks without engaging in scientific reasoning, limiting autonomy in discovery. We introduce Materials Agents for Simulation and Theory in Electronic-structure Reasoning (MASTER), an active learning framework where large language models autonomously design, execute, and interpret atomistic simulations. In MASTER, a multimodal system translates natural language into density functional theory workflows, while higher-level reasoning agents guide discovery through a hierarchy of strategies, including ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce Materials Agents for Simulation and Theory in Electronic-structure Reasoning (MASTER), an active learning framework where large language models autonomously design, execute, and interpret atomistic simulations</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Artificial intelligence is reshaping scientific exploration, but most methods automate procedural tasks without engaging in scientific reasoning, limiting autonomy in discovery</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2512.13930v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2512.13930v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Rothfarb, M. C. Davis, I. Matanovic, B. Li, E. F. Holby, and W. J. M. Kort-Kamp, &quot;Hierarchical Multi-agent Large Language Model Reasoning for Autonomous Functional Materials Discovery,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2512.13930v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397308448')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.se" data-title="a survey of aiops in the era of large language models" data-keywords="attention ros language model cs.se cs.cl" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ CS.SE</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.12472v1" target="_blank">A Survey of AIOps in the Era of Large Language Models</a>
                            </h3>
                            <p class="card-authors">Lingzhe Zhang, Tong Jia, Mengxi Jia et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.SE</span></div>

                            <div class="card-details" id="details-4397304560">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">As large language models (LLMs) grow increasingly sophisticated and pervasive, their application to various Artificial Intelligence for IT Operations (AIOps) tasks has garnered significant attention. However, a comprehensive understanding of the impact, potential, and limitations of LLMs in AIOps remains in its infancy. To address this gap, we conducted a detailed survey of LLM4AIOps, focusing on how LLMs can optimize processes and improve outcomes in this domain. We analyzed 183 research papers published between January 2020 and December 2024 to answer four key research questions (RQs). In RQ...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, a comprehensive understanding of the impact, potential, and limitations of LLMs in AIOps remains in its infancy. To address this gap, we conducted a detailed survey of LLM4AIOps, focusing on how LLMs can optimize processes and improve outcomes in this domain</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">As large language models (LLMs) grow increasingly sophisticated and pervasive, their application to various Artificial Intelligence for IT Operations (AIOps) tasks has garnered significant attention</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.12472v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.12472v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Zhang et al., &quot;A Survey of AIOps in the Era of Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.12472v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397304560')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="how do language models learn facts? dynamics, curricula and hallucinations" data-keywords="neural network attention imu language model cs.cl cs.lg" data-themes="E L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.21676v2" target="_blank">How do language models learn facts? Dynamics, curricula and hallucinations</a>
                            </h3>
                            <p class="card-authors">Nicolas Zucchet, J√∂rg Bornschein, Stephanie Chan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Attention</span><span class="keyword-tag">Imu</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4397303216">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models accumulate vast knowledge during pre-training, yet the dynamics governing this acquisition remain poorly understood. This work investigates the learning dynamics of language models on a synthetic factual recall task, uncovering three key findings: First, language models learn in three phases, exhibiting a performance plateau before acquiring precise factual knowledge. Mechanistically, this plateau coincides with the formation of attention-based circuits that support recall. Second, the training data distribution significantly impacts learning dynamics, as imbalanced distr...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large language models accumulate vast knowledge during pre-training, yet the dynamics governing this acquisition remain poorly understood</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.21676v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.21676v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. Zucchet, J. Bornschein, S. Chan, A. Lampinen, R. Pascanu, and S. De, &quot;How do language models learn facts? Dynamics, curricula and hallucinations,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.21676v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397303216')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.ai" data-title="large language models reasoning abilities under non-ideal conditions after rl-fine-tuning" data-keywords="reinforcement learning ros language model cs.ai" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Artificial Intelligence</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.04848v1" target="_blank">Large Language Models Reasoning Abilities Under Non-Ideal Conditions After RL-Fine-Tuning</a>
                            </h3>
                            <p class="card-authors">Chang Tian, Matthew B. Blaschko, Mingzhe Xing et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4397308544">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Reinforcement learning (RL) has become a key technique for enhancing the reasoning abilities of large language models (LLMs), with policy-gradient algorithms dominating the post-training stage because of their efficiency and effectiveness. However, most existing benchmarks evaluate large-language-model reasoning under idealized settings, overlooking performance in realistic, non-ideal scenarios. We identify three representative non-ideal scenarios with practical relevance: summary inference, fine-grained noise suppression, and contextual filtering. We introduce a new research direction guided ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce a new research direction guided by brain-science findings that human reasoning remains reliable under imperfect inputs</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Our results reveal that while RL fine-tuning improves baseline reasoning under idealized settings, performance declines significantly across all three non-ideal scenarios, exposing critical limitations in advanced reasoning capabilities</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Reinforcement learning (RL) has become a key technique for enhancing the reasoning abilities of large language models (LLMs), with policy-gradient algorithms dominating the post-training stage because of their efficiency and effectiveness</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.04848v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.04848v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Tian, M. B. Blaschko, M. Xing, X. Li, Y. Yue, and M. Moens, &quot;Large Language Models Reasoning Abilities Under Non-Ideal Conditions After RL-Fine-Tuning,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.04848v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397308544')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="large language models have learned to use language" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2512.12447v1" target="_blank">Large language models have learned to use language</a>
                            </h3>
                            <p class="card-authors">Gary Lupyan</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4397315264">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Acknowledging that large language models have learned to use language can open doors to breakthrough language science. Achieving these breakthroughs may require abandoning some long-held ideas about how language knowledge is evaluated and reckoning with the difficult fact that we have entered a post-Turing test era.</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Achieving these breakthroughs may require abandoning some long-held ideas about how language knowledge is evaluated and reckoning with the difficult fact that we have entered a post-Turing test era.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Acknowledging that large language models have learned to use language can open doors to breakthrough language science</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2512.12447v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2512.12447v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Lupyan, &quot;Large language models have learned to use language,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2512.12447v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397315264')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="soft inductive bias approach via explicit reasoning perspectives in inappropriate utterance detection using large language models" data-keywords="attention ros language model cs.cl" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2512.08480v1" target="_blank">Soft Inductive Bias Approach via Explicit Reasoning Perspectives in Inappropriate Utterance Detection Using Large Language Models</a>
                            </h3>
                            <p class="card-authors">Ju-Young Kim, Ji-Hong Park, Se-Yeon Lee et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398402240">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recent incidents in certain online games and communities, where anonymity is guaranteed, show that unchecked inappropriate remarks frequently escalate into verbal abuse and even criminal behavior, raising significant social concerns. Consequently, there is a growing need for research on techniques that can detect inappropriate utterances within conversational texts to help build a safer communication environment. Although large-scale language models trained on Korean corpora and chain-of-thought reasoning have recently gained attention, research applying these approaches to inappropriate utter...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this study, we propose a soft inductive bias approach that explicitly defines reasoning perspectives to guide the inference process, thereby promoting rational decision-making and preventing errors that may arise during reasoning</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Consequently, there is a growing need for research on techniques that can detect inappropriate utterances within conversational texts to help build a safer communication environment</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2512.08480v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2512.08480v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Kim, J. Park, S. Lee, S. Park, and G. Kim, &quot;Soft Inductive Bias Approach via Explicit Reasoning Perspectives in Inappropriate Utterance Detection Using Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2512.08480v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398402240')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.lg" data-title="two-stage representation learning for analyzing movement behavior dynamics in people living with dementia" data-keywords="language model cs.lg cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Machine Learning</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.09173v1" target="_blank">Two-Stage Representation Learning for Analyzing Movement Behavior Dynamics in People Living with Dementia</a>
                            </h3>
                            <p class="card-authors">Jin Cui, Alexander Capstick, Payam Barnaghi et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4398402000">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In remote healthcare monitoring, time series representation learning reveals critical patient behavior patterns from high-frequency data. This study analyzes home activity data from individuals living with dementia by proposing a two-stage, self-supervised learning approach tailored to uncover low-rank structures. The first stage converts time-series activities into text sequences encoded by a pre-trained language model, providing a rich, high-dimensional latent state space using a PageRank-based method. This PageRank vector captures latent state transitions, effectively compressing complex be...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This study analyzes home activity data from individuals living with dementia by proposing a two-stage, self-supervised learning approach tailored to uncover low-rank structures</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.09173v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.09173v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Cui, A. Capstick, P. Barnaghi, and G. Scott, &quot;Two-Stage Representation Learning for Analyzing Movement Behavior Dynamics in People Living with Dementia,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.09173v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398402000')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="investigating retrieval-augmented generation in quranic studies: a study of 13 open-source large language models" data-keywords="language model cs.cl cs.ai cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.16581v1" target="_blank">Investigating Retrieval-Augmented Generation in Quranic Studies: A Study of 13 Open-Source Large Language Models</a>
                            </h3>
                            <p class="card-authors">Zahra Khalila, Arbi Haza Nasution, Winda Monika et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.LG</span></div>

                            <div class="card-details" id="details-4398401904">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Accurate and contextually faithful responses are critical when applying large language models (LLMs) to sensitive and domain-specific tasks, such as answering queries related to quranic studies. General-purpose LLMs often struggle with hallucinations, where generated responses deviate from authoritative sources, raising concerns about their reliability in religious contexts. This challenge highlights the need for systems that can integrate domain-specific knowledge while maintaining response accuracy, relevance, and faithfulness. In this study, we investigate 13 open-source LLMs categorized in...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">This challenge highlights the need for systems that can integrate domain-specific knowledge while maintaining response accuracy, relevance, and faithfulness. A Retrieval-Augmented Generation (RAG) is used to make up for the problems that come with using separate models</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Accurate and contextually faithful responses are critical when applying large language models (LLMs) to sensitive and domain-specific tasks, such as answering queries related to quranic studies</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.16581v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.16581v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Khalila et al., &quot;Investigating Retrieval-Augmented Generation in Quranic Studies: A Study of 13 Open-Source Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.16581v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398401904')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="knowledge-driven agentic scientific corpus distillation framework for biomedical large language models training" data-keywords="gpt multi-agent language model cs.cl cs.ai q-bio.qm" data-themes="E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Multi-Agent Systems</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.19565v3" target="_blank">Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training</a>
                            </h3>
                            <p class="card-authors">Meng Xiao, Xunxin Cai, Qingqing Long et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Multi-Agent</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4397313296">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Corpus distillation for biomedical large language models (LLMs) seeks to address the pressing challenge of insufficient quantity and quality in open-source annotated scientific corpora, which remains a bottleneck for effective LLM training in biomedical research. This paper proposes a knowledge-driven, agentic framework for scientific corpus distillation, tailored explicitly for LLM training in the biomedical domain, addressing the challenge posed by the complex hierarchy of biomedical knowledge. Central to our approach is a collaborative multi-agent architecture, where specialized agents, eac...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Central to our approach is a collaborative multi-agent architecture, where specialized agents, each guided by the Medical Subject Headings (MeSH) hierarchy, work in concert to autonomously extract, synthesize, and self-evaluate high-quality textual data from vast scientific literature</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Corpus distillation for biomedical large language models (LLMs) seeks to address the pressing challenge of insufficient quantity and quality in open-source annotated scientific corpora, which remains a bottleneck for effective LLM training in biomedical research. This paper proposes a knowledge-driven, agentic framework for scientific corpus distillation, tailored explicitly for LLM training in the biomedical domain, addressing the challenge posed by the complex hierarchy of biomedical knowledge</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Corpus distillation for biomedical large language models (LLMs) seeks to address the pressing challenge of insufficient quantity and quality in open-source annotated scientific corpora, which remains a bottleneck for effective LLM training in biomedical research</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.19565v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.19565v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Xiao, X. Cai, Q. Long, C. Wang, Y. Zhou, and H. Zhu, &quot;Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.19565v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397313296')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cv" data-title="object detection with multimodal large vision-language models: an in-depth review" data-keywords="deep learning robot computer vision nlp language model cs.cv cs.ai cs.cl" data-themes="I M E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computer Vision</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.19294v2" target="_blank">Object Detection with Multimodal Large Vision-Language Models: An In-depth Review</a>
                            </h3>
                            <p class="card-authors">Ranjan Sapkota, Manoj Karkee</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Deep Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Computer Vision</span><span class="keyword-tag">Nlp</span></div>

                            <div class="card-details" id="details-4398401760">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The fusion of language and vision in large vision-language models (LVLMs) has revolutionized deep learning-based object detection by enhancing adaptability, contextual reasoning, and generalization beyond traditional architectures. This in-depth review presents a structured exploration of the state-of-the-art in LVLMs, systematically organized through a three-step research review process. First, we discuss the functioning of vision language models (VLMs) for object detection, describing how these models harness natural language processing (NLP) and computer vision (CV) techniques to revolution...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">The review also identifies a few major limitations of the current LVLM modes, proposes solutions to address those challenges, and presents a clear roadmap for the future advancement in this field</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The fusion of language and vision in large vision-language models (LVLMs) has revolutionized deep learning-based object detection by enhancing adaptability, contextual reasoning, and generalization beyond traditional architectures</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.19294v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.19294v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Sapkota, and M. Karkee, &quot;Object Detection with Multimodal Large Vision-Language Models: An In-depth Review,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.19294v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398401760')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="head-specific intervention can induce misaligned ai coordination in large language models" data-keywords="attention coordination control language model cs.cl cs.ai" data-themes="S I E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.05945v3" target="_blank">Head-Specific Intervention Can Induce Misaligned AI Coordination in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Paul Darm, Annalisa Riccardi</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Coordination</span><span class="keyword-tag">Control</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4398403056">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Robust alignment guardrails for large language models (LLMs) are becoming increasingly important with their widespread application. In contrast to previous studies, we demonstrate that inference-time activation interventions can bypass safety alignments and effectively steer model generations towards harmful AI coordination. Our method applies fine-grained interventions at specific attention heads, which we identify by probing each head in a simple binary choice task. We then show that interventions on these heads generalise to the open-ended generation setting, effectively circumventing safet...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In contrast to previous studies, we demonstrate that inference-time activation interventions can bypass safety alignments and effectively steer model generations towards harmful AI coordination</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Robust alignment guardrails for large language models (LLMs) are becoming increasingly important with their widespread application</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.05945v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.05945v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Darm, and A. Riccardi, &quot;Head-Specific Intervention Can Induce Misaligned AI Coordination in Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.05945v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398403056')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="understanding network behaviors through natural language question-answering" data-keywords="ros language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2510.21894v1" target="_blank">Understanding Network Behaviors through Natural Language Question-Answering</a>
                            </h3>
                            <p class="card-authors">Mingzhe Xing, Chang Tian, Jianan Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4398402288">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Modern large-scale networks introduce significant complexity in understanding network behaviors, increasing the risk of misconfiguration. Prior work proposed to understand network behaviors by mining network configurations, typically relying on domain-specific languages interfaced with formal models. While effective, they suffer from a steep learning curve and limited flexibility. In contrast, natural language (NL) offers a more accessible and interpretable interface, motivating recent research on NL-guided network behavior understanding. Recent advances in large language models (LLMs) further...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To tackle the above challenges, we propose NetMind, a novel framework for querying networks using NL</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, three key challenges remain: 1) numerous router devices with lengthy configuration files challenge LLM&#x27;s long-context understanding ability; 2) heterogeneity across devices and protocols impedes scalability; and 3) complex network topologies and protocols demand advanced reasoning abilities beyond the current capabilities of LLMs. To tackle the above challenges, we propose NetMind, a novel framework for querying networks using NL</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Prior work proposed to understand network behaviors by mining network configurations, typically relying on domain-specific languages interfaced with formal models</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2510.21894v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2510.21894v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Xing et al., &quot;Understanding Network Behaviors through Natural Language Question-Answering,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2510.21894v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398402288')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.ai" data-title="large language models for interpretable mental health diagnosis" data-keywords="language model cs.ai cs.lo" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Artificial Intelligence</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2501.07653v2" target="_blank">Large Language Models for Interpretable Mental Health Diagnosis</a>
                            </h3>
                            <p class="card-authors">Brian Hyeongseok Kim, Chao Wang</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.LO</span></div>

                            <div class="card-details" id="details-4398403248">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We propose a clinical decision support system (CDSS) for mental health diagnosis that combines the strengths of large language models (LLMs) and constraint logic programming (CLP). Having a CDSS is important because of the high complexity of diagnostic manuals used by mental health professionals and the danger of diagnostic errors. Our CDSS is a software tool that uses an LLM to translate diagnostic manuals to a logic program and solves the program using an off-the-shelf CLP engine to query a patient&#x27;s diagnosis based on the encoded rules and provided data. By giving domain experts the opportu...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a clinical decision support system (CDSS) for mental health diagnosis that combines the strengths of large language models (LLMs) and constraint logic programming (CLP)</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We propose a clinical decision support system (CDSS) for mental health diagnosis that combines the strengths of large language models (LLMs) and constraint logic programming (CLP)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2501.07653v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2501.07653v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. H. Kim, and C. Wang, &quot;Large Language Models for Interpretable Mental Health Diagnosis,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2501.07653v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398403248')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cr" data-title="sbfa: single sneaky bit flip attack to break large language models" data-keywords="neural network ros language model cs.cr cs.cl cs.lg" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ CS.CR</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.21843v1" target="_blank">SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models</a>
                            </h3>
                            <p class="card-authors">Jingkai Guo, Chaitali Chakrabarti, Deliang Fan</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CR</span></div>

                            <div class="card-details" id="details-4398403872">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Model integrity of Large language models (LLMs) has become a pressing security concern with their massive online deployment. Prior Bit-Flip Attacks (BFAs) -- a class of popular AI weight memory fault-injection techniques -- can severely compromise Deep Neural Networks (DNNs): as few as tens of bit flips can degrade accuracy toward random guessing. Recent studies extend BFAs to LLMs and reveal that, despite the intuition of better robustness from modularity and redundancy, only a handful of adversarial bit flips can also cause LLMs&#x27; catastrophic accuracy degradation. However, existing BFA metho...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, for the first time, we propose SBFA (Sneaky Bit-Flip Attack), which collapses LLM performance with only one single bit flip while keeping perturbed values within benign layer-wise weight distribution</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Model integrity of Large language models (LLMs) has become a pressing security concern with their massive online deployment</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.21843v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.21843v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Guo, C. Chakrabarti, and D. Fan, &quot;SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.21843v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398403872')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="commander-gpt: fully unleashing the sarcasm detection capability of multi-modal large language models" data-keywords="attention gpt nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.18681v3" target="_blank">Commander-GPT: Fully Unleashing the Sarcasm Detection Capability of Multi-Modal Large Language Models</a>
                            </h3>
                            <p class="card-authors">Yazhou Zhang, Chunwang Zou, Bo Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4398403152">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Sarcasm detection, as a crucial research direction in the field of Natural Language Processing (NLP), has attracted widespread attention. Traditional sarcasm detection tasks have typically focused on single-modal approaches (e.g., text), but due to the implicit and subtle nature of sarcasm, such methods often fail to yield satisfactory results. In recent years, researchers have shifted the focus of sarcasm detection to multi-modal approaches. However, effectively leveraging multi-modal information to accurately identify sarcastic content remains a challenge that warrants further exploration. L...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Leveraging the powerful integrated processing capabilities of Multi-Modal Large Language Models (MLLMs) for various information sources, we propose an innovative multi-modal Commander-GPT framework</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, effectively leveraging multi-modal information to accurately identify sarcastic content remains a challenge that warrants further exploration</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Traditional sarcasm detection tasks have typically focused on single-modal approaches (e.g., text), but due to the implicit and subtle nature of sarcasm, such methods often fail to yield satisfactory results</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.18681v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.18681v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Zhang, C. Zou, B. Wang, and J. Qin, &quot;Commander-GPT: Fully Unleashing the Sarcasm Detection Capability of Multi-Modal Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.18681v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398403152')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.ai" data-title="ars: adaptive reasoning suppression for efficient large reasoning language models" data-keywords="ros language model cs.ai cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Artificial Intelligence</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2510.00071v2" target="_blank">ARS: Adaptive Reasoning Suppression for Efficient Large Reasoning Language Models</a>
                            </h3>
                            <p class="card-authors">Dongqi Zheng</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398404640">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Reasoning Language Models (LRLMs or LRMs) demonstrate remarkable capabilities in complex reasoning tasks, but suffer from significant computational inefficiencies due to overthinking phenomena. Existing efficient reasoning methods face the challenge of balancing reasoning quality with inference cost reduction. We propose \textbf{Adaptive Reasoning Suppression (ARS)}, a novel training-free approach that dynamically suppresses redundant reasoning steps while preserving accuracy through adaptive certainty monitoring. ARS introduces a multi-checkpoint certainty estimation mechanism with prog...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose \textbf{Adaptive Reasoning Suppression (ARS)}, a novel training-free approach that dynamically suppresses redundant reasoning steps while preserving accuracy through adaptive certainty monitoring</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Existing efficient reasoning methods face the challenge of balancing reasoning quality with inference cost reduction</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large Reasoning Language Models (LRLMs or LRMs) demonstrate remarkable capabilities in complex reasoning tasks, but suffer from significant computational inefficiencies due to overthinking phenomena</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2510.00071v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2510.00071v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Zheng, &quot;ARS: Adaptive Reasoning Suppression for Efficient Large Reasoning Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2510.00071v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398404640')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="llm-as-a-judge: rapid evaluation of legal document recommendation for retrieval-augmented generation" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.12382v1" target="_blank">LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for Retrieval-Augmented Generation</a>
                            </h3>
                            <p class="card-authors">Anu Pradhan, Alexandra Ortan, Apurv Verma et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398402912">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The evaluation bottleneck in recommendation systems has become particularly acute with the rise of Generative AI, where traditional metrics fall short of capturing nuanced quality dimensions that matter in specialized domains like legal research. Can we trust Large Language Models to serve as reliable judges of their own kind? This paper investigates LLM-as-a-Judge as a principled approach to evaluating Retrieval-Augmented Generation systems in legal contexts, where the stakes of recommendation quality are exceptionally high.
  We tackle two fundamental questions that determine practical viabi...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Can we trust Large Language Models to serve as reliable judges of their own kind? This paper investigates LLM-as-a-Judge as a principled approach to evaluating Retrieval-Augmented Generation systems in legal contexts, where the stakes of recommendation quality are exceptionally high.
  We tackle two fundamental questions that determine practical viability: which inter-rater reliability metrics best capture the alignment between LLM and human assessments, and how do we conduct statistically sound comparisons between competing systems? Through systematic experimentation, we discover that traditional agreement metrics like Krippendorff&#x27;s alpha can be misleading in the skewed distributions typical of AI system evaluations</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.12382v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.12382v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Pradhan, A. Ortan, A. Verma, and M. Seshadri, &quot;LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for Retrieval-Augmented Generation,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.12382v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398402912')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cv" data-title="mquant: unleashing the inference potential of multimodal large language models via full static quantization" data-keywords="attention language model cs.cv cs.ai" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computer Vision</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.00425v2" target="_blank">MQuant: Unleashing the Inference Potential of Multimodal Large Language Models via Full Static Quantization</a>
                            </h3>
                            <p class="card-authors">JiangYong Yu, Sifan Zhou, Dawei Yang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4398402672">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Multimodal large language models (MLLMs) have garnered widespread attention due to their ability to understand multimodal input. However, their large parameter sizes and substantial computational demands severely hinder their practical deployment and application.While quantization is an effective way to reduce model size and inference latency, its application to MLLMs remains underexplored. In this paper, we propose MQuant, a post-training quantization (PTQ) framework designed to tackle the unique challenges of multimodal large language models (MLLMs). Conventional quantization often struggles...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose MQuant, a post-training quantization (PTQ) framework designed to tackle the unique challenges of multimodal large language models (MLLMs)</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">In this paper, we propose MQuant, a post-training quantization (PTQ) framework designed to tackle the unique challenges of multimodal large language models (MLLMs). To address these issues, MQuant introduces: Modality-Specific Static Quantization (MSQ), assigning distinct static scales for visual vs</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Multimodal large language models (MLLMs) have garnered widespread attention due to their ability to understand multimodal input</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.00425v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.00425v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Yu et al., &quot;MQuant: Unleashing the Inference Potential of Multimodal Large Language Models via Full Static Quantization,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.00425v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398402672')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.hc" data-title="large language models will change the way children think about technology and impact every interaction paradigm" data-keywords="language model cs.hc cs.ai cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Human-Computer Interaction</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.13667v1" target="_blank">Large Language Models Will Change The Way Children Think About Technology And Impact Every Interaction Paradigm</a>
                            </h3>
                            <p class="card-authors">Russell Beale</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.HC</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398403584">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents a hopeful perspective on the potentially dramatic impacts of Large Language Models on how we children learn and how they will expect to interact with technology. We review the effects of LLMs on education so far, and make the case that these effects are minor compared to the upcoming changes that are occurring. We present a small scenario and self-ethnographic study demonstrating the effects of these changes, and define five significant considerations that interactive systems designers will have to accommodate in the future.</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents a hopeful perspective on the potentially dramatic impacts of Large Language Models on how we children learn and how they will expect to interact with technology</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper presents a hopeful perspective on the potentially dramatic impacts of Large Language Models on how we children learn and how they will expect to interact with technology</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.13667v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.13667v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Beale, &quot;Large Language Models Will Change The Way Children Think About Technology And Impact Every Interaction Paradigm,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.13667v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398403584')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="multi-model synthetic training for mission-critical small language models" data-keywords="gpt ros language model cs.cl cs.ai cs.lg" data-themes="S I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.13047v1" target="_blank">Multi-Model Synthetic Training for Mission-Critical Small Language Models</a>
                            </h3>
                            <p class="card-authors">Nolan Platt, Pragyansmita Nayak</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4397311472">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Language Models (LLMs) have demonstrated remarkable capabilities across many domains, yet their appli- cation to specialized fields remains constrained by the scarcity and complexity of domain-specific training data. We present a novel approach that achieves a 261x cost reduction for maritime intelligence by using LLMs as one-time teachers rather than using them directly for inference. Our method transforms 3.2 billion Automatic Identification System (AIS) vessel tracking records into 21,543 synthetic question and answer pairs through multi-model generation (GPT-4o and o3-mini), preventi...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present a novel approach that achieves a 261x cost reduction for maritime intelligence by using LLMs as one-time teachers rather than using them directly for inference</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large Language Models (LLMs) have demonstrated remarkable capabilities across many domains, yet their appli- cation to specialized fields remains constrained by the scarcity and complexity of domain-specific training data</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.13047v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.13047v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. Platt, and P. Nayak, &quot;Multi-Model Synthetic Training for Mission-Critical Small Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.13047v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397311472')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="towards typologically aware rescoring to mitigate unfaithfulness in lower-resource languages" data-keywords="bert ros language model cs.cl" data-themes="M E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.17664v2" target="_blank">Towards Typologically Aware Rescoring to Mitigate Unfaithfulness in Lower-Resource Languages</a>
                            </h3>
                            <p class="card-authors">Tsan Tsai Chan, Xin Tong, Thi Thu Uyen Hoang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Bert</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398401472">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Multilingual large language models (LLMs) are known to more frequently generate non-faithful output in resource-constrained languages (Guerreiro et al., 2023 - arXiv:2303.16104), potentially because these typologically diverse languages are underrepresented in their training data. To mitigate unfaithfulness in such settings, we propose using computationally light auxiliary models to rescore the outputs of larger architectures. As proof of the feasibility of such an approach, we show that monolingual 4-layer BERT models pretrained from scratch on less than 700 MB of data without fine-tuning are...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To mitigate unfaithfulness in such settings, we propose using computationally light auxiliary models to rescore the outputs of larger architectures</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Multilingual large language models (LLMs) are known to more frequently generate non-faithful output in resource-constrained languages (Guerreiro et al., 2023 - arXiv:2303.16104), potentially because these typologically diverse languages are underrepresented in their training data</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.17664v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.17664v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. T. Chan, X. Tong, T. T. U. Hoang, B. Tepnadze, and W. Stempniak, &quot;Towards Typologically Aware Rescoring to Mitigate Unfaithfulness in Lower-Resource Languages,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.17664v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398401472')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.ai" data-title="from system 1 to system 2: a survey of reasoning large language models" data-keywords="language model cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Artificial Intelligence</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.17419v6" target="_blank">From System 1 to System 2: A Survey of Reasoning Large Language Models</a>
                            </h3>
                            <p class="card-authors">Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4398404688">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more accurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently, reasoning LLMs like OpenAI&#x27;s o1/o3 and DeepSeek&#x27;s R1 have demonstrated expert-lev...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.17419v6" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.17419v6" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Li et al., &quot;From System 1 to System 2: A Survey of Reasoning Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.17419v6')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398404688')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="detection of personal data in structured datasets using a large language model" data-keywords="gpt ros language model cs.cl" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.22305v1" target="_blank">Detection of Personal Data in Structured Datasets Using a Large Language Model</a>
                            </h3>
                            <p class="card-authors">Albert Agisha Ntwali, Luca R√ºck, Martin Heckmann</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398406272">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We propose a novel approach for detecting personal data in structured datasets, leveraging GPT-4o, a state-of-the-art Large Language Model. A key innovation of our method is the incorporation of contextual information: in addition to a feature&#x27;s name and values, we utilize information from other feature names within the dataset as well as the dataset description. We compare our approach to alternative methods, including Microsoft Presidio and CASSED, evaluating them on multiple datasets: DeSSI, a large synthetic dataset, datasets we collected from Kaggle and OpenML as well as MIMIC-Demo-Ext, a...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a novel approach for detecting personal data in structured datasets, leveraging GPT-4o, a state-of-the-art Large Language Model</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We propose a novel approach for detecting personal data in structured datasets, leveraging GPT-4o, a state-of-the-art Large Language Model</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.22305v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.22305v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. A. Ntwali, L. R√ºck, and M. Heckmann, &quot;Detection of Personal Data in Structured Datasets Using a Large Language Model,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.22305v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398406272')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.ai" data-title="game theory meets large language models: a systematic survey with taxonomy and new frontiers" data-keywords="language model cs.ai cs.gt cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Artificial Intelligence</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.09053v2" target="_blank">Game Theory Meets Large Language Models: A Systematic Survey with Taxonomy and New Frontiers</a>
                            </h3>
                            <p class="card-authors">Haoran Sun, Yusen Wu, Peng Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.GT</span><span class="keyword-tag">CS.LG</span></div>

                            <div class="card-details" id="details-4398402720">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Game theory is a foundational framework for analyzing strategic interactions, and its intersection with large language models (LLMs) is a rapidly growing field. However, existing surveys mainly focus narrowly on using game theory to evaluate LLM behavior. This paper provides the first comprehensive survey of the bidirectional relationship between Game Theory and LLMs. We propose a novel taxonomy that categorizes the research in this intersection into four distinct perspectives: (1) evaluating LLMs in game-based scenarios; (2) improving LLMs using game-theoretic concepts for better interpretabi...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a novel taxonomy that categorizes the research in this intersection into four distinct perspectives: (1) evaluating LLMs in game-based scenarios; (2) improving LLMs using game-theoretic concepts for better interpretability and alignment; (3) modeling the competitive landscape of LLM development and its societal impact; and (4) leveraging LLMs to advance game models and to solve corresponding game theory problems</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">We propose a novel taxonomy that categorizes the research in this intersection into four distinct perspectives: (1) evaluating LLMs in game-based scenarios; (2) improving LLMs using game-theoretic concepts for better interpretability and alignment; (3) modeling the competitive landscape of LLM development and its societal impact; and (4) leveraging LLMs to advance game models and to solve corresponding game theory problems. Furthermore, we identify key challenges and outline future research directions</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Game theory is a foundational framework for analyzing strategic interactions, and its intersection with large language models (LLMs) is a rapidly growing field</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.09053v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.09053v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Sun et al., &quot;Game Theory Meets Large Language Models: A Systematic Survey with Taxonomy and New Frontiers,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.09053v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398402720')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cr" data-title="augmenting anonymized data with ai: exploring the feasibility and limitations of large language models in data enrichment" data-keywords="language model cs.cr cs.et" data-themes="S I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ CS.CR</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.03778v1" target="_blank">Augmenting Anonymized Data with AI: Exploring the Feasibility and Limitations of Large Language Models in Data Enrichment</a>
                            </h3>
                            <p class="card-authors">Stefano Cirillo, Domenico Desiato, Giuseppe Polese et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CR</span><span class="keyword-tag">CS.ET</span></div>

                            <div class="card-details" id="details-4398405312">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Language Models (LLMs) have demonstrated advanced capabilities in both text generation and comprehension, and their application to data archives might facilitate the privatization of sensitive information about the data subjects. In fact, the information contained in data often includes sensitive and personally identifiable details. This data, if not safeguarded, may bring privacy risks in terms of both disclosure and identification. Furthermore, the application of anonymisation techniques, such as k-anonymity, can lead to a significant reduction in the amount of data within data sources...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To this end, we designed new ad-hoc prompt template engineering strategies to perform anonymized Data Augmentation and assess the effectiveness of LLM-based approaches in providing anonymized data</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large Language Models (LLMs) have demonstrated advanced capabilities in both text generation and comprehension, and their application to data archives might facilitate the privatization of sensitive information about the data subjects</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.03778v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.03778v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Cirillo, D. Desiato, G. Polese, M. M. L. Sebillo, and G. Solimando, &quot;Augmenting Anonymized Data with AI: Exploring the Feasibility and Limitations of Large Language Models in Data Enrichment,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.03778v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398405312')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.ai" data-title="language games as the pathway to artificial superhuman intelligence" data-keywords="multi-agent ros language model cs.ai cs.cl cs.ma" data-themes="E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Artificial Intelligence</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2501.18924v1" target="_blank">Language Games as the Pathway to Artificial Superhuman Intelligence</a>
                            </h3>
                            <p class="card-authors">Ying Wen, Ziyu Wan, Shao Zhang</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Multi-Agent</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4398403536">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The evolution of large language models (LLMs) toward artificial superhuman intelligence (ASI) hinges on data reproduction, a cyclical process in which models generate, curate and retrain on novel data to refine capabilities. Current methods, however, risk getting stuck in a data reproduction trap: optimizing outputs within fixed human-generated distributions in a closed loop leads to stagnation, as models merely recombine existing knowledge rather than explore new frontiers. In this paper, we propose language games as a pathway to expanded data reproduction, breaking this cycle through three m...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose language games as a pathway to expanded data reproduction, breaking this cycle through three mechanisms: (1) \textit{role fluidity}, which enhances data diversity and coverage by enabling multi-agent systems to dynamically shift roles across tasks; (2) \textit{reward variety}, embedding multiple feedback criteria that can drive complex intelligent behaviors; and (3) \textit{rule plasticity}, iteratively evolving interaction constraints to foster learnability, thereby injecting continual novelty</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The evolution of large language models (LLMs) toward artificial superhuman intelligence (ASI) hinges on data reproduction, a cyclical process in which models generate, curate and retrain on novel data to refine capabilities</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2501.18924v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2501.18924v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Wen, Z. Wan, and S. Zhang, &quot;Language Games as the Pathway to Artificial Superhuman Intelligence,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2501.18924v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398403536')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="language-conditioned world model improves policy generalization by reading environmental descriptions" data-keywords="reinforcement learning attention planning cs.cl cs.lg" data-themes="S I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2511.22904v1" target="_blank">Language-conditioned world model improves policy generalization by reading environmental descriptions</a>
                            </h3>
                            <p class="card-authors">Anh Nguyen, Stefan Lee</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Attention</span><span class="keyword-tag">Planning</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398415776">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">To interact effectively with humans in the real world, it is important for agents to understand language that describes the dynamics of the environment--that is, how the environment behaves--rather than just task instructions specifying &quot;what to do&quot;. Understanding this dynamics-descriptive language is important for human-agent interaction and agent behavior. Recent work address this problem using a model-based approach: language is incorporated into a world model, which is then used to learn a behavior policy. However, these existing methods either do not demonstrate policy generalization to u...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a model-based reinforcement learning approach, where a language-conditioned world model is trained through interaction with the environment, and a policy is learned from this model--without planning or expert demonstrations</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Recent work address this problem using a model-based approach: language is incorporated into a world model, which is then used to learn a behavior policy</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Recent work address this problem using a model-based approach: language is incorporated into a world model, which is then used to learn a behavior policy</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2511.22904v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2511.22904v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Nguyen, and S. Lee, &quot;Language-conditioned world model improves policy generalization by reading environmental descriptions,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2511.22904v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398415776')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="exploring gender bias in large language models: an in-depth dive into the german language" data-keywords="perception ros language model cs.cl cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.16557v1" target="_blank">Exploring Gender Bias in Large Language Models: An In-depth Dive into the German Language</a>
                            </h3>
                            <p class="card-authors">Kristin Gnadt, David Thulke, Simone Kopeinik et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Perception</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398402336">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In recent years, various methods have been proposed to evaluate gender bias in large language models (LLMs). A key challenge lies in the transferability of bias measurement methods initially developed for the English language when applied to other languages. This work aims to contribute to this research strand by presenting five German datasets for gender bias evaluation in LLMs. The datasets are grounded in well-established concepts of gender bias and are accessible through multiple methodologies. Our findings, reported for eight multilingual LLM models, reveal unique challenges associated wi...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">A key challenge lies in the transferability of bias measurement methods initially developed for the English language when applied to other languages. Our findings, reported for eight multilingual LLM models, reveal unique challenges associated with gender bias in German, including the ambiguous interpretation of male occupational terms and the influence of seemingly neutral nouns on gender perception</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In recent years, various methods have been proposed to evaluate gender bias in large language models (LLMs)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.16557v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.16557v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Gnadt, D. Thulke, S. Kopeinik, and R. Schl√ºter, &quot;Exploring Gender Bias in Large Language Models: An In-depth Dive into the German Language,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.16557v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398402336')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="metasc: test-time safety specification optimization for language models" data-keywords="optimization ros language model cs.cl cs.ai" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.07985v2" target="_blank">MetaSC: Test-Time Safety Specification Optimization for Language Models</a>
                            </h3>
                            <p class="card-authors">V√≠ctor Gallego</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Optimization</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398405648">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We propose a novel dynamic safety framework that optimizes language model (LM) safety reasoning at inference time without modifying model weights. Building on recent advances in self-critique methods, our approach leverages a meta-critique mechanism that iteratively updates safety prompts-termed specifications-to drive the critique and revision process adaptively. This test-time optimization not only improves performance against adversarial jailbreak requests but also in diverse general safety-related tasks, such as avoiding moral harm or pursuing honest responses. Our empirical evaluations ac...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a novel dynamic safety framework that optimizes language model (LM) safety reasoning at inference time without modifying model weights</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We propose a novel dynamic safety framework that optimizes language model (LM) safety reasoning at inference time without modifying model weights</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.07985v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.07985v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'V. Gallego, &quot;MetaSC: Test-Time Safety Specification Optimization for Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.07985v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398405648')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="untangling the influence of typology, data and model architecture on ranking transfer languages for cross-lingual pos tagging" data-keywords="lstm ros cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.19979v1" target="_blank">Untangling the Influence of Typology, Data and Model Architecture on Ranking Transfer Languages for Cross-Lingual POS Tagging</a>
                            </h3>
                            <p class="card-authors">Enora Rice, Ali Marashian, Hannah Haynie et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Lstm</span><span class="keyword-tag">Ros</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398404256">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Cross-lingual transfer learning is an invaluable tool for overcoming data scarcity, yet selecting a suitable transfer language remains a challenge. The precise roles of linguistic typology, training data, and model architecture in transfer language choice are not fully understood. We take a holistic approach, examining how both dataset-specific and fine-grained typological features influence transfer language selection for part-of-speech tagging, considering two different sources for morphosyntactic features. While previous work examines these dynamics in the context of bilingual biLSTMS, we e...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Cross-lingual transfer learning is an invaluable tool for overcoming data scarcity, yet selecting a suitable transfer language remains a challenge</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The precise roles of linguistic typology, training data, and model architecture in transfer language choice are not fully understood</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.19979v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.19979v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'E. Rice, A. Marashian, H. Haynie, K. v. d. Wense, and A. Palmer, &quot;Untangling the Influence of Typology, Data and Model Architecture on Ranking Transfer Languages for Cross-Lingual POS Tagging,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.19979v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398404256')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.lg" data-title="understanding reasoning in thinking language models via steering vectors" data-keywords="control ros language model cs.lg cs.ai" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Machine Learning</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.18167v4" target="_blank">Understanding Reasoning in Thinking Language Models via Steering Vectors</a>
                            </h3>
                            <p class="card-authors">Constantin Venhoff, Iv√°n Arcuschin, Philip Torr et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Control</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span></div>

                            <div class="card-details" id="details-4398405168">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recent advances in large language models (LLMs) have led to the development of thinking language models that generate extensive internal reasoning chains before producing responses. While these models achieve improved performance, controlling their reasoning processes remains challenging. This work presents a steering approach for thinking LLMs by analyzing and manipulating specific reasoning behaviors in DeepSeek-R1-Distill models. Through a systematic experiment on 500 tasks across 10 diverse categories, we identify several reasoning behaviors exhibited by thinking models, including expressi...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We demonstrate that these behaviors are mediated by linear directions in the model&#x27;s activation space and can be controlled using steering vectors</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Recent advances in large language models (LLMs) have led to the development of thinking language models that generate extensive internal reasoning chains before producing responses</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.18167v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.18167v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Venhoff, I. Arcuschin, P. Torr, A. Conmy, and N. Nanda, &quot;Understanding Reasoning in Thinking Language Models via Steering Vectors,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.18167v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398405168')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="q-bio.gn" data-title="deepseq: high-throughput single-cell rna sequencing data labeling via web search-augmented agentic generative ai foundation models" data-keywords="q-bio.gn cs.ai cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Q-BIO.GN</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.13817v1" target="_blank">DeepSeq: High-Throughput Single-Cell RNA Sequencing Data Labeling via Web Search-Augmented Agentic Generative AI Foundation Models</a>
                            </h3>
                            <p class="card-authors">Saleem A. Al Dajani, Abel Sanchez, John R. Williams</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Q-BIO.GN</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.LG</span></div>

                            <div class="card-details" id="details-4398402960">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Generative AI foundation models offer transformative potential for processing structured biological data, particularly in single-cell RNA sequencing, where datasets are rapidly scaling toward billions of cells. We propose the use of agentic foundation models with real-time web search to automate the labeling of experimental data, achieving up to 82.5% accuracy. This addresses a key bottleneck in supervised learning for structured omics data by increasing annotation throughput without manual curation and human error. Our approach enables the development of virtual cell foundation models capable...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose the use of agentic foundation models with real-time web search to automate the labeling of experimental data, achieving up to 82.5% accuracy</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Generative AI foundation models offer transformative potential for processing structured biological data, particularly in single-cell RNA sequencing, where datasets are rapidly scaling toward billions of cells</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.13817v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.13817v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. A. A. Dajani, A. Sanchez, and J. R. Williams, &quot;DeepSeq: High-Throughput Single-Cell RNA Sequencing Data Labeling via Web Search-Augmented Agentic Generative AI Foundation Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.13817v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398402960')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="enhancing small language models for cross-lingual generalized zero-shot classification with soft prompt tuning" data-keywords="ros nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.19469v2" target="_blank">Enhancing Small Language Models for Cross-Lingual Generalized Zero-Shot Classification with Soft Prompt Tuning</a>
                            </h3>
                            <p class="card-authors">Fred Philippy, Siwen Guo, Cedric Lothritz et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398407184">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In NLP, Zero-Shot Classification (ZSC) has become essential for enabling models to classify text into categories unseen during training, particularly in low-resource languages and domains where labeled data is scarce. While pretrained language models (PLMs) have shown promise in ZSC, they often rely on large training datasets or external knowledge, limiting their applicability in multilingual and low-resource scenarios. Recent approaches leveraging natural language prompts reduce the dependence on large training datasets but struggle to effectively incorporate available labeled data from relat...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address these challenges, we introduce RoSPrompt, a lightweight and data-efficient approach for training soft prompts that enhance cross-lingual ZSC while ensuring robust generalization across data distribution shifts</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To address these challenges, we introduce RoSPrompt, a lightweight and data-efficient approach for training soft prompts that enhance cross-lingual ZSC while ensuring robust generalization across data distribution shifts</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In NLP, Zero-Shot Classification (ZSC) has become essential for enabling models to classify text into categories unseen during training, particularly in low-resource languages and domains where labeled data is scarce</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.19469v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.19469v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. Philippy, S. Guo, C. Lothritz, J. Klein, and T. F. Bissyand√©, &quot;Enhancing Small Language Models for Cross-Lingual Generalized Zero-Shot Classification with Soft Prompt Tuning,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.19469v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398407184')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="studies with impossible languages falsify lms as models of human language" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2511.11389v1" target="_blank">Studies with impossible languages falsify LMs as models of human language</a>
                            </h3>
                            <p class="card-authors">Jeffrey S. Bowers, Jeff Mitchell</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398406944">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">According to Futrell and Mahowald [arXiv:2501.17047], both infants and language models (LMs) find attested languages easier to learn than impossible languages that have unnatural structures. We review the literature and show that LMs often learn attested and many impossible languages equally well. Difficult to learn impossible languages are simply more complex (or random). LMs are missing human inductive biases that support language acquisition.</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Difficult to learn impossible languages are simply more complex (or random)</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">According to Futrell and Mahowald [arXiv:2501.17047], both infants and language models (LMs) find attested languages easier to learn than impossible languages that have unnatural structures</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2511.11389v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2511.11389v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. S. Bowers, and J. Mitchell, &quot;Studies with impossible languages falsify LMs as models of human language,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2511.11389v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398406944')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="benchmarking adversarial robustness to bias elicitation in large language models: scalable automated assessment with llm-as-a-judge" data-keywords="manipulation ros language model cs.cl cs.ai" data-themes="E S M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.07887v2" target="_blank">Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge</a>
                            </h3>
                            <p class="card-authors">Riccardo Cantini, Alessio Orsino, Massimo Ruggiero et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Manipulation</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398401520">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The growing integration of Large Language Models (LLMs) into critical societal domains has raised concerns about embedded biases that can perpetuate stereotypes and undermine fairness. Such biases may stem from historical inequalities in training data, linguistic imbalances, or adversarial manipulation. Despite mitigation efforts, recent studies show that LLMs remain vulnerable to adversarial attacks that elicit biased outputs. This work proposes a scalable benchmarking framework to assess LLM robustness to adversarial bias elicitation. Our methodology involves: (i) systematically probing mode...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Our methodology involves: (i) systematically probing models across multiple tasks targeting diverse sociocultural biases, (ii) quantifying robustness through safety scores using an LLM-as-a-Judge approach, and (iii) employing jailbreak techniques to reveal safety vulnerabilities</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The growing integration of Large Language Models (LLMs) into critical societal domains has raised concerns about embedded biases that can perpetuate stereotypes and undermine fairness</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.07887v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.07887v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Cantini, A. Orsino, M. Ruggiero, and D. Talia, &quot;Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.07887v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398401520')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="distinct social-linguistic processing between humans and large audio-language models: evidence from model-brain alignment" data-keywords="ros language model cs.cl q-bio.nc" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.19586v2" target="_blank">Distinct social-linguistic processing between humans and large audio-language models: Evidence from model-brain alignment</a>
                            </h3>
                            <p class="card-authors">Hanlin Wu, Xufeng Duan, Zhenguang Cai</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">Q-BIO.NC</span></div>

                            <div class="card-details" id="details-4398415824">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Voice-based AI development faces unique challenges in processing both linguistic and paralinguistic information. This study compares how large audio-language models (LALMs) and humans integrate speaker characteristics during speech comprehension, asking whether LALMs process speaker-contextualized language in ways that parallel human cognitive mechanisms. We compared two LALMs&#x27; (Qwen2-Audio and Ultravox 0.5) processing patterns with human EEG responses. Using surprisal and entropy metrics from the models, we analyzed their sensitivity to speaker-content incongruency across social stereotype vi...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Voice-based AI development faces unique challenges in processing both linguistic and paralinguistic information. These findings reveal both the potential and limitations of current LALMs in processing speaker-contextualized language, and suggest differences in social-linguistic processing mechanisms between humans and LALMs.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This study compares how large audio-language models (LALMs) and humans integrate speaker characteristics during speech comprehension, asking whether LALMs process speaker-contextualized language in ways that parallel human cognitive mechanisms</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.19586v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.19586v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Wu, X. Duan, and Z. Cai, &quot;Distinct social-linguistic processing between humans and large audio-language models: Evidence from model-brain alignment,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.19586v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398415824')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="tiny language models" data-keywords="transformer bert ros nlp language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.14871v2" target="_blank">Tiny language models</a>
                            </h3>
                            <p class="card-authors">Ronit D. Gross, Yarden Tzach, Tal Halevi et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Transformer</span><span class="keyword-tag">Bert</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Nlp</span></div>

                            <div class="card-details" id="details-4398403728">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">A prominent achievement of natural language processing (NLP) is its ability to understand and generate meaningful human language. This capability relies on complex feedforward transformer block architectures pre-trained on large language models (LLMs). However, LLM pre-training is currently feasible only for a few dominant companies due to the immense computational resources required, limiting broader research participation. This creates a critical need for more accessible alternatives. In this study, we explore whether tiny language models (TLMs) exhibit the same key qualitative features of L...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We demonstrate that TLMs exhibit a clear performance gap between pre-trained and non-pre-trained models across classification tasks, indicating the effectiveness of pre-training, even at a tiny scale</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">We demonstrate that TLMs exhibit a clear performance gap between pre-trained and non-pre-trained models across classification tasks, indicating the effectiveness of pre-training, even at a tiny scale. The performance gap increases with the size of the pre-training dataset and with greater overlap between tokens in the pre-training and classification datasets</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This capability relies on complex feedforward transformer block architectures pre-trained on large language models (LLMs)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.14871v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.14871v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. D. Gross, Y. Tzach, T. Halevi, E. Koresh, and I. Kanter, &quot;Tiny language models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.14871v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398403728')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="sim-to-real transfer in deep reinforcement learning for bipedal locomotion" data-keywords="reinforcement learning robot bipedal locomotion control simulation imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2511.06465v1" target="_blank">Sim-to-Real Transfer in Deep Reinforcement Learning for Bipedal Locomotion</a>
                            </h3>
                            <p class="card-authors">Lingfan Bao, Tianhu Peng, Chengxu Zhou</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399336320">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This chapter addresses the critical challenge of simulation-to-reality (sim-to-real) transfer for deep reinforcement learning (DRL) in bipedal locomotion. After contextualizing the problem within various control architectures, we dissect the ``curse of simulation&#x27;&#x27; by analyzing the primary sources of sim-to-real gap: robot dynamics, contact modeling, state estimation, and numerical solvers. Building on this diagnosis, we structure the solutions around two complementary philosophies. The first is to shrink the gap through model-centric strategies that systematically improve the simulator&#x27;s phys...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">This chapter addresses the critical challenge of simulation-to-reality (sim-to-real) transfer for deep reinforcement learning (DRL) in bipedal locomotion. After contextualizing the problem within various control architectures, we dissect the ``curse of simulation&#x27;&#x27; by analyzing the primary sources of sim-to-real gap: robot dynamics, contact modeling, state estimation, and numerical solvers</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">After contextualizing the problem within various control architectures, we dissect the ``curse of simulation&#x27;&#x27; by analyzing the primary sources of sim-to-real gap: robot dynamics, contact modeling, state estimation, and numerical solvers</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2511.06465v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2511.06465v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Bao, T. Peng, and C. Zhou, &quot;Sim-to-Real Transfer in Deep Reinforcement Learning for Bipedal Locomotion,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2511.06465v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399336320')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="learning bipedal locomotion on gear-driven humanoid robot using foot-mounted imus" data-keywords="reinforcement learning robot humanoid bipedal locomotion imu cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.00614v2" target="_blank">Learning Bipedal Locomotion on Gear-Driven Humanoid Robot Using Foot-Mounted IMUs</a>
                            </h3>
                            <p class="card-authors">Sotaro Katayama, Yuta Koda, Norio Nagatsuka et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span></div>

                            <div class="card-details" id="details-4399335264">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Sim-to-real reinforcement learning (RL) for humanoid robots with high-gear ratio actuators remains challenging due to complex actuator dynamics and the absence of torque sensors. To address this, we propose a novel RL framework leveraging foot-mounted inertial measurement units (IMUs). Instead of pursuing detailed actuator modeling and system identification, we utilize foot-mounted IMU measurements to enhance rapid stabilization capabilities over challenging terrains. Additionally, we propose symmetric data augmentation dedicated to the proposed observation space and random network distillatio...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address this, we propose a novel RL framework leveraging foot-mounted inertial measurement units (IMUs)</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">To address this, we propose a novel RL framework leveraging foot-mounted inertial measurement units (IMUs)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.00614v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.00614v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Katayama, Y. Koda, N. Nagatsuka, and M. Kinoshita, &quot;Learning Bipedal Locomotion on Gear-Driven Humanoid Robot Using Foot-Mounted IMUs,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.00614v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399335264')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="hierarchical reduced-order model predictive control for robust locomotion on humanoid robots" data-keywords="robot humanoid locomotion planning control simulation ros imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.04722v1" target="_blank">Hierarchical Reduced-Order Model Predictive Control for Robust Locomotion on Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Adrian B. Ghansah, Sergio A. Esteban, Aaron D. Ames</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Planning</span></div>

                            <div class="card-details" id="details-4399336080">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">As humanoid robots enter real-world environments, ensuring robust locomotion across diverse environments is crucial. This paper presents a computationally efficient hierarchical control framework for humanoid robot locomotion based on reduced-order models -- enabling versatile step planning and incorporating arm and torso dynamics to better stabilize the walking. At the high level, we use the step-to-step dynamics of the ALIP model to simultaneously optimize over step periods, step lengths, and ankle torques via nonlinear MPC. The ALIP trajectories are used as references to a linear MPC framew...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents a computationally efficient hierarchical control framework for humanoid robot locomotion based on reduced-order models -- enabling versatile step planning and incorporating arm and torso dynamics to better stabilize the walking</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper presents a computationally efficient hierarchical control framework for humanoid robot locomotion based on reduced-order models -- enabling versatile step planning and incorporating arm and torso dynamics to better stabilize the walking</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.04722v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.04722v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. B. Ghansah, S. A. Esteban, and A. D. Ames, &quot;Hierarchical Reduced-Order Model Predictive Control for Robust Locomotion on Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.04722v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399336080')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="learning sim-to-real humanoid locomotion in 15 minutes" data-keywords="reinforcement learning robot humanoid locomotion control simulation imu cs.ro cs.ai cs.lg" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2512.01996v1" target="_blank">Learning Sim-to-Real Humanoid Locomotion in 15 Minutes</a>
                            </h3>
                            <p class="card-authors">Younggyo Seo, Carmelo Sferrazza, Juyue Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399336224">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Massively parallel simulation has reduced reinforcement learning (RL) training time for robots from days to minutes. However, achieving fast and reliable sim-to-real RL for humanoid control remains difficult due to the challenges introduced by factors such as high dimensionality and domain randomization. In this work, we introduce a simple and practical recipe based on off-policy RL algorithms, i.e., FastSAC and FastTD3, that enables rapid training of humanoid locomotion policies in just 15 minutes with a single RTX 4090 GPU. Our simple recipe stabilizes off-policy RL algorithms at massive sca...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we introduce a simple and practical recipe based on off-policy RL algorithms, i.e., FastSAC and FastTD3, that enables rapid training of humanoid locomotion policies in just 15 minutes with a single RTX 4090 GPU</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, achieving fast and reliable sim-to-real RL for humanoid control remains difficult due to the challenges introduced by factors such as high dimensionality and domain randomization</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In this work, we introduce a simple and practical recipe based on off-policy RL algorithms, i.e., FastSAC and FastTD3, that enables rapid training of humanoid locomotion policies in just 15 minutes with a single RTX 4090 GPU</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2512.01996v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2512.01996v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Seo, C. Sferrazza, J. Chen, G. Shi, R. Duan, and P. Abbeel, &quot;Learning Sim-to-Real Humanoid Locomotion in 15 Minutes,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2512.01996v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399336224')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="romoco: robotic motion control toolbox for reduced-order model-based locomotion on bipedal and humanoid robots" data-keywords="robot humanoid bipedal locomotion control simulation ros imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.19545v1" target="_blank">RoMoCo: Robotic Motion Control Toolbox for Reduced-Order Model-Based Locomotion on Bipedal and Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Min Dai, Aaron D. Ames</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399337040">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We present RoMoCo, an open-source C++ toolbox for the synthesis and evaluation of reduced-order model-based planners and whole-body controllers for bipedal and humanoid robots. RoMoCo&#x27;s modular architecture unifies state-of-the-art planners and whole-body locomotion controllers under a consistent API, enabling rapid prototyping and reproducible benchmarking. By leveraging reduced-order models for platform-agnostic gait generation, RoMoCo enables flexible controller design across diverse robots. We demonstrate its versatility and performance through extensive simulations on the Cassie, Unitree ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present RoMoCo, an open-source C++ toolbox for the synthesis and evaluation of reduced-order model-based planners and whole-body controllers for bipedal and humanoid robots</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We present RoMoCo, an open-source C++ toolbox for the synthesis and evaluation of reduced-order model-based planners and whole-body controllers for bipedal and humanoid robots</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.19545v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.19545v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Dai, and A. D. Ames, &quot;RoMoCo: Robotic Motion Control Toolbox for Reduced-Order Model-Based Locomotion on Bipedal and Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.19545v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399337040')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="learning getting-up policies for real-world humanoid robots" data-keywords="robot humanoid locomotion control cs.ro cs.lg" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.12152v2" target="_blank">Learning Getting-Up Policies for Real-World Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Xialin He, Runpei Dong, Zixuan Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Control</span></div>

                            <div class="card-details" id="details-4399334688">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Automatic fall recovery is a crucial prerequisite before humanoid robots can be reliably deployed. Hand-designing controllers for getting up is difficult because of the varied configurations a humanoid can end up in after a fall and the challenging terrains humanoid robots are expected to operate on. This paper develops a learning framework to produce controllers that enable humanoid robots to get up from varying configurations on varying terrains. Unlike previous successful applications of learning to humanoid locomotion, the getting-up task involves complex contact patterns (which necessitat...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Hand-designing controllers for getting up is difficult because of the varied configurations a humanoid can end up in after a fall and the challenging terrains humanoid robots are expected to operate on. We address these challenges through a two-phase approach that induces a curriculum</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper develops a learning framework to produce controllers that enable humanoid robots to get up from varying configurations on varying terrains</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.12152v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.12152v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. He, R. Dong, Z. Chen, and S. Gupta, &quot;Learning Getting-Up Policies for Real-World Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.12152v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399334688')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="robot trains robot: automatic real-world policy adaptation and learning for humanoids" data-keywords="reinforcement learning robot humanoid locomotion simulation imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.12252v2" target="_blank">Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids</a>
                            </h3>
                            <p class="card-authors">Kaizhe Hu, Haochen Shi, Yao He et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399335504">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Simulation-based reinforcement learning (RL) has significantly advanced humanoid locomotion tasks, yet direct real-world RL from scratch or adapting from pretrained policies remains rare, limiting the full potential of humanoid robots. Real-world learning, despite being crucial for overcoming the sim-to-real gap, faces substantial challenges related to safety, reward design, and learning efficiency. To address these limitations, we propose Robot-Trains-Robot (RTR), a novel framework where a robotic arm teacher actively supports and guides a humanoid robot student. The RTR system provides prote...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address these limitations, we propose Robot-Trains-Robot (RTR), a novel framework where a robotic arm teacher actively supports and guides a humanoid robot student</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Real-world learning, despite being crucial for overcoming the sim-to-real gap, faces substantial challenges related to safety, reward design, and learning efficiency. To address these limitations, we propose Robot-Trains-Robot (RTR), a novel framework where a robotic arm teacher actively supports and guides a humanoid robot student</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">To address these limitations, we propose Robot-Trains-Robot (RTR), a novel framework where a robotic arm teacher actively supports and guides a humanoid robot student</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.12252v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.12252v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Hu, H. Shi, Y. He, W. Wang, C. K. Liu, and S. Song, &quot;Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.12252v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399335504')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="robust humanoid walking on compliant and uneven terrain with deep reinforcement learning" data-keywords="reinforcement learning robot humanoid bipedal locomotion control simulation imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.13619v1" target="_blank">Robust Humanoid Walking on Compliant and Uneven Terrain with Deep Reinforcement Learning</a>
                            </h3>
                            <p class="card-authors">Rohan P. Singh, Mitsuharu Morisawa, Mehdi Benallegue et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span></div>

                            <div class="card-details" id="details-4399334928">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">For the deployment of legged robots in real-world environments, it is essential to develop robust locomotion control methods for challenging terrains that may exhibit unexpected deformability and irregularity. In this paper, we explore the application of sim-to-real deep reinforcement learning (RL) for the design of bipedal locomotion controllers for humanoid robots on compliant and uneven terrains. Our key contribution is to show that a simple training curriculum for exposing the RL agent to randomized terrains in simulation can achieve robust walking on a real humanoid robot using only propr...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a new control policy to enable modification of the observed clock signal, leading to adaptive gait frequencies depending on the terrain and command velocity</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">We train an end-to-end bipedal locomotion policy using the proposed approach, and show extensive real-robot demonstration on the HRP-5P humanoid over several difficult terrains inside and outside the lab environment</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">For the deployment of legged robots in real-world environments, it is essential to develop robust locomotion control methods for challenging terrains that may exhibit unexpected deformability and irregularity</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.13619v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.13619v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. P. Singh, M. Morisawa, M. Benallegue, Z. Xie, and F. Kanehiro, &quot;Robust Humanoid Walking on Compliant and Uneven Terrain with Deep Reinforcement Learning,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.13619v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399334928')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="mash: cooperative-heterogeneous multi-agent reinforcement learning for single humanoid robot locomotion" data-keywords="reinforcement learning robot humanoid locomotion multi-agent multi-robot cooperative control cs.ro cs.ai" data-themes="I L R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.10423v1" target="_blank">MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion</a>
                            </h3>
                            <p class="card-authors">Qi Liu, Xiaopeng Zhang, Mingshan Tan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-R">Multi-Robot</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399338768">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper proposes a novel method to enhance locomotion for a single humanoid robot through cooperative-heterogeneous multi-agent deep reinforcement learning (MARL). While most existing methods typically employ single-agent reinforcement learning algorithms for a single humanoid robot or MARL algorithms for multi-robot system tasks, we propose a distinct paradigm: applying cooperative-heterogeneous MARL to optimize locomotion for a single humanoid robot. The proposed method, multi-agent reinforcement learning for single humanoid locomotion (MASH), treats each limb (legs and arms) as an indepe...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">While most existing methods typically employ single-agent reinforcement learning algorithms for a single humanoid robot or MARL algorithms for multi-robot system tasks, we propose a distinct paradigm: applying cooperative-heterogeneous MARL to optimize locomotion for a single humanoid robot</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper proposes a novel method to enhance locomotion for a single humanoid robot through cooperative-heterogeneous multi-agent deep reinforcement learning (MARL)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.10423v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.10423v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Q. Liu, X. Zhang, M. Tan, S. Ma, J. Ding, and Y. Li, &quot;MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.10423v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399338768')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="periodic bipedal gait learning using reward composition based on a novel gait planner for humanoid robots" data-keywords="reinforcement learning robot humanoid bipedal locomotion planning cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.08416v1" target="_blank">Periodic Bipedal Gait Learning Using Reward Composition Based on a Novel Gait Planner for Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Bolin Li, Linwei Sun, Xuecong Huang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span></div>

                            <div class="card-details" id="details-4399335840">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents a periodic bipedal gait learning method using reward composition, integrated with a real-time gait planner for humanoid robots. First, we introduce a novel gait planner that incorporates dynamics to design the desired joint trajectory. In the gait design process, the 3D robot model is decoupled into two 2D models, which are then approximated as hybrid inverted pendulums (H-LIP) for trajectory planning. The gait planner operates in parallel in real time within the robot&#x27;s learning environment. Second, based on this gait planner, we design three effective reward functions wit...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents a periodic bipedal gait learning method using reward composition, integrated with a real-time gait planner for humanoid robots</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper presents a periodic bipedal gait learning method using reward composition, integrated with a real-time gait planner for humanoid robots</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.08416v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.08416v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. Li, L. Sun, X. Huang, Y. Jiang, and L. Zhu, &quot;Periodic Bipedal Gait Learning Using Reward Composition Based on a Novel Gait Planner for Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.08416v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399335840')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="constrained reinforcement learning for unstable point-feet bipedal locomotion applied to the bolt robot" data-keywords="reinforcement learning robot bipedal locomotion control cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.02194v1" target="_blank">Constrained Reinforcement Learning for Unstable Point-Feet Bipedal Locomotion Applied to the Bolt Robot</a>
                            </h3>
                            <p class="card-authors">Constant Roux, Elliot Chane-Sane, Ludovic De Matte√Øs et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399347504">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Bipedal locomotion is a key challenge in robotics, particularly for robots like Bolt, which have a point-foot design. This study explores the control of such underactuated robots using constrained reinforcement learning, addressing their inherent instability, lack of arms, and limited foot actuation. We present a methodology that leverages Constraints-as-Terminations and domain randomization techniques to enable sim-to-real transfer. Through a series of qualitative and quantitative experiments, we evaluate our approach in terms of balance maintenance, velocity control, and responses to slip an...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present a methodology that leverages Constraints-as-Terminations and domain randomization techniques to enable sim-to-real transfer</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Bipedal locomotion is a key challenge in robotics, particularly for robots like Bolt, which have a point-foot design</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We present a methodology that leverages Constraints-as-Terminations and domain randomization techniques to enable sim-to-real transfer</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.02194v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.02194v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Roux et al., &quot;Constrained Reinforcement Learning for Unstable Point-Feet Bipedal Locomotion Applied to the Bolt Robot,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.02194v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399347504')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="lipm-guided reinforcement learning for stable and perceptive locomotion in bipedal robots" data-keywords="robot bipedal locomotion simulation ros camera imu cs.ro" data-themes="S I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.09106v2" target="_blank">LIPM-Guided Reinforcement Learning for Stable and Perceptive Locomotion in Bipedal Robots</a>
                            </h3>
                            <p class="card-authors">Haokai Su, Haoxiang Luo, Shunpeng Yang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Simulation</span></div>

                            <div class="card-details" id="details-4399337088">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Achieving stable and robust perceptive locomotion for bipedal robots in unstructured outdoor environments remains a critical challenge due to complex terrain geometry and susceptibility to external disturbances. In this work, we propose a novel reward design inspired by the Linear Inverted Pendulum Model (LIPM) to enable perceptive and stable locomotion in the wild. The LIPM provides theoretical guidance for dynamic balance by regulating the center of mass (CoM) height and the torso orientation. These are key factors for terrain-aware locomotion, as they help ensure a stable viewpoint for the ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose a novel reward design inspired by the Linear Inverted Pendulum Model (LIPM) to enable perceptive and stable locomotion in the wild</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Achieving stable and robust perceptive locomotion for bipedal robots in unstructured outdoor environments remains a critical challenge due to complex terrain geometry and susceptibility to external disturbances</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In this work, we propose a novel reward design inspired by the Linear Inverted Pendulum Model (LIPM) to enable perceptive and stable locomotion in the wild</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.09106v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.09106v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Su, H. Luo, S. Yang, K. Jiang, W. Zhang, and H. Chen, &quot;LIPM-Guided Reinforcement Learning for Stable and Perceptive Locomotion in Bipedal Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.09106v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399337088')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="natural humanoid robot locomotion with generative motion prior" data-keywords="robot humanoid locomotion simulation imu cs.ro cs.lg" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.09015v1" target="_blank">Natural Humanoid Robot Locomotion with Generative Motion Prior</a>
                            </h3>
                            <p class="card-authors">Haodong Zhang, Liang Zhang, Zhenghan Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Simulation</span></div>

                            <div class="card-details" id="details-4399348464">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Natural and lifelike locomotion remains a fundamental challenge for humanoid robots to interact with human society. However, previous methods either neglect motion naturalness or rely on unstable and ambiguous style rewards. In this paper, we propose a novel Generative Motion Prior (GMP) that provides fine-grained motion-level supervision for the task of natural humanoid robot locomotion. To leverage natural human motions, we first employ whole-body motion retargeting to effectively transfer them to the robot. Subsequently, we train a generative model offline to predict future natural referenc...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose a novel Generative Motion Prior (GMP) that provides fine-grained motion-level supervision for the task of natural humanoid robot locomotion</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Natural and lifelike locomotion remains a fundamental challenge for humanoid robots to interact with human society</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">However, previous methods either neglect motion naturalness or rely on unstable and ambiguous style rewards</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.09015v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.09015v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Zhang, L. Zhang, Z. Chen, L. Chen, Y. Wang, and R. Xiong, &quot;Natural Humanoid Robot Locomotion with Generative Motion Prior,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.09015v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399348464')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="humanoid agent via embodied chain-of-action reasoning with multimodal foundation models for zero-shot loco-manipulation" data-keywords="robot humanoid locomotion manipulation coordination perception ros cs.ro cs.ai" data-themes="E I M L R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.09532v3" target="_blank">Humanoid Agent via Embodied Chain-of-Action Reasoning with Multimodal Foundation Models for Zero-Shot Loco-Manipulation</a>
                            </h3>
                            <p class="card-authors">Congcong Wen, Geeta Chandra Raju Bethala, Yu Hao et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Manipulation</span></div>

                            <div class="card-details" id="details-4399334976">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid loco-manipulation, which integrates whole-body locomotion with dexterous manipulation, remains a fundamental challenge in robotics. Beyond whole-body coordination and balance, a central difficulty lies in understanding human instructions and translating them into coherent sequences of embodied actions. Recent advances in foundation models provide transferable multimodal representations and reasoning capabilities, yet existing efforts remain largely restricted to either locomotion or manipulation in isolation, with limited applicability to humanoid settings. In this paper, we propose H...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose Humanoid-COA, the first humanoid agent framework that integrates foundation model reasoning with an Embodied Chain-of-Action (CoA) mechanism for zero-shot loco-manipulation</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Humanoid loco-manipulation, which integrates whole-body locomotion with dexterous manipulation, remains a fundamental challenge in robotics. Beyond whole-body coordination and balance, a central difficulty lies in understanding human instructions and translating them into coherent sequences of embodied actions</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Recent advances in foundation models provide transferable multimodal representations and reasoning capabilities, yet existing efforts remain largely restricted to either locomotion or manipulation in isolation, with limited applicability to humanoid settings</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.09532v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.09532v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Wen et al., &quot;Humanoid Agent via Embodied Chain-of-Action Reasoning with Multimodal Foundation Models for Zero-Shot Loco-Manipulation,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.09532v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399334976')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="polygmap: a perceptive locomotion framework for humanoid robot stair climbing" data-keywords="robot humanoid locomotion perception planning lidar camera imu sensor fusion cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2510.12346v1" target="_blank">PolygMap: A Perceptive Locomotion Framework for Humanoid Robot Stair Climbing</a>
                            </h3>
                            <p class="card-authors">Bingquan Li, Ning Wang, Tianwei Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Perception</span></div>

                            <div class="card-details" id="details-4399334784">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recently, biped robot walking technology has been significantly developed, mainly in the context of a bland walking scheme. To emulate human walking, robots need to step on the positions they see in unknown spaces accurately. In this paper, we present PolyMap, a perception-based locomotion planning framework for humanoid robots to climb stairs. Our core idea is to build a real-time polygonal staircase plane semantic map, followed by a footstep planar using these polygonal plane segments. These plane segmentation and visual odometry are done by multi-sensor fusion(LiDAR, RGB-D camera and IMUs)....</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we present PolyMap, a perception-based locomotion planning framework for humanoid robots to climb stairs</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In this paper, we present PolyMap, a perception-based locomotion planning framework for humanoid robots to climb stairs</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2510.12346v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2510.12346v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. Li, N. Wang, T. Zhang, Z. He, and Y. Wu, &quot;PolygMap: A Perceptive Locomotion Framework for Humanoid Robot Stair Climbing,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2510.12346v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399334784')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="unified multi-rate model predictive control for a jet-powered humanoid robot" data-keywords="robot humanoid control simulation mujoco imu cs.ro eess.sy" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2505.16478v2" target="_blank">Unified Multi-Rate Model Predictive Control for a Jet-Powered Humanoid Robot</a>
                            </h3>
                            <p class="card-authors">Davide Gorbani, Giuseppe L&#x27;Erario, Hosameldin Awadalla Omer Mohamed et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Control</span><span class="keyword-tag">Simulation</span></div>

                            <div class="card-details" id="details-4399337280">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We propose a novel Model Predictive Control (MPC) framework for a jet-powered flying humanoid robot. The controller is based on a linearised centroidal momentum model to represent the flight dynamics, augmented with a second-order nonlinear model to explicitly account for the slow and nonlinear dynamics of jet propulsion. A key contribution is the introduction of a multi-rate MPC formulation that handles the different actuation rates of the robot&#x27;s joints and jet engines while embedding the jet dynamics directly into the predictive model. We validated the framework using the jet-powered humano...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a novel Model Predictive Control (MPC) framework for a jet-powered flying humanoid robot</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We propose a novel Model Predictive Control (MPC) framework for a jet-powered flying humanoid robot</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2505.16478v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2505.16478v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Gorbani, G. L&#x27;Erario, H. A. O. Mohamed, and D. Pucci, &quot;Unified Multi-Rate Model Predictive Control for a Jet-Powered Humanoid Robot,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2505.16478v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399337280')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="stride: automating reward design, deep reinforcement learning training and feedback optimization in humanoid robotics locomotion" data-keywords="reinforcement learning robot humanoid locomotion coordination control optimization ros imu language model" data-themes="I E L R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.04692v3" target="_blank">STRIDE: Automating Reward Design, Deep Reinforcement Learning Training and Feedback Optimization in Humanoid Robotics Locomotion</a>
                            </h3>
                            <p class="card-authors">Zhenwei Wu, Jinxiong Lu, Yuxiao Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399335792">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robotics presents significant challenges in artificial intelligence, requiring precise coordination and control of high-degree-of-freedom systems. Designing effective reward functions for deep reinforcement learning (DRL) in this domain remains a critical bottleneck, demanding extensive manual effort, domain expertise, and iterative refinement. To overcome these challenges, we introduce STRIDE, a novel framework built on agentic engineering to automate reward design, DRL training, and feedback optimization for humanoid robot locomotion tasks. By combining the structured principles of ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To overcome these challenges, we introduce STRIDE, a novel framework built on agentic engineering to automate reward design, DRL training, and feedback optimization for humanoid robot locomotion tasks</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Humanoid robotics presents significant challenges in artificial intelligence, requiring precise coordination and control of high-degree-of-freedom systems. To overcome these challenges, we introduce STRIDE, a novel framework built on agentic engineering to automate reward design, DRL training, and feedback optimization for humanoid robot locomotion tasks</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">To overcome these challenges, we introduce STRIDE, a novel framework built on agentic engineering to automate reward design, DRL training, and feedback optimization for humanoid robot locomotion tasks</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.04692v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.04692v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Wu, J. Lu, Y. Chen, Y. Liu, Y. Zhuang, and L. Hu, &quot;STRIDE: Automating Reward Design, Deep Reinforcement Learning Training and Feedback Optimization in Humanoid Robotics Locomotion,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.04692v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399335792')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="styleloco: generative adversarial distillation for natural humanoid robot locomotion" data-keywords="reinforcement learning robot humanoid locomotion simulation ros imu imitation learning cs.ro cs.ai" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.15082v1" target="_blank">StyleLoco: Generative Adversarial Distillation for Natural Humanoid Robot Locomotion</a>
                            </h3>
                            <p class="card-authors">Le Ma, Ziyu Meng, Tengyu Liu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399336848">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots are anticipated to acquire a wide range of locomotion capabilities while ensuring natural movement across varying speeds and terrains. Existing methods encounter a fundamental dilemma in learning humanoid locomotion: reinforcement learning with handcrafted rewards can achieve agile locomotion but produces unnatural gaits, while Generative Adversarial Imitation Learning (GAIL) with motion capture data yields natural movements but suffers from unstable training processes and restricted agility. Integrating these approaches proves challenging due to the inherent heterogeneity betw...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address this, we introduce StyleLoco, a novel two-stage framework that bridges this gap through a Generative Adversarial Distillation (GAD) process</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To address this, we introduce StyleLoco, a novel two-stage framework that bridges this gap through a Generative Adversarial Distillation (GAD) process. This approach effectively combines the agility of reinforcement learning with the natural fluidity of human-like movements while mitigating the instability issues commonly associated with adversarial training</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Existing methods encounter a fundamental dilemma in learning humanoid locomotion: reinforcement learning with handcrafted rewards can achieve agile locomotion but produces unnatural gaits, while Generative Adversarial Imitation Learning (GAIL) with motion capture data yields natural movements but suffers from unstable training processes and restricted agility</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.15082v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.15082v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Ma et al., &quot;StyleLoco: Generative Adversarial Distillation for Natural Humanoid Robot Locomotion,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.15082v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399336848')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="rhino: learning real-time humanoid-human-object interaction from human demonstrations" data-keywords="robot humanoid locomotion manipulation control cs.ro cs.hc cs.lg" data-themes="S E I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.13134v1" target="_blank">RHINO: Learning Real-Time Humanoid-Human-Object Interaction from Human Demonstrations</a>
                            </h3>
                            <p class="card-authors">Jingxiao Chen, Xinyao Li, Jiahang Cao et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Manipulation</span></div>

                            <div class="card-details" id="details-4399337520">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots have shown success in locomotion and manipulation. Despite these basic abilities, humanoids are still required to quickly understand human instructions and react based on human interaction signals to become valuable assistants in human daily life. Unfortunately, most existing works only focus on multi-stage interactions, treating each task separately, and neglecting real-time feedback. In this work, we aim to empower humanoid robots with real-time reaction abilities to achieve various tasks, allowing human to interrupt robots at any time, and making robots respond to humans imm...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To support such abilities, we propose a general humanoid-human-object interaction framework, named RHINO, i.e., Real-time Humanoid-human Interaction and Object manipulation</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">To support such abilities, we propose a general humanoid-human-object interaction framework, named RHINO, i.e., Real-time Humanoid-human Interaction and Object manipulation</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.13134v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.13134v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Chen et al., &quot;RHINO: Learning Real-Time Humanoid-Human-Object Interaction from Human Demonstrations,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.13134v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399337520')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="learning vision-driven reactive soccer skills for humanoid robots" data-keywords="reinforcement learning robot humanoid coordination perception control ros cs.ro" data-themes="S I R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2511.03996v1" target="_blank">Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Yushi Wang, Changsheng Luo, Penghui Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-R">Multi-Robot</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Coordination</span></div>

                            <div class="card-details" id="details-4399337424">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid soccer poses a representative challenge for embodied intelligence, requiring robots to operate within a tightly coupled perception-action loop. However, existing systems typically rely on decoupled modules, resulting in delayed responses and incoherent behaviors in dynamic environments, while real-world perceptual limitations further exacerbate these issues. In this work, we present a unified reinforcement learning-based controller that enables humanoid robots to acquire reactive soccer skills through the direct integration of visual perception and motion control. Our approach extends...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we present a unified reinforcement learning-based controller that enables humanoid robots to acquire reactive soccer skills through the direct integration of visual perception and motion control</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Humanoid soccer poses a representative challenge for embodied intelligence, requiring robots to operate within a tightly coupled perception-action loop. However, existing systems typically rely on decoupled modules, resulting in delayed responses and incoherent behaviors in dynamic environments, while real-world perceptual limitations further exacerbate these issues</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Our approach extends Adversarial Motion Priors to perceptual settings in real-world dynamic environments, bridging motion imitation and visually grounded dynamic control</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2511.03996v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2511.03996v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Wang et al., &quot;Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2511.03996v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399337424')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="end-to-end humanoid robot safe and comfortable locomotion policy" data-keywords="reinforcement learning robot humanoid locomotion navigation perception control optimization lidar cs.ro" data-themes="S I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.07611v1" target="_blank">End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy</a>
                            </h3>
                            <p class="card-authors">Zifan Wang, Xun Yang, Jianzhuang Zhao et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399336752">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The deployment of humanoid robots in unstructured, human-centric environments requires navigation capabilities that extend beyond simple locomotion to include robust perception, provable safety, and socially aware behavior. Current reinforcement learning approaches are often limited by blind controllers that lack environmental awareness or by vision-based systems that fail to perceive complex 3D obstacles. In this work, we present an end-to-end locomotion policy that directly maps raw, spatio-temporal LiDAR point clouds to motor commands, enabling robust navigation in cluttered dynamic scenes....</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we present an end-to-end locomotion policy that directly maps raw, spatio-temporal LiDAR point clouds to motor commands, enabling robust navigation in cluttered dynamic scenes</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">We formulate the control problem as a Constrained Markov Decision Process (CMDP) to formally separate safety from task objectives</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Current reinforcement learning approaches are often limited by blind controllers that lack environmental awareness or by vision-based systems that fail to perceive complex 3D obstacles</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.07611v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.07611v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Wang et al., &quot;End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.07611v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399336752')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="child (controller for humanoid imitation and live demonstration): a whole-body humanoid teleoperation system" data-keywords="robot humanoid manipulation control cs.ro" data-themes="I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.00162v2" target="_blank">CHILD (Controller for Humanoid Imitation and Live Demonstration): a Whole-Body Humanoid Teleoperation System</a>
                            </h3>
                            <p class="card-authors">Noboru Myers, Obin Kwon, Sankalp Yamsani et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Manipulation</span><span class="keyword-tag">Control</span></div>

                            <div class="card-details" id="details-4399348656">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recent advances in teleoperation have demonstrated robots performing complex manipulation tasks. However, existing works rarely support whole-body joint-level teleoperation for humanoid robots, limiting the diversity of tasks that can be accomplished. This work presents Controller for Humanoid Imitation and Live Demonstration (CHILD), a compact reconfigurable teleoperation system that enables joint level control over humanoid robots. CHILD fits within a standard baby carrier, allowing the operator control over all four limbs, and supports both direct joint mapping for full-body control and loc...</p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.00162v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.00162v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. Myers, O. Kwon, S. Yamsani, and J. Kim, &quot;CHILD (Controller for Humanoid Imitation and Live Demonstration): a Whole-Body Humanoid Teleoperation System,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.00162v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399348656')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="rl-augmented adaptive model predictive control for bipedal locomotion over challenging terrain" data-keywords="reinforcement learning robot humanoid bipedal locomotion control simulation ros isaac imu" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.18466v1" target="_blank">RL-augmented Adaptive Model Predictive Control for Bipedal Locomotion over Challenging Terrain</a>
                            </h3>
                            <p class="card-authors">Junnosuke Kamohara, Feiyang Wu, Chinmayee Wamorkar et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span></div>

                            <div class="card-details" id="details-4399339200">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Model predictive control (MPC) has demonstrated effectiveness for humanoid bipedal locomotion; however, its applicability in challenging environments, such as rough and slippery terrain, is limited by the difficulty of modeling terrain interactions. In contrast, reinforcement learning (RL) has achieved notable success in training robust locomotion policies over diverse terrain, yet it lacks guarantees of constraint satisfaction and often requires substantial reward shaping. Recent efforts in combining MPC and RL have shown promise of taking the best of both worlds, but they are primarily restr...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose an RL-augmented MPC framework tailored for bipedal locomotion over rough and slippery terrain</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Model predictive control (MPC) has demonstrated effectiveness for humanoid bipedal locomotion; however, its applicability in challenging environments, such as rough and slippery terrain, is limited by the difficulty of modeling terrain interactions</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Model predictive control (MPC) has demonstrated effectiveness for humanoid bipedal locomotion; however, its applicability in challenging environments, such as rough and slippery terrain, is limited by the difficulty of modeling terrain interactions</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.18466v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.18466v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Kamohara, F. Wu, C. Wamorkar, S. Hutchinson, and Y. Zhao, &quot;RL-augmented Adaptive Model Predictive Control for Bipedal Locomotion over Challenging Terrain,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.18466v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399339200')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="vmts: vision-assisted teacher-student reinforcement learning for multi-terrain locomotion in bipedal robots" data-keywords="reinforcement learning robot bipedal locomotion perception control ros cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.07049v2" target="_blank">VMTS: Vision-Assisted Teacher-Student Reinforcement Learning for Multi-Terrain Locomotion in Bipedal Robots</a>
                            </h3>
                            <p class="card-authors">Fu Chen, Rui Wan, Peidong Liu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399349232">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Bipedal robots, due to their anthropomorphic design, offer substantial potential across various applications, yet their control is hindered by the complexity of their structure. Currently, most research focuses on proprioception-based methods, which lack the capability to overcome complex terrain. While visual perception is vital for operation in human-centric environments, its integration complicates control further. Recent reinforcement learning (RL) approaches have shown promise in enhancing legged robot locomotion, particularly with proprioception-based methods. However, terrain adaptabili...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we introduce a novel mixture of experts teacher-student network RL strategy, which enhances the performance of teacher-student policies based on visual inputs through a simple yet effective approach</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, terrain adaptability, especially for bipedal robots, remains a significant challenge, with most research focusing on flat-terrain scenarios</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Currently, most research focuses on proprioception-based methods, which lack the capability to overcome complex terrain</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.07049v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.07049v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. Chen, R. Wan, P. Liu, N. Zheng, and B. Zhou, &quot;VMTS: Vision-Assisted Teacher-Student Reinforcement Learning for Multi-Terrain Locomotion in Bipedal Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.07049v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399349232')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="booster gym: an end-to-end reinforcement learning framework for humanoid robot locomotion" data-keywords="reinforcement learning robot humanoid locomotion simulation imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.15132v1" target="_blank">Booster Gym: An End-to-End Reinforcement Learning Framework for Humanoid Robot Locomotion</a>
                            </h3>
                            <p class="card-authors">Yushi Wang, Penghui Chen, Xinyu Han et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399347888">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recent advancements in reinforcement learning (RL) have led to significant progress in humanoid robot locomotion, simplifying the design and training of motion policies in simulation. However, the numerous implementation details make transferring these policies to real-world robots a challenging task. To address this, we have developed a comprehensive code framework that covers the entire process from training to deployment, incorporating common RL training methods, domain randomization, reward function design, and solutions for handling parallel structures. This library is made available as a...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">To address this, we have developed a comprehensive code framework that covers the entire process from training to deployment, incorporating common RL training methods, domain randomization, reward function design, and solutions for handling parallel structures</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.15132v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.15132v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Wang, P. Chen, X. Han, F. Wu, and M. Zhao, &quot;Booster Gym: An End-to-End Reinforcement Learning Framework for Humanoid Robot Locomotion,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.15132v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399347888')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="mechanical intelligence-aware curriculum reinforcement learning for humanoids with parallel actuation" data-keywords="reinforcement learning robot humanoid locomotion control simulation mujoco imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.00273v3" target="_blank">Mechanical Intelligence-Aware Curriculum Reinforcement Learning for Humanoids with Parallel Actuation</a>
                            </h3>
                            <p class="card-authors">Yusuke Tanaka, Alvin Zhu, Quanyou Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399348704">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Reinforcement learning (RL) has enabled advances in humanoid robot locomotion, yet most learning frameworks do not account for mechanical intelligence embedded in parallel actuation mechanisms due to limitations in simulator support for closed kinematic chains. This omission can lead to inaccurate motion modeling and suboptimal policies, particularly for robots with high actuation complexity. This paper presents general formulations and simulation methods for three types of parallel mechanisms: a differential pulley, a five-bar linkage, and a four-bar linkage, and trains a parallel-mechanism a...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents general formulations and simulation methods for three types of parallel mechanisms: a differential pulley, a five-bar linkage, and a four-bar linkage, and trains a parallel-mechanism aware policy through an end-to-end curriculum RL framework for BRUCE, a kid-sized humanoid robot</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Reinforcement learning (RL) has enabled advances in humanoid robot locomotion, yet most learning frameworks do not account for mechanical intelligence embedded in parallel actuation mechanisms due to limitations in simulator support for closed kinematic chains</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Reinforcement learning (RL) has enabled advances in humanoid robot locomotion, yet most learning frameworks do not account for mechanical intelligence embedded in parallel actuation mechanisms due to limitations in simulator support for closed kinematic chains</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.00273v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.00273v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Tanaka, A. Zhu, Q. Wang, Y. Liu, and D. Hong, &quot;Mechanical Intelligence-Aware Curriculum Reinforcement Learning for Humanoids with Parallel Actuation,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.00273v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399348704')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="a framework for optimal ankle design of humanoid robots" data-keywords="robot humanoid optimization ros cs.ro" data-themes="M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.16469v1" target="_blank">A Framework for Optimal Ankle Design of Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Guglielmo Cervettini, Roberto Mauceri, Alex Coppola et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">Ros</span></div>

                            <div class="card-details" id="details-4399347456">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The design of the humanoid ankle is critical for safe and efficient ground interaction. Key factors such as mechanical compliance and motor mass distribution have driven the adoption of parallel mechanism architectures. However, selecting the optimal configuration depends on both actuator availability and task requirements. We propose a unified methodology for the design and evaluation of parallel ankle mechanisms. A multi-objective optimization synthesizes the mechanism geometry, the resulting solutions are evaluated using a scalar cost function that aggregates key performance metrics for cro...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a unified methodology for the design and evaluation of parallel ankle mechanisms</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Key factors such as mechanical compliance and motor mass distribution have driven the adoption of parallel mechanism architectures</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.16469v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.16469v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Cervettini et al., &quot;A Framework for Optimal Ankle Design of Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.16469v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399347456')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="learning humanoid standing-up control across diverse postures" data-keywords="reinforcement learning robot humanoid locomotion manipulation control simulation ros imu cs.ro" data-themes="S I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.08378v2" target="_blank">Learning Humanoid Standing-up Control across Diverse Postures</a>
                            </h3>
                            <p class="card-authors">Tao Huang, Junli Ren, Huayi Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399349136">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Standing-up control is crucial for humanoid robots, with the potential for integration into current locomotion and loco-manipulation systems, such as fall recovery. Existing approaches are either limited to simulations that overlook hardware constraints or rely on predefined ground-specific motion trajectories, failing to enable standing up across postures in real-world scenes. To bridge this gap, we present HoST (Humanoid Standing-up Control), a reinforcement learning framework that learns standing-up control from scratch, enabling robust sim-to-real transfer across diverse postures. HoST eff...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To bridge this gap, we present HoST (Humanoid Standing-up Control), a reinforcement learning framework that learns standing-up control from scratch, enabling robust sim-to-real transfer across diverse postures</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To bridge this gap, we present HoST (Humanoid Standing-up Control), a reinforcement learning framework that learns standing-up control from scratch, enabling robust sim-to-real transfer across diverse postures</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Existing approaches are either limited to simulations that overlook hardware constraints or rely on predefined ground-specific motion trajectories, failing to enable standing up across postures in real-world scenes</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.08378v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.08378v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. Huang et al., &quot;Learning Humanoid Standing-up Control across Diverse Postures,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.08378v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399349136')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="a unified and general humanoid whole-body controller for versatile locomotion" data-keywords="robot humanoid locomotion manipulation control simulation imu cs.ro cs.ai" data-themes="S I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.03206v3" target="_blank">A Unified and General Humanoid Whole-Body Controller for Versatile Locomotion</a>
                            </h3>
                            <p class="card-authors">Yufei Xue, Wentao Dong, Minghuan Liu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Manipulation</span></div>

                            <div class="card-details" id="details-4399336896">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Locomotion is a fundamental skill for humanoid robots. However, most existing works make locomotion a single, tedious, unextendable, and unconstrained movement. This limits the kinematic capabilities of humanoid robots. In contrast, humans possess versatile athletic abilities-running, jumping, hopping, and finely adjusting gait parameters such as frequency and foot height. In this paper, we investigate solutions to bring such versatility into humanoid locomotion and thereby propose HugWBC: a unified and general humanoid whole-body controller for versatile locomotion. By designing a general com...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">By designing a general command space in the aspect of tasks and behaviors, along with advanced techniques like symmetrical loss and intervention training for learning a whole-body humanoid controlling policy in simulation, HugWBC enables real-world humanoid robots to produce various natural gaits, including walking, jumping, standing, and hopping, with customizable parameters such as frequency, foot swing height, further combined with different body height, waist rotation, and body pitch</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.03206v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.03206v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Xue, W. Dong, M. Liu, W. Zhang, and J. Pang, &quot;A Unified and General Humanoid Whole-Body Controller for Versatile Locomotion,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.03206v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399336896')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="unitracker: learning universal whole-body motion tracker for humanoid robots" data-keywords="robot humanoid control simulation ros imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.07356v3" target="_blank">UniTracker: Learning Universal Whole-Body Motion Tracker for Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Kangning Yin, Weishuai Zeng, Ke Fan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Control</span><span class="keyword-tag">Simulation</span></div>

                            <div class="card-details" id="details-4399349184">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Achieving expressive and generalizable whole-body motion control is essential for deploying humanoid robots in real-world environments. In this work, we propose UniTracker, a three-stage training framework that enables robust and scalable motion tracking across a wide range of human behaviors. In the first stage, we train a teacher policy with privileged observations to generate high-quality actions. In the second stage, we introduce a Conditional Variational Autoencoder (CVAE) to model a universal student policy that can be deployed directly on real hardware. The CVAE structure allows the pol...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose UniTracker, a three-stage training framework that enables robust and scalable motion tracking across a wide range of human behaviors</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">The CVAE structure allows the policy to learn a global latent representation of motion, enhancing generalization to unseen behaviors and addressing the limitations of standard MLP-based policies under partial observations. In the third stage, we introduce a fast adaptation module that fine-tunes the universal policy on harder motion sequences that are difficult to track directly</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In this work, we propose UniTracker, a three-stage training framework that enables robust and scalable motion tracking across a wide range of human behaviors</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.07356v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.07356v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Yin et al., &quot;UniTracker: Learning Universal Whole-Body Motion Tracker for Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.07356v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399349184')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="learning humanoid locomotion with world model reconstruction" data-keywords="robot humanoid locomotion control ros cs.ro cs.lg" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.16230v1" target="_blank">Learning Humanoid Locomotion with World Model Reconstruction</a>
                            </h3>
                            <p class="card-authors">Wandong Sun, Long Chen, Yongbo Su et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Control</span></div>

                            <div class="card-details" id="details-4399335984">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots are designed to navigate environments accessible to humans using their legs. However, classical research has primarily focused on controlled laboratory settings, resulting in a gap in developing controllers for navigating complex real-world terrains. This challenge mainly arises from the limitations and noise in sensor data, which hinder the robot&#x27;s understanding of itself and the environment. In this study, we introduce World Model Reconstruction (WMR), an end-to-end learning-based approach for blind humanoid locomotion across challenging terrains. We propose training an estim...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this study, we introduce World Model Reconstruction (WMR), an end-to-end learning-based approach for blind humanoid locomotion across challenging terrains</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, classical research has primarily focused on controlled laboratory settings, resulting in a gap in developing controllers for navigating complex real-world terrains. This challenge mainly arises from the limitations and noise in sensor data, which hinder the robot&#x27;s understanding of itself and the environment</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In this study, we introduce World Model Reconstruction (WMR), an end-to-end learning-based approach for blind humanoid locomotion across challenging terrains</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.16230v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.16230v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. Sun, L. Chen, Y. Su, B. Cao, Y. Liu, and Z. Xie, &quot;Learning Humanoid Locomotion with World Model Reconstruction,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.16230v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399335984')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="a hierarchical framework for humanoid locomotion with supernumerary limbs" data-keywords="robot humanoid locomotion control simulation imu imitation learning cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2512.00077v1" target="_blank">A Hierarchical Framework for Humanoid Locomotion with Supernumerary Limbs</a>
                            </h3>
                            <p class="card-authors">Bowen Zhi</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Control</span></div>

                            <div class="card-details" id="details-4399349472">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The integration of Supernumerary Limbs (SLs) on humanoid robots poses a significant stability challenge due to the dynamic perturbations they introduce. This thesis addresses this issue by designing a novel hierarchical control architecture to improve humanoid locomotion stability with SLs. The core of this framework is a decoupled strategy that combines learning-based locomotion with model-based balancing. The low-level component consists of a walking gait for a Unitree H1 humanoid through imitation learning and curriculum learning. The high-level component actively utilizes the SLs for dynam...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">The integration of Supernumerary Limbs (SLs) on humanoid robots poses a significant stability challenge due to the dynamic perturbations they introduce. This thesis addresses this issue by designing a novel hierarchical control architecture to improve humanoid locomotion stability with SLs</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This thesis addresses this issue by designing a novel hierarchical control architecture to improve humanoid locomotion stability with SLs</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2512.00077v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2512.00077v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. Zhi, &quot;A Hierarchical Framework for Humanoid Locomotion with Supernumerary Limbs,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2512.00077v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399349472')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="descriptive model-based learning and control for bipedal locomotion" data-keywords="robot bipedal locomotion planning control imu cs.ro eess.sy" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2511.00512v1" target="_blank">Descriptive Model-based Learning and Control for Bipedal Locomotion</a>
                            </h3>
                            <p class="card-authors">Suraj Kumar, Andy Ruina</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Planning</span></div>

                            <div class="card-details" id="details-4399337376">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Bipedal balance is challenging due to its multi-phase, hybrid nature and high-dimensional state space. Traditional balance control approaches for bipedal robots rely on low-dimensional models for locomotion planning and reactive control, constraining the full robot to behave like these simplified models. This involves tracking preset reference paths for the Center of Mass and upper body obtained through low-dimensional models, often resulting in inefficient walking patterns with bent knees. However, we observe that bipedal balance is inherently low-dimensional and can be effectively described ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose a novel control approach that avoids prescribing a low-dimensional model to the full model</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Traditional balance control approaches for bipedal robots rely on low-dimensional models for locomotion planning and reactive control, constraining the full robot to behave like these simplified models</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2511.00512v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2511.00512v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Kumar, and A. Ruina, &quot;Descriptive Model-based Learning and Control for Bipedal Locomotion,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2511.00512v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399337376')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="preference-conditioned multi-objective rl for integrated command tracking and force compliance in humanoid locomotion" data-keywords="robot humanoid locomotion navigation optimization simulation imu cs.ro" data-themes="S I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2510.10851v1" target="_blank">Preference-Conditioned Multi-Objective RL for Integrated Command Tracking and Force Compliance in Humanoid Locomotion</a>
                            </h3>
                            <p class="card-authors">Tingxuan Leng, Yushi Wang, Tinglong Zheng et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Navigation</span></div>

                            <div class="card-details" id="details-4399349568">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid locomotion requires not only accurate command tracking for navigation but also compliant responses to external forces during human interaction. Despite significant progress, existing RL approaches mainly emphasize robustness, yielding policies that resist external forces but lack compliance-particularly challenging for inherently unstable humanoids. In this work, we address this by formulating humanoid locomotion as a multi-objective optimization problem that balances command tracking and external force compliance. We introduce a preference-conditioned multi-objective RL (MORL) framew...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce a preference-conditioned multi-objective RL (MORL) framework that integrates rigid command following and compliant behaviors within a single omnidirectional locomotion policy</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">In this work, we address this by formulating humanoid locomotion as a multi-objective optimization problem that balances command tracking and external force compliance</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Despite significant progress, existing RL approaches mainly emphasize robustness, yielding policies that resist external forces but lack compliance-particularly challenging for inherently unstable humanoids</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2510.10851v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2510.10851v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. Leng, Y. Wang, T. Zheng, C. Luo, and M. Zhao, &quot;Preference-Conditioned Multi-Objective RL for Integrated Command Tracking and Force Compliance in Humanoid Locomotion,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2510.10851v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399349568')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="inekformer: a hybrid state estimator for humanoid robots" data-keywords="deep learning transformer robot humanoid bipedal locomotion control cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2511.16306v1" target="_blank">InEKFormer: A Hybrid State Estimator for Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Lasse Hohmeyer, Mihaela Popescu, Ivan Bergonzani et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Deep Learning</span><span class="keyword-tag">Transformer</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span></div>

                            <div class="card-details" id="details-4399336128">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots have great potential for a wide range of applications, including industrial and domestic use, healthcare, and search and rescue missions. However, bipedal locomotion in different environments is still a challenge when it comes to performing stable and dynamic movements. This is where state estimation plays a crucial role, providing fast and accurate feedback of the robot&#x27;s floating base state to the motion controller. Although classical state estimation methods such as Kalman filters are widely used in robotics, they require expert knowledge to fine-tune the noise parameters. D...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose the InEKFormer, a novel hybrid state estimation method that incorporates an invariant extended Kalman filter (InEKF) and a Transformer network</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, bipedal locomotion in different environments is still a challenge when it comes to performing stable and dynamic movements. The results indicate the potential of Transformers in humanoid state estimation, but also highlight the need for robust autoregressive training in these high-dimensional problems.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Although classical state estimation methods such as Kalman filters are widely used in robotics, they require expert knowledge to fine-tune the noise parameters</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2511.16306v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2511.16306v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Hohmeyer, M. Popescu, I. Bergonzani, D. Mronga, and F. Kirchner, &quot;InEKFormer: A Hybrid State Estimator for Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2511.16306v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399336128')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="h-zero: cross-humanoid locomotion pretraining enables few-shot novel embodiment transfer" data-keywords="robot humanoid locomotion control simulation ros imu cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2512.00971v1" target="_blank">H-Zero: Cross-Humanoid Locomotion Pretraining Enables Few-shot Novel Embodiment Transfer</a>
                            </h3>
                            <p class="card-authors">Yunfeng Lin, Minghuan Liu, Yufei Xue et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Control</span></div>

                            <div class="card-details" id="details-4399348320">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The rapid advancement of humanoid robotics has intensified the need for robust and adaptable controllers to enable stable and efficient locomotion across diverse platforms. However, developing such controllers remains a significant challenge because existing solutions are tailored to specific robot designs, requiring extensive tuning of reward functions, physical parameters, and training hyperparameters for each embodiment. To address this challenge, we introduce H-Zero, a cross-humanoid locomotion pretraining pipeline that learns a generalizable humanoid base policy. We show that pretraining ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address this challenge, we introduce H-Zero, a cross-humanoid locomotion pretraining pipeline that learns a generalizable humanoid base policy</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, developing such controllers remains a significant challenge because existing solutions are tailored to specific robot designs, requiring extensive tuning of reward functions, physical parameters, and training hyperparameters for each embodiment. To address this challenge, we introduce H-Zero, a cross-humanoid locomotion pretraining pipeline that learns a generalizable humanoid base policy</p></div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2512.00971v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2512.00971v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Lin et al., &quot;H-Zero: Cross-Humanoid Locomotion Pretraining Enables Few-shot Novel Embodiment Transfer,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2512.00971v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399348320')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="learning to walk in costume: adversarial motion priors for aesthetically constrained humanoids" data-keywords="reinforcement learning robot humanoid locomotion cs.ro cs.ai eess.sy" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.05581v1" target="_blank">Learning to Walk in Costume: Adversarial Motion Priors for Aesthetically Constrained Humanoids</a>
                            </h3>
                            <p class="card-authors">Arturo Flores Alvarez, Fatemeh Zargarbashi, Havel Liu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399349280">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We present a Reinforcement Learning (RL)-based locomotion system for Cosmo, a custom-built humanoid robot designed for entertainment applications. Unlike traditional humanoids, entertainment robots present unique challenges due to aesthetic-driven design choices. Cosmo embodies these with a disproportionately large head (16% of total mass), limited sensing, and protective shells that considerably restrict movement. To address these challenges, we apply Adversarial Motion Priors (AMP) to enable the robot to learn natural-looking movements while maintaining physical stability. We develop tailore...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present a Reinforcement Learning (RL)-based locomotion system for Cosmo, a custom-built humanoid robot designed for entertainment applications</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Unlike traditional humanoids, entertainment robots present unique challenges due to aesthetic-driven design choices. To address these challenges, we apply Adversarial Motion Priors (AMP) to enable the robot to learn natural-looking movements while maintaining physical stability</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We develop tailored domain randomization techniques and specialized reward structures to ensure safe sim-to-real, protecting valuable hardware components during deployment</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.05581v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.05581v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. F. Alvarez et al., &quot;Learning to Walk in Costume: Adversarial Motion Priors for Aesthetically Constrained Humanoids,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.05581v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399349280')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="beamdojo: learning agile humanoid locomotion on sparse footholds" data-keywords="reinforcement learning robot humanoid locomotion simulation lidar imu cs.ro cs.ai cs.lg" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.10363v3" target="_blank">BeamDojo: Learning Agile Humanoid Locomotion on Sparse Footholds</a>
                            </h3>
                            <p class="card-authors">Huayi Wang, Zirui Wang, Junli Ren et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399349088">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Traversing risky terrains with sparse footholds poses a significant challenge for humanoid robots, requiring precise foot placements and stable locomotion. Existing learning-based approaches often struggle on such complex terrains due to sparse foothold rewards and inefficient learning processes. To address these challenges, we introduce BeamDojo, a reinforcement learning (RL) framework designed for enabling agile humanoid locomotion on sparse footholds. BeamDojo begins by introducing a sampling-based foothold reward tailored for polygonal feet, along with a double critic to balancing the lear...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address these challenges, we introduce BeamDojo, a reinforcement learning (RL) framework designed for enabling agile humanoid locomotion on sparse footholds</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Traversing risky terrains with sparse footholds poses a significant challenge for humanoid robots, requiring precise foot placements and stable locomotion. To address these challenges, we introduce BeamDojo, a reinforcement learning (RL) framework designed for enabling agile humanoid locomotion on sparse footholds</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Existing learning-based approaches often struggle on such complex terrains due to sparse foothold rewards and inefficient learning processes</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.10363v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.10363v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Wang et al., &quot;BeamDojo: Learning Agile Humanoid Locomotion on Sparse Footholds,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.10363v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399349088')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="humanoid whole-body badminton via multi-stage reinforcement learning" data-keywords="robot humanoid locomotion manipulation control simulation ros imu cs.ro" data-themes="S I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2511.11218v2" target="_blank">Humanoid Whole-Body Badminton via Multi-Stage Reinforcement Learning</a>
                            </h3>
                            <p class="card-authors">Chenhao Liu, Leyun Jiang, Yibo Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Manipulation</span></div>

                            <div class="card-details" id="details-4399348224">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots have demonstrated strong capabilities for interacting with static scenes across locomotion, manipulation, and more challenging loco-manipulation tasks. Yet the real world is dynamic, and quasi-static interactions are insufficient to cope with diverse environmental conditions. As a step toward more dynamic interaction scenarios, we present a reinforcement-learning-based training pipeline that produces a unified whole-body controller for humanoid badminton, enabling coordinated lower-body footwork and upper-body striking without motion priors or expert demonstrations. Training fo...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">As a step toward more dynamic interaction scenarios, we present a reinforcement-learning-based training pipeline that produces a unified whole-body controller for humanoid badminton, enabling coordinated lower-body footwork and upper-body striking without motion priors or expert demonstrations</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">To validate the framework, we conduct five sets of experiments in both simulation and the real world</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2511.11218v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2511.11218v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Liu, L. Jiang, Y. Wang, K. Yao, J. Fu, and X. Ren, &quot;Humanoid Whole-Body Badminton via Multi-Stage Reinforcement Learning,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2511.11218v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399348224')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="towards embodiment scaling laws in robot locomotion" data-keywords="robot locomotion control simulation ros imu cs.ro cs.ai cs.lg" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2505.05753v2" target="_blank">Towards Embodiment Scaling Laws in Robot Locomotion</a>
                            </h3>
                            <p class="card-authors">Bo Ai, Liu Dai, Nico Bohlinger et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Control</span><span class="keyword-tag">Simulation</span></div>

                            <div class="card-details" id="details-4399335072">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Cross-embodiment generalization underpins the vision of building generalist embodied agents for any robot, yet its enabling factors remain poorly understood. We investigate embodiment scaling laws, the hypothesis that increasing the number of training embodiments improves generalization to unseen ones, using robot locomotion as a test bed. We procedurally generate ~1,000 embodiments with topological, geometric, and joint-level kinematic variations, and train policies on random subsets. We observe positive scaling trends supporting the hypothesis, and find that embodiment scaling enables substa...</p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2505.05753v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2505.05753v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. Ai et al., &quot;Towards Embodiment Scaling Laws in Robot Locomotion,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2505.05753v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399335072')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="bracing for impact: robust humanoid push recovery and locomotion with reduced order models" data-keywords="robot humanoid locomotion control simulation imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2505.11495v1" target="_blank">Bracing for Impact: Robust Humanoid Push Recovery and Locomotion with Reduced Order Models</a>
                            </h3>
                            <p class="card-authors">Lizhi Yang, Blake Werner, Adrian B. Ghansah et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Control</span></div>

                            <div class="card-details" id="details-4399336704">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Push recovery during locomotion will facilitate the deployment of humanoid robots in human-centered environments. In this paper, we present a unified framework for walking control and push recovery for humanoid robots, leveraging the arms for push recovery while dynamically walking. The key innovation is to use the environment, such as walls, to facilitate push recovery by combining Single Rigid Body model predictive control (SRB-MPC) with Hybrid Linear Inverted Pendulum (HLIP) dynamics to enable robust locomotion, push detection, and recovery by utilizing the robot&#x27;s arms to brace against suc...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we present a unified framework for walking control and push recovery for humanoid robots, leveraging the arms for push recovery while dynamically walking</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In this paper, we present a unified framework for walking control and push recovery for humanoid robots, leveraging the arms for push recovery while dynamically walking</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2505.11495v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2505.11495v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Yang, B. Werner, A. B. Ghansah, and A. D. Ames, &quot;Bracing for Impact: Robust Humanoid Push Recovery and Locomotion with Reduced Order Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2505.11495v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399336704')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="synthetic data pipelines for adaptive, mission-ready militarized humanoids" data-keywords="humanoid navigation perception imu cs.ro" data-themes="S">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2512.14411v1" target="_blank">Synthetic Data Pipelines for Adaptive, Mission-Ready Militarized Humanoids</a>
                            </h3>
                            <p class="card-authors">Mohammed Ayman Habib, Aldo Petruzzelli</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Navigation</span><span class="keyword-tag">Perception</span><span class="keyword-tag">Imu</span></div>

                            <div class="card-details" id="details-4399349040">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Omnia presents a synthetic data driven pipeline to accelerate the training, validation, and deployment readiness of militarized humanoids. The approach converts first-person spatial observations captured from point-of-view recordings, smart glasses, augmented reality headsets, and spatial browsing workflows into scalable, mission-specific synthetic datasets for humanoid autonomy. By generating large volumes of high-fidelity simulated scenarios and pairing them with automated labeling and model training, the pipeline enables rapid iteration on perception, navigation, and decision-making capabil...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The approach converts first-person spatial observations captured from point-of-view recordings, smart glasses, augmented reality headsets, and spatial browsing workflows into scalable, mission-specific synthetic datasets for humanoid autonomy</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2512.14411v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2512.14411v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. A. Habib, and A. Petruzzelli, &quot;Synthetic Data Pipelines for Adaptive, Mission-Ready Militarized Humanoids,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2512.14411v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399349040')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="extended hybrid zero dynamics for bipedal walking of the knee-less robot slider" data-keywords="reinforcement learning robot humanoid bipedal locomotion control cs.ro eess.sy" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.01165v2" target="_blank">Extended Hybrid Zero Dynamics for Bipedal Walking of the Knee-less Robot SLIDER</a>
                            </h3>
                            <p class="card-authors">Rui Zong, Martin Liang, Yuntian Fang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span></div>

                            <div class="card-details" id="details-4399336800">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Knee-less bipedal robots like SLIDER have the advantage of ultra-lightweight legs and improved walking energy efficiency compared to traditional humanoid robots. In this paper, we firstly introduce an improved hardware design of the SLIDER bipedal robot with new line-feet and more optimized mass distribution that enables higher locomotion speeds. Secondly, we propose an extended Hybrid Zero Dynamics (eHZD) method, which can be applied to prismatic joint robots like SLIDER. The eHZD method is then used to generate a library of gaits with varying reference velocities in an offline way. Thirdly, ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Secondly, we propose an extended Hybrid Zero Dynamics (eHZD) method, which can be applied to prismatic joint robots like SLIDER</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Secondly, we propose an extended Hybrid Zero Dynamics (eHZD) method, which can be applied to prismatic joint robots like SLIDER</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.01165v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.01165v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Zong et al., &quot;Extended Hybrid Zero Dynamics for Bipedal Walking of the Knee-less Robot SLIDER,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.01165v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399336800')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="flam: foundation model-based body stabilization for humanoid locomotion and manipulation" data-keywords="reinforcement learning attention robot humanoid locomotion manipulation control cs.ro cs.lg" data-themes="E I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.22249v1" target="_blank">FLAM: Foundation Model-Based Body Stabilization for Humanoid Locomotion and Manipulation</a>
                            </h3>
                            <p class="card-authors">Xianqi Zhang, Hongliang Wei, Wenrui Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Attention</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span></div>

                            <div class="card-details" id="details-4399336272">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots have attracted significant attention in recent years. Reinforcement Learning (RL) is one of the main ways to control the whole body of humanoid robots. RL enables agents to complete tasks by learning from environment interactions, guided by task rewards. However, existing RL methods rarely explicitly consider the impact of body stability on humanoid locomotion and manipulation. Achieving high performance in whole-body control remains a challenge for RL methods that rely solely on task rewards. In this paper, we propose a Foundation model-based method for humanoid Locomotion And...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose a Foundation model-based method for humanoid Locomotion And Manipulation (FLAM for short)</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Achieving high performance in whole-body control remains a challenge for RL methods that rely solely on task rewards</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">However, existing RL methods rarely explicitly consider the impact of body stability on humanoid locomotion and manipulation</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.22249v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.22249v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. Zhang, H. Wei, W. Wang, X. Wang, X. Fan, and D. Zhao, &quot;FLAM: Foundation Model-Based Body Stabilization for Humanoid Locomotion and Manipulation,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.22249v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399336272')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="no more blind spots: learning vision-based omnidirectional bipedal locomotion for challenging terrain" data-keywords="reinforcement learning bipedal locomotion control simulation imu cs.ro cs.ai" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.11929v1" target="_blank">No More Blind Spots: Learning Vision-Based Omnidirectional Bipedal Locomotion for Challenging Terrain</a>
                            </h3>
                            <p class="card-authors">Mohitvishnu S. Gadde, Pranay Dugar, Ashish Malik et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Control</span></div>

                            <div class="card-details" id="details-4399349664">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Effective bipedal locomotion in dynamic environments, such as cluttered indoor spaces or uneven terrain, requires agile and adaptive movement in all directions. This necessitates omnidirectional terrain sensing and a controller capable of processing such input. We present a learning framework for vision-based omnidirectional bipedal locomotion, enabling seamless movement using depth images. A key challenge is the high computational cost of rendering omnidirectional depth images in simulation, making traditional sim-to-real reinforcement learning (RL) impractical. Our method combines a robust b...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present a learning framework for vision-based omnidirectional bipedal locomotion, enabling seamless movement using depth images</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">A key challenge is the high computational cost of rendering omnidirectional depth images in simulation, making traditional sim-to-real reinforcement learning (RL) impractical</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We present a learning framework for vision-based omnidirectional bipedal locomotion, enabling seamless movement using depth images</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.11929v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.11929v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. S. Gadde, P. Dugar, A. Malik, and A. Fern, &quot;No More Blind Spots: Learning Vision-Based Omnidirectional Bipedal Locomotion for Challenging Terrain,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.11929v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399349664')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="robust rl control for bipedal locomotion with closed kinematic chains" data-keywords="reinforcement learning robot bipedal locomotion control ros cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.10164v1" target="_blank">Robust RL Control for Bipedal Locomotion with Closed Kinematic Chains</a>
                            </h3>
                            <p class="card-authors">Egor Maslennikov, Eduard Zaliaev, Nikita Dudorov et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399335600">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Developing robust locomotion controllers for bipedal robots with closed kinematic chains presents unique challenges, particularly since most reinforcement learning (RL) approaches simplify these parallel mechanisms into serial models during training. We demonstrate that this simplification significantly impairs sim-to-real transfer by failing to capture essential aspects such as joint coupling, friction dynamics, and motor-space control characteristics. In this work, we present an RL framework that explicitly incorporates closed-chain dynamics and validate it on our custom-built robot TopA. Ou...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We demonstrate that this simplification significantly impairs sim-to-real transfer by failing to capture essential aspects such as joint coupling, friction dynamics, and motor-space control characteristics</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Developing robust locomotion controllers for bipedal robots with closed kinematic chains presents unique challenges, particularly since most reinforcement learning (RL) approaches simplify these parallel mechanisms into serial models during training</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Developing robust locomotion controllers for bipedal robots with closed kinematic chains presents unique challenges, particularly since most reinforcement learning (RL) approaches simplify these parallel mechanisms into serial models during training</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.10164v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.10164v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'E. Maslennikov et al., &quot;Robust RL Control for Bipedal Locomotion with Closed Kinematic Chains,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.10164v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399335600')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="humanoid everyday: a comprehensive robotic dataset for open-world humanoid manipulation" data-keywords="robot humanoid locomotion manipulation control ros lidar cs.ro cs.lg" data-themes="S E I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2510.08807v1" target="_blank">Humanoid Everyday: A Comprehensive Robotic Dataset for Open-World Humanoid Manipulation</a>
                            </h3>
                            <p class="card-authors">Zhenyu Zhao, Hongyi Jing, Xiawei Liu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Manipulation</span></div>

                            <div class="card-details" id="details-4398443696">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">From loco-motion to dextrous manipulation, humanoid robots have made remarkable strides in demonstrating complex full-body capabilities. However, the majority of current robot learning datasets and benchmarks mainly focus on stationary robot arms, and the few existing humanoid datasets are either confined to fixed environments or limited in task diversity, often lacking human-humanoid interaction and lower-body locomotion. Moreover, there are a few standardized evaluation platforms for benchmarking learning-based policies on humanoid data. In this work, we present Humanoid Everyday, a large-sc...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we present Humanoid Everyday, a large-scale and diverse humanoid manipulation dataset characterized by extensive task variety involving dextrous object manipulation, human-humanoid interaction, locomotion-integrated actions, and more</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">In addition, we conduct an analysis of representative policy learning methods on our dataset, providing insights into their strengths and limitations across different task categories</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In addition, we conduct an analysis of representative policy learning methods on our dataset, providing insights into their strengths and limitations across different task categories</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2510.08807v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2510.08807v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Zhao et al., &quot;Humanoid Everyday: A Comprehensive Robotic Dataset for Open-World Humanoid Manipulation,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2510.08807v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398443696')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="reinforcement learning with data bootstrapping for dynamic subgoal pursuit in humanoid robot navigation" data-keywords="reinforcement learning robot humanoid bipedal locomotion navigation control simulation ros imu" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.02206v1" target="_blank">Reinforcement Learning with Data Bootstrapping for Dynamic Subgoal Pursuit in Humanoid Robot Navigation</a>
                            </h3>
                            <p class="card-authors">Chengyang Peng, Zhihao Zhang, Shiting Gong et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span></div>

                            <div class="card-details" id="details-4398443120">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Safe and real-time navigation is fundamental for humanoid robot applications. However, existing bipedal robot navigation frameworks often struggle to balance computational efficiency with the precision required for stable locomotion. We propose a novel hierarchical framework that continuously generates dynamic subgoals to guide the robot through cluttered environments. Our method comprises a high-level reinforcement learning (RL) planner for subgoal selection in a robot-centric coordinate system and a low-level Model Predictive Control (MPC) based planner which produces robust walking gaits to...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a novel hierarchical framework that continuously generates dynamic subgoals to guide the robot through cluttered environments</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">However, existing bipedal robot navigation frameworks often struggle to balance computational efficiency with the precision required for stable locomotion</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.02206v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.02206v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Peng, Z. Zhang, S. Gong, S. Agrawal, K. A. Redmill, and A. Hereid, &quot;Reinforcement Learning with Data Bootstrapping for Dynamic Subgoal Pursuit in Humanoid Robot Navigation,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.02206v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398443120')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="hwc-loco: a hierarchical whole-body control approach to robust humanoid locomotion" data-keywords="robot humanoid locomotion control optimization ros imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.00923v3" target="_blank">HWC-Loco: A Hierarchical Whole-Body Control Approach to Robust Humanoid Locomotion</a>
                            </h3>
                            <p class="card-authors">Sixu Lin, Guanren Qiao, Yunxin Tai et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Control</span></div>

                            <div class="card-details" id="details-4398442976">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots, capable of assuming human roles in various workplaces, have become essential to embodied intelligence. However, as robots with complex physical structures, learning a control model that can operate robustly across diverse environments remains inherently challenging, particularly under the discrepancies between training and deployment environments. In this study, we propose HWC-Loco, a robust whole-body control algorithm tailored for humanoid locomotion tasks. By reformulating policy learning as a robust optimization problem, HWC-Loco explicitly learns to recover from safety-cr...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this study, we propose HWC-Loco, a robust whole-body control algorithm tailored for humanoid locomotion tasks</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">By reformulating policy learning as a robust optimization problem, HWC-Loco explicitly learns to recover from safety-critical scenarios. To tackle this challenge, HWC-Loco leverages a hierarchical policy for robust control</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">However, as robots with complex physical structures, learning a control model that can operate robustly across diverse environments remains inherently challenging, particularly under the discrepancies between training and deployment environments</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.00923v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.00923v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Lin, G. Qiao, Y. Tai, A. Li, K. Jia, and G. Liu, &quot;HWC-Loco: A Hierarchical Whole-Body Control Approach to Robust Humanoid Locomotion,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.00923v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398442976')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="doublyaware: dual planning and policy awareness for temporal difference learning in humanoid locomotion" data-keywords="reinforcement learning robot humanoid locomotion planning control simulation imu cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.12095v1" target="_blank">DoublyAware: Dual Planning and Policy Awareness for Temporal Difference Learning in Humanoid Locomotion</a>
                            </h3>
                            <p class="card-authors">Khang Nguyen, An T. Le, Jan Peters et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4398432320">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Achieving robust robot learning for humanoid locomotion is a fundamental challenge in model-based reinforcement learning (MBRL), where environmental stochasticity and randomness can hinder efficient exploration and learning stability. The environmental, so-called aleatoric, uncertainty can be amplified in high-dimensional action spaces with complex contact dynamics, and further entangled with epistemic uncertainty in the models during learning phases. In this work, we propose DoublyAware, an uncertainty-aware extension of Temporal Difference Model Predictive Control (TD-MPC) that explicitly de...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose DoublyAware, an uncertainty-aware extension of Temporal Difference Model Predictive Control (TD-MPC) that explicitly decomposes uncertainty into two disjoint interpretable components, i.e., planning and policy uncertainties</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Achieving robust robot learning for humanoid locomotion is a fundamental challenge in model-based reinforcement learning (MBRL), where environmental stochasticity and randomness can hinder efficient exploration and learning stability</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Achieving robust robot learning for humanoid locomotion is a fundamental challenge in model-based reinforcement learning (MBRL), where environmental stochasticity and randomness can hinder efficient exploration and learning stability</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.12095v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.12095v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Nguyen, A. T. Le, J. Peters, and M. N. Vu, &quot;DoublyAware: Dual Planning and Policy Awareness for Temporal Difference Learning in Humanoid Locomotion,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.12095v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398432320')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="anticipatory and adaptive footstep streaming for teleoperated bipedal robots" data-keywords="robot humanoid locomotion cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.11802v1" target="_blank">Anticipatory and Adaptive Footstep Streaming for Teleoperated Bipedal Robots</a>
                            </h3>
                            <p class="card-authors">Luigi Penco, Beomyeong Park, Stefan Fasano et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">CS.RO</span></div>

                            <div class="card-details" id="details-4398443552">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Achieving seamless synchronization between user and robot motion in teleoperation, particularly during high-speed tasks, remains a significant challenge. In this work, we propose a novel approach for transferring stepping motions from the user to the robot in real-time. Instead of directly replicating user foot poses, we retarget user steps to robot footstep locations, allowing the robot to utilize its own dynamics for locomotion, ensuring better balance and stability. Our method anticipates user footsteps to minimize delays between when the user initiates and completes a step and when the rob...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose a novel approach for transferring stepping motions from the user to the robot in real-time</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Achieving seamless synchronization between user and robot motion in teleoperation, particularly during high-speed tasks, remains a significant challenge. Additionally, the system autonomously adjusts the robot&#x27;s steps to account for its surrounding terrain, overcoming challenges posed by environmental mismatches between the user&#x27;s flat-ground setup and the robot&#x27;s uneven terrain</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In this work, we propose a novel approach for transferring stepping motions from the user to the robot in real-time</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.11802v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.11802v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Penco et al., &quot;Anticipatory and Adaptive Footstep Streaming for Teleoperated Bipedal Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.11802v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398443552')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="humanoid locomotion and manipulation: current progress and challenges in control, planning, and learning" data-keywords="robot humanoid locomotion manipulation planning control imitation learning cs.ro" data-themes="S E I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2501.02116v2" target="_blank">Humanoid Locomotion and Manipulation: Current Progress and Challenges in Control, Planning, and Learning</a>
                            </h3>
                            <p class="card-authors">Zhaoyuan Gu, Junheng Li, Wenlan Shen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Manipulation</span></div>

                            <div class="card-details" id="details-4398433328">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots hold great potential to perform various human-level skills, involving unified locomotion and manipulation in real-world settings. Driven by advances in machine learning and the strength of existing model-based approaches, these capabilities have progressed rapidly, but often separately. This survey offers a comprehensive overview of the state-of-the-art in humanoid locomotion and manipulation (HLM), with a focus on control, planning, and learning methods. We first review the model-based methods that have been the backbone of humanoid robotics for the past three decades. We disc...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Finally, we compare the strengths and limitations of model-based and learning-based paradigms from multiple perspectives, such as robustness, computational efficiency, versatility, and generalizability, and suggest potential solutions to existing challenges.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Driven by advances in machine learning and the strength of existing model-based approaches, these capabilities have progressed rapidly, but often separately</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2501.02116v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2501.02116v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Gu et al., &quot;Humanoid Locomotion and Manipulation: Current Progress and Challenges in Control, Planning, and Learning,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2501.02116v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398433328')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="distillation-ppo: a novel two-stage reinforcement learning framework for humanoid robot perceptive locomotion" data-keywords="reinforcement learning attention robot humanoid locomotion control imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.08299v1" target="_blank">Distillation-PPO: A Novel Two-Stage Reinforcement Learning Framework for Humanoid Robot Perceptive Locomotion</a>
                            </h3>
                            <p class="card-authors">Qiang Zhang, Gang Han, Jingkai Sun et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Attention</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span></div>

                            <div class="card-details" id="details-4398443360">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In recent years, humanoid robots have garnered significant attention from both academia and industry due to their high adaptability to environments and human-like characteristics. With the rapid advancement of reinforcement learning, substantial progress has been made in the walking control of humanoid robots. However, existing methods still face challenges when dealing with complex environments and irregular terrains. In the field of perceptive locomotion, existing approaches are generally divided into two-stage methods and end-to-end methods. Two-stage methods first train a teacher policy in...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, existing methods still face challenges when dealing with complex environments and irregular terrains. However, due to the lack of supervision from a teacher policy, end-to-end methods often face difficulties in training and exhibit unstable performance in real-world applications</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">However, existing methods still face challenges when dealing with complex environments and irregular terrains</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.08299v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.08299v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Q. Zhang et al., &quot;Distillation-PPO: A Novel Two-Stage Reinforcement Learning Framework for Humanoid Robot Perceptive Locomotion,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.08299v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398443360')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="what can you say to a robot? capability communication leads to more natural conversations" data-keywords="robot cs.ro cs.hc" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.01448v1" target="_blank">What Can You Say to a Robot? Capability Communication Leads to More Natural Conversations</a>
                            </h3>
                            <p class="card-authors">Merle M. Reimann, Koen V. Hindriks, Florian A. Kunneman et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">CS.RO</span><span class="keyword-tag">CS.HC</span></div>

                            <div class="card-details" id="details-4398445136">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">When encountering a robot in the wild, it is not inherently clear to human users what the robot&#x27;s capabilities are. When encountering misunderstandings or problems in spoken interaction, robots often just apologize and move on, without additional effort to make sure the user understands what happened. We set out to compare the effect of two speech based capability communication strategies (proactive, reactive) to a robot without such a strategy, in regard to the user&#x27;s rating of and their behavior during the interaction. For this, we conducted an in-person user study with 120 participants who ...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">When encountering misunderstandings or problems in spoken interaction, robots often just apologize and move on, without additional effort to make sure the user understands what happened</p></div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.01448v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.01448v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. M. Reimann, K. V. Hindriks, F. A. Kunneman, C. Oertel, G. Skantze, and I. Leite, &quot;What Can You Say to a Robot? Capability Communication Leads to More Natural Conversations,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.01448v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398445136')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="learning differentiable reachability maps for optimization-based humanoid motion generation" data-keywords="neural network robot humanoid manipulation planning optimization cs.ro" data-themes="I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.11275v1" target="_blank">Learning Differentiable Reachability Maps for Optimization-based Humanoid Motion Generation</a>
                            </h3>
                            <p class="card-authors">Masaki Murooka, Iori Kumagai, Mitsuharu Morisawa et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Manipulation</span></div>

                            <div class="card-details" id="details-4398445664">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">To reduce the computational cost of humanoid motion generation, we introduce a new approach to representing robot kinematic reachability: the differentiable reachability map. This map is a scalar-valued function defined in the task space that takes positive values only in regions reachable by the robot&#x27;s end-effector. A key feature of this representation is that it is continuous and differentiable with respect to task-space coordinates, enabling its direct use as constraints in continuous optimization for humanoid motion planning. We describe a method to learn such differentiable reachability ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To reduce the computational cost of humanoid motion generation, we introduce a new approach to representing robot kinematic reachability: the differentiable reachability map</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">By incorporating the learned reachability map as a constraint, we formulate humanoid motion generation as a continuous optimization problem. We demonstrate that the proposed approach efficiently solves various motion planning problems, including footstep planning, multi-contact motion planning, and loco-manipulation planning for humanoid robots.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">To reduce the computational cost of humanoid motion generation, we introduce a new approach to representing robot kinematic reachability: the differentiable reachability map</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.11275v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.11275v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Murooka, I. Kumagai, M. Morisawa, and F. Kanehiro, &quot;Learning Differentiable Reachability Maps for Optimization-based Humanoid Motion Generation,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.11275v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398445664')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="llm-based ambiguity detection in natural language instructions for collaborative surgical robots" data-keywords="robot language model cs.ro cs.hc" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.11525v1" target="_blank">LLM-based ambiguity detection in natural language instructions for collaborative surgical robots</a>
                            </h3>
                            <p class="card-authors">Ana Davila, Jacinto Colan, Yasuhisa Hasegawa</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.RO</span><span class="keyword-tag">CS.HC</span></div>

                            <div class="card-details" id="details-4398433472">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Ambiguity in natural language instructions poses significant risks in safety-critical human-robot interaction, particularly in domains such as surgery. To address this, we propose a framework that uses Large Language Models (LLMs) for ambiguity detection specifically designed for collaborative surgical scenarios. Our method employs an ensemble of LLM evaluators, each configured with distinct prompting techniques to identify linguistic, contextual, procedural, and critical ambiguities. A chain-of-thought evaluator is included to systematically analyze instruction structure for potential issues....</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address this, we propose a framework that uses Large Language Models (LLMs) for ambiguity detection specifically designed for collaborative surgical scenarios</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">A chain-of-thought evaluator is included to systematically analyze instruction structure for potential issues</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">To address this, we propose a framework that uses Large Language Models (LLMs) for ambiguity detection specifically designed for collaborative surgical scenarios</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.11525v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.11525v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Davila, J. Colan, and Y. Hasegawa, &quot;LLM-based ambiguity detection in natural language instructions for collaborative surgical robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.11525v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398433472')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="ppf: pre-training and preservative fine-tuning of humanoid locomotion via model-assumption-based regularization" data-keywords="reinforcement learning robot humanoid locomotion control simulation ros imu cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.09833v2" target="_blank">PPF: Pre-training and Preservative Fine-tuning of Humanoid Locomotion via Model-Assumption-based Regularization</a>
                            </h3>
                            <p class="card-authors">Hyunyoung Jung, Zhaoyuan Gu, Ye Zhao et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4398443504">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid locomotion is a challenging task due to its inherent complexity and high-dimensional dynamics, as well as the need to adapt to diverse and unpredictable environments. In this work, we introduce a novel learning framework for effectively training a humanoid locomotion policy that imitates the behavior of a model-based controller while extending its capabilities to handle more complex locomotion tasks, such as more challenging terrain and higher velocity commands. Our framework consists of three key components: pre-training through imitation of the model-based controller, fine-tuning vi...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we introduce a novel learning framework for effectively training a humanoid locomotion policy that imitates the behavior of a model-based controller while extending its capabilities to handle more complex locomotion tasks, such as more challenging terrain and higher velocity commands</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In this work, we introduce a novel learning framework for effectively training a humanoid locomotion policy that imitates the behavior of a model-based controller while extending its capabilities to handle more complex locomotion tasks, such as more challenging terrain and higher velocity commands</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.09833v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.09833v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Jung, Z. Gu, Y. Zhao, H. Park, and S. Ha, &quot;PPF: Pre-training and Preservative Fine-tuning of Humanoid Locomotion via Model-Assumption-based Regularization,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.09833v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398443504')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cv" data-title="thinking in 360¬∞: humanoid visual search in the wild" data-keywords="humanoid control cs.cv" data-themes="S M E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computer Vision</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2511.20351v2" target="_blank">Thinking in 360¬∞: Humanoid Visual Search in the Wild</a>
                            </h3>
                            <p class="card-authors">Heyang Yu, Yinan Han, Xiangyu Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Control</span><span class="keyword-tag">CS.CV</span></div>

                            <div class="card-details" id="details-4398444032">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humans rely on the synergistic control of head (cephalomotor) and eye (oculomotor) to efficiently search for visual information in 360¬∞. However, prior approaches to visual search are limited to a static image, neglecting the physical embodiment and its interaction with the 3D world. How can we develop embodied visual search agents as efficient as humans while bypassing the constraints imposed by real-world hardware? To this end, we propose humanoid visual search where a humanoid agent actively rotates its head to search for objects or paths in an immersive world represented by a 360¬∞ panorami...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">How can we develop embodied visual search agents as efficient as humans while bypassing the constraints imposed by real-world hardware? To this end, we propose humanoid visual search where a humanoid agent actively rotates its head to search for objects or paths in an immersive world represented by a 360¬∞ panoramic image</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Notably, the lower ceiling of path search reveals its inherent difficulty, which we attribute to the demand for sophisticated spatial commonsense. Our results not only show a promising path forward but also quantify the immense challenge that remains in building MLLM agents that can be seamlessly integrated into everyday human life.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">However, prior approaches to visual search are limited to a static image, neglecting the physical embodiment and its interaction with the 3D world</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2511.20351v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2511.20351v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Yu et al., &quot;Thinking in 360¬∞: Humanoid Visual Search in the Wild,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2511.20351v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398444032')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="dribble master: learning agile humanoid dribbling through legged locomotion" data-keywords="reinforcement learning robot humanoid locomotion manipulation perception control simulation ros camera" data-themes="I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2505.12679v2" target="_blank">Dribble Master: Learning Agile Humanoid Dribbling Through Legged Locomotion</a>
                            </h3>
                            <p class="card-authors">Zhuoheng Wang, Jinyin Zhou, Qi Wu</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4398443648">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid soccer dribbling is a highly challenging task that demands dexterous ball manipulation while maintaining dynamic balance. Traditional rule-based methods often struggle to achieve accurate ball control due to their reliance on fixed walking patterns and limited adaptability to real-time ball dynamics. To address these challenges, we propose a two-stage curriculum learning framework that enables a humanoid robot to acquire dribbling skills without explicit dynamics or predefined trajectories. In the first stage, the robot learns basic locomotion skills; in the second stage, we fine-tune...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address these challenges, we propose a two-stage curriculum learning framework that enables a humanoid robot to acquire dribbling skills without explicit dynamics or predefined trajectories</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To address these challenges, we propose a two-stage curriculum learning framework that enables a humanoid robot to acquire dribbling skills without explicit dynamics or predefined trajectories</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Traditional rule-based methods often struggle to achieve accurate ball control due to their reliance on fixed walking patterns and limited adaptability to real-time ball dynamics</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2505.12679v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2505.12679v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Wang, J. Zhou, and Q. Wu, &quot;Dribble Master: Learning Agile Humanoid Dribbling Through Legged Locomotion,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2505.12679v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398443648')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="ctbc: contact-triggered blind climbing for wheeled bipedal robots with instruction learning and reinforcement learning" data-keywords="attention robot bipedal locomotion cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.02986v2" target="_blank">CTBC: Contact-Triggered Blind Climbing for Wheeled Bipedal Robots with Instruction Learning and Reinforcement Learning</a>
                            </h3>
                            <p class="card-authors">Rankun Li, Hao Wang, Qi Li et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4398445856">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In recent years, wheeled bipedal robots have gained increasing attention due to their advantages in mobility, such as high-speed locomotion on flat terrain. However, their performance on complex environments (e.g., staircases) remains inferior to that of traditional legged robots. To overcome this limitation, we propose a general contact-triggered blind climbing (CTBC) framework for wheeled bipedal robots. Upon detecting wheel-obstacle contact, the robot triggers a leg-lifting motion to overcome the obstacle. By leveraging a strongly-guided feedforward trajectory, our method enables the robot ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To overcome this limitation, we propose a general contact-triggered blind climbing (CTBC) framework for wheeled bipedal robots</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To overcome this limitation, we propose a general contact-triggered blind climbing (CTBC) framework for wheeled bipedal robots</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">To overcome this limitation, we propose a general contact-triggered blind climbing (CTBC) framework for wheeled bipedal robots</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.02986v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.02986v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Li et al., &quot;CTBC: Contact-Triggered Blind Climbing for Wheeled Bipedal Robots with Instruction Learning and Reinforcement Learning,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.02986v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398445856')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2025" data-category="other" data-title="leg state estimation for quadruped robot by using probabilistic model with proprioceptive feedback" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-105008431336&amp;origin=resultslist" target="_blank">Leg State Estimation for Quadruped Robot by Using Probabilistic Model With Proprioceptive Feedback</a>
                            </h3>
                            <p class="card-authors">Sun J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398446432">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-105008431336&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TMECH.2024.3421251" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. J., &quot;Leg State Estimation for Quadruped Robot by Using Probabilistic Model With Proprioceptive Feedback,&quot; IEEE ASME Transactions on Mechatronics, 2025. doi: 10.1109/TMECH.2024.3421251. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-105008431336&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398446432')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2025" data-category="other" data-title="learning-based legged locomotion: state of the art and future perspectives" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85216854259&amp;origin=resultslist" target="_blank">Learning-based legged locomotion: State of the art and future perspectives</a>
                            </h3>
                            <p class="card-authors">Ha S.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398447680">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85216854259&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1177/02783649241312698" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. S., &quot;Learning-based legged locomotion: State of the art and future perspectives,&quot; International Journal of Robotics Research, 2025. doi: 10.1177/02783649241312698. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85216854259&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398447680')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2025" data-category="other" data-title="a novel adaptive dynamic optimal balance control method for wheel-legged robot" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85205929380&amp;origin=resultslist" target="_blank">A novel adaptive dynamic optimal balance control method for wheel-legged robot</a>
                            </h3>
                            <p class="card-authors">Liu X.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398448592">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85205929380&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1016/j.apm.2024.115737" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. X., &quot;A novel adaptive dynamic optimal balance control method for wheel-legged robot,&quot; Applied Mathematical Modelling, 2025. doi: 10.1016/j.apm.2024.115737. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85205929380&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398448592')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2025" data-category="other" data-title="design and control of skater: a wheeled- bipedal robot with high-speed turning robustness and terrain adaptability" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-105003137460&amp;origin=resultslist" target="_blank">Design and Control of SKATER: A Wheeled- Bipedal Robot With High-Speed Turning Robustness and Terrain Adaptability</a>
                            </h3>
                            <p class="card-authors">Wang Y.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398170608">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-105003137460&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TMECH.2024.3420390" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. Y., &quot;Design and Control of SKATER: A Wheeled- Bipedal Robot With High-Speed Turning Robustness and Terrain Adaptability,&quot; IEEE ASME Transactions on Mechatronics, 2025. doi: 10.1109/TMECH.2024.3420390. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-105003137460&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398170608')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2025" data-category="other" data-title="nonsmooth trajectory optimization for wheeled balancing robots with contact switches and impacts" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-86000429293&amp;origin=resultslist" target="_blank">Nonsmooth Trajectory Optimization for Wheeled Balancing Robots With Contact Switches and Impacts</a>
                            </h3>
                            <p class="card-authors">Klemm V.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398171664">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-86000429293&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2023.3326334" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. V., &quot;Nonsmooth Trajectory Optimization for Wheeled Balancing Robots With Contact Switches and Impacts,&quot; IEEE Transactions on Robotics, 2025. doi: 10.1109/TRO.2023.3326334. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-86000429293&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398171664')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section year-section" data-year="2024">
                <h2 class="section-header">üìÖ 2024 <span class="section-count">(143 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="a mini-review on mobile manipulators with variable autonomy" data-keywords="robot language model cs.ro cs.hc" data-themes="E L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.10887v1" target="_blank">A Mini-Review on Mobile Manipulators with Variable Autonomy</a>
                            </h3>
                            <p class="card-authors">Cesar Alan Contreras, Alireza Rastegarpanah, Rustam Stolkin et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.RO</span><span class="keyword-tag">CS.HC</span></div>

                            <div class="card-details" id="details-4397314544">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents a mini-review of the current state of research in mobile manipulators with variable levels of autonomy, emphasizing their associated challenges and application environments. The need for mobile manipulators in different environments is evident due to the unique challenges and risks each presents. Many systems deployed in these environments are not fully autonomous, requiring human-robot teaming to ensure safe and reliable operations under uncertainties. Through this analysis, we identify gaps and challenges in the literature on Variable Autonomy, including cognitive workloa...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents a mini-review of the current state of research in mobile manipulators with variable levels of autonomy, emphasizing their associated challenges and application environments</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">This paper presents a mini-review of the current state of research in mobile manipulators with variable levels of autonomy, emphasizing their associated challenges and application environments. The need for mobile manipulators in different environments is evident due to the unique challenges and risks each presents</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Through this analysis, we identify gaps and challenges in the literature on Variable Autonomy, including cognitive workload and communication delays, and propose future directions, including whole-body Variable Autonomy for mobile manipulators, virtual reality frameworks, and large language models to reduce operators&#x27; complexity and cognitive load in some challenging and uncertain scenarios.</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.10887v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.10887v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. A. Contreras, A. Rastegarpanah, R. Stolkin, and M. Chiou, &quot;A Mini-Review on Mobile Manipulators with Variable Autonomy,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.10887v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397314544')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="is self-knowledge and action consistent or not: investigating large language model&#x27;s personality" data-keywords="language model cs.cl cs.cy" data-themes="S M E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.14679v2" target="_blank">Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model&#x27;s Personality</a>
                            </h3>
                            <p class="card-authors">Yiming Ai, Zhiwei He, Ziyin Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.CY</span></div>

                            <div class="card-details" id="details-4397313248">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In this study, we delve into the validity of conventional personality questionnaires in capturing the human-like personality traits of Large Language Models (LLMs). Our objective is to assess the congruence between the personality traits LLMs claim to possess and their demonstrated tendencies in real-world scenarios. By conducting an extensive examination of LLM outputs against observed human response patterns, we aim to understand the disjunction between self-knowledge and action in LLMs.</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In this study, we delve into the validity of conventional personality questionnaires in capturing the human-like personality traits of Large Language Models (LLMs)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.14679v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.14679v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Ai et al., &quot;Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model&#x27;s Personality,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.14679v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397313248')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="large language models lack understanding of character composition of words" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.11357v3" target="_blank">Large Language Models Lack Understanding of Character Composition of Words</a>
                            </h3>
                            <p class="card-authors">Andrew Shin, Kunitake Kaneko</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4397308688">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) have demonstrated remarkable performances on a wide range of natural language tasks. Yet, LLMs&#x27; successes have been largely restricted to tasks concerning words, sentences, or documents, and it remains questionable how much they understand the minimal units of text, namely characters. In this paper, we examine contemporary LLMs regarding their ability to understand character composition of words, and show that most of them fail to reliably carry out even the simple tasks that can be handled by humans with perfection. We analyze their behaviors with comparison to to...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large language models (LLMs) have demonstrated remarkable performances on a wide range of natural language tasks</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.11357v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.11357v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Shin, and K. Kaneko, &quot;Large Language Models Lack Understanding of Character Composition of Words,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.11357v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397308688')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="unmasking the shadows of ai: investigating deceptive capabilities in large language models" data-keywords="language model cs.cl cs.ai" data-themes="S I M E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.09676v1" target="_blank">Unmasking the Shadows of AI: Investigating Deceptive Capabilities in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Linge Guo</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4397304800">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This research critically navigates the intricate landscape of AI deception, concentrating on deceptive behaviours of Large Language Models (LLMs). My objective is to elucidate this issue, examine the discourse surrounding it, and subsequently delve into its categorization and ramifications. The essay initiates with an evaluation of the AI Safety Summit 2023 (ASS) and introduction of LLMs, emphasising multidimensional biases that underlie their deceptive behaviours.The literature review covers four types of deception categorised: Strategic deception, Imitation, Sycophancy, and Unfaithful Reason...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">My objective is to elucidate this issue, examine the discourse surrounding it, and subsequently delve into its categorization and ramifications. Lastly, I take an evaluative stance on various aspects related to navigating the persistent challenges of the deceptive AI</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This research critically navigates the intricate landscape of AI deception, concentrating on deceptive behaviours of Large Language Models (LLMs)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.09676v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.09676v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Guo, &quot;Unmasking the Shadows of AI: Investigating Deceptive Capabilities in Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.09676v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397304800')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="self-cognition in large language models: an exploratory study" data-keywords="ros language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.01505v1" target="_blank">Self-Cognition in Large Language Models: An Exploratory Study</a>
                            </h3>
                            <p class="card-authors">Dongping Chen, Jiawen Shi, Yao Wan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4397308016">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">While Large Language Models (LLMs) have achieved remarkable success across various applications, they also raise concerns regarding self-cognition. In this paper, we perform a pioneering study to explore self-cognition in LLMs. Specifically, we first construct a pool of self-cognition instruction prompts to evaluate where an LLM exhibits self-cognition and four well-designed principles to quantify LLMs&#x27; self-cognition. Our study reveals that 4 of the 48 models on Chatbot Arena--specifically Command R, Claude3-Opus, Llama-3-70b-Instruct, and Reka-core--demonstrate some level of detectable self-...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">While Large Language Models (LLMs) have achieved remarkable success across various applications, they also raise concerns regarding self-cognition</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.01505v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.01505v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Chen, J. Shi, Y. Wan, P. Zhou, N. Z. Gong, and L. Sun, &quot;Self-Cognition in Large Language Models: An Exploratory Study,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.01505v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397308016')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="a critical review of causal reasoning benchmarks for large language models" data-keywords="language model cs.lg cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Machine Learning</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.08029v1" target="_blank">A Critical Review of Causal Reasoning Benchmarks for Large Language Models</a>
                            </h3>
                            <p class="card-authors">Linying Yang, Vik Shirvaikar, Oscar Clivio et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4397314496">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Numerous benchmarks aim to evaluate the capabilities of Large Language Models (LLMs) for causal inference and reasoning. However, many of them can likely be solved through the retrieval of domain knowledge, questioning whether they achieve their purpose. In this review, we present a comprehensive overview of LLM benchmarks for causality. We highlight how recent benchmarks move towards a more thorough definition of causal reasoning by incorporating interventional or counterfactual reasoning. We derive a set of criteria that a useful benchmark or set of benchmarks should aim to satisfy. We hope ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this review, we present a comprehensive overview of LLM benchmarks for causality</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Numerous benchmarks aim to evaluate the capabilities of Large Language Models (LLMs) for causal inference and reasoning</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.08029v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.08029v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Yang, V. Shirvaikar, O. Clivio, and F. Falck, &quot;A Critical Review of Causal Reasoning Benchmarks for Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.08029v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397314496')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="not all experts are equal: efficient expert pruning and skipping for mixture-of-experts large language models" data-keywords="ros imu language model cs.cl cs.ai cs.lg" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.14800v2" target="_blank">Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models</a>
                            </h3>
                            <p class="card-authors">Xudong Lu, Qi Liu, Yuhui Xu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Imu</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4397307200">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">A pivotal advancement in the progress of large language models (LLMs) is the emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer parameters, but it is still hard to deploy them due to their immense parameter sizes. Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques. Specifically, we propose, for the first time to our best knowledge, post-tr...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">A pivotal advancement in the progress of large language models (LLMs) is the emergence of the Mixture-of-Experts (MoE) LLMs</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.14800v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.14800v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. Lu et al., &quot;Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.14800v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397307200')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cr" data-title="attacks on third-party apis of large language models" data-keywords="ros language model cs.cr cs.ai cs.cl" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ CS.CR</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2404.16891v1" target="_blank">Attacks on Third-Party APIs of Large Language Models</a>
                            </h3>
                            <p class="card-authors">Wanru Zhao, Vidit Khazanchi, Haodi Xing et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CR</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4397084880">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language model (LLM) services have recently begun offering a plugin ecosystem to interact with third-party API services. This innovation enhances the capabilities of LLMs, but it also introduces risks, as these plugins developed by various third parties cannot be easily trusted. This paper proposes a new attacking framework to examine security and safety vulnerabilities within LLM platforms that incorporate third-party services. Applying our framework specifically to widely used LLMs, we identify real-world malicious attacks across various domains on third-party APIs that can imperceptib...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">The paper discusses the unique challenges posed by third-party API integration and offers strategic possibilities to improve the security and safety of LLM ecosystems moving forward</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large language model (LLM) services have recently begun offering a plugin ecosystem to interact with third-party API services</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2404.16891v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2404.16891v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. Zhao, V. Khazanchi, H. Xing, X. He, Q. Xu, and N. D. Lane, &quot;Attacks on Third-Party APIs of Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2404.16891v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397084880')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="exploring large language models to generate easy to read content" data-keywords="ros nlp language model cs.cl cs.hc" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.20046v1" target="_blank">Exploring Large Language Models to generate Easy to Read content</a>
                            </h3>
                            <p class="card-authors">Paloma Mart√≠nez, Lourdes Moreno, Alberto Ramos</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4397085024">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Ensuring text accessibility and understandability are essential goals, particularly for individuals with cognitive impairments and intellectual disabilities, who encounter challenges in accessing information across various mediums such as web pages, newspapers, administrative tasks, or health documents. Initiatives like Easy to Read and Plain Language guidelines aim to simplify complex texts; however, standardizing these guidelines remains challenging and often involves manual processes. This work presents an exploratory investigation into leveraging Artificial Intelligence (AI) and Natural La...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Ensuring text accessibility and understandability are essential goals, particularly for individuals with cognitive impairments and intellectual disabilities, who encounter challenges in accessing information across various mediums such as web pages, newspapers, administrative tasks, or health documents</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This work presents an exploratory investigation into leveraging Artificial Intelligence (AI) and Natural Language Processing (NLP) approaches to systematically simplify Spanish texts into Easy to Read formats, with a focus on utilizing Large Language Models (LLMs) for simplifying texts, especially in generating Easy to Read content</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.20046v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.20046v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Mart√≠nez, L. Moreno, and A. Ramos, &quot;Exploring Large Language Models to generate Easy to Read content,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.20046v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397085024')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cv" data-title="prunevid: visual token pruning for efficient video large language models" data-keywords="ros language model cs.cv" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computer Vision</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.16117v1" target="_blank">PruneVid: Visual Token Pruning for Efficient Video Large Language Models</a>
                            </h3>
                            <p class="card-authors">Xiaohu Huang, Hao Zhou, Kai Han</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span></div>

                            <div class="card-details" id="details-4397313680">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In this paper, we introduce PruneVid, a visual token pruning method designed to enhance the efficiency of multi-modal video understanding. Large Language Models (LLMs) have shown promising performance in video tasks due to their extended capabilities in comprehending visual modalities. However, the substantial redundancy in video data presents significant computational challenges for LLMs. To address this issue, we introduce a training-free method that 1) minimizes video redundancy by merging spatial-temporal tokens, and 2) leverages LLMs&#x27; reasoning capabilities to selectively prune visual fea...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we introduce PruneVid, a visual token pruning method designed to enhance the efficiency of multi-modal video understanding</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, the substantial redundancy in video data presents significant computational challenges for LLMs. To address this issue, we introduce a training-free method that 1) minimizes video redundancy by merging spatial-temporal tokens, and 2) leverages LLMs&#x27; reasoning capabilities to selectively prune visual features relevant to question tokens, enhancing model efficiency</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In this paper, we introduce PruneVid, a visual token pruning method designed to enhance the efficiency of multi-modal video understanding</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.16117v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.16117v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. Huang, H. Zhou, and K. Han, &quot;PruneVid: Visual Token Pruning for Efficient Video Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.16117v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397313680')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="scaling behavior of machine translation with large language models under prompt injection attacks" data-keywords="language model cs.cl" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.09832v1" target="_blank">Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks</a>
                            </h3>
                            <p class="card-authors">Zhifan Sun, Antonio Valerio Miceli-Barone</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4397304320">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Language Models (LLMs) are increasingly becoming the preferred foundation platforms for many Natural Language Processing tasks such as Machine Translation, owing to their quality often comparable to or better than task-specific models, and the simplicity of specifying the task through natural language instructions or in-context examples. Their generality, however, opens them up to subversion by end users who may embed into their requests instructions that cause the model to behave in unauthorized and possibly unsafe ways. In this work we study these Prompt Injection Attacks (PIAs) on mul...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce a new benchmark data set and we discover that on multiple language pairs and injected prompts written in English, larger models under certain conditions may become more susceptible to successful attacks, an instance of the Inverse Scaling phenomenon (McKenzie et al., 2023)</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large Language Models (LLMs) are increasingly becoming the preferred foundation platforms for many Natural Language Processing tasks such as Machine Translation, owing to their quality often comparable to or better than task-specific models, and the simplicity of specifying the task through natural language instructions or in-context examples</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.09832v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.09832v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Sun, and A. V. Miceli-Barone, &quot;Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.09832v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397304320')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="babysit a language model from scratch: interactive language learning by trials and demonstrations" data-keywords="control language model cs.cl cs.ai" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.13828v2" target="_blank">Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations</a>
                            </h3>
                            <p class="card-authors">Ziqiao Ma, Zekun Wang, Joyce Chai</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Control</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4397314400">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humans are efficient language learners and inherently social creatures. Our language development is largely shaped by our social interactions, for example, the demonstration and feedback from caregivers. Contrary to human language learning, recent advancements in large language models have primarily adopted a non-interactive training paradigm, and refined pre-trained models through feedback afterward. In this work, we explore how corrective feedback from interactions influences neural language acquisition from scratch through systematically controlled experiments, assessing whether it contribu...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce a trial-and-demonstration (TnD) learning framework that incorporates three distinct components: student trials, teacher demonstrations, and a reward conditioned on language competence at various developmental stages</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Contrary to human language learning, recent advancements in large language models have primarily adopted a non-interactive training paradigm, and refined pre-trained models through feedback afterward</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.13828v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.13828v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Ma, Z. Wang, and J. Chai, &quot;Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.13828v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397314400')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="behavioral bias of vision-language models: a behavioral finance view" data-keywords="gpt language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.15256v1" target="_blank">Behavioral Bias of Vision-Language Models: A Behavioral Finance View</a>
                            </h3>
                            <p class="card-authors">Yuhang Xiao, Yudi Lin, Ming-Chang Chiu</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4397306144">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Vision-Language Models (LVLMs) evolve rapidly as Large Language Models (LLMs) was equipped with vision modules to create more human-like models. However, we should carefully evaluate their applications in different domains, as they may possess undesired biases. Our work studies the potential behavioral biases of LVLMs from a behavioral finance perspective, an interdisciplinary subject that jointly considers finance and psychology. We propose an end-to-end framework, from data collection to new evaluation metrics, to assess LVLMs&#x27; reasoning capabilities and the dynamic behaviors manifeste...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose an end-to-end framework, from data collection to new evaluation metrics, to assess LVLMs&#x27; reasoning capabilities and the dynamic behaviors manifested in two established human financial behavioral biases: recency bias and authority bias</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large Vision-Language Models (LVLMs) evolve rapidly as Large Language Models (LLMs) was equipped with vision modules to create more human-like models</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.15256v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.15256v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Xiao, Y. Lin, and M. Chiu, &quot;Behavioral Bias of Vision-Language Models: A Behavioral Finance View,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.15256v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397306144')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.hc" data-title="exploring bengali religious dialect biases in large language models with evaluation perspectives" data-keywords="gpt ros language model cs.hc cs.cl cs.cy" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Human-Computer Interaction</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.18376v1" target="_blank">Exploring Bengali Religious Dialect Biases in Large Language Models with Evaluation Perspectives</a>
                            </h3>
                            <p class="card-authors">Azmine Toushik Wasi, Raima Islam, Mst Rafia Islam et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.HC</span></div>

                            <div class="card-details" id="details-4397307920">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">While Large Language Models (LLM) have created a massive technological impact in the past decade, allowing for human-enabled applications, they can produce output that contains stereotypes and biases, especially when using low-resource languages. This can be of great ethical concern when dealing with sensitive topics such as religion. As a means toward making LLMS more fair, we explore bias from a religious perspective in Bengali, focusing specifically on two main religious dialects: Hindu and Muslim-majority dialects. Here, we perform different experiments and audit showing the comparative an...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">While Large Language Models (LLM) have created a massive technological impact in the past decade, allowing for human-enabled applications, they can produce output that contains stereotypes and biases, especially when using low-resource languages</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.18376v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.18376v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. T. Wasi, R. Islam, M. R. Islam, T. H. Rafi, and D. Chae, &quot;Exploring Bengali Religious Dialect Biases in Large Language Models with Evaluation Perspectives,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.18376v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397307920')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="exploring advanced large language models with llmsuite" data-keywords="reinforcement learning transformer gpt language model cs.cl cs.cv" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.12036v2" target="_blank">Exploring Advanced Large Language Models with LLMsuite</a>
                            </h3>
                            <p class="card-authors">Giorgio Roffo</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Transformer</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4397304944">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This tutorial explores the advancements and challenges in the development of Large Language Models (LLMs) such as ChatGPT and Gemini. It addresses inherent limitations like temporal knowledge cutoffs, mathematical inaccuracies, and the generation of incorrect information, proposing solutions like Retrieval Augmented Generation (RAG), Program-Aided Language Models (PAL), and frameworks such as ReAct and LangChain. The integration of these techniques enhances LLM performance and reliability, especially in multi-step reasoning and complex task execution. The paper also covers fine-tuning strategi...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">This tutorial explores the advancements and challenges in the development of Large Language Models (LLMs) such as ChatGPT and Gemini. It addresses inherent limitations like temporal knowledge cutoffs, mathematical inaccuracies, and the generation of incorrect information, proposing solutions like Retrieval Augmented Generation (RAG), Program-Aided Language Models (PAL), and frameworks such as ReAct and LangChain</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This tutorial explores the advancements and challenges in the development of Large Language Models (LLMs) such as ChatGPT and Gemini</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.12036v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.12036v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Roffo, &quot;Exploring Advanced Large Language Models with LLMsuite,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.12036v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397304944')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="wkvquant: quantizing weight and key/value cache for large language models gains more" data-keywords="attention optimization ros language model cs.lg cs.ai cs.cl" data-themes="S I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Machine Learning</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.12065v2" target="_blank">WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More</a>
                            </h3>
                            <p class="card-authors">Yuxuan Yue, Zhihang Yuan, Haojie Duanmu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4397304896">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process. This paper addresses these challenges by focusing on the quantization of LLMs, a technique that reduces memory consumption by converting model parameters and activations into low-bit integers. We critically analyze the existing quantization approaches, identifying their limitations in balancing the accuracy and efficiency of the quantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ fr...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To advance beyond these limitations, we propose WKVQuant, a PTQ framework especially designed for quantizing weights and the key/value (KV) cache of LLMs</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process. This paper addresses these challenges by focusing on the quantization of LLMs, a technique that reduces memory consumption by converting model parameters and activations into low-bit integers</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.12065v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.12065v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Yue, Z. Yuan, H. Duanmu, S. Zhou, J. Wu, and L. Nie, &quot;WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.12065v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397304896')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="unraveling arithmetic in large language models: the role of algebraic structures" data-keywords="transformer attention language model cs.lg cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Machine Learning</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2411.16260v3" target="_blank">Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures</a>
                            </h3>
                            <p class="card-authors">Fu-Chieh Chang, You-Chen Lin, Pei-Yuan Wu</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Transformer</span><span class="keyword-tag">Attention</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span></div>

                            <div class="card-details" id="details-4397306960">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) have demonstrated remarkable mathematical capabilities, largely driven by chain-of-thought (CoT) prompting, which decomposes complex reasoning into step-by-step solutions. This approach has enabled significant advancements, as evidenced by performance on benchmarks like GSM8K and MATH. However, the mechanisms underlying LLMs&#x27; ability to perform arithmetic in a single step of CoT remain poorly understood. Existing studies debate whether LLMs encode numerical values or rely on symbolic reasoning, while others explore attention and multi-layered processing in arithmet...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose that LLMs learn arithmetic by capturing algebraic structures, such as commutativity and identity properties</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">We empirically demonstrate that LLMs can learn algebraic structures using a custom dataset of arithmetic problems, as well as providing theoretical evidence showing that, under specific configurations of weights and biases, the transformer-based LLMs can generate embeddings that remain invariant to both permutations of input tokens and the presence of identity elements</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large language models (LLMs) have demonstrated remarkable mathematical capabilities, largely driven by chain-of-thought (CoT) prompting, which decomposes complex reasoning into step-by-step solutions</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2411.16260v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2411.16260v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. Chang, Y. Lin, and P. Wu, &quot;Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2411.16260v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397306960')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="fs-rag: a frame semantics based approach for improved factual accuracy in large language models" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2406.16167v1" target="_blank">FS-RAG: A Frame Semantics Based Approach for Improved Factual Accuracy in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Harish Tayyar Madabushi</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4397307296">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We present a novel extension to Retrieval Augmented Generation with the goal of mitigating factual inaccuracies in the output of large language models. Specifically, our method draws on the cognitive linguistic theory of frame semantics for the indexing and retrieval of factual information relevant to helping large language models answer queries. We conduct experiments to demonstrate the effectiveness of this method both in terms of retrieval effectiveness and in terms of the relevance of the frames and frame relations automatically generated. Our results show that this novel mechanism of Fram...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present a novel extension to Retrieval Augmented Generation with the goal of mitigating factual inaccuracies in the output of large language models</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We present a novel extension to Retrieval Augmented Generation with the goal of mitigating factual inaccuracies in the output of large language models</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2406.16167v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2406.16167v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. T. Madabushi, &quot;FS-RAG: A Frame Semantics Based Approach for Improved Factual Accuracy in Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2406.16167v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397307296')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="precise length control in large language models" data-keywords="control language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.11937v1" target="_blank">Precise Length Control in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Bradley Butcher, Michael O&#x27;Keefe, James Titchener</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Control</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4397313536">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Language Models (LLMs) are increasingly used in production systems, powering applications such as chatbots, summarization, and question answering. Despite their success, controlling the length of their response remains a significant challenge, particularly for tasks requiring structured outputs or specific levels of detail. In this work, we propose a method to adapt pre-trained decoder-only LLMs for precise control of response length. Our approach incorporates a secondary length-difference positional encoding (LDPE) into the input embeddings, which counts down to a user-set response term...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose a method to adapt pre-trained decoder-only LLMs for precise control of response length</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Despite their success, controlling the length of their response remains a significant challenge, particularly for tasks requiring structured outputs or specific levels of detail</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large Language Models (LLMs) are increasingly used in production systems, powering applications such as chatbots, summarization, and question answering</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.11937v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.11937v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. Butcher, M. O&#x27;Keefe, and J. Titchener, &quot;Precise Length Control in Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.11937v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397313536')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="instruction-tuned large language models for machine translation in the medical domain" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.16440v2" target="_blank">Instruction-tuned Large Language Models for Machine Translation in the Medical Domain</a>
                            </h3>
                            <p class="card-authors">Miguel Rios</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4397314304">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Language Models (LLMs) have shown promising results on machine translation for high resource language pairs and domains. However, in specialised domains (e.g. medical) LLMs have shown lower performance compared to standard neural machine translation models. The consistency in the machine translation of terminology is crucial for users, researchers, and translators in specialised domains. In this study, we compare the performance between baseline LLMs and instruction-tuned LLMs in the medical domain. In addition, we introduce terminology from specialised medical dictionaries into the inst...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In addition, we introduce terminology from specialised medical dictionaries into the instruction formatted datasets for fine-tuning LLMs</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large Language Models (LLMs) have shown promising results on machine translation for high resource language pairs and domains</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.16440v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.16440v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Rios, &quot;Instruction-tuned Large Language Models for Machine Translation in the Medical Domain,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.16440v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397314304')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="learning from failure: integrating negative examples when fine-tuning large language models as agents" data-keywords="control optimization language model cs.cl" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.11651v2" target="_blank">Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents</a>
                            </h3>
                            <p class="card-authors">Renxi Wang, Haonan Li, Xudong Han et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Control</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4397313584">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools such as search engines. However, LLMs are optimized for language generation instead of tool use during training or alignment, limiting their effectiveness as agents. To resolve this problem, previous work has first collected interaction trajectories between LLMs and environments, using only trajectories that successfully finished the task to fine-tune smaller models, making fine-tuning data scarce and acquiring it both difficult and costly. Discarding failed trajectories also ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We further analyze the inference results and find that our method provides a better trade-off between valuable information and errors in unsuccessful trajectories</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To resolve this problem, previous work has first collected interaction trajectories between LLMs and environments, using only trajectories that successfully finished the task to fine-tune smaller models, making fine-tuning data scarce and acquiring it both difficult and costly</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools such as search engines</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.11651v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.11651v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Wang, H. Li, X. Han, Y. Zhang, and T. Baldwin, &quot;Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.11651v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397313584')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="bridging large language models and graph structure learning models for robust representation learning" data-keywords="ros language model cs.lg cs.ai" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Machine Learning</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.12096v1" target="_blank">Bridging Large Language Models and Graph Structure Learning Models for Robust Representation Learning</a>
                            </h3>
                            <p class="card-authors">Guangxin Su, Yifan Zhu, Wenjie Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4397311280">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Graph representation learning, involving both node features and graph structures, is crucial for real-world applications but often encounters pervasive noise. State-of-the-art methods typically address noise by focusing separately on node features with large language models (LLMs) and on graph structures with graph structure learning models (GSLMs). In this paper, we introduce LangGSL, a robust framework that integrates the complementary strengths of pre-trained language models and GSLMs to jointly enhance both node feature and graph structure learning. In LangGSL, we first leverage LLMs to fi...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we introduce LangGSL, a robust framework that integrates the complementary strengths of pre-trained language models and GSLMs to jointly enhance both node feature and graph structure learning</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">State-of-the-art methods typically address noise by focusing separately on node features with large language models (LLMs) and on graph structures with graph structure learning models (GSLMs)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.12096v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.12096v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Su, Y. Zhu, W. Zhang, H. Wang, and Y. Zhang, &quot;Bridging Large Language Models and Graph Structure Learning Models for Robust Representation Learning,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.12096v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397311280')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="unforgettable generalization in language models" data-keywords="transformer ros language model cs.lg cs.cl" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Machine Learning</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.02228v1" target="_blank">Unforgettable Generalization in Language Models</a>
                            </h3>
                            <p class="card-authors">Eric Zhang, Leshem Chosen, Jacob Andreas</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Transformer</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span></div>

                            <div class="card-details" id="details-4397305040">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">When language models (LMs) are trained to forget (or &quot;unlearn&#x27;&#x27;) a skill, how precisely does their behavior change? We study the behavior of transformer LMs in which tasks have been forgotten via fine-tuning on randomized labels. Such LMs learn to generate near-random predictions for individual examples in the &quot;training&#x27;&#x27; set used for forgetting. Across tasks, however, LMs exhibit extreme variability in whether LM predictions change on examples outside the training set. In some tasks (like entailment classification), forgetting generalizes robustly, and causes models to produce uninformative p...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Dataset difficulty is not predictive of whether a behavior can be forgotten; instead, generalization in forgetting is (weakly) predicted by the confidence of LMs&#x27; initial task predictions and the variability of LM representations of training data, with low confidence and low variability both associated with greater generalization. Our results highlight the difficulty and unpredictability of performing targeted skill removal from models via fine-tuning.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">When language models (LMs) are trained to forget (or &quot;unlearn&#x27;&#x27;) a skill, how precisely does their behavior change? We study the behavior of transformer LMs in which tasks have been forgotten via fine-tuning on randomized labels</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.02228v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.02228v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'E. Zhang, L. Chosen, and J. Andreas, &quot;Unforgettable Generalization in Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.02228v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397305040')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="detecting mode collapse in language models via narration" data-keywords="reinforcement learning gpt simulation imu language model cs.cl cs.ai" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.04477v1" target="_blank">Detecting Mode Collapse in Language Models via Narration</a>
                            </h3>
                            <p class="card-authors">Sil Hamilton</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Simulation</span><span class="keyword-tag">Imu</span></div>

                            <div class="card-details" id="details-4398403392">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">No two authors write alike. Personal flourishes invoked in written narratives, from lexicon to rhetorical devices, imply a particular author--what literary theorists label the implied or virtual author; distinct from the real author or narrator of a text. Early large language models trained on unfiltered training sets drawn from a variety of discordant sources yielded incoherent personalities, problematic for conversational tasks but proving useful for sampling literature from multiple perspectives. Successes in alignment research in recent years have allowed researchers to impose subjectively...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Our method and results are significant for researchers seeking to employ language models in sociological simulations.</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Early large language models trained on unfiltered training sets drawn from a variety of discordant sources yielded incoherent personalities, problematic for conversational tasks but proving useful for sampling literature from multiple perspectives</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Early large language models trained on unfiltered training sets drawn from a variety of discordant sources yielded incoherent personalities, problematic for conversational tasks but proving useful for sampling literature from multiple perspectives</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.04477v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.04477v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Hamilton, &quot;Detecting Mode Collapse in Language Models via Narration,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.04477v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398403392')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="pallm: evaluating and enhancing palliative care conversations with large language models" data-keywords="gpt imu nlp language model cs.cl cs.hc" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.15188v2" target="_blank">PALLM: Evaluating and Enhancing PALLiative Care Conversations with Large Language Models</a>
                            </h3>
                            <p class="card-authors">Zhiyuan Wang, Fangxu Yuan, Virginia LeBaron et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Imu</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4398402768">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Effective patient-provider communication is crucial in clinical care, directly impacting patient outcomes and quality of life. Traditional evaluation methods, such as human ratings, patient feedback, and provider self-assessments, are often limited by high costs and scalability issues. Although existing natural language processing (NLP) techniques show promise, they struggle with the nuances of clinical communication and require sensitive clinical data for training, reducing their effectiveness in real-world applications. Emerging large language models (LLMs) offer a new approach to assessing ...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Traditional evaluation methods, such as human ratings, patient feedback, and provider self-assessments, are often limited by high costs and scalability issues</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Traditional evaluation methods, such as human ratings, patient feedback, and provider self-assessments, are often limited by high costs and scalability issues</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.15188v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.15188v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Wang, F. Yuan, V. LeBaron, T. Flickinger, and L. E. Barnes, &quot;PALLM: Evaluating and Enhancing PALLiative Care Conversations with Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.15188v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398402768')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cv" data-title="exploring the frontier of vision-language models: a survey of current methodologies and future directions" data-keywords="language model cs.cv cs.ai cs.cl" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computer Vision</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2404.07214v4" target="_blank">Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions</a>
                            </h3>
                            <p class="card-authors">Akash Ghosh, Arkadeep Acharya, Sriparna Saha et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398403104">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The advent of Large Language Models (LLMs) has significantly reshaped the trajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable limitation, as they are primarily adept at processing textual information. To address this constraint, researchers have endeavored to integrate visual capabilities with LLMs, resulting in the emergence of Vision-Language Models (VLMs). These advanced models are instrumental in tackling more intricate tasks such as image captioning and visual question answering. In our comprehensive survey paper, we delve into the key advancements within the realm...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Nevertheless, these LLMs exhibit a notable limitation, as they are primarily adept at processing textual information. Our classification organizes VLMs into three distinct categories: models dedicated to vision-language understanding, models that process multimodal inputs to generate unimodal (textual) outputs and models that both accept and produce multimodal inputs and outputs.This classification is based on their respective capabilities and functionalities in processing and generating various modalities of data.We meticulously dissect each model, offering an extensive analysis of its foundational architecture, training data sources, as well as its strengths and limitations wherever possible, providing readers with a comprehensive understanding of its essential components</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The advent of Large Language Models (LLMs) has significantly reshaped the trajectory of the AI revolution</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2404.07214v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2404.07214v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Ghosh, A. Acharya, S. Saha, V. Jain, and A. Chadha, &quot;Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2404.07214v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398403104')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="efficacy of large language models in systematic reviews" data-keywords="gpt language model cs.cl cs.lg" data-themes="M E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.04646v2" target="_blank">Efficacy of Large Language Models in Systematic Reviews</a>
                            </h3>
                            <p class="card-authors">Aaditya Shah, Shridhar Mehendale, Siddha Kanthi</p>
                            <div class="theme-tags"><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.LG</span></div>

                            <div class="card-details" id="details-4398402384">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This study investigates the effectiveness of Large Language Models (LLMs) in interpreting existing literature through a systematic review of the relationship between Environmental, Social, and Governance (ESG) factors and financial performance. The primary objective is to assess how LLMs can replicate a systematic review on a corpus of ESG-focused papers. We compiled and hand-coded a database of 88 relevant papers published from March 2020 to May 2024. Additionally, we used a set of 238 papers from a previous systematic review of ESG literature from January 2015 to February 2020. We evaluated ...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This study investigates the effectiveness of Large Language Models (LLMs) in interpreting existing literature through a systematic review of the relationship between Environmental, Social, and Governance (ESG) factors and financial performance</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.04646v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.04646v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Shah, S. Mehendale, and S. Kanthi, &quot;Efficacy of Large Language Models in Systematic Reviews,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.04646v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398402384')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="thames: an end-to-end tool for hallucination mitigation and evaluation in large language models" data-keywords="gpt ros language model cs.cl" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.11353v3" target="_blank">THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Mengfei Liang, Archish Arun, Zekun Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398401856">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Hallucination, the generation of factually incorrect content, is a growing challenge in Large Language Models (LLMs). Existing detection and mitigation methods are often isolated and insufficient for domain-specific needs, lacking a standardized pipeline. This paper introduces THaMES (Tool for Hallucination Mitigations and EvaluationS), an integrated framework and library addressing this gap. THaMES offers an end-to-end solution for evaluating and mitigating hallucinations in LLMs, featuring automated test set generation, multifaceted benchmarking, and adaptable mitigation strategies. It autom...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Hallucination, the generation of factually incorrect content, is a growing challenge in Large Language Models (LLMs). This paper introduces THaMES (Tool for Hallucination Mitigations and EvaluationS), an integrated framework and library addressing this gap</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Hallucination, the generation of factually incorrect content, is a growing challenge in Large Language Models (LLMs)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.11353v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.11353v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Liang et al., &quot;THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.11353v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398401856')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="are compressed language models less subgroup robust?" data-keywords="bert language model cs.lg cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Machine Learning</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.17811v1" target="_blank">Are Compressed Language Models Less Subgroup Robust?</a>
                            </h3>
                            <p class="card-authors">Leonidas Gee, Andrea Zugarini, Novi Quadrianto</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Bert</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398402576">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">To reduce the inference cost of large language models, model compression is increasingly used to create smaller scalable models. However, little is known about their robustness to minority subgroups defined by the labels and attributes of a dataset. In this paper, we investigate the effects of 18 different compression methods and settings on the subgroup robustness of BERT language models. We show that worst-group performance does not depend on model size alone, but also on the compression method used. Additionally, we find that model compression does not always worsen the performance on minor...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">To reduce the inference cost of large language models, model compression is increasingly used to create smaller scalable models</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.17811v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.17811v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Gee, A. Zugarini, and N. Quadrianto, &quot;Are Compressed Language Models Less Subgroup Robust?,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.17811v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398402576')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="can large language models (or humans) disentangle text?" data-keywords="language model cs.cl" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.16584v2" target="_blank">Can Large Language Models (or Humans) Disentangle Text?</a>
                            </h3>
                            <p class="card-authors">Nicolas Audinet de Pieuchon, Adel Daoud, Connor Thomas Jerzak et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398404160">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We investigate the potential of large language models (LLMs) to disentangle text variables--to remove the textual traces of an undesired forbidden variable in a task sometimes known as text distillation and closely related to the fairness in AI and causal inference literature. We employ a range of various LLM approaches in an attempt to disentangle text by identifying and removing information about a target variable while preserving other relevant signals. We show that in the strong test of removing sentiment, the statistical association between the processed text and sentiment is still detect...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">This suggests there may be limited separability between concept variables in some text contexts, highlighting limitations of methods relying on text-level transformations and also raising questions about the robustness of disentanglement methods that achieve statistical independence in representation space.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We investigate the potential of large language models (LLMs) to disentangle text variables--to remove the textual traces of an undesired forbidden variable in a task sometimes known as text distillation and closely related to the fairness in AI and causal inference literature</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.16584v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.16584v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. A. d. Pieuchon, A. Daoud, C. T. Jerzak, M. Johansson, and R. Johansson, &quot;Can Large Language Models (or Humans) Disentangle Text?,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.16584v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398404160')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cr" data-title="large language models merging for enhancing the link stealing attack on graph neural networks" data-keywords="neural network gnn ros language model cs.cr cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ CS.CR</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.05830v1" target="_blank">Large Language Models Merging for Enhancing the Link Stealing Attack on Graph Neural Networks</a>
                            </h3>
                            <p class="card-authors">Faqian Guan, Tianqing Zhu, Wenhan Chang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Gnn</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4398401712">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Graph Neural Networks (GNNs), specifically designed to process the graph data, have achieved remarkable success in various applications. Link stealing attacks on graph data pose a significant privacy threat, as attackers aim to extract sensitive relationships between nodes (entities), potentially leading to academic misconduct, fraudulent transactions, or other malicious activities. Previous studies have primarily focused on single datasets and did not explore cross-dataset attacks, let alone attacks that leverage the combined knowledge of multiple attackers. However, we find that an attacker ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose a novel link stealing attack method that takes advantage of cross-dataset and Large Language Models (LLMs)</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">However, we find that an attacker can combine the data knowledge of multiple attackers to create a more effective attack model, which can be referred to cross-dataset attacks</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.05830v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.05830v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. Guan, T. Zhu, W. Chang, W. Ren, and W. Zhou, &quot;Large Language Models Merging for Enhancing the Link Stealing Attack on Graph Neural Networks,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.05830v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398401712')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="fanal -- financial activity news alerting language modeling framework" data-keywords="bert gpt optimization language model cs.cl cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.03527v1" target="_blank">FANAL -- Financial Activity News Alerting Language Modeling Framework</a>
                            </h3>
                            <p class="card-authors">Urjitkumar Patel, Fang-Chun Yeh, Chinmay Gondhalekar et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Bert</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4398402624">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In the rapidly evolving financial sector, the accurate and timely interpretation of market news is essential for stakeholders needing to navigate unpredictable events. This paper introduces FANAL (Financial Activity News Alerting Language Modeling Framework), a specialized BERT-based framework engineered for real-time financial event detection and analysis, categorizing news into twelve distinct financial categories. FANAL leverages silver-labeled data processed through XGBoost and employs advanced fine-tuning techniques, alongside ORBERT (Odds Ratio BERT), a novel variant of BERT fine-tuned w...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper introduces FANAL (Financial Activity News Alerting Language Modeling Framework), a specialized BERT-based framework engineered for real-time financial event detection and analysis, categorizing news into twelve distinct financial categories</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.03527v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.03527v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'U. Patel, F. Yeh, C. Gondhalekar, and H. Nalluri, &quot;FANAL -- Financial Activity News Alerting Language Modeling Framework,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.03527v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398402624')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cr" data-title="adashield: safeguarding multimodal large language models from structure-based attack via adaptive shield prompting" data-keywords="language model cs.cr cs.ai" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ CS.CR</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.09513v1" target="_blank">AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting</a>
                            </h3>
                            <p class="card-authors">Yu Wang, Xiaogeng Liu, Yu Li et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CR</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4398401616">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">With the advent and widespread deployment of Multimodal Large Language Models (MLLMs), the imperative to ensure their safety has become increasingly pronounced. However, with the integration of additional modalities, MLLMs are exposed to new vulnerabilities, rendering them prone to structured-based jailbreak attacks, where semantic content (e.g., &quot;harmful text&quot;) has been injected into the images to mislead MLLMs. In this work, we aim to defend against such threats. Specifically, we propose \textbf{Ada}ptive \textbf{Shield} Prompting (\textbf{AdaShield}), which prepends inputs with defense prom...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Specifically, we propose \textbf{Ada}ptive \textbf{Shield} Prompting (\textbf{AdaShield}), which prepends inputs with defense prompts to defend MLLMs against structure-based jailbreak attacks without fine-tuning MLLMs or training additional modules (e.g., post-stage content detector)</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">With the advent and widespread deployment of Multimodal Large Language Models (MLLMs), the imperative to ensure their safety has become increasingly pronounced</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.09513v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.09513v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Wang, X. Liu, Y. Li, M. Chen, and C. Xiao, &quot;AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.09513v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398401616')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="measuring the inconsistency of large language models in preferential ranking" data-keywords="language model cs.cl" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.08851v1" target="_blank">Measuring the Inconsistency of Large Language Models in Preferential Ranking</a>
                            </h3>
                            <p class="card-authors">Xiutian Zhao, Ke Wang, Wei Peng</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398402096">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Despite large language models&#x27; (LLMs) recent advancements, their bias and hallucination issues persist, and their ability to offer consistent preferential rankings remains underexplored. This study investigates the capacity of LLMs to provide consistent ordinal preferences, a crucial aspect in scenarios with dense decision space or lacking absolute answers. We introduce a formalization of consistency based on order theory, outlining criteria such as transitivity, asymmetry, reversibility, and independence from irrelevant alternatives. Our diagnostic experiments on selected state-of-the-art LLM...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce a formalization of consistency based on order theory, outlining criteria such as transitivity, asymmetry, reversibility, and independence from irrelevant alternatives</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Despite large language models&#x27; (LLMs) recent advancements, their bias and hallucination issues persist, and their ability to offer consistent preferential rankings remains underexplored. These findings highlight a significant inconsistency in LLM-generated preferential rankings, underscoring the need for further research to address these limitations.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Despite large language models&#x27; (LLMs) recent advancements, their bias and hallucination issues persist, and their ability to offer consistent preferential rankings remains underexplored</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.08851v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.08851v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. Zhao, K. Wang, and W. Peng, &quot;Measuring the Inconsistency of Large Language Models in Preferential Ranking,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.08851v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398402096')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="overview of the first workshop on language models for low-resource languages (loreslm 2025)" data-keywords="nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.16365v1" target="_blank">Overview of the First Workshop on Language Models for Low-Resource Languages (LoResLM 2025)</a>
                            </h3>
                            <p class="card-authors">Hansi Hettiarachchi, Tharindu Ranasinghe, Paul Rayson et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4398403632">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The first Workshop on Language Models for Low-Resource Languages (LoResLM 2025) was held in conjunction with the 31st International Conference on Computational Linguistics (COLING 2025) in Abu Dhabi, United Arab Emirates. This workshop mainly aimed to provide a forum for researchers to share and discuss their ongoing work on language models (LMs) focusing on low-resource languages, following the recent advancements in neural language models and their linguistic biases towards high-resource languages. LoResLM 2025 attracted notable interest from the natural language processing (NLP) community, ...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The first Workshop on Language Models for Low-Resource Languages (LoResLM 2025) was held in conjunction with the 31st International Conference on Computational Linguistics (COLING 2025) in Abu Dhabi, United Arab Emirates</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.16365v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.16365v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Hettiarachchi et al., &quot;Overview of the First Workshop on Language Models for Low-Resource Languages (LoResLM 2025),&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.16365v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398403632')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="activation sparsity opportunities for compressing general large language models" data-keywords="optimization language model cs.lg cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Machine Learning</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.12178v2" target="_blank">Activation Sparsity Opportunities for Compressing General Large Language Models</a>
                            </h3>
                            <p class="card-authors">Nobel Dhar, Bobin Deng, Md Romyull Islam et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Optimization</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4398402864">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Deploying local AI models, such as Large Language Models (LLMs), to edge devices can substantially enhance devices&#x27; independent capabilities, alleviate the server&#x27;s burden, and lower the response time. Owing to these tremendous potentials, many big tech companies have released several lightweight Small Language Models (SLMs) to bridge this gap. However, we still have huge motivations to deploy more powerful (LLMs) AI models on edge devices and enhance their smartness level. Unlike the conventional approaches for AI model compression, we investigate activation sparsity. The activation sparsity ...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Owing to these tremendous potentials, many big tech companies have released several lightweight Small Language Models (SLMs) to bridge this gap</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Deploying local AI models, such as Large Language Models (LLMs), to edge devices can substantially enhance devices&#x27; independent capabilities, alleviate the server&#x27;s burden, and lower the response time</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.12178v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.12178v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. Dhar, B. Deng, M. R. Islam, K. F. A. Nasif, L. Zhao, and K. Suo, &quot;Activation Sparsity Opportunities for Compressing General Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.12178v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398402864')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="how important is tokenization in french medical masked language models?" data-keywords="ros nlp language model cs.cl cs.ai cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.15010v2" target="_blank">How Important Is Tokenization in French Medical Masked Language Models?</a>
                            </h3>
                            <p class="card-authors">Yanis Labrak, Adrien Bazoge, Beatrice Daille et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398404400">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Subword tokenization has become the prevailing standard in the field of natural language processing (NLP) over recent years, primarily due to the widespread utilization of pre-trained language models. This shift began with Byte-Pair Encoding (BPE) and was later followed by the adoption of SentencePiece and WordPiece. While subword tokenization consistently outperforms character and word-level tokenization, the precise factors contributing to its success remain unclear. Key aspects such as the optimal segmentation granularity for diverse tasks and languages, the influence of data sources on tok...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Subword tokenization has become the prevailing standard in the field of natural language processing (NLP) over recent years, primarily due to the widespread utilization of pre-trained language models</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.15010v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.15010v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Labrak, A. Bazoge, B. Daille, M. Rouvier, and R. Dufour, &quot;How Important Is Tokenization in French Medical Masked Language Models?,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.15010v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398404400')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.ai" data-title="from code to play: benchmarking program search for games using large language models" data-keywords="control ros language model cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Artificial Intelligence</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.04057v2" target="_blank">From Code to Play: Benchmarking Program Search for Games Using Large Language Models</a>
                            </h3>
                            <p class="card-authors">Manuel Eberhardinger, James Goodman, Alexander Dockhorn et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Control</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4398402816">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) have shown impressive capabilities in generating program code, opening exciting opportunities for applying program synthesis to games. In this work, we explore the potential of LLMs to directly synthesize usable code for a wide range of gaming applications, focusing on two programming languages, Python and Java. We use an evolutionary hill-climbing algorithm, where the mutations and seeds of the initial programs are controlled by LLMs. For Python, the framework covers various game-related tasks, including five miniature versions of Atari games, ten levels of Baba i...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Trying many models on a problem and using the best results across them is more reliable than using just one.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large language models (LLMs) have shown impressive capabilities in generating program code, opening exciting opportunities for applying program synthesis to games</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.04057v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.04057v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Eberhardinger et al., &quot;From Code to Play: Benchmarking Program Search for Games Using Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.04057v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398402816')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cr" data-title="on trojan signatures in large language models of code" data-keywords="computer vision language model cs.cr cs.lg cs.se" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ CS.CR</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.16896v2" target="_blank">On Trojan Signatures in Large Language Models of Code</a>
                            </h3>
                            <p class="card-authors">Aftab Hussain, Md Rafiqul Islam Rabin, Mohammad Amin Alipour</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Computer Vision</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CR</span><span class="keyword-tag">CS.LG</span></div>

                            <div class="card-details" id="details-4398404496">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Trojan signatures, as described by Fields et al. (2021), are noticeable differences in the distribution of the trojaned class parameters (weights) and the non-trojaned class parameters of the trojaned model, that can be used to detect the trojaned model. Fields et al. (2021) found trojan signatures in computer vision classification tasks with image models, such as, Resnet, WideResnet, Densenet, and VGG. In this paper, we investigate such signatures in the classifier layer parameters of large language models of source code.
  Our results suggest that trojan signatures could not generalize to LL...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To the best of our knowledge, this is the first work to examine weight-based trojan signature revelation techniques for large-language models of code and furthermore to demonstrate that detecting trojans only from the weights in such models is a hard problem.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">(2021), are noticeable differences in the distribution of the trojaned class parameters (weights) and the non-trojaned class parameters of the trojaned model, that can be used to detect the trojaned model</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.16896v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.16896v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Hussain, M. R. I. Rabin, and M. A. Alipour, &quot;On Trojan Signatures in Large Language Models of Code,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.16896v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398404496')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.ce" data-title="leveraging large language models for institutional portfolio management: persona-based ensembles" data-keywords="ros language model cs.ce cs.ma" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ CS.CE</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2411.19515v1" target="_blank">Leveraging Large Language Models for Institutional Portfolio Management: Persona-Based Ensembles</a>
                            </h3>
                            <p class="card-authors">Yoshia Abe, Shuhei Matsuo, Ryoma Kondo et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CE</span><span class="keyword-tag">CS.MA</span></div>

                            <div class="card-details" id="details-4398404064">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) have demonstrated promising performance in various financial applications, though their potential in complex investment strategies remains underexplored. To address this gap, we investigate how LLMs can predict price movements in stock and bond portfolios using economic indicators, enabling portfolio adjustments akin to those employed by institutional investors. Additionally, we explore the impact of incorporating different personas within LLMs, using an ensemble approach to leverage their diverse predictions. Our findings show that LLM-based strategies, especially...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To address this gap, we investigate how LLMs can predict price movements in stock and bond portfolios using economic indicators, enabling portfolio adjustments akin to those employed by institutional investors</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large language models (LLMs) have demonstrated promising performance in various financial applications, though their potential in complex investment strategies remains underexplored</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2411.19515v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2411.19515v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Abe, S. Matsuo, R. Kondo, and R. Hisano, &quot;Leveraging Large Language Models for Institutional Portfolio Management: Persona-Based Ensembles,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2411.19515v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398404064')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="evaluating class membership relations in knowledge graphs using large language models" data-keywords="gpt language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2404.17000v1" target="_blank">Evaluating Class Membership Relations in Knowledge Graphs using Large Language Models</a>
                            </h3>
                            <p class="card-authors">Bradley P. Allen, Paul T. Groth</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4398401952">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">A backbone of knowledge graphs are their class membership relations, which assign entities to a given class. As part of the knowledge engineering process, we propose a new method for evaluating the quality of these relations by processing descriptions of a given entity and class using a zero-shot chain-of-thought classifier that uses a natural language intensional definition of a class. We evaluate the method using two publicly available knowledge graphs, Wikidata and CaLiGraph, and 7 large language models. Using the gpt-4-0125-preview large language model, the method&#x27;s classification performa...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">As part of the knowledge engineering process, we propose a new method for evaluating the quality of these relations by processing descriptions of a given entity and class using a zero-shot chain-of-thought classifier that uses a natural language intensional definition of a class</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">As part of the knowledge engineering process, we propose a new method for evaluating the quality of these relations by processing descriptions of a given entity and class using a zero-shot chain-of-thought classifier that uses a natural language intensional definition of a class</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2404.17000v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2404.17000v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. P. Allen, and P. T. Groth, &quot;Evaluating Class Membership Relations in Knowledge Graphs using Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2404.17000v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398401952')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="social bias in large language models for bangla: an empirical study on gender and religious bias" data-keywords="nlp language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.03536v3" target="_blank">Social Bias in Large Language Models For Bangla: An Empirical Study on Gender and Religious Bias</a>
                            </h3>
                            <p class="card-authors">Jayanta Sadhu, Maneesha Rani Saha, Rifat Shahriyar</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398404352">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The rapid growth of Large Language Models (LLMs) has put forward the study of biases as a crucial field. It is important to assess the influence of different types of biases embedded in LLMs to ensure fair use in sensitive fields. Although there have been extensive works on bias assessment in English, such efforts are rare and scarce for a major language like Bangla. In this work, we examine two types of social biases in LLM generated outputs for Bangla language. Our main contributions in this work are: (1) bias studies on two different social biases for Bangla, (2) a curated dataset for bias ...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The rapid growth of Large Language Models (LLMs) has put forward the study of biases as a crucial field</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.03536v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.03536v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Sadhu, M. R. Saha, and R. Shahriyar, &quot;Social Bias in Large Language Models For Bangla: An Empirical Study on Gender and Religious Bias,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.03536v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398404352')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="eess.as" data-title="speechprompt: prompting speech language models for speech processing tasks" data-keywords="language model eess.as cs.ai cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ EESS.AS</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.13040v1" target="_blank">SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks</a>
                            </h3>
                            <p class="card-authors">Kai-Wei Chang, Haibin Wu, Yu-Kai Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">EESS.AS</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398404304">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Prompting has become a practical method for utilizing pre-trained language models (LMs). This approach offers several advantages. It allows an LM to adapt to new tasks with minimal training and parameter updates, thus achieving efficiency in both storage and computation. Additionally, prompting modifies only the LM&#x27;s inputs and harnesses the generative capabilities of language models to address various downstream tasks in a unified manner. This significantly reduces the need for human labor in designing task-specific models. These advantages become even more evident as the number of tasks serv...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Prompting has become a practical method for utilizing pre-trained language models (LMs)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.13040v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.13040v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Chang et al., &quot;SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.13040v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398404304')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="q-bio.qm" data-title="gene-associated disease discovery powered by large language models" data-keywords="language model q-bio.qm cs.ir" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Q-BIO.QM</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.09490v1" target="_blank">Gene-associated Disease Discovery Powered by Large Language Models</a>
                            </h3>
                            <p class="card-authors">Jiayu Chang, Shiyu Wang, Chen Ling et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">Q-BIO.QM</span><span class="keyword-tag">CS.IR</span></div>

                            <div class="card-details" id="details-4398404448">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The intricate relationship between genetic variation and human diseases has been a focal point of medical research, evidenced by the identification of risk genes regarding specific diseases. The advent of advanced genome sequencing techniques has significantly improved the efficiency and cost-effectiveness of detecting these genetic markers, playing a crucial role in disease diagnosis and forming the basis for clinical decision-making and early risk assessment. To overcome the limitations of existing databases that record disease-gene associations from existing literature, which often lack rea...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To overcome the limitations of existing databases that record disease-gene associations from existing literature, which often lack real-time updates, we propose a novel framework employing Large Language Models (LLMs) for the discovery of diseases associated with specific genes</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To overcome the limitations of existing databases that record disease-gene associations from existing literature, which often lack real-time updates, we propose a novel framework employing Large Language Models (LLMs) for the discovery of diseases associated with specific genes</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The advent of advanced genome sequencing techniques has significantly improved the efficiency and cost-effectiveness of detecting these genetic markers, playing a crucial role in disease diagnosis and forming the basis for clinical decision-making and early risk assessment</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.09490v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.09490v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Chang, S. Wang, C. Ling, Z. Qin, and L. Zhao, &quot;Gene-associated Disease Discovery Powered by Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.09490v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398404448')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.ai" data-title="graph-of-thought: utilizing large language models to solve complex and dynamic business problems" data-keywords="ros language model cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Artificial Intelligence</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.06801v2" target="_blank">Graph-of-Thought: Utilizing Large Language Models to Solve Complex and Dynamic Business Problems</a>
                            </h3>
                            <p class="card-authors">Ye Li</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4398405360">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents Graph-of-Thought (GoT), a new model for workflow automation that enhances the flexibility and efficiency of Large Language Models (LLMs) in complex task execution. GoT advances beyond traditional linear and tree-like cognitive models with a graph structure that enables dynamic path selection. The open-source engine GoTFlow demonstrates the practical application of GoT, facilitating automated, data-driven decision-making across various domains. Despite challenges in complexity and transparency, GoTFlow&#x27;s potential for improving business processes is significant, promising ad...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents Graph-of-Thought (GoT), a new model for workflow automation that enhances the flexibility and efficiency of Large Language Models (LLMs) in complex task execution</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Despite challenges in complexity and transparency, GoTFlow&#x27;s potential for improving business processes is significant, promising advancements in both efficiency and decision quality with continuous development.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper presents Graph-of-Thought (GoT), a new model for workflow automation that enhances the flexibility and efficiency of Large Language Models (LLMs) in complex task execution</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.06801v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.06801v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Li, &quot;Graph-of-Thought: Utilizing Large Language Models to Solve Complex and Dynamic Business Problems,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.06801v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398405360')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="clinical information extraction for low-resource languages with few-shot learning using pre-trained language models and prompting" data-keywords="language model cs.cl cs.ai cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.13369v2" target="_blank">Clinical information extraction for Low-resource languages with Few-shot learning using Pre-trained language models and Prompting</a>
                            </h3>
                            <p class="card-authors">Phillip Richter-Pechanski, Philipp Wiesenbach, Dominic M. Schwab et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.LG</span></div>

                            <div class="card-details" id="details-4398404112">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Automatic extraction of medical information from clinical documents poses several challenges: high costs of required clinical expertise, limited interpretability of model predictions, restricted computational resources and privacy regulations. Recent advances in domain-adaptation and prompting methods showed promising results with minimal training data using lightweight masked language models, which are suited for well-established interpretability methods. We are first to present a systematic evaluation of these methods in a low-resource setting, by performing multi-class section classificatio...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We demonstrate that a lightweight, domain-adapted pretrained model, prompted with just 20 shots, outperforms a traditional classification model by 30.5% accuracy</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Automatic extraction of medical information from clinical documents poses several challenges: high costs of required clinical expertise, limited interpretability of model predictions, restricted computational resources and privacy regulations</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Automatic extraction of medical information from clinical documents poses several challenges: high costs of required clinical expertise, limited interpretability of model predictions, restricted computational resources and privacy regulations</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.13369v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.13369v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Richter-Pechanski et al., &quot;Clinical information extraction for Low-resource languages with Few-shot learning using Pre-trained language models and Prompting,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.13369v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398404112')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="causal reasoning in large language models: a knowledge graph approach" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.11588v1" target="_blank">Causal Reasoning in Large Language Models: A Knowledge Graph Approach</a>
                            </h3>
                            <p class="card-authors">Yejin Kim, Eojin Kang, Juae Kim et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398405552">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) typically improve performance by either retrieving semantically similar information, or enhancing reasoning abilities through structured prompts like chain-of-thought. While both strategies are considered crucial, it remains unclear which has a greater impact on model performance or whether a combination of both is necessary. This paper answers this question by proposing a knowledge graph (KG)-based random-walk reasoning approach that leverages causal relationships. We conduct experiments on the commonsense question answering task that is based on a KG. The KG inhe...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large language models (LLMs) typically improve performance by either retrieving semantically similar information, or enhancing reasoning abilities through structured prompts like chain-of-thought</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.11588v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.11588v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Kim, E. Kang, J. Kim, and H. H. Huang, &quot;Causal Reasoning in Large Language Models: A Knowledge Graph Approach,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.11588v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398405552')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="pediatricsgpt: large language models as chinese medical assistants for pediatric applications" data-keywords="gpt optimization ros language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.19266v4" target="_blank">PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications</a>
                            </h3>
                            <p class="card-authors">Dingkang Yang, Jinjie Wei, Dongling Xiao et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4398405888">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Developing intelligent pediatric consultation systems offers promising prospects for improving diagnostic efficiency, especially in China, where healthcare resources are scarce. Despite recent advances in Large Language Models (LLMs) for Chinese medicine, their performance is sub-optimal in pediatric applications due to inadequate instruction data and vulnerable training procedures. To address the above issues, this paper builds PedCorpus, a high-quality dataset of over 300,000 multi-task instructions from pediatric textbooks, guidelines, and knowledge graph resources to fulfil diverse diagnos...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Upon well-designed PedCorpus, we propose PediatricsGPT, the first Chinese pediatric LLM assistant built on a systematic and robust training pipeline</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To address the above issues, this paper builds PedCorpus, a high-quality dataset of over 300,000 multi-task instructions from pediatric textbooks, guidelines, and knowledge graph resources to fulfil diverse diagnostic demands</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Despite recent advances in Large Language Models (LLMs) for Chinese medicine, their performance is sub-optimal in pediatric applications due to inadequate instruction data and vulnerable training procedures</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.19266v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.19266v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Yang et al., &quot;PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.19266v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398405888')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="beyond data quantity: key factors driving performance in multilingual language models" data-keywords="ros language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.12500v1" target="_blank">Beyond Data Quantity: Key Factors Driving Performance in Multilingual Language Models</a>
                            </h3>
                            <p class="card-authors">Sina Bagheri Nezhad, Ameeta Agrawal, Rhitabrat Pokharel</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4398403488">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Multilingual language models (MLLMs) are crucial for handling text across various languages, yet they often show performance disparities due to differences in resource availability and linguistic characteristics. While the impact of pre-train data percentage and model size on performance is well-known, our study reveals additional critical factors that significantly influence MLLM effectiveness. Analyzing a wide range of features, including geographical, linguistic, and resource-related aspects, we focus on the SIB-200 dataset for classification and the Flores-200 dataset for machine translati...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Multilingual language models (MLLMs) are crucial for handling text across various languages, yet they often show performance disparities due to differences in resource availability and linguistic characteristics</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.12500v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.12500v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. B. Nezhad, A. Agrawal, and R. Pokharel, &quot;Beyond Data Quantity: Key Factors Driving Performance in Multilingual Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.12500v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398403488')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="a comprehensive survey of scientific large language models and their applications in scientific discovery" data-keywords="ros language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2406.10833v3" target="_blank">A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery</a>
                            </h3>
                            <p class="card-authors">Yu Zhang, Xiusi Chen, Bowen Jin et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398406800">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In many scientific fields, large language models (LLMs) have revolutionized the way text and other modalities of data (e.g., molecules and proteins) are handled, achieving superior performance in various applications and augmenting the scientific discovery process. Nevertheless, previous surveys on scientific LLMs often concentrate on one or two fields or a single modality. In this paper, we aim to provide a more holistic view of the research landscape by unveiling cross-field and cross-modal connections between scientific LLMs regarding their architectures and pre-training techniques. To this...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In many scientific fields, large language models (LLMs) have revolutionized the way text and other modalities of data (e.g., molecules and proteins) are handled, achieving superior performance in various applications and augmenting the scientific discovery process</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2406.10833v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2406.10833v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Zhang et al., &quot;A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2406.10833v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398406800')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="a survey of large language models for arabic language and its dialects" data-keywords="ros language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.20238v2" target="_blank">A Survey of Large Language Models for Arabic Language and its Dialects</a>
                            </h3>
                            <p class="card-authors">Malak Mashaabi, Shahad Al-Khalifa, Hend Al-Khalifa</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4398405072">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This survey offers a comprehensive overview of Large Language Models (LLMs) designed for Arabic language and its dialects. It covers key architectures, including encoder-only, decoder-only, and encoder-decoder models, along with the datasets used for pre-training, spanning Classical Arabic, Modern Standard Arabic, and Dialectal Arabic. The study also explores monolingual, bilingual, and multilingual LLMs, analyzing their architectures and performance across downstream tasks, such as sentiment analysis, named entity recognition, and question answering. Furthermore, it assesses the openness of A...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">It concludes by identifying key challenges and opportunities for future research and stressing the need for more inclusive and representative models.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This survey offers a comprehensive overview of Large Language Models (LLMs) designed for Arabic language and its dialects</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.20238v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.20238v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Mashaabi, S. Al-Khalifa, and H. Al-Khalifa, &quot;A Survey of Large Language Models for Arabic Language and its Dialects,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.20238v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398405072')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="a few-shot approach for relation extraction domain adaptation using large language models" data-keywords="deep learning transformer ros language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.02377v1" target="_blank">A Few-Shot Approach for Relation Extraction Domain Adaptation using Large Language Models</a>
                            </h3>
                            <p class="card-authors">Vanni Zavarella, Juan Carlos Gamero-Salinas, Sergio Consoli</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Deep Learning</span><span class="keyword-tag">Transformer</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4398403296">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Knowledge graphs (KGs) have been successfully applied to the analysis of complex scientific and technological domains, with automatic KG generation methods typically building upon relation extraction models capturing fine-grained relations between domain entities in text. While these relations are fully applicable across scientific areas, existing models are trained on few domain-specific datasets such as SciERC and do not perform well on new target domains. In this paper, we experiment with leveraging in-context learning capabilities of Large Language Models to perform schema-constrained data...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Knowledge graphs (KGs) have been successfully applied to the analysis of complex scientific and technological domains, with automatic KG generation methods typically building upon relation extraction models capturing fine-grained relations between domain entities in text</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.02377v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.02377v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'V. Zavarella, J. C. Gamero-Salinas, and S. Consoli, &quot;A Few-Shot Approach for Relation Extraction Domain Adaptation using Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.02377v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398403296')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="fast vocabulary transfer for language model compression" data-keywords="language model cs.cl cs.ai cs.lg" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.09977v1" target="_blank">Fast Vocabulary Transfer for Language Model Compression</a>
                            </h3>
                            <p class="card-authors">Leonidas Gee, Andrea Zugarini, Leonardo Rigutini et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.LG</span></div>

                            <div class="card-details" id="details-4398407040">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Real-world business applications require a trade-off between language model performance and size. We propose a new method for model compression that relies on vocabulary transfer. We evaluate the method on various vertical domains and downstream tasks. Our results indicate that vocabulary transfer can be effectively used in combination with other compression techniques, yielding a significant reduction in model size and inference time while marginally compromising on performance.</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a new method for model compression that relies on vocabulary transfer</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Real-world business applications require a trade-off between language model performance and size</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.09977v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.09977v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Gee, A. Zugarini, L. Rigutini, and P. Torroni, &quot;Fast Vocabulary Transfer for Language Model Compression,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.09977v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398407040')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.ai" data-title="revealing hidden bias in ai: lessons from large language models" data-keywords="gpt ros language model cs.ai cs.cy" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Artificial Intelligence</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.16927v1" target="_blank">Revealing Hidden Bias in AI: Lessons from Large Language Models</a>
                            </h3>
                            <p class="card-authors">Django Beatty, Kritsada Masanthia, Teepakorn Kaphol et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4398402528">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">As large language models (LLMs) become integral to recruitment processes, concerns about AI-induced bias have intensified. This study examines biases in candidate interview reports generated by Claude 3.5 Sonnet, GPT-4o, Gemini 1.5, and Llama 3.1 405B, focusing on characteristics such as gender, race, and age. We evaluate the effectiveness of LLM-based anonymization in reducing these biases. Findings indicate that while anonymization reduces certain biases, particularly gender bias, the degree of effectiveness varies across models and bias types. Notably, Llama 3.1 405B exhibited the lowest ov...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Moreover, our methodology of comparing anonymized and non-anonymized data reveals a novel approach to assessing inherent biases in LLMs beyond recruitment applications</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">As large language models (LLMs) become integral to recruitment processes, concerns about AI-induced bias have intensified</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.16927v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.16927v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Beatty, K. Masanthia, T. Kaphol, and N. Sethi, &quot;Revealing Hidden Bias in AI: Lessons from Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.16927v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398402528')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="native vs non-native language prompting: a comparative analysis" data-keywords="nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.07054v2" target="_blank">Native vs Non-Native Language Prompting: A Comparative Analysis</a>
                            </h3>
                            <p class="card-authors">Mohamed Bayan Kmainasi, Rakif Khan, Ali Ezzat Shahroor et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4398405984">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) have shown remarkable abilities in different fields, including standard Natural Language Processing (NLP) tasks. To elicit knowledge from LLMs, prompts play a key role, consisting of natural language instructions. Most open and closed source LLMs are trained on available labeled and unlabeled resources--digital content such as text, images, audio, and videos. Hence, these models have better knowledge for high-resourced languages but struggle with low-resourced languages. Since prompts play a crucial role in understanding their capabilities, the language used for pr...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large language models (LLMs) have shown remarkable abilities in different fields, including standard Natural Language Processing (NLP) tasks</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.07054v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.07054v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. B. Kmainasi, R. Khan, A. E. Shahroor, B. Bendou, M. Hasanain, and F. Alam, &quot;Native vs Non-Native Language Prompting: A Comparative Analysis,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.07054v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398405984')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="pragmatic competence evaluation of large language models for the korean language" data-keywords="gpt language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.12675v2" target="_blank">Pragmatic Competence Evaluation of Large Language Models for the Korean Language</a>
                            </h3>
                            <p class="card-authors">Dojun Park, Jiwoo Lee, Hyeyun Jeong et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398405216">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Benchmarks play a significant role in the current evaluation of Large Language Models (LLMs), yet they often overlook the models&#x27; abilities to capture the nuances of human language, primarily focusing on evaluating embedded knowledge and technical skills. To address this gap, our study evaluates how well LLMs understand context-dependent expressions from a pragmatic standpoint, specifically in Korean. We use both Multiple-Choice Questions (MCQs) for automatic evaluation and Open-Ended Questions (OEQs) assessed by human experts. Our results show that GPT-4 leads with scores of 81.11 in MCQs and...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To address this gap, our study evaluates how well LLMs understand context-dependent expressions from a pragmatic standpoint, specifically in Korean</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Benchmarks play a significant role in the current evaluation of Large Language Models (LLMs), yet they often overlook the models&#x27; abilities to capture the nuances of human language, primarily focusing on evaluating embedded knowledge and technical skills</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.12675v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.12675v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Park, J. Lee, H. Jeong, S. Park, and S. Lee, &quot;Pragmatic Competence Evaluation of Large Language Models for the Korean Language,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.12675v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398405216')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cv" data-title="integrating large language models into a tri-modal architecture for automated depression classification on the daic-woz" data-keywords="lstm gpt ros language model cs.cv cs.ai cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computer Vision</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.19340v5" target="_blank">Integrating Large Language Models into a Tri-Modal Architecture for Automated Depression Classification on the DAIC-WOZ</a>
                            </h3>
                            <p class="card-authors">Santosh V. Patapati</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Lstm</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4398403968">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Major Depressive Disorder (MDD) is a pervasive mental health condition that affects 300 million people worldwide. This work presents a novel, BiLSTM-based tri-modal model-level fusion architecture for the binary classification of depression from clinical interview recordings. The proposed architecture incorporates Mel Frequency Cepstral Coefficients, Facial Action Units, and uses a two-shot learning based GPT-4 model to process text data. This is the first work to incorporate large language models into a multi-modal architecture for this task. It achieves impressive results on the DAIC-WOZ AVE...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">It achieves impressive results on the DAIC-WOZ AVEC 2016 Challenge cross-validation split and Leave-One-Subject-Out cross-validation split, surpassing all baseline models and multiple state-of-the-art models</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This work presents a novel, BiLSTM-based tri-modal model-level fusion architecture for the binary classification of depression from clinical interview recordings</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.19340v5" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.19340v5" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. V. Patapati, &quot;Integrating Large Language Models into a Tri-Modal Architecture for Automated Depression Classification on the DAIC-WOZ,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.19340v5')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398403968')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="the generation gap: exploring age bias in the value systems of large language models" data-keywords="ros nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2404.08760v4" target="_blank">The Generation Gap: Exploring Age Bias in the Value Systems of Large Language Models</a>
                            </h3>
                            <p class="card-authors">Siyang Liu, Trish Maturi, Bowen Yi et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398402192">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We explore the alignment of values in Large Language Models (LLMs) with specific age groups, leveraging data from the World Value Survey across thirteen categories. Through a diverse set of prompts tailored to ensure response robustness, we find a general inclination of LLM values towards younger demographics, especially when compared to the US population. Although a general inclination can be observed, we also found that this inclination toward younger groups can be different across different value categories. Additionally, we explore the impact of incorporating age identity information in pr...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Additionally, we explore the impact of incorporating age identity information in prompts and observe challenges in mitigating value discrepancies with different age cohorts</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We explore the alignment of values in Large Language Models (LLMs) with specific age groups, leveraging data from the World Value Survey across thirteen categories</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2404.08760v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2404.08760v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Liu, T. Maturi, B. Yi, S. Shen, and R. Mihalcea, &quot;The Generation Gap: Exploring Age Bias in the Value Systems of Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2404.08760v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398402192')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="automatic generation of question hints for mathematics problems using large language models in educational technology" data-keywords="gpt imu language model cs.cl cs.ai" data-themes="M E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2411.03495v1" target="_blank">Automatic Generation of Question Hints for Mathematics Problems using Large Language Models in Educational Technology</a>
                            </h3>
                            <p class="card-authors">Junior Cedric Tonga, Benjamin Clement, Pierre-Yves Oudeyer</p>
                            <div class="theme-tags"><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Imu</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398407808">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The automatic generation of hints by Large Language Models (LLMs) within Intelligent Tutoring Systems (ITSs) has shown potential to enhance student learning. However, generating pedagogically sound hints that address student misconceptions and adhere to specific educational objectives remains challenging. This work explores using LLMs (GPT-4o and Llama-3-8B-instruct) as teachers to generate effective hints for students simulated through LLMs (GPT-3.5-turbo, Llama-3-8B-Instruct, or Mistral-7B-instruct-v0.3) tackling math exercises designed for human high-school students, and designed using cogn...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present here the study of several dimensions: 1) identifying error patterns made by simulated students on secondary-level math exercises; 2) developing various prompts for GPT-4o as a teacher and evaluating their effectiveness in generating hints that enable simulated students to self-correct; and 3) testing the best-performing prompts, based on their ability to produce relevant hints and facilitate error correction, with Llama-3-8B-Instruct as the teacher, allowing for a performance comparison with GPT-4o</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Also the problem-solving and response revision capabilities of the LLMs as students, particularly GPT-3.5-turbo, improved significantly after receiving hints, especially at lower temperature settings</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The automatic generation of hints by Large Language Models (LLMs) within Intelligent Tutoring Systems (ITSs) has shown potential to enhance student learning</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2411.03495v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2411.03495v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. C. Tonga, B. Clement, and P. Oudeyer, &quot;Automatic Generation of Question Hints for Mathematics Problems using Large Language Models in Educational Technology,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2411.03495v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398407808')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="understanding survey paper taxonomy about large language models via graph representation learning" data-keywords="language model cs.cl cs.ai cs.ir" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.10409v1" target="_blank">Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning</a>
                            </h3>
                            <p class="card-authors">Jun Zhuang, Casey Kennington</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.IR</span></div>

                            <div class="card-details" id="details-4398406464">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">As new research on Large Language Models (LLMs) continues, it is difficult to keep up with new research and models. To help researchers synthesize the new research many have written survey papers, but even those have become numerous. In this paper, we develop a method to automatically assign survey papers to a taxonomy. We collect the metadata of 144 LLM survey papers and explore three paradigms to classify papers within the taxonomy. Our work indicates that leveraging graph structure information on co-category graphs can significantly outperform the language models in two paradigms; pre-train...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we develop a method to automatically assign survey papers to a taxonomy</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">As new research on Large Language Models (LLMs) continues, it is difficult to keep up with new research and models</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">As new research on Large Language Models (LLMs) continues, it is difficult to keep up with new research and models</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.10409v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.10409v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Zhuang, and C. Kennington, &quot;Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.10409v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398406464')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.ne" data-title="evolutionary computation in the era of large language model: survey and roadmap" data-keywords="optimization ros language model cs.ne cs.ai cs.cl" data-themes="M E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Neural & Evolutionary</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.10034v3" target="_blank">Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap</a>
                            </h3>
                            <p class="card-authors">Xingyu Wu, Sheng-hao Wu, Jibin Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Optimization</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.NE</span></div>

                            <div class="card-details" id="details-4398404592">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride towards artificial general intelligence. The interplay between LLMs and evolutionary algorithms (EAs), despite differing in objectives and methodologies, share a common pursuit of applicability in complex problems. Meanwhile, EA can provide an optimization framework for LLM&#x27;s further enhancement under black-box settings, empowering LLM with flexible global search capacities. On the other hand, the abundant domain knowledge inhere...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">The interplay between LLMs and evolutionary algorithms (EAs), despite differing in objectives and methodologies, share a common pursuit of applicability in complex problems. The identified challenges and future directions offer guidance for researchers and practitioners to unlock the full potential of this innovative collaboration in propelling advancements in optimization and artificial intelligence</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large language models (LLMs) have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride towards artificial general intelligence</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.10034v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.10034v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. Wu, S. Wu, J. Wu, L. Feng, and K. C. Tan, &quot;Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.10034v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398404592')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="conditional and modal reasoning in large language models" data-keywords="ros language model cs.cl cs.ai cs.lo" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.17169v4" target="_blank">Conditional and Modal Reasoning in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Wesley H. Holliday, Matthew Mandelkern, Cedegao E. Zhang</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4398407232">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The reasoning abilities of large language models (LLMs) are the topic of a growing body of research in AI and cognitive science. In this paper, we probe the extent to which twenty-nine LLMs are able to distinguish logically correct inferences from logically fallacious ones. We focus on inference patterns involving conditionals (e.g., &#x27;If Ann has a queen, then Bob has a jack&#x27;) and epistemic modals (e.g., &#x27;Ann might have an ace&#x27;, &#x27;Bob must have a king&#x27;). These inferences have been of special interest to logicians, philosophers, and linguists, since they play a central role in the fundamental hum...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">These results highlight gaps in basic logical reasoning in today&#x27;s LLMs.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The reasoning abilities of large language models (LLMs) are the topic of a growing body of research in AI and cognitive science</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.17169v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.17169v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. H. Holliday, M. Mandelkern, and C. E. Zhang, &quot;Conditional and Modal Reasoning in Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.17169v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398407232')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="facilitating large language model russian adaptation with learned embedding propagation" data-keywords="gpt language model cs.cl cs.ai" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.21140v1" target="_blank">Facilitating large language model Russian adaptation with Learned Embedding Propagation</a>
                            </h3>
                            <p class="card-authors">Mikhail Tikhomirov, Daniil Chernyshev</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4398406512">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Rapid advancements of large language model (LLM) technologies led to the introduction of powerful open-source instruction-tuned LLMs that have the same text generation quality as the state-of-the-art counterparts such as GPT-4. While the emergence of such models accelerates the adoption of LLM technologies in sensitive-information environments the authors of such models don not disclose the training data necessary for replication of the results thus making the achievements model-exclusive. Since those open-source models are also multilingual this in turn reduces the benefits of training a lang...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address the limitations and cut the costs of the language adaptation pipeline we propose Learned Embedding Propagation (LEP)</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To address the limitations and cut the costs of the language adaptation pipeline we propose Learned Embedding Propagation (LEP)</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Rapid advancements of large language model (LLM) technologies led to the introduction of powerful open-source instruction-tuned LLMs that have the same text generation quality as the state-of-the-art counterparts such as GPT-4</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.21140v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.21140v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Tikhomirov, and D. Chernyshev, &quot;Facilitating large language model Russian adaptation with Learned Embedding Propagation,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.21140v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398406512')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="emergent world models and latent variable estimation in chess-playing language models" data-keywords="gpt language model cs.lg cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Machine Learning</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.15498v2" target="_blank">Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models</a>
                            </h3>
                            <p class="card-authors">Adam Karvonen</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398405264">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Language models have shown unprecedented capabilities, sparking debate over the source of their performance. Is it merely the outcome of learning syntactic patterns and surface level statistics, or do they extract semantics and a world model from the text? Prior work by Li et al. investigated this by training a GPT model on synthetic, randomly generated Othello games and found that the model learned an internal representation of the board state. We extend this work into the more complex domain of chess, training on real games and investigating our model&#x27;s internal representations using linear ...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Language models have shown unprecedented capabilities, sparking debate over the source of their performance</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.15498v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.15498v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Karvonen, &quot;Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.15498v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398405264')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="learning humanoid locomotion over challenging terrain" data-keywords="reinforcement learning transformer robot humanoid locomotion control ros cs.ro cs.lg" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.03654v1" target="_blank">Learning Humanoid Locomotion over Challenging Terrain</a>
                            </h3>
                            <p class="card-authors">Ilija Radosavovic, Sarthak Kamat, Trevor Darrell et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Transformer</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span></div>

                            <div class="card-details" id="details-4399337328">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots can, in principle, use their legs to go almost anywhere. Developing controllers capable of traversing diverse terrains, however, remains a considerable challenge. Classical controllers are hard to generalize broadly while the learning-based methods have primarily focused on gentle terrains. Here, we present a learning-based approach for blind humanoid locomotion capable of traversing challenging natural and man-made terrain. Our method uses a transformer model to predict the next action based on the history of proprioceptive observations and actions. The model is first pre-trai...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Here, we present a learning-based approach for blind humanoid locomotion capable of traversing challenging natural and man-made terrain</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Developing controllers capable of traversing diverse terrains, however, remains a considerable challenge</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Classical controllers are hard to generalize broadly while the learning-based methods have primarily focused on gentle terrains</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.03654v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.03654v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'I. Radosavovic, S. Kamat, T. Darrell, and J. Malik, &quot;Learning Humanoid Locomotion over Challenging Terrain,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.03654v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399337328')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="learning bipedal walking for humanoid robots in challenging environments with obstacle avoidance" data-keywords="reinforcement learning robot humanoid bipedal locomotion cs.ro cs.lg" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.08212v1" target="_blank">Learning Bipedal Walking for Humanoid Robots in Challenging Environments with Obstacle Avoidance</a>
                            </h3>
                            <p class="card-authors">Marwan Hamze, Mitsuharu Morisawa, Eiichi Yoshida</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span></div>

                            <div class="card-details" id="details-4399335168">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Deep reinforcement learning has seen successful implementations on humanoid robots to achieve dynamic walking. However, these implementations have been so far successful in simple environments void of obstacles. In this paper, we aim to achieve bipedal locomotion in an environment where obstacles are present using a policy-based reinforcement learning. By adding simple distance reward terms to a state of art reward function that can achieve basic bipedal locomotion, the trained policy succeeds in navigating the robot towards the desired destination without colliding with the obstacles along th...</p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.08212v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.08212v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Hamze, M. Morisawa, and E. Yoshida, &quot;Learning Bipedal Walking for Humanoid Robots in Challenging Environments with Obstacle Avoidance,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.08212v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399335168')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="whole-body humanoid robot locomotion with human reference" data-keywords="reinforcement learning robot humanoid locomotion imitation learning cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.18294v4" target="_blank">Whole-body Humanoid Robot Locomotion with Human Reference</a>
                            </h3>
                            <p class="card-authors">Qiang Zhang, Peter Cui, David Yan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399336464">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recently, humanoid robots have made significant advances in their ability to perform challenging tasks due to the deployment of Reinforcement Learning (RL), however, the inherent complexity of humanoid robots, including the difficulty of designing complicated reward functions and training entire sophisticated systems, still poses a notable challenge. To conquer these challenges, after many iterations and in-depth investigations, we have meticulously developed a full-size humanoid robot, &quot;Adam&quot;, whose innovative structural design greatly improves the efficiency and effectiveness of the imitatio...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Recently, humanoid robots have made significant advances in their ability to perform challenging tasks due to the deployment of Reinforcement Learning (RL), however, the inherent complexity of humanoid robots, including the difficulty of designing complicated reward functions and training entire sophisticated systems, still poses a notable challenge. To conquer these challenges, after many iterations and in-depth investigations, we have meticulously developed a full-size humanoid robot, &quot;Adam&quot;, whose innovative structural design greatly improves the efficiency and effectiveness of the imitation learning process</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In addition, we have developed a novel imitation learning framework based on an adversarial motion prior, which applies not only to Adam but also to humanoid robots in general</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.18294v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.18294v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Q. Zhang et al., &quot;Whole-body Humanoid Robot Locomotion with Human Reference,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.18294v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399336464')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="birodiff: diffusion policies for bipedal robot locomotion on unseen terrains" data-keywords="diffusion robot bipedal locomotion control simulation imu cs.ro cs.ai eess.sy" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.05424v1" target="_blank">BiRoDiff: Diffusion policies for bipedal robot locomotion on unseen terrains</a>
                            </h3>
                            <p class="card-authors">GVS Mothish, Manan Tayal, Shishir Kolathaya</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Diffusion</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399336656">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Locomotion on unknown terrains is essential for bipedal robots to handle novel real-world challenges, thus expanding their utility in disaster response and exploration. In this work, we introduce a lightweight framework that learns a single walking controller that yields locomotion on multiple terrains. We have designed a real-time robot controller based on diffusion models, which not only captures multiple behaviours with different velocities in a single policy but also generalizes well for unseen terrains. Our controller learns with offline data, which is better than online learning in aspec...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we introduce a lightweight framework that learns a single walking controller that yields locomotion on multiple terrains</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Locomotion on unknown terrains is essential for bipedal robots to handle novel real-world challenges, thus expanding their utility in disaster response and exploration</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In this work, we introduce a lightweight framework that learns a single walking controller that yields locomotion on multiple terrains</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.05424v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.05424v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Mothish, M. Tayal, and S. Kolathaya, &quot;BiRoDiff: Diffusion policies for bipedal robot locomotion on unseen terrains,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.05424v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399336656')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="humanplus: humanoid shadowing and imitation from humans" data-keywords="reinforcement learning robot humanoid perception control simulation camera imu cs.ro cs.ai" data-themes="I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2406.10454v1" target="_blank">HumanPlus: Humanoid Shadowing and Imitation from Humans</a>
                            </h3>
                            <p class="card-authors">Zipeng Fu, Qingqing Zhao, Qi Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Perception</span></div>

                            <div class="card-details" id="details-4399335936">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">One of the key arguments for building robots that have similar form factors to human beings is that we can leverage the massive human data for training. Yet, doing so has remained challenging in practice due to the complexities in humanoid perception and control, lingering physical gaps between humanoids and humans in morphologies and actuation, and lack of a data pipeline for humanoids to learn autonomous skills from egocentric vision. In this paper, we introduce a full-stack system for humanoids to learn motion and autonomous skills from human data. We first train a low-level policy in simul...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we introduce a full-stack system for humanoids to learn motion and autonomous skills from human data</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Yet, doing so has remained challenging in practice due to the complexities in humanoid perception and control, lingering physical gaps between humanoids and humans in morphologies and actuation, and lack of a data pipeline for humanoids to learn autonomous skills from egocentric vision</p></div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2406.10454v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2406.10454v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Fu, Q. Zhao, Q. Wu, G. Wetzstein, and C. Finn, &quot;HumanPlus: Humanoid Shadowing and Imitation from Humans,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2406.10454v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399335936')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="generalizable humanoid manipulation with 3d diffusion policies" data-keywords="diffusion robot humanoid manipulation lidar cs.ro cs.cv cs.lg" data-themes="S M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.10803v3" target="_blank">Generalizable Humanoid Manipulation with 3D Diffusion Policies</a>
                            </h3>
                            <p class="card-authors">Yanjie Ze, Zixuan Chen, Wenhao Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Diffusion</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Manipulation</span></div>

                            <div class="card-details" id="details-4399348080">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots capable of autonomous operation in diverse environments have long been a goal for roboticists. However, autonomous manipulation by humanoid robots has largely been restricted to one specific scene, primarily due to the difficulty of acquiring generalizable skills and the expensiveness of in-the-wild humanoid robot data. In this work, we build a real-world robotic system to address this challenging problem. Our system is mainly an integration of 1) a whole-upper-body robotic teleoperation system to acquire human-like robot data, 2) a 25-DoF humanoid robot platform with a height-...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, autonomous manipulation by humanoid robots has largely been restricted to one specific scene, primarily due to the difficulty of acquiring generalizable skills and the expensiveness of in-the-wild humanoid robot data. In this work, we build a real-world robotic system to address this challenging problem</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Our system is mainly an integration of 1) a whole-upper-body robotic teleoperation system to acquire human-like robot data, 2) a 25-DoF humanoid robot platform with a height-adjustable cart and a 3D LiDAR sensor, and 3) an improved 3D Diffusion Policy learning algorithm for humanoid robots to learn from noisy human data</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.10803v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.10803v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Ze et al., &quot;Generalizable Humanoid Manipulation with 3D Diffusion Policies,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.10803v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399348080')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="real-time polygonal semantic mapping for humanoid robot stair climbing" data-keywords="diffusion robot humanoid planning cs.ro cs.cv" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2411.01919v1" target="_blank">Real-Time Polygonal Semantic Mapping for Humanoid Robot Stair Climbing</a>
                            </h3>
                            <p class="card-authors">Teng Bin, Jianming Yao, Tin Lun Lam et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Diffusion</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Planning</span></div>

                            <div class="card-details" id="details-4399347696">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We present a novel algorithm for real-time planar semantic mapping tailored for humanoid robots navigating complex terrains such as staircases. Our method is adaptable to any odometry input and leverages GPU-accelerated processes for planar extraction, enabling the rapid generation of globally consistent semantic maps. We utilize an anisotropic diffusion filter on depth images to effectively minimize noise from gradient jumps while preserving essential edge details, enhancing normal vector images&#x27; accuracy and smoothness. Both the anisotropic diffusion and the RANSAC-based plane extraction pro...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present a novel algorithm for real-time planar semantic mapping tailored for humanoid robots navigating complex terrains such as staircases</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We present a novel algorithm for real-time planar semantic mapping tailored for humanoid robots navigating complex terrains such as staircases</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2411.01919v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2411.01919v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. Bin, J. Yao, T. L. Lam, and T. Zhang, &quot;Real-Time Polygonal Semantic Mapping for Humanoid Robot Stair Climbing,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2411.01919v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399347696')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="deep reinforcement learning for bipedal locomotion: a brief survey" data-keywords="reinforcement learning robot bipedal locomotion control cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2404.17070v7" target="_blank">Deep Reinforcement Learning for Bipedal Locomotion: A Brief Survey</a>
                            </h3>
                            <p class="card-authors">Lingfan Bao, Joseph Humphreys, Tianhu Peng et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399349424">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Bipedal robots are gaining global recognition due to their potential applications and advancements in artificial intelligence, particularly through Deep Reinforcement Learning (DRL). While DRL has significantly advanced bipedal locomotion, the development of a unified framework capable of handling a wide range of tasks remains an ongoing challenge. This survey systematically categorises, compares, and analyses existing DRL frameworks for bipedal locomotion, organising them into end-to-end and hierarchical control schemes. End-to-end frameworks are evaluated based on their learning approaches, ...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">While DRL has significantly advanced bipedal locomotion, the development of a unified framework capable of handling a wide range of tasks remains an ongoing challenge. We provide a detailed evaluation of the composition, strengths, limitations, and capabilities of each framework</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">While DRL has significantly advanced bipedal locomotion, the development of a unified framework capable of handling a wide range of tasks remains an ongoing challenge</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2404.17070v7" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2404.17070v7" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Bao, J. Humphreys, T. Peng, and C. Zhou, &quot;Deep Reinforcement Learning for Bipedal Locomotion: A Brief Survey,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2404.17070v7')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399349424')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="capture point control in thruster-assisted bipedal locomotion" data-keywords="robot bipedal locomotion manipulation control simulation imu cs.ro eess.sy" data-themes="I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2406.14799v1" target="_blank">Capture Point Control in Thruster-Assisted Bipedal Locomotion</a>
                            </h3>
                            <p class="card-authors">Shreyansh Pitroda, Aditya Bondada, Kaushik Venkatesh Krishnamurthy et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Manipulation</span></div>

                            <div class="card-details" id="details-4399336992">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Despite major advancements in control design that are robust to unplanned disturbances, bipedal robots are still susceptible to falling over and struggle to negotiate rough terrains. By utilizing thrusters in our bipedal robot, we can perform additional posture manipulation and expand the modes of locomotion to enhance the robot&#x27;s stability and ability to negotiate rough and difficult-to-navigate terrains. In this paper, we present our efforts in designing a controller based on capture point control for our thruster-assisted walking model named Harpy and explore its control design possibilitie...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we present our efforts in designing a controller based on capture point control for our thruster-assisted walking model named Harpy and explore its control design possibilities</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">By utilizing thrusters in our bipedal robot, we can perform additional posture manipulation and expand the modes of locomotion to enhance the robot&#x27;s stability and ability to negotiate rough and difficult-to-navigate terrains</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In this paper, we present our efforts in designing a controller based on capture point control for our thruster-assisted walking model named Harpy and explore its control design possibilities</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2406.14799v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2406.14799v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Pitroda et al., &quot;Capture Point Control in Thruster-Assisted Bipedal Locomotion,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2406.14799v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399336992')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="reinforcement learning for versatile, dynamic, and robust bipedal locomotion control" data-keywords="reinforcement learning robot bipedal locomotion control simulation ros imu cs.ro cs.ai" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.16889v2" target="_blank">Reinforcement Learning for Versatile, Dynamic, and Robust Bipedal Locomotion Control</a>
                            </h3>
                            <p class="card-authors">Zhongyu Li, Xue Bin Peng, Pieter Abbeel et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399335552">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents a comprehensive study on using deep reinforcement learning (RL) to create dynamic locomotion controllers for bipedal robots. Going beyond focusing on a single locomotion skill, we develop a general control solution that can be used for a range of dynamic bipedal skills, from periodic walking and running to aperiodic jumping and standing. Our RL-based controller incorporates a novel dual-history architecture, utilizing both a long-term and short-term input/output (I/O) history of the robot. This control architecture, when trained through the proposed end-to-end RL approach, ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents a comprehensive study on using deep reinforcement learning (RL) to create dynamic locomotion controllers for bipedal robots</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Our RL-based controller incorporates a novel dual-history architecture, utilizing both a long-term and short-term input/output (I/O) history of the robot</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.16889v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.16889v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Li, X. B. Peng, P. Abbeel, S. Levine, G. Berseth, and K. Sreenath, &quot;Reinforcement Learning for Versatile, Dynamic, and Robust Bipedal Locomotion Control,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.16889v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399335552')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="autotuning bipedal locomotion mpc with grfm-net for efficient sim-to-real transfer" data-keywords="robot humanoid bipedal locomotion control optimization simulation imu cs.ro cs.ai" data-themes="I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.15710v1" target="_blank">Autotuning Bipedal Locomotion MPC with GRFM-Net for Efficient Sim-to-Real Transfer</a>
                            </h3>
                            <p class="card-authors">Qianzhong Chen, Junheng Li, Sheng Cheng et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399347936">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Bipedal locomotion control is essential for humanoid robots to navigate complex, human-centric environments. While optimization-based control designs are popular for integrating sophisticated models of humanoid robots, they often require labor-intensive manual tuning. In this work, we address the challenges of parameter selection in bipedal locomotion control using DiffTune, a model-based autotuning method that leverages differential programming for efficient parameter learning. A major difficulty lies in balancing model fidelity with differentiability. We address this difficulty using a low-f...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">In this work, we address the challenges of parameter selection in bipedal locomotion control using DiffTune, a model-based autotuning method that leverages differential programming for efficient parameter learning. A major difficulty lies in balancing model fidelity with differentiability</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">While optimization-based control designs are popular for integrating sophisticated models of humanoid robots, they often require labor-intensive manual tuning</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.15710v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.15710v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Q. Chen, J. Li, S. Cheng, N. Hovakimyan, and Q. Nguyen, &quot;Autotuning Bipedal Locomotion MPC with GRFM-Net for Efficient Sim-to-Real Transfer,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.15710v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399347936')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2024" data-category="nlin.ao" data-title="self-organized attractoring in locomoting animals and robots: an emerging field" data-keywords="robot locomotion coordination control imu nlin.ao cond-mat.dis-nn" data-themes="S I L R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ NLIN.AO</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.13581v1" target="_blank">Self-organized attractoring in locomoting animals and robots: an emerging field</a>
                            </h3>
                            <p class="card-authors">Bulcs√∫ S√°ndor, Claudius Gros</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Coordination</span><span class="keyword-tag">Control</span></div>

                            <div class="card-details" id="details-4399348992">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Locomotion may be induced on three levels. On a classical level, actuators and limbs follow the sequence of open-loop top-down control signals they receive. Limbs may move alternatively on their own, which implies that interlimb coordination must be mediated either by the body or via decentralized inter-limb signaling. In this case, when embodiment is present, two types of controllers are conceivable for the actuators of the limbs, local pacemaker circuits and control principles based on self-organized embodiment. The latter, self-organized control, is based on limit cycles and chaotic attract...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Here we review the progress made within the framework of self-organized embodiment, with a particular focus on the concept of attractoring</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.13581v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.13581v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. S√°ndor, and C. Gros, &quot;Self-organized attractoring in locomoting animals and robots: an emerging field,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.13581v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399348992')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="achieving stable high-speed locomotion for humanoid robots with deep reinforcement learning" data-keywords="reinforcement learning robot humanoid locomotion control simulation imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.16611v1" target="_blank">Achieving Stable High-Speed Locomotion for Humanoid Robots with Deep Reinforcement Learning</a>
                            </h3>
                            <p class="card-authors">Xinming Zhang, Xianghui Wang, Lerong Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399334832">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots offer significant versatility for performing a wide range of tasks, yet their basic ability to walk and run, especially at high velocities, remains a challenge. This letter presents a novel method that combines deep reinforcement learning with kinodynamic priors to achieve stable locomotion control (KSLC). KSLC promotes coordinated arm movements to counteract destabilizing forces, enhancing overall stability. Compared to the baseline method, KSLC provides more accurate tracking of commanded velocities and better generalization in velocity control. In simulation tests, the KSLC-...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Humanoid robots offer significant versatility for performing a wide range of tasks, yet their basic ability to walk and run, especially at high velocities, remains a challenge</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This letter presents a novel method that combines deep reinforcement learning with kinodynamic priors to achieve stable locomotion control (KSLC)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.16611v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.16611v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. Zhang, X. Wang, L. Zhang, G. Guo, X. Shen, and W. Zhang, &quot;Achieving Stable High-Speed Locomotion for Humanoid Robots with Deep Reinforcement Learning,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.16611v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399334832')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="online dnn-driven nonlinear mpc for stylistic humanoid robot walking with step adjustment" data-keywords="neural network robot humanoid locomotion control optimization cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.07849v1" target="_blank">Online DNN-driven Nonlinear MPC for Stylistic Humanoid Robot Walking with Step Adjustment</a>
                            </h3>
                            <p class="card-authors">Giulio Romualdi, Paolo Maria Viceconte, Lorenzo Moretti et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399348608">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents a three-layered architecture that enables stylistic locomotion with online contact location adjustment. Our method combines an autoregressive Deep Neural Network (DNN) acting as a trajectory generation layer with a model-based trajectory adjustment and trajectory control layers. The DNN produces centroidal and postural references serving as an initial guess and regularizer for the other layers. Being the DNN trained on human motion capture data, the resulting robot motion exhibits locomotion patterns, resembling a human walking style. The trajectory adjustment layer utilize...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents a three-layered architecture that enables stylistic locomotion with online contact location adjustment</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper presents a three-layered architecture that enables stylistic locomotion with online contact location adjustment</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.07849v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.07849v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Romualdi et al., &quot;Online DNN-driven Nonlinear MPC for Stylistic Humanoid Robot Walking with Step Adjustment,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.07849v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399348608')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="rl-augmented mpc framework for agile and robust bipedal footstep locomotion planning and control" data-keywords="reinforcement learning robot humanoid bipedal locomotion planning control cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.17683v1" target="_blank">RL-augmented MPC Framework for Agile and Robust Bipedal Footstep Locomotion Planning and Control</a>
                            </h3>
                            <p class="card-authors">Seung Hyeon Bang, Carlos Arribalzaga Jov√©, Luis Sentis</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span></div>

                            <div class="card-details" id="details-4399348848">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper proposes an online bipedal footstep planning strategy that combines model predictive control (MPC) and reinforcement learning (RL) to achieve agile and robust bipedal maneuvers. While MPC-based foot placement controllers have demonstrated their effectiveness in achieving dynamic locomotion, their performance is often limited by the use of simplified models and assumptions. To address this challenge, we develop a novel foot placement controller that leverages a learned policy to bridge the gap between the use of a simplified model and the more complex full-order robot system. Specifi...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address this challenge, we develop a novel foot placement controller that leverages a learned policy to bridge the gap between the use of a simplified model and the more complex full-order robot system</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To address this challenge, we develop a novel foot placement controller that leverages a learned policy to bridge the gap between the use of a simplified model and the more complex full-order robot system</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper proposes an online bipedal footstep planning strategy that combines model predictive control (MPC) and reinforcement learning (RL) to achieve agile and robust bipedal maneuvers</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.17683v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.17683v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. H. Bang, C. A. Jov√©, and L. Sentis, &quot;RL-augmented MPC Framework for Agile and Robust Bipedal Footstep Locomotion Planning and Control,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.17683v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399348848')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="the duke humanoid: design and control for energy efficient bipedal locomotion using passive dynamics" data-keywords="reinforcement learning robot humanoid locomotion simulation imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.19795v2" target="_blank">The Duke Humanoid: Design and Control For Energy Efficient Bipedal Locomotion Using Passive Dynamics</a>
                            </h3>
                            <p class="card-authors">Boxi Xia, Bokuan Li, Jacob Lee et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399335696">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We present the Duke Humanoid, an open-source 10-degrees-of-freedom humanoid, as an extensible platform for locomotion research. The design mimics human physiology, with symmetrical body alignment in the frontal plane to maintain static balance with straight knees. We develop a reinforcement learning policy that can be deployed zero-shot on the hardware for velocity-tracking walking tasks. Additionally, to enhance energy efficiency in locomotion, we propose an end-to-end reinforcement learning algorithm that encourages the robot to leverage passive dynamics. Our experimental results show that o...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present the Duke Humanoid, an open-source 10-degrees-of-freedom humanoid, as an extensible platform for locomotion research</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Additionally, to enhance energy efficiency in locomotion, we propose an end-to-end reinforcement learning algorithm that encourages the robot to leverage passive dynamics</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.19795v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.19795v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. Xia, B. Li, J. Lee, M. Scutari, and B. Chen, &quot;The Duke Humanoid: Design and Control For Energy Efficient Bipedal Locomotion Using Passive Dynamics,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.19795v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399335696')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="harmon: whole-body motion generation of humanoid robots from language descriptions" data-keywords="robot humanoid imu language model cs.ro cs.ai" data-themes="S I E L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.12773v1" target="_blank">Harmon: Whole-Body Motion Generation of Humanoid Robots from Language Descriptions</a>
                            </h3>
                            <p class="card-authors">Zhenyu Jiang, Yuqi Xie, Jinhan Li et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Imu</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4399349520">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots, with their human-like embodiment, have the potential to integrate seamlessly into human environments. Critical to their coexistence and cooperation with humans is the ability to understand natural language communications and exhibit human-like behaviors. This work focuses on generating diverse whole-body motions for humanoid robots from language descriptions. We leverage human motion priors from extensive human motion datasets to initialize humanoid motions and employ the commonsense reasoning capabilities of Vision Language Models (VLMs) to edit and refine these motions. Our ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Our approach demonstrates the capability to produce natural, expressive, and text-aligned humanoid motions, validated through both simulated and real-world experiments</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We leverage human motion priors from extensive human motion datasets to initialize humanoid motions and employ the commonsense reasoning capabilities of Vision Language Models (VLMs) to edit and refine these motions</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.12773v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.12773v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Jiang, Y. Xie, J. Li, Y. Yuan, Y. Zhu, and Y. Zhu, &quot;Harmon: Whole-Body Motion Generation of Humanoid Robots from Language Descriptions,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.12773v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399349520')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="robust-locomotion-by-logic: perturbation-resilient bipedal locomotion via signal temporal logic guided model predictive control" data-keywords="robot bipedal locomotion planning control optimization simulation ros imu cs.ro" data-themes="I E L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.15993v2" target="_blank">Robust-Locomotion-by-Logic: Perturbation-Resilient Bipedal Locomotion via Signal Temporal Logic Guided Model Predictive Control</a>
                            </h3>
                            <p class="card-authors">Zhaoyuan Gu, Yuntian Zhao, Yipu Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Planning</span></div>

                            <div class="card-details" id="details-4399348512">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This study introduces a robust planning framework that utilizes a model predictive control (MPC) approach, enhanced by incorporating signal temporal logic (STL) specifications. This marks the first-ever study to apply STL-guided trajectory optimization for bipedal locomotion, specifically designed to handle both translational and orientational perturbations. Existing recovery strategies often struggle with reasoning complex task logic and evaluating locomotion robustness systematically, making them susceptible to failures caused by inappropriate recovery strategies or lack of robustness. To ad...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address these issues, we design an analytical stability metric for bipedal locomotion and quantify this metric using STL specifications, which guide the generation of recovery trajectories to achieve maximum robustness degree</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To address these issues, we design an analytical stability metric for bipedal locomotion and quantify this metric using STL specifications, which guide the generation of recovery trajectories to achieve maximum robustness degree</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This study introduces a robust planning framework that utilizes a model predictive control (MPC) approach, enhanced by incorporating signal temporal logic (STL) specifications</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.15993v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.15993v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Gu et al., &quot;Robust-Locomotion-by-Logic: Perturbation-Resilient Bipedal Locomotion via Signal Temporal Logic Guided Model Predictive Control,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.15993v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399348512')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="humanoidbench: simulated humanoid benchmark for whole-body locomotion and manipulation" data-keywords="reinforcement learning robot humanoid locomotion manipulation imu cs.ro cs.ai cs.lg" data-themes="I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.10506v2" target="_blank">HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation</a>
                            </h3>
                            <p class="card-authors">Carmelo Sferrazza, Dun-Ming Huang, Xingyu Lin et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399349376">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots hold great promise in assisting humans in diverse environments and tasks, due to their flexibility and adaptability leveraging human-like morphology. However, research in humanoid robots is often bottlenecked by the costly and fragile hardware setups. To accelerate algorithmic research in humanoid robots, we present a high-dimensional, simulated robot learning benchmark, HumanoidBench, featuring a humanoid robot equipped with dexterous hands and a variety of challenging whole-body manipulation and locomotion tasks. Our findings reveal that state-of-the-art reinforcement learnin...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To accelerate algorithmic research in humanoid robots, we present a high-dimensional, simulated robot learning benchmark, HumanoidBench, featuring a humanoid robot equipped with dexterous hands and a variety of challenging whole-body manipulation and locomotion tasks</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">With HumanoidBench, we provide the robotics community with a platform to identify the challenges arising when solving diverse tasks with humanoid robots, facilitating prompt verification of algorithms and ideas</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">To accelerate algorithmic research in humanoid robots, we present a high-dimensional, simulated robot learning benchmark, HumanoidBench, featuring a humanoid robot equipped with dexterous hands and a variety of challenging whole-body manipulation and locomotion tasks</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.10506v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.10506v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Sferrazza, D. Huang, X. Lin, Y. Lee, and P. Abbeel, &quot;HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.10506v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399349376')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="guiding collision-free humanoid multi-contact locomotion using convex kinematic relaxations and dynamic optimization" data-keywords="robot humanoid navigation planning optimization simulation imu cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.08335v1" target="_blank">Guiding Collision-Free Humanoid Multi-Contact Locomotion using Convex Kinematic Relaxations and Dynamic Optimization</a>
                            </h3>
                            <p class="card-authors">Carlos Gonzalez, Luis Sentis</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Navigation</span><span class="keyword-tag">Planning</span></div>

                            <div class="card-details" id="details-4399348176">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots rely on multi-contact planners to navigate a diverse set of environments, including those that are unstructured and highly constrained. To synthesize stable multi-contact plans within a reasonable time frame, most planners assume statically stable motions or rely on reduced order models. However, these approaches can also render the problem infeasible in the presence of large obstacles or when operating near kinematic and dynamic limits. To that end, we propose a new multi-contact framework that leverages recent advancements in relaxing collision-free path planning into a conve...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To that end, we propose a new multi-contact framework that leverages recent advancements in relaxing collision-free path planning into a convex optimization problem, extending it to be applicable to humanoid multi-contact navigation</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, these approaches can also render the problem infeasible in the presence of large obstacles or when operating near kinematic and dynamic limits. To that end, we propose a new multi-contact framework that leverages recent advancements in relaxing collision-free path planning into a convex optimization problem, extending it to be applicable to humanoid multi-contact navigation</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">To synthesize stable multi-contact plans within a reasonable time frame, most planners assume statically stable motions or rely on reduced order models</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.08335v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.08335v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Gonzalez, and L. Sentis, &quot;Guiding Collision-Free Humanoid Multi-Contact Locomotion using Convex Kinematic Relaxations and Dynamic Optimization,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.08335v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399348176')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="quadratic programming optimization for bio-inspired thruster-assisted bipedal locomotion on inclined slopes" data-keywords="robot bipedal locomotion manipulation control optimization simulation imu cs.ro" data-themes="I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2411.12968v1" target="_blank">Quadratic Programming Optimization for Bio-Inspired Thruster-Assisted Bipedal Locomotion on Inclined Slopes</a>
                            </h3>
                            <p class="card-authors">Shreyansh Pitroda, Eric Sihite, Kaushik Venkatesh Krishnamurthy et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Manipulation</span></div>

                            <div class="card-details" id="details-4399349328">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Our work aims to make significant strides in understanding unexplored locomotion control paradigms based on the integration of posture manipulation and thrust vectoring. These techniques are commonly seen in nature, such as Chukar birds using their wings to run on a nearly vertical wall. In this work, we show quadratic programming with contact constraints which is then given to the whole body controller to map on robot states to produce a thruster-assisted slope walking controller for our state-of-the-art Harpy platform. Harpy is a bipedal robot capable of legged-aerial locomotion using its le...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">These techniques are commonly seen in nature, such as Chukar birds using their wings to run on a nearly vertical wall</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2411.12968v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2411.12968v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Pitroda et al., &quot;Quadratic Programming Optimization for Bio-Inspired Thruster-Assisted Bipedal Locomotion on Inclined Slopes,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2411.12968v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399349328')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="learning velocity-based humanoid locomotion: massively parallel learning with brax and mjx" data-keywords="reinforcement learning robot humanoid locomotion simulation imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.05148v1" target="_blank">Learning Velocity-based Humanoid Locomotion: Massively Parallel Learning with Brax and MJX</a>
                            </h3>
                            <p class="card-authors">William Thibault, William Melek, Katja Mombaur</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399348896">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid locomotion is a key skill to bring humanoids out of the lab and into the real-world. Many motion generation methods for locomotion have been proposed including reinforcement learning (RL). RL locomotion policies offer great versatility and generalizability along with the ability to experience new knowledge to improve over time. This work presents a velocity-based RL locomotion policy for the REEM-C robot. The policy uses a periodic reward formulation and is implemented in Brax/MJX for fast training. Simulation results for the policy are demonstrated with future experimental results in...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Many motion generation methods for locomotion have been proposed including reinforcement learning (RL)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.05148v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.05148v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. Thibault, W. Melek, and K. Mombaur, &quot;Learning Velocity-based Humanoid Locomotion: Massively Parallel Learning with Brax and MJX,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.05148v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399348896')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="bipedal safe navigation over uncertain rough terrain: unifying terrain mapping and locomotion stability" data-keywords="robot bipedal locomotion navigation planning simulation mujoco imu cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.16356v2" target="_blank">Bipedal Safe Navigation over Uncertain Rough Terrain: Unifying Terrain Mapping and Locomotion Stability</a>
                            </h3>
                            <p class="card-authors">Kasidit Muenprasitivej, Jesse Jiang, Abdulaziz Shamsah et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Navigation</span></div>

                            <div class="card-details" id="details-4399335024">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We study the problem of bipedal robot navigation in complex environments with uncertain and rough terrain. In particular, we consider a scenario in which the robot is expected to reach a desired goal location by traversing an environment with uncertain terrain elevation. Such terrain uncertainties induce not only untraversable regions but also robot motion perturbations. Thus, the problems of terrain mapping and locomotion stability are intertwined. We evaluate three different kernels for Gaussian process (GP) regression to learn the terrain elevation. We also learn the motion deviation result...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a hierarchical locomotion-dynamics-aware sampling-based navigation planner</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">We study the problem of bipedal robot navigation in complex environments with uncertain and rough terrain. Thus, the problems of terrain mapping and locomotion stability are intertwined</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We also learn the motion deviation resulting from both the terrain as well as the discrepancy between the reduced-order Prismatic Inverted Pendulum Model used for planning and the full-order locomotion dynamics</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.16356v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.16356v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Muenprasitivej, J. Jiang, A. Shamsah, S. Coogan, and Y. Zhao, &quot;Bipedal Safe Navigation over Uncertain Rough Terrain: Unifying Terrain Mapping and Locomotion Stability,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.16356v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399335024')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="learning generic and dynamic locomotion of humanoids across discrete terrains" data-keywords="reinforcement learning neural network robot humanoid locomotion control optimization simulation imu cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.17227v2" target="_blank">Learning Generic and Dynamic Locomotion of Humanoids Across Discrete Terrains</a>
                            </h3>
                            <p class="card-authors">Shangqun Yu, Nisal Perera, Daniel Marew et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span></div>

                            <div class="card-details" id="details-4399336560">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper addresses the challenge of terrain-adaptive dynamic locomotion in humanoid robots, a problem traditionally tackled by optimization-based methods or reinforcement learning (RL). Optimization-based methods, such as model-predictive control, excel in finding optimal reaction forces and achieving agile locomotion, especially in quadruped, but struggle with the nonlinear hybrid dynamics of legged systems and the real-time computation of step location, timing, and reaction forces. Conversely, RL-based methods show promise in navigating dynamic and rough terrains but are limited by their e...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce a novel locomotion architecture that integrates a neural network policy, trained through RL in simplified environments, with a state-of-the-art motion controller combining model-predictive control (MPC) and whole-body impulse control (WBIC)</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">This paper addresses the challenge of terrain-adaptive dynamic locomotion in humanoid robots, a problem traditionally tackled by optimization-based methods or reinforcement learning (RL)</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper addresses the challenge of terrain-adaptive dynamic locomotion in humanoid robots, a problem traditionally tackled by optimization-based methods or reinforcement learning (RL)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.17227v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.17227v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Yu, N. Perera, D. Marew, and D. Kim, &quot;Learning Generic and Dynamic Locomotion of Humanoids Across Discrete Terrains,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.17227v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399336560')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="advancing humanoid locomotion: mastering challenging terrains with denoising world model learning" data-keywords="reinforcement learning neural network robot humanoid locomotion control cs.ro cs.ai eess.sy" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.14472v1" target="_blank">Advancing Humanoid Locomotion: Mastering Challenging Terrains with Denoising World Model Learning</a>
                            </h3>
                            <p class="card-authors">Xinyang Gu, Yen-Jen Wang, Xiang Zhu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span></div>

                            <div class="card-details" id="details-4399336176">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots, with their human-like skeletal structure, are especially suited for tasks in human-centric environments. However, this structure is accompanied by additional challenges in locomotion controller design, especially in complex real-world environments. As a result, existing humanoid robots are limited to relatively simple terrains, either with model-based control or model-free reinforcement learning. In this work, we introduce Denoising World Model Learning (DWL), an end-to-end reinforcement learning framework for humanoid locomotion control, which demonstrates the world&#x27;s first h...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we introduce Denoising World Model Learning (DWL), an end-to-end reinforcement learning framework for humanoid locomotion control, which demonstrates the world&#x27;s first humanoid robot to master real-world challenging terrains such as snowy and inclined land in the wild, up and down stairs, and extremely uneven terrains</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, this structure is accompanied by additional challenges in locomotion controller design, especially in complex real-world environments</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">As a result, existing humanoid robots are limited to relatively simple terrains, either with model-based control or model-free reinforcement learning</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.14472v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.14472v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. Gu et al., &quot;Advancing Humanoid Locomotion: Mastering Challenging Terrains with Denoising World Model Learning,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.14472v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399336176')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="learning humanoid locomotion with perceptive internal model" data-keywords="robot humanoid locomotion perception control simulation ros camera imu cs.ro" data-themes="I E L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2411.14386v1" target="_blank">Learning Humanoid Locomotion with Perceptive Internal Model</a>
                            </h3>
                            <p class="card-authors">Junfeng Long, Junli Ren, Moji Shi et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Perception</span></div>

                            <div class="card-details" id="details-4399347600">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In contrast to quadruped robots that can navigate diverse terrains using a &quot;blind&quot; policy, humanoid robots require accurate perception for stable locomotion due to their high degrees of freedom and inherently unstable morphology. However, incorporating perceptual signals often introduces additional disturbances to the system, potentially reducing its robustness, generalizability, and efficiency. This paper presents the Perceptive Internal Model (PIM), which relies on onboard, continuously updated elevation maps centered around the robot to perceive its surroundings. We train the policy using g...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents the Perceptive Internal Model (PIM), which relies on onboard, continuously updated elevation maps centered around the robot to perceive its surroundings</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper presents the Perceptive Internal Model (PIM), which relies on onboard, continuously updated elevation maps centered around the robot to perceive its surroundings</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2411.14386v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2411.14386v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Long et al., &quot;Learning Humanoid Locomotion with Perceptive Internal Model,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2411.14386v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399347600')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="stride: an open-source, low-cost, and versatile bipedal robot platform for research and education" data-keywords="robot bipedal locomotion control cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.02648v1" target="_blank">STRIDE: An Open-Source, Low-Cost, and Versatile Bipedal Robot Platform for Research and Education</a>
                            </h3>
                            <p class="card-authors">Yuhao Huang, Yicheng Zeng, Xiaobin Xiong</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Control</span></div>

                            <div class="card-details" id="details-4399348944">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In this paper, we present STRIDE, a Simple, Terrestrial, Reconfigurable, Intelligent, Dynamic, and Educational bipedal platform. STRIDE aims to propel bipedal robotics research and education by providing a cost-effective implementation with step-by-step instructions for building a bipedal robotic platform while providing flexible customizations via a modular and durable design. Moreover, a versatile terrain setup and a quantitative disturbance injection system are augmented to the robot platform to replicate natural terrains and push forces that can be used to evaluate legged locomotion in pra...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we present STRIDE, a Simple, Terrestrial, Reconfigurable, Intelligent, Dynamic, and Educational bipedal platform</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Our work with the open-soured implementation shows that STRIDE is a highly versatile and durable platform that can be used in research and education to evaluate locomotion algorithms, mechanical designs, and robust and adaptative controls.</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.02648v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.02648v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Huang, Y. Zeng, and X. Xiong, &quot;STRIDE: An Open-Source, Low-Cost, and Versatile Bipedal Robot Platform for Research and Education,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.02648v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399348944')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2024" data-category="cs.hc" data-title="users&#x27; perception on appropriateness of robotic coaching assistant&#x27;s disclosure behaviors" data-keywords="robot perception cs.hc" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Human-Computer Interaction</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.10550v1" target="_blank">Users&#x27; Perception on Appropriateness of Robotic Coaching Assistant&#x27;s Disclosure Behaviors</a>
                            </h3>
                            <p class="card-authors">Atikkhan Faridkhan Nilgar, Manuel Dietrich, Kristof Van Laerhoven</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Perception</span><span class="keyword-tag">CS.HC</span></div>

                            <div class="card-details" id="details-4399348416">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Social robots have emerged as valuable contributors to individuals&#x27; well-being coaching. Notably, their integration into long-term human coaching trials shows particular promise, emphasizing a complementary role alongside human coaches rather than outright replacement. In this context, robots serve as supportive entities during coaching sessions, offering insights based on their knowledge about users&#x27; well-being and activity. Traditionally, such insights have been gathered through methods like written self-reports or wearable data visualizations. However, the disclosure of people&#x27;s information...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Traditionally, such insights have been gathered through methods like written self-reports or wearable data visualizations</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.10550v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.10550v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. F. Nilgar, M. Dietrich, and K. V. Laerhoven, &quot;Users&#x27; Perception on Appropriateness of Robotic Coaching Assistant&#x27;s Disclosure Behaviors,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.10550v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399348416')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="humanoid-gym: reinforcement learning for humanoid robot with zero-shot sim2real transfer" data-keywords="reinforcement learning robot humanoid locomotion simulation mujoco isaac imu cs.ro cs.ai" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2404.05695v2" target="_blank">Humanoid-Gym: Reinforcement Learning for Humanoid Robot with Zero-Shot Sim2Real Transfer</a>
                            </h3>
                            <p class="card-authors">Xinyang Gu, Yen-Jen Wang, Jianyu Chen</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399336608">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid-Gym is an easy-to-use reinforcement learning (RL) framework based on Nvidia Isaac Gym, designed to train locomotion skills for humanoid robots, emphasizing zero-shot transfer from simulation to the real-world environment. Humanoid-Gym also integrates a sim-to-sim framework from Isaac Gym to Mujoco that allows users to verify the trained policies in different physical simulations to ensure the robustness and generalization of the policies. This framework is verified by RobotEra&#x27;s XBot-S (1.2-meter tall humanoid robot) and XBot-L (1.65-meter tall humanoid robot) in a real-world environm...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Humanoid-Gym is an easy-to-use reinforcement learning (RL) framework based on Nvidia Isaac Gym, designed to train locomotion skills for humanoid robots, emphasizing zero-shot transfer from simulation to the real-world environment</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2404.05695v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2404.05695v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. Gu, Y. Wang, and J. Chen, &quot;Humanoid-Gym: Reinforcement Learning for Humanoid Robot with Zero-Shot Sim2Real Transfer,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2404.05695v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399336608')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="robot vulnerability and the elicitation of user empathy" data-keywords="robot cs.ro cs.hc" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.02684v1" target="_blank">Robot Vulnerability and the Elicitation of User Empathy</a>
                            </h3>
                            <p class="card-authors">Morten Roed Frederiksen, Katrin Fischer, Maja Matariƒá</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">CS.RO</span><span class="keyword-tag">CS.HC</span></div>

                            <div class="card-details" id="details-4398432704">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper describes a between-subjects Amazon Mechanical Turk study (n = 220) that investigated how a robot&#x27;s affective narrative influences its ability to elicit empathy in human observers. We first conducted a pilot study to develop and validate the robot&#x27;s affective narratives. Then, in the full study, the robot used one of three different affective narrative strategies (funny, sad, neutral) while becoming less functional at its shopping task over the course of the interaction. As the functionality of the robot degraded, participants were repeatedly asked if they were willing to help the r...</p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.02684v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.02684v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. R. Frederiksen, K. Fischer, and M. Matariƒá, &quot;Robot Vulnerability and the Elicitation of User Empathy,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.02684v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398432704')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="socially acceptable bipedal robot navigation via social zonotope network model predictive control" data-keywords="neural network robot bipedal locomotion navigation planning control simulation imu cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2406.17151v1" target="_blank">Socially Acceptable Bipedal Robot Navigation via Social Zonotope Network Model Predictive Control</a>
                            </h3>
                            <p class="card-authors">Abdulaziz Shamsah, Krishanu Agarwal, Nigam Katta et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4398443168">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This study addresses the challenge of social bipedal navigation in a dynamic, human-crowded environment, a research area largely underexplored in legged robot navigation. We present a zonotope-based framework that couples prediction and motion planning for a bipedal ego-agent to account for bidirectional influence with the surrounding pedestrians. This framework incorporates a Social Zonotope Network (SZN), a neural network that predicts future pedestrian reachable sets and plans future socially acceptable reachable set for the ego-agent. SZN generates the reachable sets as zonotopes for effic...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present a zonotope-based framework that couples prediction and motion planning for a bipedal ego-agent to account for bidirectional influence with the surrounding pedestrians</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">This study addresses the challenge of social bipedal navigation in a dynamic, human-crowded environment, a research area largely underexplored in legged robot navigation</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We present a zonotope-based framework that couples prediction and motion planning for a bipedal ego-agent to account for bidirectional influence with the surrounding pedestrians</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2406.17151v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2406.17151v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Shamsah, K. Agarwal, N. Katta, A. Raju, S. Kousik, and Y. Zhao, &quot;Socially Acceptable Bipedal Robot Navigation via Social Zonotope Network Model Predictive Control,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2406.17151v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398443168')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="play everywhere: a temporal logic based game environment independent approach for playing soccer with robots" data-keywords="robot cs.ro cs.ai" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.12628v1" target="_blank">Play Everywhere: A Temporal Logic based Game Environment Independent Approach for Playing Soccer with Robots</a>
                            </h3>
                            <p class="card-authors">Vincenzo Suriani, Emanuele Musumeci, Daniele Nardi et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">CS.RO</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4398443072">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Robots playing soccer often rely on hard-coded behaviors that struggle to generalize when the game environment change. In this paper, we propose a temporal logic based approach that allows robots&#x27; behaviors and goals to adapt to the semantics of the environment. In particular, we present a hierarchical representation of soccer in which the robot selects the level of operation based on the perceived semantic characteristics of the environment, thus modifying dynamically the set of rules and goals to apply. The proposed approach enables the robot to operate in unstructured environments, just as ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose a temporal logic based approach that allows robots&#x27; behaviors and goals to adapt to the semantics of the environment</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In this paper, we propose a temporal logic based approach that allows robots&#x27; behaviors and goals to adapt to the semantics of the environment</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.12628v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.12628v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'V. Suriani, E. Musumeci, D. Nardi, and D. D. Bloisi, &quot;Play Everywhere: A Temporal Logic based Game Environment Independent Approach for Playing Soccer with Robots,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.12628v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398443072')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="human impression of humanoid robots mirroring social cues" data-keywords="robot humanoid perception control ros imu cs.ro cs.hc" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.12076v1" target="_blank">Human Impression of Humanoid Robots Mirroring Social Cues</a>
                            </h3>
                            <p class="card-authors">Di Fu, Fares Abawi, Philipp Allgeuer et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Perception</span><span class="keyword-tag">Control</span></div>

                            <div class="card-details" id="details-4398443264">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Mirroring non-verbal social cues such as affect or movement can enhance human-human and human-robot interactions in the real world. The robotic platforms and control methods also impact people&#x27;s perception of human-robot interaction. However, limited studies have compared robot imitation across different platforms and control methods. Our research addresses this gap by conducting two experiments comparing people&#x27;s perception of affective mirroring between the iCub and Pepper robots and movement mirroring between vision-based iCub control and Inertial Measurement Unit (IMU)-based iCub control. ...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Our research addresses this gap by conducting two experiments comparing people&#x27;s perception of affective mirroring between the iCub and Pepper robots and movement mirroring between vision-based iCub control and Inertial Measurement Unit (IMU)-based iCub control</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The robotic platforms and control methods also impact people&#x27;s perception of human-robot interaction</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.12076v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.12076v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Fu, F. Abawi, P. Allgeuer, and S. Wermter, &quot;Human Impression of Humanoid Robots Mirroring Social Cues,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.12076v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398443264')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="eess.sy" data-title="structural optimization of lightweight bipedal robot via serl" data-keywords="reinforcement learning robot bipedal locomotion optimization eess.sy cs.ai" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Signal Processing</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.15632v1" target="_blank">Structural Optimization of Lightweight Bipedal Robot via SERL</a>
                            </h3>
                            <p class="card-authors">Yi Cheng, Chenxi Han, Yuheng Min et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4398433232">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Designing a bipedal robot is a complex and challenging task, especially when dealing with a multitude of structural parameters. Traditional design methods often rely on human intuition and experience. However, such approaches are time-consuming, labor-intensive, lack theoretical guidance and hard to obtain optimal design results within vast design spaces, thus failing to full exploit the inherent performance potential of robots. In this context, this paper introduces the SERL (Structure Evolution Reinforcement Learning) algorithm, which combines reinforcement learning for locomotion tasks with...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Additionally, to assess the performance gap between our designed robot and the current state-of-the-art robots, we compared Wow Orin with mainstream bipedal robots Cassie and Unitree H1</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Traditional design methods often rely on human intuition and experience</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.15632v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.15632v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Cheng, C. Han, Y. Min, L. Ye, H. Liu, and H. Liu, &quot;Structural Optimization of Lightweight Bipedal Robot via SERL,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.15632v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398433232')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="what am i? evaluating the effect of language fluency and task competency on the perception of a social robot" data-keywords="robot perception cs.ro" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.11085v1" target="_blank">What Am I? Evaluating the Effect of Language Fluency and Task Competency on the Perception of a Social Robot</a>
                            </h3>
                            <p class="card-authors">Shahira Ali, Haley N. Green, Tariq Iqbal</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Perception</span><span class="keyword-tag">CS.RO</span></div>

                            <div class="card-details" id="details-4398445280">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recent advancements in robot capabilities have enabled them to interact with people in various human-social environments (HSEs). In many of these environments, the perception of the robot often depends on its capabilities, e.g., task competency, language fluency, etc. To enable fluent human-robot interaction (HRI) in HSEs, it is crucial to understand the impact of these capabilities on the perception of the robot. Although many works have investigated the effects of various robot capabilities on the robot&#x27;s perception separately, in this paper, we present a large-scale HRI study (n = 60) to in...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Although many works have investigated the effects of various robot capabilities on the robot&#x27;s perception separately, in this paper, we present a large-scale HRI study (n = 60) to investigate the combined impact of both language fluency and task competency on the perception of a robot</p></div>
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.11085v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.11085v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Ali, H. N. Green, and T. Iqbal, &quot;What Am I? Evaluating the Effect of Language Fluency and Task Competency on the Perception of a Social Robot,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.11085v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398445280')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="anymal parkour: learning agile navigation for quadrupedal robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">High Impact</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85187730333&amp;origin=resultslist" target="_blank">ANYmal parkour: Learning agile navigation for quadrupedal robots</a>
                            </h3>
                            <p class="card-authors">Hoeller D.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398444080">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85187730333&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1126/scirobotics.adi7566" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. D., &quot;ANYmal parkour: Learning agile navigation for quadrupedal robots,&quot; Science Robotics, 2024. doi: 10.1126/scirobotics.adi7566. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85187730333&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398444080')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="optimization-based control for dynamic legged robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">High Impact</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85174809687&amp;origin=resultslist" target="_blank">Optimization-Based Control for Dynamic Legged Robots</a>
                            </h3>
                            <p class="card-authors">Wensing P.M.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398432752">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85174809687&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2023.3324580" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. P.M., &quot;Optimization-Based Control for Dynamic Legged Robots,&quot; IEEE Transactions on Robotics, 2024. doi: 10.1109/TRO.2023.3324580. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85174809687&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398432752')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="rapid locomotion via reinforcement learning" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">High Impact</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85181503675&amp;origin=resultslist" target="_blank">Rapid locomotion via reinforcement learning</a>
                            </h3>
                            <p class="card-authors">Margolis G.B.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398434000">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85181503675&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1177/02783649231224053" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. G.B., &quot;Rapid locomotion via reinforcement learning,&quot; International Journal of Robotics Research, 2024. doi: 10.1177/02783649231224053. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85181503675&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398434000')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="learning robust autonomous navigation and locomotion for wheeled-legged robots" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">High Impact</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85191487739&amp;origin=resultslist" target="_blank">Learning robust autonomous navigation and locomotion for wheeled-legged robots</a>
                            </h3>
                            <p class="card-authors">Lee J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398445568">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85191487739&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1126/scirobotics.adi9641" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. J., &quot;Learning robust autonomous navigation and locomotion for wheeled-legged robots,&quot; Science Robotics, 2024. doi: 10.1126/scirobotics.adi9641. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85191487739&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398445568')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="fast contact-implicit model predictive control" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">High Impact</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85182382427&amp;origin=resultslist" target="_blank">Fast Contact-Implicit Model Predictive Control</a>
                            </h3>
                            <p class="card-authors">Le Cleac&#x27;h S.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398445760">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85182382427&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2024.3351554" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. C. S., &quot;Fast Contact-Implicit Model Predictive Control,&quot; IEEE Transactions on Robotics, 2024. doi: 10.1109/TRO.2024.3351554. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85182382427&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398445760')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="icub3 avatar system: enabling remote fully immersive embodiment of humanoid robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85183335405&amp;origin=resultslist" target="_blank">iCub3 avatar system: Enabling remote fully immersive embodiment of humanoid robots</a>
                            </h3>
                            <p class="card-authors">Dafarra S.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398442928">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85183335405&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1126/scirobotics.adh3834" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. S., &quot;iCub3 avatar system: Enabling remote fully immersive embodiment of humanoid robots,&quot; Science Robotics, 2024. doi: 10.1126/scirobotics.adh3834. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85183335405&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398442928')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="electrohydraulic musculoskeletal robotic leg for agile, adaptive, yet energy-efficient locomotion" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85203331069&amp;origin=resultslist" target="_blank">Electrohydraulic musculoskeletal robotic leg for agile, adaptive, yet energy-efficient locomotion</a>
                            </h3>
                            <p class="card-authors">Buchner T.J.K.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398443888">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85203331069&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1038/s41467-024-51568-3" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. T.J.K., &quot;Electrohydraulic musculoskeletal robotic leg for agile, adaptive, yet energy-efficient locomotion,&quot; Nature Communications, 2024. doi: 10.1038/s41467-024-51568-3. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85203331069&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398443888')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2024" data-category="other" data-title="biohybrid bipedal robot powered by skeletal muscle tissue" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85184781793&amp;origin=resultslist" target="_blank">Biohybrid bipedal robot powered by skeletal muscle tissue</a>
                            </h3>
                            <p class="card-authors">Kinjo R.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398433280">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85184781793&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1016/j.matt.2023.12.035" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. R., &quot;Biohybrid bipedal robot powered by skeletal muscle tissue,&quot; Matter, 2024. doi: 10.1016/j.matt.2023.12.035. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85184781793&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398433280')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="not only rewards but also constraints: applications on legged robot locomotion" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85193256463&amp;origin=resultslist" target="_blank">Not only Rewards but Also Constraints: Applications on Legged Robot Locomotion</a>
                            </h3>
                            <p class="card-authors">Kim Y.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398433184">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85193256463&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2024.3400935" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Y., &quot;Not only Rewards but Also Constraints: Applications on Legged Robot Locomotion,&quot; IEEE Transactions on Robotics, 2024. doi: 10.1109/TRO.2024.3400935. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85193256463&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398433184')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="development of bioinspired multimodal underwater robot ‚Äúhero-blue‚Äù for walking, swimming, and crawling" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85182952443&amp;origin=resultslist" target="_blank">Development of Bioinspired Multimodal Underwater Robot ‚ÄúHERO-BLUE‚Äù for Walking, Swimming, and Crawling</a>
                            </h3>
                            <p class="card-authors">Kim T.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398444176">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85182952443&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2024.3353040" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. T., &quot;Development of Bioinspired Multimodal Underwater Robot ‚ÄúHERO-BLUE‚Äù for Walking, Swimming, and Crawling,&quot; IEEE Transactions on Robotics, 2024. doi: 10.1109/TRO.2024.3353040. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85182952443&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398444176')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="lifelike agility and play in quadrupedal robots using reinforcement learning and generative pre-trained models" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85197508387&amp;origin=resultslist" target="_blank">Lifelike agility and play in quadrupedal robots using reinforcement learning and generative pre-trained models</a>
                            </h3>
                            <p class="card-authors">Han L.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398444320">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85197508387&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1038/s42256-024-00861-3" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. L., &quot;Lifelike agility and play in quadrupedal robots using reinforcement learning and generative pre-trained models,&quot; Nature Machine Intelligence, 2024. doi: 10.1038/s42256-024-00861-3. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85197508387&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398444320')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2024" data-category="other" data-title="adaptive-force-based control of dynamic legged locomotion over uneven terrain" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85189350789&amp;origin=resultslist" target="_blank">Adaptive-Force-Based Control of Dynamic Legged Locomotion Over Uneven Terrain</a>
                            </h3>
                            <p class="card-authors">Sombolestan M.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398444656">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85189350789&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2024.3381554" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. M., &quot;Adaptive-Force-Based Control of Dynamic Legged Locomotion Over Uneven Terrain,&quot; IEEE Transactions on Robotics, 2024. doi: 10.1109/TRO.2024.3381554. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85189350789&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398444656')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2024" data-category="other" data-title="max: a wheeled-legged quadruped robot for multimodal agile locomotion" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85181580701&amp;origin=resultslist" target="_blank">Max: A Wheeled-Legged Quadruped Robot for Multimodal Agile Locomotion</a>
                            </h3>
                            <p class="card-authors">Zhou Q.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398444704">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85181580701&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TASE.2023.3345876" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Q., &quot;Max: A Wheeled-Legged Quadruped Robot for Multimodal Agile Locomotion,&quot; IEEE Transactions on Automation Science and Engineering, 2024. doi: 10.1109/TASE.2023.3345876. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85181580701&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398444704')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="Scopus" data-year="2024" data-category="other" data-title="compliant motion control of wheel-legged humanoid robot on rough terrains" data-keywords="" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85174819735&amp;origin=resultslist" target="_blank">Compliant Motion Control of Wheel-Legged Humanoid Robot on Rough Terrains</a>
                            </h3>
                            <p class="card-authors">Zhao L.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398444896">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85174819735&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TMECH.2023.3320762" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. L., &quot;Compliant Motion Control of Wheel-Legged Humanoid Robot on Rough Terrains,&quot; IEEE ASME Transactions on Mechatronics, 2024. doi: 10.1109/TMECH.2023.3320762. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85174819735&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398444896')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="deep ensemble learning approach for lower limb movement recognition from multichannel semg signals" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85185144957&amp;origin=resultslist" target="_blank">Deep ensemble learning approach for lower limb movement recognition from multichannel sEMG signals</a>
                            </h3>
                            <p class="card-authors">Tokas P.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398445088">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85185144957&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1007/s00521-024-09465-9" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. P., &quot;Deep ensemble learning approach for lower limb movement recognition from multichannel sEMG signals,&quot; Neural Computing and Applications, 2024. doi: 10.1007/s00521-024-09465-9. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85185144957&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398445088')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="liquid-metal soft electronics coupled with multi-legged robots for targeted delivery in the gastrointestinal tract" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85187259087&amp;origin=resultslist" target="_blank">Liquid-metal soft electronics coupled with multi-legged robots for targeted delivery in the gastrointestinal tract</a>
                            </h3>
                            <p class="card-authors">Ye Z.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398444368">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85187259087&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1016/j.device.2023.100181" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Z., &quot;Liquid-metal soft electronics coupled with multi-legged robots for targeted delivery in the gastrointestinal tract,&quot; Device, 2024. doi: 10.1016/j.device.2023.100181. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85187259087&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398444368')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2024" data-category="other" data-title="ai-cpg: adaptive imitated central pattern generators for bipedal locomotion learned through reinforced reflex neural networks" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85190737819&amp;origin=resultslist" target="_blank">AI-CPG: Adaptive Imitated Central Pattern Generators for Bipedal Locomotion Learned Through Reinforced Reflex Neural Networks</a>
                            </h3>
                            <p class="card-authors">Li G.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398446000">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85190737819&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2024.3388842" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. G., &quot;AI-CPG: Adaptive Imitated Central Pattern Generators for Bipedal Locomotion Learned Through Reinforced Reflex Neural Networks,&quot; IEEE Robotics and Automation Letters, 2024. doi: 10.1109/LRA.2024.3388842. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85190737819&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398446000')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2024" data-category="other" data-title="cdm-mpc: an integrated dynamic planning and control framework for bipedal robots jumping" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85195411305&amp;origin=resultslist" target="_blank">CDM-MPC: An Integrated Dynamic Planning and Control Framework for Bipedal Robots Jumping</a>
                            </h3>
                            <p class="card-authors">He Z.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398446288">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85195411305&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2024.3408487" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Z., &quot;CDM-MPC: An Integrated Dynamic Planning and Control Framework for Bipedal Robots Jumping,&quot; IEEE Robotics and Automation Letters, 2024. doi: 10.1109/LRA.2024.3408487. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85195411305&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398446288')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2024" data-category="other" data-title="hybrid internal model: learning agile legged locomotion with simulated robot response" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85198305369&amp;origin=resultslist" target="_blank">HYBRID INTERNAL MODEL: LEARNING AGILE LEGGED LOCOMOTION WITH SIMULATED ROBOT RESPONSE</a>
                            </h3>
                            <p class="card-authors">Long J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398446144">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85198305369&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. J., &quot;HYBRID INTERNAL MODEL: LEARNING AGILE LEGGED LOCOMOTION WITH SIMULATED ROBOT RESPONSE,&quot; 12th International Conference on Learning Representations Iclr 2024, 2024. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85198305369&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398446144')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="dipper: diffusion-based 2d path planner applied on legged robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85196223831&amp;origin=resultslist" target="_blank">DiPPeR: Diffusion-based 2D Path Planner applied on Legged Robots</a>
                            </h3>
                            <p class="card-authors">Liu J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398446240">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85196223831&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/ICRA57147.2024.10610013" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. J., &quot;DiPPeR: Diffusion-based 2D Path Planner applied on Legged Robots,&quot; Proceedings IEEE International Conference on Robotics and Automation, 2024. doi: 10.1109/ICRA57147.2024.10610013. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85196223831&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398446240')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="revisiting reward design and evaluation for robust humanoid standing and walking" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85216463921&amp;origin=resultslist" target="_blank">Revisiting Reward Design and Evaluation for Robust Humanoid Standing and Walking</a>
                            </h3>
                            <p class="card-authors">Van Marum B.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398446528">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85216463921&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/IROS58592.2024.10802680" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'V. M. B., &quot;Revisiting Reward Design and Evaluation for Robust Humanoid Standing and Walking,&quot; IEEE International Conference on Intelligent Robots and Systems, 2024. doi: 10.1109/IROS58592.2024.10802680. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85216463921&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398446528')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="moral: learning morphologically adaptive locomotion controller for quadrupedal robots on challenging terrains" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85187977237&amp;origin=resultslist" target="_blank">MorAL: Learning Morphologically Adaptive Locomotion Controller for Quadrupedal Robots on Challenging Terrains</a>
                            </h3>
                            <p class="card-authors">Luo Z.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398446672">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85187977237&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2024.3375086" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Z., &quot;MorAL: Learning Morphologically Adaptive Locomotion Controller for Quadrupedal Robots on Challenging Terrains,&quot; IEEE Robotics and Automation Letters, 2024. doi: 10.1109/LRA.2024.3375086. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85187977237&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398446672')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2024" data-category="other" data-title="humanoid loco-manipulations using combined fast dense 3d tracking and slam with wide-angle depth-images" data-keywords="" data-themes="I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85162718807&amp;origin=resultslist" target="_blank">Humanoid Loco-Manipulations Using Combined Fast Dense 3D Tracking and SLAM with Wide-Angle Depth-Images</a>
                            </h3>
                            <p class="card-authors">Chappellet K.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398446336">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85162718807&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TASE.2023.3283497" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. K., &quot;Humanoid Loco-Manipulations Using Combined Fast Dense 3D Tracking and SLAM with Wide-Angle Depth-Images,&quot; IEEE Transactions on Automation Science and Engineering, 2024. doi: 10.1109/TASE.2023.3283497. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85162718807&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398446336')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2024" data-category="other" data-title="highly maneuverable humanoid running via 3d slip+foot dynamics" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85179809683&amp;origin=resultslist" target="_blank">Highly Maneuverable Humanoid Running via 3D SLIP+Foot Dynamics</a>
                            </h3>
                            <p class="card-authors">Sovukluk S.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398447296">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85179809683&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3342668" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. S., &quot;Highly Maneuverable Humanoid Running via 3D SLIP+Foot Dynamics,&quot; IEEE Robotics and Automation Letters, 2024. doi: 10.1109/LRA.2023.3342668. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85179809683&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398447296')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="Scopus" data-year="2024" data-category="other" data-title="marine sediment sampling with an underwater legged robot: a user-driven sampling approach for microplastic analysis" data-keywords="" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85182359859&amp;origin=resultslist" target="_blank">Marine Sediment Sampling With an Underwater Legged Robot: A User-Driven Sampling Approach for Microplastic Analysis</a>
                            </h3>
                            <p class="card-authors">Astolfi A.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398447488">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85182359859&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/MRA.2023.3341288" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. A., &quot;Marine Sediment Sampling With an Underwater Legged Robot: A User-Driven Sampling Approach for Microplastic Analysis,&quot; IEEE Robotics and Automation Magazine, 2024. doi: 10.1109/MRA.2023.3341288. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85182359859&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398447488')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2024" data-category="other" data-title="a new method for finding the proper initial conditions in passive locomotion of bipedal robotic systems" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85178144535&amp;origin=resultslist" target="_blank">A new method for finding the proper initial conditions in passive locomotion of bipedal robotic systems</a>
                            </h3>
                            <p class="card-authors">Fazel R.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398447344">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85178144535&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1016/j.cnsns.2023.107693" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. R., &quot;A new method for finding the proper initial conditions in passive locomotion of bipedal robotic systems,&quot; Communications in Nonlinear Science and Numerical Simulation, 2024. doi: 10.1016/j.cnsns.2023.107693. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85178144535&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398447344')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="optimization-based flocking control and mpc-based gait synchronization control for multiple quadruped robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85182362384&amp;origin=resultslist" target="_blank">Optimization-Based Flocking Control and MPC-Based Gait Synchronization Control for Multiple Quadruped Robots</a>
                            </h3>
                            <p class="card-authors">Liu K.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398447632">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85182362384&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2024.3350372" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. K., &quot;Optimization-Based Flocking Control and MPC-Based Gait Synchronization Control for Multiple Quadruped Robots,&quot; IEEE Robotics and Automation Letters, 2024. doi: 10.1109/LRA.2024.3350372. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85182362384&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398447632')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="robust jumping with an articulated soft quadruped via trajectory optimization and iterative learning" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85177025212&amp;origin=resultslist" target="_blank">Robust Jumping with an Articulated Soft Quadruped Via Trajectory Optimization and Iterative Learning</a>
                            </h3>
                            <p class="card-authors">Ding J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398447824">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85177025212&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3331288" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. J., &quot;Robust Jumping with an Articulated Soft Quadruped Via Trajectory Optimization and Iterative Learning,&quot; IEEE Robotics and Automation Letters, 2024. doi: 10.1109/LRA.2023.3331288. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85177025212&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398447824')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2024" data-category="other" data-title="learning agile locomotion and adaptive behaviors via rl-augmented mpc" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85202431823&amp;origin=resultslist" target="_blank">Learning Agile Locomotion and Adaptive Behaviors via RL-augmented MPC</a>
                            </h3>
                            <p class="card-authors">Chen Y.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398447920">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85202431823&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/ICRA57147.2024.10610453" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Y., &quot;Learning Agile Locomotion and Adaptive Behaviors via RL-augmented MPC,&quot; Proceedings IEEE International Conference on Robotics and Automation, 2024. doi: 10.1109/ICRA57147.2024.10610453. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85202431823&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398447920')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="learning to walk in confined spaces using 3d representation" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85202444899&amp;origin=resultslist" target="_blank">Learning to walk in confined spaces using 3D representation</a>
                            </h3>
                            <p class="card-authors">Miki T.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398447536">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85202444899&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/ICRA57147.2024.10610271" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. T., &quot;Learning to walk in confined spaces using 3D representation,&quot; Proceedings IEEE International Conference on Robotics and Automation, 2024. doi: 10.1109/ICRA57147.2024.10610271. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85202444899&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398447536')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2024" data-category="other" data-title="cts: concurrent teacher-student reinforcement learning for legged locomotion" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85204230435&amp;origin=resultslist" target="_blank">CTS: Concurrent Teacher-Student Reinforcement Learning for Legged Locomotion</a>
                            </h3>
                            <p class="card-authors">Wang H.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398448400">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85204230435&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2024.3457379" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. H., &quot;CTS: Concurrent Teacher-Student Reinforcement Learning for Legged Locomotion,&quot; IEEE Robotics and Automation Letters, 2024. doi: 10.1109/LRA.2024.3457379. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85204230435&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398448400')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="Scopus" data-year="2024" data-category="other" data-title="design, motions, capabilities, and applications of quadruped robots: a comprehensive review" data-keywords="" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85201620126&amp;origin=resultslist" target="_blank">Design, motions, capabilities, and applications of quadruped robots: a comprehensive review</a>
                            </h3>
                            <p class="card-authors">Majithia A.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398170368">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85201620126&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.3389/fmech.2024.1448681" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. A., &quot;Design, motions, capabilities, and applications of quadruped robots: a comprehensive review,&quot; Frontiers in Mechanical Engineering, 2024. doi: 10.3389/fmech.2024.1448681. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85201620126&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398170368')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="leg-kilo: robust kinematic-inertial-lidar odometry for dynamic legged robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85200800452&amp;origin=resultslist" target="_blank">Leg-KILO: Robust Kinematic-Inertial-Lidar Odometry for Dynamic Legged Robots</a>
                            </h3>
                            <p class="card-authors">Ou G.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398170464">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85200800452&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2024.3440730" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'O. G., &quot;Leg-KILO: Robust Kinematic-Inertial-Lidar Odometry for Dynamic Legged Robots,&quot; IEEE Robotics and Automation Letters, 2024. doi: 10.1109/LRA.2024.3440730. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85200800452&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398170464')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="leveraging large language models for comprehensive locomotion control in humanoid robots design" data-keywords="" data-themes="I E L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85207356903&amp;origin=resultslist" target="_blank">Leveraging large language models for comprehensive locomotion control in humanoid robots design</a>
                            </h3>
                            <p class="card-authors">Sun S.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398170560">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85207356903&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1016/j.birob.2024.100187" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. S., &quot;Leveraging large language models for comprehensive locomotion control in humanoid robots design,&quot; Biomimetic Intelligence and Robotics, 2024. doi: 10.1016/j.birob.2024.100187. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85207356903&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398170560')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="staf: interaction-based design and evaluation of sensorized terrain-adaptive foot for legged robot traversing on soft slopes" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85183993768&amp;origin=resultslist" target="_blank">STAF: Interaction-Based Design and Evaluation of Sensorized Terrain-Adaptive Foot for Legged Robot Traversing on Soft Slopes</a>
                            </h3>
                            <p class="card-authors">Yao C.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398170656">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85183993768&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TMECH.2024.3350183" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. C., &quot;STAF: Interaction-Based Design and Evaluation of Sensorized Terrain-Adaptive Foot for Legged Robot Traversing on Soft Slopes,&quot; IEEE ASME Transactions on Mechatronics, 2024. doi: 10.1109/TMECH.2024.3350183. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85183993768&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398170656')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="bioinspired soft spine enables small-scale robotic rat to conquer challenging environments" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85166578780&amp;origin=resultslist" target="_blank">Bioinspired Soft Spine Enables Small-Scale Robotic Rat to Conquer Challenging Environments</a>
                            </h3>
                            <p class="card-authors">Wang R.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398170800">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85166578780&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1089/soro.2022.0220" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. R., &quot;Bioinspired Soft Spine Enables Small-Scale Robotic Rat to Conquer Challenging Environments,&quot; Soft Robotics, 2024. doi: 10.1089/soro.2022.0220. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85166578780&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398170800')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="on second-order derivatives of rigid-body dynamics: theory and implementation" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85186988695&amp;origin=resultslist" target="_blank">On Second-Order Derivatives of Rigid-Body Dynamics: Theory and Implementation</a>
                            </h3>
                            <p class="card-authors">Singh S.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398170848">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85186988695&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2024.3370002" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. S., &quot;On Second-Order Derivatives of Rigid-Body Dynamics: Theory and Implementation,&quot; IEEE Transactions on Robotics, 2024. doi: 10.1109/TRO.2024.3370002. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85186988695&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398170848')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2024" data-category="other" data-title="adaptive bipedal robot walking on industrial pipes under neural multimodal locomotion control: toward robotic out-pipe inspection" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85165403812&amp;origin=resultslist" target="_blank">Adaptive Bipedal Robot Walking on Industrial Pipes Under Neural Multimodal Locomotion Control: Toward Robotic Out-Pipe Inspection</a>
                            </h3>
                            <p class="card-authors">Srisuchinnawong A.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398170896">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85165403812&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TMECH.2023.3293950" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. A., &quot;Adaptive Bipedal Robot Walking on Industrial Pipes Under Neural Multimodal Locomotion Control: Toward Robotic Out-Pipe Inspection,&quot; IEEE ASME Transactions on Mechatronics, 2024. doi: 10.1109/TMECH.2023.3293950. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85165403812&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398170896')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="biomimetic soft-legged robotic locomotion, interactions and transitions in terrestrial, aquatic and multiple environments" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85190800381&amp;origin=resultslist" target="_blank">Biomimetic soft-legged robotic locomotion, interactions and transitions in terrestrial, aquatic and multiple environments</a>
                            </h3>
                            <p class="card-authors">Yu Z.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398171040">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85190800381&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1016/j.susmat.2024.e00930" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Z., &quot;Biomimetic soft-legged robotic locomotion, interactions and transitions in terrestrial, aquatic and multiple environments,&quot; Sustainable Materials and Technologies, 2024. doi: 10.1016/j.susmat.2024.e00930. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85190800381&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398171040')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="pie: parkour with implicit-explicit learning framework for legged robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85203996501&amp;origin=resultslist" target="_blank">PIE: Parkour With Implicit-Explicit Learning Framework for Legged Robots</a>
                            </h3>
                            <p class="card-authors">Luo S.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398171184">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85203996501&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2024.3459797" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. S., &quot;PIE: Parkour With Implicit-Explicit Learning Framework for Legged Robots,&quot; IEEE Robotics and Automation Letters, 2024. doi: 10.1109/LRA.2024.3459797. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85203996501&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398171184')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="learning risk-aware quadrupedal locomotion using distributional reinforcement learning" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85200564118&amp;origin=resultslist" target="_blank">Learning Risk-Aware Quadrupedal Locomotion using Distributional Reinforcement Learning</a>
                            </h3>
                            <p class="card-authors">Schneider L.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398171424">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85200564118&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/ICRA57147.2024.10610137" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. L., &quot;Learning Risk-Aware Quadrupedal Locomotion using Distributional Reinforcement Learning,&quot; Proceedings IEEE International Conference on Robotics and Automation, 2024. doi: 10.1109/ICRA57147.2024.10610137. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85200564118&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398171424')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="a survey on legged robots: advances, technologies and applications" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85206246595&amp;origin=resultslist" target="_blank">A survey on legged robots: Advances, technologies and applications</a>
                            </h3>
                            <p class="card-authors">Wu Z.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398171520">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85206246595&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1016/j.engappai.2024.109418" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. Z., &quot;A survey on legged robots: Advances, technologies and applications,&quot; Engineering Applications of Artificial Intelligence, 2024. doi: 10.1016/j.engappai.2024.109418. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85206246595&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398171520')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2024" data-category="other" data-title="comparative analysis of reinforcement learning algorithms for bipedal robot locomotion" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85181844587&amp;origin=resultslist" target="_blank">Comparative Analysis of Reinforcement Learning Algorithms for Bipedal Robot Locomotion</a>
                            </h3>
                            <p class="card-authors">Aydogmus O.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398171712">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85181844587&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/ACCESS.2023.3344393" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. O., &quot;Comparative Analysis of Reinforcement Learning Algorithms for Bipedal Robot Locomotion,&quot; IEEE Access, 2024. doi: 10.1109/ACCESS.2023.3344393. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85181844587&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398171712')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="design and multimodal locomotion plan of a hexapod robot with improved knee joints" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85189527908&amp;origin=resultslist" target="_blank">Design and multimodal locomotion plan of a hexapod robot with improved knee joints</a>
                            </h3>
                            <p class="card-authors">Xu K.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398171856">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85189527908&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1002/rob.22324" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. K., &quot;Design and multimodal locomotion plan of a hexapod robot with improved knee joints,&quot; Journal of Field Robotics, 2024. doi: 10.1002/rob.22324. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85189527908&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398171856')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section year-section" data-year="2023">
                <h2 class="section-header">üìÖ 2023 <span class="section-count">(152 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.hc" data-title="exploring large language models to facilitate variable autonomy for human-robot teaming" data-keywords="transformer gpt robot multi-robot language model cs.hc cs.ai cs.ro" data-themes="E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Human-Computer Interaction</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.07214v3" target="_blank">Exploring Large Language Models to Facilitate Variable Autonomy for Human-Robot Teaming</a>
                            </h3>
                            <p class="card-authors">Younes Lakhnati, Max Pascher, Jens Gerken</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Transformer</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Robot</span></div>

                            <div class="card-details" id="details-4397313104">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In a rapidly evolving digital landscape autonomous tools and robots are becoming commonplace. Recognizing the significance of this development, this paper explores the integration of Large Language Models (LLMs) like Generative pre-trained transformer (GPT) into human-robot teaming environments to facilitate variable autonomy through the means of verbal human-robot communication. In this paper, we introduce a novel framework for such a GPT-powered multi-robot testbed environment, based on a Unity Virtual Reality (VR) setting. This system allows users to interact with robot agents through natur...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we introduce a novel framework for such a GPT-powered multi-robot testbed environment, based on a Unity Virtual Reality (VR) setting</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">By means of OpenAI&#x27;s function calling, we bridge the gap between unstructured natural language input and structure robot actions</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Recognizing the significance of this development, this paper explores the integration of Large Language Models (LLMs) like Generative pre-trained transformer (GPT) into human-robot teaming environments to facilitate variable autonomy through the means of verbal human-robot communication</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.07214v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.07214v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Lakhnati, M. Pascher, and J. Gerken, &quot;Exploring Large Language Models to Facilitate Variable Autonomy for Human-Robot Teaming,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.07214v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397313104')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="making large language models better reasoners with alignment" data-keywords="language model cs.cl cs.ai cs.lg" data-themes="I M E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.02144v1" target="_blank">Making Large Language Models Better Reasoners with Alignment</a>
                            </h3>
                            <p class="card-authors">Peiyi Wang, Lei Li, Liang Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.LG</span></div>

                            <div class="card-details" id="details-4397307344">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Reasoning is a cognitive process of using evidence to reach a sound conclusion. The reasoning capability is essential for large language models (LLMs) to serve as the brain of the artificial general intelligence agent. Recent studies reveal that fine-tuning LLMs on data with the chain of thought (COT) reasoning process can significantly enhance their reasoning capabilities. However, we find that the fine-tuned LLMs suffer from an \textit{Assessment Misalignment} problem, i.e., they frequently assign higher scores to subpar COTs, leading to potential limitations in their reasoning abilities. To...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address this problem, we introduce an \textit{Alignment Fine-Tuning (AFT)} paradigm, which involves three steps: 1) fine-tuning LLMs with COT training data; 2) generating multiple COT responses for each question, and categorizing them into positive and negative ones based on whether they achieve the correct answer; 3) calibrating the scores of positive and negative responses given by LLMs with a novel constraint alignment loss</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, we find that the fine-tuned LLMs suffer from an \textit{Assessment Misalignment} problem, i.e., they frequently assign higher scores to subpar COTs, leading to potential limitations in their reasoning abilities. To address this problem, we introduce an \textit{Alignment Fine-Tuning (AFT)} paradigm, which involves three steps: 1) fine-tuning LLMs with COT training data; 2) generating multiple COT responses for each question, and categorizing them into positive and negative ones based on whether they achieve the correct answer; 3) calibrating the scores of positive and negative responses given by LLMs with a novel constraint alignment loss</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The reasoning capability is essential for large language models (LLMs) to serve as the brain of the artificial general intelligence agent</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.02144v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.02144v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Wang et al., &quot;Making Large Language Models Better Reasoners with Alignment,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.02144v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397307344')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="beneath the surface: unveiling harmful memes with multimodal reasoning distilled from large language models" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.05434v1" target="_blank">Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models</a>
                            </h3>
                            <p class="card-authors">Hongzhan Lin, Ziyang Luo, Jing Ma et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4397305568">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The age of social media is rife with memes. Understanding and detecting harmful memes pose a significant challenge due to their implicit meaning that is not explicitly conveyed through the surface text and image. However, existing harmful meme detection approaches only recognize superficial harm-indicative signals in an end-to-end classification manner but ignore in-depth cognition of the meme text and image. In this paper, we attempt to detect harmful memes based on advanced reasoning over the interplay of multimodal information in memes. Inspired by the success of Large Language Models (LLMs...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Then we propose a novel generative framework to learn reasonable thoughts from LLMs for better multimodal fusion and lightweight fine-tuning, which consists of two training stages: 1) Distill multimodal reasoning knowledge from LLMs; and 2) Fine-tune the generative framework to infer harmfulness</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Understanding and detecting harmful memes pose a significant challenge due to their implicit meaning that is not explicitly conveyed through the surface text and image</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">However, existing harmful meme detection approaches only recognize superficial harm-indicative signals in an end-to-end classification manner but ignore in-depth cognition of the meme text and image</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.05434v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.05434v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Lin, Z. Luo, J. Ma, and L. Chen, &quot;Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.05434v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397305568')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="all languages matter: on the multilingual safety of large language models" data-keywords="gpt ros language model cs.cl cs.ai" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2310.00905v2" target="_blank">All Languages Matter: On the Multilingual Safety of Large Language Models</a>
                            </h3>
                            <p class="card-authors">Wenxuan Wang, Zhaopeng Tu, Chang Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4397308064">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Safety lies at the core of developing and deploying large language models (LLMs). However, previous safety benchmarks only concern the safety in one language, e.g. the majority language in the pretraining data such as English. In this work, we build the first multilingual safety benchmark for LLMs, XSafety, in response to the global deployment of LLMs in practice. XSafety covers 14 kinds of commonly used safety issues across 10 languages that span several language families. We utilize XSafety to empirically study the multilingual safety for 4 widely-used LLMs, including both close-API and open...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In addition, we propose several simple and effective prompting methods to improve the multilingual safety of ChatGPT by evoking safety knowledge and improving cross-lingual generalization of safety alignment</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">XSafety covers 14 kinds of commonly used safety issues across 10 languages that span several language families</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Safety lies at the core of developing and deploying large language models (LLMs)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2310.00905v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2310.00905v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. Wang et al., &quot;All Languages Matter: On the Multilingual Safety of Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2310.00905v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397308064')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cv" data-title="a survey on multimodal large language models" data-keywords="gpt language model cs.cv cs.ai cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computer Vision</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2306.13549v4" target="_blank">A Survey on Multimodal Large Language Models</a>
                            </h3>
                            <p class="card-authors">Shukang Yin, Chaoyou Fu, Sirui Zhao et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4397308304">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. I...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To conclude the paper, we discuss existing challenges and point out promising research directions</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2306.13549v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2306.13549v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Yin et al., &quot;A Survey on Multimodal Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2306.13549v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397308304')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="teal: tokenize and embed all for multi-modal large language models" data-keywords="language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2311.04589v3" target="_blank">TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models</a>
                            </h3>
                            <p class="card-authors">Zhen Yang, Yingxue Zhang, Fandong Meng et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4397304512">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Despite Multi-modal Large Language Models (MM-LLMs) have made exciting strides recently, they are still struggling to efficiently model the interactions among multi-modal inputs and the generation in non-textual modalities. In this work, we propose TEAL (Tokenize and Embed ALl)}, an approach to treat the input from any modality as a token sequence and learn a joint embedding space for all modalities. Specifically, for the input from any modality, TEAL first discretizes it into a token sequence with the off-the-shelf tokenizer and embeds the token sequence into a joint embedding space with a le...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose TEAL (Tokenize and Embed ALl)}, an approach to treat the input from any modality as a token sequence and learn a joint embedding space for all modalities</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Despite Multi-modal Large Language Models (MM-LLMs) have made exciting strides recently, they are still struggling to efficiently model the interactions among multi-modal inputs and the generation in non-textual modalities</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2311.04589v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2311.04589v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Yang, Y. Zhang, F. Meng, and J. Zhou, &quot;TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2311.04589v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397304512')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="large language models in ambulatory devices for home health diagnostics: a case study of sickle cell anemia management" data-keywords="language model cs.cl" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2305.03715v1" target="_blank">Large Language Models in Ambulatory Devices for Home Health Diagnostics: A case study of Sickle Cell Anemia Management</a>
                            </h3>
                            <p class="card-authors">Oluwatosin Ogundare, Subuola Sofolahan</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4397304704">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This study investigates the potential of an ambulatory device that incorporates Large Language Models (LLMs) in cadence with other specialized ML models to assess anemia severity in sickle cell patients in real time. The device would rely on sensor data that measures angiogenic material levels to assess anemia severity, providing real-time information to patients and clinicians to reduce the frequency of vaso-occlusive crises because of the early detection of anemia severity, allowing for timely interventions and potentially reducing the likelihood of serious complications. The main challenges...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">The main challenges in developing such a device are the creation of a reliable non-invasive tool for angiogenic level assessment, a biophysics model and the practical consideration of an LLM communicating with emergency personnel on behalf of an incapacitated patient. A possible system is proposed, and the limitations of this approach are discussed.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This study investigates the potential of an ambulatory device that incorporates Large Language Models (LLMs) in cadence with other specialized ML models to assess anemia severity in sickle cell patients in real time</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2305.03715v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2305.03715v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'O. Ogundare, and S. Sofolahan, &quot;Large Language Models in Ambulatory Devices for Home Health Diagnostics: A case study of Sickle Cell Anemia Management,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2305.03715v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397304704')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="larabench: benchmarking arabic ai with large language models" data-keywords="gpt ros nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2305.14982v2" target="_blank">LAraBench: Benchmarking Arabic AI with Large Language Models</a>
                            </h3>
                            <p class="card-authors">Ahmed Abdelali, Hamdy Mubarak, Shammur Absar Chowdhury et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4397312192">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recent advancements in Large Language Models (LLMs) have significantly influenced the landscape of language and speech research. Despite this progress, these models lack specific benchmarking against state-of-the-art (SOTA) models tailored to particular languages and tasks. LAraBench addresses this gap for Arabic Natural Language Processing (NLP) and Speech Processing tasks, including sequence tagging and content classification across different domains. We utilized models such as GPT-3.5-turbo, GPT-4, BLOOMZ, Jais-13b-chat, Whisper, and USM, employing zero and few-shot learning techniques to t...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">LAraBench addresses this gap for Arabic Natural Language Processing (NLP) and Speech Processing tasks, including sequence tagging and content classification across different domains. Our analysis focused on measuring the performance gap between SOTA models and LLMs</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Recent advancements in Large Language Models (LLMs) have significantly influenced the landscape of language and speech research</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2305.14982v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2305.14982v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Abdelali et al., &quot;LAraBench: Benchmarking Arabic AI with Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2305.14982v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397312192')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="neuron-level knowledge attribution in large language models" data-keywords="attention ros language model cs.cl cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.12141v4" target="_blank">Neuron-Level Knowledge Attribution in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Zeping Yu, Sophia Ananiadou</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4397304656">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Identifying important neurons for final predictions is essential for understanding the mechanisms of large language models. Due to computational constraints, current attribution techniques struggle to operate at neuron level. In this paper, we propose a static method for pinpointing significant neurons. Compared to seven other methods, our approach demonstrates superior performance across three metrics. Additionally, since most static methods typically only identify &quot;value neurons&quot; directly contributing to the final prediction, we propose a method for identifying &quot;query neurons&quot; which activate...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose a static method for pinpointing significant neurons</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Identifying important neurons for final predictions is essential for understanding the mechanisms of large language models</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.12141v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.12141v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Yu, and S. Ananiadou, &quot;Neuron-Level Knowledge Attribution in Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.12141v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397304656')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="factuality challenges in the era of large language models" data-keywords="attention gpt ros language model cs.cl cs.ai cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2310.05189v2" target="_blank">Factuality Challenges in the Era of Large Language Models</a>
                            </h3>
                            <p class="card-authors">Isabelle Augenstein, Timothy Baldwin, Meeyoung Cha et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4397306480">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The emergence of tools based on Large Language Models (LLMs), such as OpenAI&#x27;s ChatGPT, Microsoft&#x27;s Bing Chat, and Google&#x27;s Bard, has garnered immense public attention. These incredibly useful, natural-sounding tools mark significant advances in natural language generation, yet they exhibit a propensity to generate false, erroneous, or misleading content -- commonly referred to as &quot;hallucinations.&quot; Moreover, LLMs can be exploited for malicious applications, such as generating false but credible-sounding content and profiles at scale. This poses a significant challenge to society in terms of th...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">This poses a significant challenge to society in terms of the potential deception of users and the increasing dissemination of inaccurate information</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The emergence of tools based on Large Language Models (LLMs), such as OpenAI&#x27;s ChatGPT, Microsoft&#x27;s Bing Chat, and Google&#x27;s Bard, has garnered immense public attention</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2310.05189v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2310.05189v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'I. Augenstein et al., &quot;Factuality Challenges in the Era of Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2310.05189v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397306480')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="can chatgpt be your personal medical assistant?" data-keywords="gpt language model cs.cl cs.si" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.12006v1" target="_blank">Can ChatGPT be Your Personal Medical Assistant?</a>
                            </h3>
                            <p class="card-authors">Md. Rafiul Biswas, Ashhadul Islam, Zubair Shah et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.SI</span></div>

                            <div class="card-details" id="details-4397314448">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The advanced large language model (LLM) ChatGPT has shown its potential in different domains and remains unbeaten due to its characteristics compared to other LLMs. This study aims to evaluate the potential of using a fine-tuned ChatGPT model as a personal medical assistant in the Arabic language. To do so, this study uses publicly available online questions and answering datasets in Arabic language. There are almost 430K questions and answers for 20 disease-specific categories. GPT-3.5-turbo model was fine-tuned with a portion of this dataset. The performance of this fine-tuned model was eval...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The advanced large language model (LLM) ChatGPT has shown its potential in different domains and remains unbeaten due to its characteristics compared to other LLMs</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.12006v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.12006v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. R. Biswas, A. Islam, Z. Shah, W. Zaghouani, and S. B. Belhaouari, &quot;Can ChatGPT be Your Personal Medical Assistant?,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.12006v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397314448')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="large language models and multimodal retrieval for visual word sense disambiguation" data-keywords="transformer language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2310.14025v1" target="_blank">Large Language Models and Multimodal Retrieval for Visual Word Sense Disambiguation</a>
                            </h3>
                            <p class="card-authors">Anastasia Kritharoula, Maria Lymperaiou, Giorgos Stamou</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Transformer</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4397313200">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Visual Word Sense Disambiguation (VWSD) is a novel challenging task with the goal of retrieving an image among a set of candidates, which better represents the meaning of an ambiguous word within a given context. In this paper, we make a substantial step towards unveiling this interesting task by applying a varying set of approaches. Since VWSD is primarily a text-image retrieval task, we explore the latest transformer-based methods for multimodal retrieval. Additionally, we utilize Large Language Models (LLMs) as knowledge bases to enhance the given phrases and resolve ambiguity related to th...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">We also study VWSD as a unimodal problem by converting to text-to-text and image-to-image retrieval, as well as question-answering (QA), to fully explore the capabilities of relevant models</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In this paper, we make a substantial step towards unveiling this interesting task by applying a varying set of approaches</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2310.14025v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2310.14025v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Kritharoula, M. Lymperaiou, and G. Stamou, &quot;Large Language Models and Multimodal Retrieval for Visual Word Sense Disambiguation,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2310.14025v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397313200')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="open sesame! universal black box jailbreaking of large language models" data-keywords="language model cs.cl cs.cv cs.ne" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.01446v4" target="_blank">Open Sesame! Universal Black Box Jailbreaking of Large Language Models</a>
                            </h3>
                            <p class="card-authors">Raz Lapid, Ron Langberg, Moshe Sipper</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.CV</span><span class="keyword-tag">CS.NE</span></div>

                            <div class="card-details" id="details-4397304608">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs), designed to provide helpful and safe responses, often rely on alignment techniques to align with user intent and social guidelines. Unfortunately, this alignment can be exploited by malicious actors seeking to manipulate an LLM&#x27;s outputs for unintended purposes. In this paper we introduce a novel approach that employs a genetic algorithm (GA) to manipulate LLMs when model architecture and parameters are inaccessible. The GA attack works by optimizing a universal adversarial prompt that -- when combined with a user&#x27;s query -- disrupts the attacked model&#x27;s alignment...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper we introduce a novel approach that employs a genetic algorithm (GA) to manipulate LLMs when model architecture and parameters are inaccessible</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Our novel approach systematically reveals a model&#x27;s limitations and vulnerabilities by uncovering instances where its responses deviate from expected behavior</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large language models (LLMs), designed to provide helpful and safe responses, often rely on alignment techniques to align with user intent and social guidelines</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.01446v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.01446v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Lapid, R. Langberg, and M. Sipper, &quot;Open Sesame! Universal Black Box Jailbreaking of Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.01446v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397304608')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="demystifying instruction mixing for fine-tuning large language models" data-keywords="ros nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.10793v3" target="_blank">Demystifying Instruction Mixing for Fine-tuning Large Language Models</a>
                            </h3>
                            <p class="card-authors">Renxi Wang, Haonan Li, Minghao Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4397306192">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Instruction tuning significantly enhances the performance of large language models (LLMs) across various tasks. However, the procedure to optimizing the mixing of instruction datasets for LLM fine-tuning is still poorly understood. This study categorizes instructions into three primary types: NLP downstream tasks, coding, and general chat. We explore the effects of instruction tuning on different combinations of datasets on LLM performance, and find that certain instruction types are more advantageous for specific applications but can negatively impact other areas. This work provides insights ...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Instruction tuning significantly enhances the performance of large language models (LLMs) across various tasks</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.10793v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.10793v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Wang et al., &quot;Demystifying Instruction Mixing for Fine-tuning Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.10793v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397306192')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="response: emergent analogical reasoning in large language models" data-keywords="gpt ros language model cs.cl cs.ai" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2308.16118v2" target="_blank">Response: Emergent analogical reasoning in large language models</a>
                            </h3>
                            <p class="card-authors">Damian Hodel, Jevin West</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4397307104">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In their recent Nature Human Behaviour paper, &quot;Emergent analogical reasoning in large language models,&quot; (Webb, Holyoak, and Lu, 2023) the authors argue that &quot;large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems.&quot; In this response, we provide counterexamples of the letter string analogies. In our tests, GPT-3 fails to solve simplest variations of the original tasks, whereas human performance remains consistently high across all modified versions. Zero-shot reasoning is an extraordinary claim that requires extraord...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">In their recent Nature Human Behaviour paper, &quot;Emergent analogical reasoning in large language models,&quot; (Webb, Holyoak, and Lu, 2023) the authors argue that &quot;large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems.&quot; In this response, we provide counterexamples of the letter string analogies</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In their recent Nature Human Behaviour paper, &quot;Emergent analogical reasoning in large language models,&quot; (Webb, Holyoak, and Lu, 2023) the authors argue that &quot;large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems.&quot; In this response, we provide counterexamples of the letter string analogies</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2308.16118v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2308.16118v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Hodel, and J. West, &quot;Response: Emergent analogical reasoning in large language models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2308.16118v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397307104')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="using large language models to provide explanatory feedback to human tutors" data-keywords="language model cs.cl cs.ai cs.hc" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2306.15498v1" target="_blank">Using Large Language Models to Provide Explanatory Feedback to Human Tutors</a>
                            </h3>
                            <p class="card-authors">Jionghao Lin, Danielle R. Thomas, Feifei Han et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.HC</span></div>

                            <div class="card-details" id="details-4397304992">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Research demonstrates learners engaging in the process of producing explanations to support their reasoning, can have a positive impact on learning. However, providing learners real-time explanatory feedback often presents challenges related to classification accuracy, particularly in domain-specific environments, containing situationally complex and nuanced responses. We present two approaches for supplying tutors real-time feedback within an online lesson on how to give students effective praise. This work-in-progress demonstrates considerable accuracy in binary classification for corrective...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present two approaches for supplying tutors real-time feedback within an online lesson on how to give students effective praise</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, providing learners real-time explanatory feedback often presents challenges related to classification accuracy, particularly in domain-specific environments, containing situationally complex and nuanced responses</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We present two approaches for supplying tutors real-time feedback within an online lesson on how to give students effective praise</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2306.15498v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2306.15498v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Lin et al., &quot;Using Large Language Models to Provide Explanatory Feedback to Human Tutors,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2306.15498v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397304992')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="wizardlm: empowering large pre-trained language models to follow complex instructions" data-keywords="gpt nlp language model cs.cl cs.ai" data-themes="E L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2304.12244v3" target="_blank">WizardLM: Empowering large pre-trained language models to follow complex instructions</a>
                            </h3>
                            <p class="card-authors">Can Xu, Qingfeng Sun, Kai Zheng et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398401664">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data t...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Training large language models (LLMs) with open-domain instruction following data brings colossal success</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2304.12244v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2304.12244v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Xu et al., &quot;WizardLM: Empowering large pre-trained language models to follow complex instructions,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2304.12244v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398401664')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.lg" data-title="pb-llm: partially binarized large language models" data-keywords="gpt language model cs.lg cs.ai cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Machine Learning</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2310.00034v2" target="_blank">PB-LLM: Partially Binarized Large Language Models</a>
                            </h3>
                            <p class="card-authors">Yuzhang Shang, Zhihang Yuan, Qiang Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4398403200">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper explores network binarization, a radical form of quantization, compressing model weights to a single bit, specifically for Large Language Models (LLMs) compression. Due to previous binarization methods collapsing LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of quantized LLMs. Specifically, our exploration first uncovers the ineffectiveness of naive applications of existing binarization algorithms and highlights the imperative role of salient weights in achieving...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Due to previous binarization methods collapsing LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of quantized LLMs</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper explores network binarization, a radical form of quantization, compressing model weights to a single bit, specifically for Large Language Models (LLMs) compression</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2310.00034v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2310.00034v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Shang, Z. Yuan, Q. Wu, and Z. Dong, &quot;PB-LLM: Partially Binarized Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2310.00034v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398403200')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="prompting and fine-tuning open-sourced large language models for stance classification" data-keywords="ros language model cs.cl cs.ai" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.13734v2" target="_blank">Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification</a>
                            </h3>
                            <p class="card-authors">Iain J. Cruickshank, Lynnette Hui Xian Ng</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4398402432">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Stance classification, the task of predicting the viewpoint of an author on a subject of interest, has long been a focal point of research in domains ranging from social science to machine learning. Current stance detection methods rely predominantly on manual annotation of sentences, followed by training a supervised machine learning model. However, this manual annotation process requires laborious annotation effort, and thus hampers its potential to generalize across different contexts. In this work, we investigate the use of Large Language Models (LLMs) as a stance detection methodology tha...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Current stance detection methods rely predominantly on manual annotation of sentences, followed by training a supervised machine learning model</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.13734v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.13734v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'I. J. Cruickshank, and L. H. X. Ng, &quot;Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.13734v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398402432')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="tidybot: personalized robot assistance with large language models" data-keywords="robot perception planning language model cs.ro cs.ai cs.cl" data-themes="S I M E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2305.05658v2" target="_blank">TidyBot: Personalized Robot Assistance with Large Language Models</a>
                            </h3>
                            <p class="card-authors">Jimmy Wu, Rika Antonova, Adam Kan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Perception</span><span class="keyword-tag">Planning</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4398403440">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people&#x27;s preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We also demonstrate our approach on a real-world mobile manipulator called TidyBot, which successfully puts away 85.0% of objects in real-world test scenarios.</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">A key challenge is determining the proper place to put each object, as people&#x27;s preferences can vary greatly depending on personal taste or cultural background</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2305.05658v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2305.05658v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Wu et al., &quot;TidyBot: Personalized Robot Assistance with Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2305.05658v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398403440')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.ai" data-title="kglens: towards efficient and effective knowledge probing of large language models with knowledge graphs" data-keywords="simulation ros imu language model cs.ai cs.cl cs.lg" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Artificial Intelligence</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.11539v3" target="_blank">KGLens: Towards Efficient and Effective Knowledge Probing of Large Language Models with Knowledge Graphs</a>
                            </h3>
                            <p class="card-authors">Shangshang Zheng, He Bai, Yizhe Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Simulation</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Imu</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4398403680">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Language Models (LLMs) might hallucinate facts, while curated Knowledge Graph (KGs) are typically factually reliable especially with domain-specific knowledge. Measuring the alignment between KGs and LLMs can effectively probe the factualness and identify the knowledge blind spots of LLMs. However, verifying the LLMs over extensive KGs can be expensive. In this paper, we present KGLens, a Thompson-sampling-inspired framework aimed at effectively and efficiently measuring the alignment between KGs and LLMs. KGLens features a graph-guided question generator for converting KGs into natural ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we present KGLens, a Thompson-sampling-inspired framework aimed at effectively and efficiently measuring the alignment between KGs and LLMs</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large Language Models (LLMs) might hallucinate facts, while curated Knowledge Graph (KGs) are typically factually reliable especially with domain-specific knowledge</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.11539v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.11539v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Zheng, H. Bai, Y. Zhang, Y. Su, X. Niu, and N. Jaitly, &quot;KGLens: Towards Efficient and Effective Knowledge Probing of Large Language Models with Knowledge Graphs,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.11539v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398403680')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.se" data-title="metal: metamorphic testing framework for analyzing large-language model qualities" data-keywords="language model cs.se cs.ai cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ CS.SE</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.06056v1" target="_blank">METAL: Metamorphic Testing Framework for Analyzing Large-Language Model Qualities</a>
                            </h3>
                            <p class="card-authors">Sangwon Hyun, Mingyu Guo, M. Ali Babar</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.SE</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398401376">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large-Language Models (LLMs) have shifted the paradigm of natural language data processing. However, their black-boxed and probabilistic characteristics can lead to potential risks in the quality of outputs in diverse LLM applications. Recent studies have tested Quality Attributes (QAs), such as robustness or fairness, of LLMs by generating adversarial input texts. However, existing studies have limited their coverage of QAs and tasks in LLMs and are difficult to extend. Additionally, these studies have only used one evaluation metric, Attack Success Rate (ASR), to assess the effectiveness of ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a MEtamorphic Testing for Analyzing LLMs (METAL) framework to address these issues by applying Metamorphic Testing (MT) techniques</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, existing studies have limited their coverage of QAs and tasks in LLMs and are difficult to extend. We propose a MEtamorphic Testing for Analyzing LLMs (METAL) framework to address these issues by applying Metamorphic Testing (MT) techniques</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large-Language Models (LLMs) have shifted the paradigm of natural language data processing</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.06056v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.06056v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Hyun, M. Guo, and M. A. Babar, &quot;METAL: Metamorphic Testing Framework for Analyzing Large-Language Model Qualities,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.06056v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398401376')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="speaker attribution in german parliamentary debates with qlora-adapted large language models" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.09902v2" target="_blank">Speaker attribution in German parliamentary debates with QLoRA-adapted large language models</a>
                            </h3>
                            <p class="card-authors">Tobias Bornheim, Niklas Grieger, Patrick Gustav Blaneck et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398403008">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The growing body of political texts opens up new opportunities for rich insights into political dynamics and ideologies but also increases the workload for manual analysis. Automated speaker attribution, which detects who said what to whom in a speech event and is closely related to semantic role labeling, is an important processing step for computational text analysis. We study the potential of the large language model family Llama 2 to automate speaker attribution in German parliamentary debates from 2017-2021. We fine-tune Llama 2 with QLoRA, an efficient training strategy, and observe our ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We fine-tune Llama 2 with QLoRA, an efficient training strategy, and observe our approach to achieve competitive performance in the GermEval 2023 Shared Task On Speaker Attribution in German News Articles and Parliamentary Debates</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We study the potential of the large language model family Llama 2 to automate speaker attribution in German parliamentary debates from 2017-2021</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.09902v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.09902v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. Bornheim, N. Grieger, P. G. Blaneck, and S. Bialonski, &quot;Speaker attribution in German parliamentary debates with QLoRA-adapted large language models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.09902v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398403008')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="eess.as" data-title="acoustic prompt tuning: empowering large language models with audition capabilities" data-keywords="ros language model eess.as" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ EESS.AS</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.00249v2" target="_blank">Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities</a>
                            </h3>
                            <p class="card-authors">Jinhua Liang, Xubo Liu, Wenwu Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">EESS.AS</span></div>

                            <div class="card-details" id="details-4398401568">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The auditory system plays a substantial role in shaping the overall human perceptual experience. While prevailing large language models (LLMs) and visual language models (VLMs) have shown their promise in solving a wide variety of language and vision understanding tasks, only a few of them can be generalised to the audio domain without compromising their domain-specific capability. In this work, we introduce Acoustic Prompt Tuning (APT), a new adapter extending LLMs and VLMs to the audio domain by injecting audio embeddings to the input of LLMs, namely soft prompting. Specifically, APT applies...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we introduce Acoustic Prompt Tuning (APT), a new adapter extending LLMs and VLMs to the audio domain by injecting audio embeddings to the input of LLMs, namely soft prompting</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">While prevailing large language models (LLMs) and visual language models (VLMs) have shown their promise in solving a wide variety of language and vision understanding tasks, only a few of them can be generalised to the audio domain without compromising their domain-specific capability</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.00249v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.00249v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Liang, X. Liu, W. Wang, M. D. Plumbley, H. Phan, and E. Benetos, &quot;Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.00249v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398401568')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="a zero-shot and few-shot study of instruction-finetuned large language models applied to clinical and biomedical tasks" data-keywords="bert gpt nlp language model cs.cl cs.ai cs.lg" data-themes="S I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2307.12114v3" target="_blank">A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks</a>
                            </h3>
                            <p class="card-authors">Yanis Labrak, Mickael Rouvier, Richard Dufour</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Bert</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4398404016">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We evaluate four state-of-the-art instruction-tuned large language models (LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13 real-world clinical and biomedical natural language processing (NLP) tasks in English, such as named-entity recognition (NER), question-answering (QA), relation extraction (RE), etc. Our overall results demonstrate that the evaluated LLMs begin to approach performance of state-of-the-art models in zero- and few-shot scenarios for most tasks, and particularly well for the QA task, even though they have never seen examples from these tasks before. Ho...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We evaluate four state-of-the-art instruction-tuned large language models (LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13 real-world clinical and biomedical natural language processing (NLP) tasks in English, such as named-entity recognition (NER), question-answering (QA), relation extraction (RE), etc</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2307.12114v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2307.12114v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Labrak, M. Rouvier, and R. Dufour, &quot;A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2307.12114v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398404016')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="a variable autonomy approach for an automated weeding platform" data-keywords="robot imu cs.ro" data-themes="S">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2303.05461v1" target="_blank">A Variable Autonomy approach for an Automated Weeding Platform</a>
                            </h3>
                            <p class="card-authors">Ionut Moraru, Tsvetan Zhivkov, Shaun Coutts et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Imu</span><span class="keyword-tag">CS.RO</span></div>

                            <div class="card-details" id="details-4398402144">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Climate change, increase in world population and the war in Ukraine have led nations such as the UK to put a larger focus on food security, while simultaneously trying to halt declines in biodiversity and reduce risks to human health posed by chemically-reliant farming practices. Achieving these goals simultaneously will require novel approaches and accelerating the deployment of Agri-Robotics from the lab and into the field. In this paper we describe the ARWAC robot platform for mechanical weeding. We explain why the mechanical weeding approach is beneficial compared to the use of pesticides ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Thereafter, we present the system design and processing pipeline for generating a course of action for the robot to follow, such that it removes as many weeds as possible</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Achieving these goals simultaneously will require novel approaches and accelerating the deployment of Agri-Robotics from the lab and into the field</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2303.05461v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2303.05461v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'I. Moraru, T. Zhivkov, S. Coutts, D. Li, and E. I. Sklar, &quot;A Variable Autonomy approach for an Automated Weeding Platform,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2303.05461v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398402144')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cv" data-title="evaluation and enhancement of semantic grounding in large vision-language models" data-keywords="ros language model cs.cv cs.cl" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computer Vision</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.04041v2" target="_blank">Evaluation and Enhancement of Semantic Grounding in Large Vision-Language Models</a>
                            </h3>
                            <p class="card-authors">Jiaying Lu, Jinmeng Rao, Kezhen Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398401424">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Vision-Language Models (LVLMs) offer remarkable benefits for a variety of vision-language tasks. However, a challenge hindering their application in real-world scenarios, particularly regarding safety, robustness, and reliability, is their constrained semantic grounding ability, which pertains to connecting language to the physical-world entities or concepts referenced in images. Therefore, a crucial need arises for a comprehensive study to assess the semantic grounding ability of widely used LVLMs. Despite the significance, sufficient investigation in this direction is currently lacking...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address this issue, we propose a data-centric enhancement method that aims to improve LVLMs&#x27; semantic grounding ability through multimodal instruction tuning on fine-grained conversations</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, a challenge hindering their application in real-world scenarios, particularly regarding safety, robustness, and reliability, is their constrained semantic grounding ability, which pertains to connecting language to the physical-world entities or concepts referenced in images. Our work bridges this gap by designing a pipeline for generating large-scale evaluation datasets covering fine-grained semantic information, such as color, number, material, etc., along with a thorough assessment of seven popular LVLMs&#x27; semantic grounding ability</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large Vision-Language Models (LVLMs) offer remarkable benefits for a variety of vision-language tasks</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.04041v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.04041v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Lu et al., &quot;Evaluation and Enhancement of Semantic Grounding in Large Vision-Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.04041v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398401424')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cv" data-title="pushing boundaries: exploring zero shot object classification with large multimodal models" data-keywords="ros language model cs.cv cs.si" data-themes="S M E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computer Vision</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.00127v1" target="_blank">Pushing Boundaries: Exploring Zero Shot Object Classification with Large Multimodal Models</a>
                            </h3>
                            <p class="card-authors">Ashhadul Islam, Md. Rafiul Biswas, Wajdi Zaghouani et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span><span class="keyword-tag">CS.SI</span></div>

                            <div class="card-details" id="details-4398404544">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">$ $The synergy of language and vision models has given rise to Large Language and Vision Assistant models (LLVAs), designed to engage users in rich conversational experiences intertwined with image-based queries. These comprehensive multimodal models seamlessly integrate vision encoders with Large Language Models (LLMs), expanding their applications in general-purpose language and visual comprehension. The advent of Large Multimodal Models (LMMs) heralds a new era in Artificial Intelligence (AI) assistance, extending the horizons of AI utilization. This paper takes a unique perspective on LMMs...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">$ $The synergy of language and vision models has given rise to Large Language and Vision Assistant models (LLVAs), designed to engage users in rich conversational experiences intertwined with image-based queries</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.00127v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.00127v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Islam, M. R. Biswas, W. Zaghouani, S. B. Belhaouari, and Z. Shah, &quot;Pushing Boundaries: Exploring Zero Shot Object Classification with Large Multimodal Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2401.00127v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398404544')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.se" data-title="assurance for autonomy -- jpl&#x27;s past research, lessons learned, and future directions" data-keywords="robot control cs.se cs.ro" data-themes="M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ CS.SE</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2305.11902v1" target="_blank">Assurance for Autonomy -- JPL&#x27;s past research, lessons learned, and future directions</a>
                            </h3>
                            <p class="card-authors">Martin S. Feather, Alessandro Pinto</p>
                            <div class="theme-tags"><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Control</span><span class="keyword-tag">CS.SE</span><span class="keyword-tag">CS.RO</span></div>

                            <div class="card-details" id="details-4398404880">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Robotic space missions have long depended on automation, defined in the 2015 NASA Technology Roadmaps as &quot;the automatically-controlled operation of an apparatus, process, or system using a pre-planned set of instructions (e.g., a command sequence),&quot; to react to events when a rapid response is required. Autonomy, defined there as &quot;the capacity of a system to achieve goals while operating independently from external control,&quot; is required when a wide variation in circumstances precludes responses being pre-planned, instead autonomy follows an on-board deliberative process to determine the situati...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">This paper summarizes over two decades of this research, and offers a vision of where further work is needed to address open issues.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">To remedy this situation, researchers in JPL&#x27;s software assurance group have been involved in the development of techniques specific to the assurance of autonomy</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2305.11902v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2305.11902v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. S. Feather, and A. Pinto, &quot;Assurance for Autonomy -- JPL&#x27;s past research, lessons learned, and future directions,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2305.11902v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398404880')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cv" data-title="lvlm-ehub: a comprehensive evaluation benchmark for large vision-language models" data-keywords="gpt language model cs.cv cs.ai" data-themes="M E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computer Vision</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2306.09265v1" target="_blank">LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models</a>
                            </h3>
                            <p class="card-authors">Peng Xu, Wenqi Shao, Kaipeng Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4397314352">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Vision-Language Models (LVLMs) have recently played a dominant role in multimodal vision-language learning. Despite the great success, it lacks a holistic evaluation of their efficacy. This paper presents a comprehensive evaluation of publicly available large multimodal models by building a LVLM evaluation Hub (LVLM-eHub). Our LVLM-eHub consists of $8$ representative LVLMs such as InstructBLIP and MiniGPT-4, which are thoroughly evaluated by a quantitative capability evaluation and an online arena platform. The former evaluates $6$ categories of multimodal capabilities of LVLMs such as v...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents a comprehensive evaluation of publicly available large multimodal models by building a LVLM evaluation Hub (LVLM-eHub)</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Second, instruction-tuned LVLM with moderate instruction-following data may result in object hallucination issues (i.e., generate objects that are inconsistent with target images in the descriptions). Third, employing a multi-turn reasoning evaluation framework can mitigate the issue of object hallucination, shedding light on developing an effective pipeline for LVLM evaluation</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large Vision-Language Models (LVLMs) have recently played a dominant role in multimodal vision-language learning</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2306.09265v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2306.09265v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Xu et al., &quot;LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2306.09265v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4397314352')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="wizardcoder: empowering code large language models with evol-instruct" data-keywords="nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2306.08568v2" target="_blank">WizardCoder: Empowering Code Large Language Models with Evol-Instruct</a>
                            </h3>
                            <p class="card-authors">Ziyang Luo, Can Xu, Pu Zhao et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4398404928">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2306.08568v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2306.08568v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Luo et al., &quot;WizardCoder: Empowering Code Large Language Models with Evol-Instruct,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2306.08568v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398404928')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cv" data-title="mme: a comprehensive evaluation benchmark for multimodal large language models" data-keywords="perception optimization language model cs.cv" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computer Vision</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2306.13394v5" target="_blank">MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models</a>
                            </h3>
                            <p class="card-authors">Chaoyou Fu, Peixian Chen, Yunhang Shen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Perception</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span></div>

                            <div class="card-details" id="details-4398403920">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image. However, it is difficult for these case studies to fully reflect the performance of MLLM, lacking a comprehensive evaluation. In this paper, we fill in this blank, presenting the first comprehensive MLLM Evaluation benchmark MME. It measures both perception and cognition abilities on a total of 14 subtasks. In order to avoid data leakage that may arise from direct use of public datasets for evaluation, the ...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, it is difficult for these case studies to fully reflect the performance of MLLM, lacking a comprehensive evaluation</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2306.13394v5" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2306.13394v5" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Fu et al., &quot;MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2306.13394v5')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398403920')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="learning and autonomy for extraterrestrial terrain sampling: an experience report from owlat deployment" data-keywords="control cs.ro" data-themes="S I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2311.17405v2" target="_blank">Learning and Autonomy for Extraterrestrial Terrain Sampling: An Experience Report from OWLAT Deployment</a>
                            </h3>
                            <p class="card-authors">Pranay Thangeda, Ashish Goel, Erica Tevere et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Control</span><span class="keyword-tag">CS.RO</span></div>

                            <div class="card-details" id="details-4398406896">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Extraterrestrial autonomous lander missions increasingly demand adaptive capabilities to handle the unpredictable and diverse nature of the terrain. This paper discusses the deployment of a Deep Meta-Learning with Controlled Deployment Gaps (CoDeGa) trained model for terrain scooping tasks in Ocean Worlds Lander Autonomy Testbed (OWLAT) at NASA Jet Propulsion Laboratory. The CoDeGa-powered scooping strategy is designed to adapt to novel terrains, selecting scooping actions based on the available RGB-D image data and limited experience. The paper presents our experiences with transferring the s...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">This paper discusses the deployment of a Deep Meta-Learning with Controlled Deployment Gaps (CoDeGa) trained model for terrain scooping tasks in Ocean Worlds Lander Autonomy Testbed (OWLAT) at NASA Jet Propulsion Laboratory</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper discusses the deployment of a Deep Meta-Learning with Controlled Deployment Gaps (CoDeGa) trained model for terrain scooping tasks in Ocean Worlds Lander Autonomy Testbed (OWLAT) at NASA Jet Propulsion Laboratory</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2311.17405v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2311.17405v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Thangeda et al., &quot;Learning and Autonomy for Extraterrestrial Terrain Sampling: An Experience Report from OWLAT Deployment,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2311.17405v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398406896')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="turkish native language identification v2" data-keywords="cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2307.14850v6" target="_blank">Turkish Native Language Identification V2</a>
                            </h3>
                            <p class="card-authors">Ahmet Yavuz Uluslu, Gerold Schneider</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398404784">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents the first application of Native Language Identification (NLI) for the Turkish language. NLI is the task of automatically identifying an individual&#x27;s native language (L1) based on their writing or speech in a non-native language (L2). While most NLI research has focused on L2 English, our study extends this scope to L2 Turkish by analyzing a corpus of texts written by native speakers of Albanian, Arabic and Persian. We leverage a cleaned version of the Turkish Learner Corpus and demonstrate the effectiveness of syntactic features, comparing a structural Part-of-Speech n-gram...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents the first application of Native Language Identification (NLI) for the Turkish language</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We leverage a cleaned version of the Turkish Learner Corpus and demonstrate the effectiveness of syntactic features, comparing a structural Part-of-Speech n-gram model to a hybrid model that retains function words</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2307.14850v6" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2307.14850v6" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Y. Uluslu, and G. Schneider, &quot;Turkish Native Language Identification V2,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2307.14850v6')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398404784')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="a comprehensive review of state-of-the-art methods for java code generation from natural language text" data-keywords="deep learning transformer rnn nlp cs.cl" data-themes="I M E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2306.06371v1" target="_blank">A Comprehensive Review of State-of-The-Art Methods for Java Code Generation from Natural Language Text</a>
                            </h3>
                            <p class="card-authors">Jessica L√≥pez Espejel, Mahaman Sanoussi Yahaya Alassan, El Mehdi Chouham et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Deep Learning</span><span class="keyword-tag">Transformer</span><span class="keyword-tag">Rnn</span><span class="keyword-tag">Nlp</span></div>

                            <div class="card-details" id="details-4398405744">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Java Code Generation consists in generating automatically Java code from a Natural Language Text. This NLP task helps in increasing programmers&#x27; productivity by providing them with immediate solutions to the simplest and most repetitive tasks. Code generation is a challenging task because of the hard syntactic rules and the necessity of a deep understanding of the semantic aspect of the programming language. Many works tried to tackle this task using either RNN-based, or Transformer-based models. The latter achieved remarkable advancement in the domain and they can be divided into three groups...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">We focus on the most important methods and present their merits and limitations, as well as the objective functions used by the community</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Many works tried to tackle this task using either RNN-based, or Transformer-based models</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2306.06371v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2306.06371v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. L. Espejel, M. S. Y. Alassan, E. M. Chouham, W. Dahhane, and E. H. Ettifouri, &quot;A Comprehensive Review of State-of-The-Art Methods for Java Code Generation from Natural Language Text,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2306.06371v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398405744')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="using large language models for knowledge engineering (llmke): a case study on wikidata" data-keywords="ros language model cs.cl cs.ai" data-themes="M E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.08491v1" target="_blank">Using Large Language Models for Knowledge Engineering (LLMKE): A Case Study on Wikidata</a>
                            </h3>
                            <p class="card-authors">Bohui Zhang, Ioannis Reklos, Nitisha Jain et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4398404976">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In this work, we explore the use of Large Language Models (LLMs) for knowledge engineering tasks in the context of the ISWC 2023 LM-KBC Challenge. For this task, given subject and relation pairs sourced from Wikidata, we utilize pre-trained LLMs to produce the relevant objects in string format and link them to their respective Wikidata QIDs. We developed a pipeline using LLMs for Knowledge Engineering (LLMKE), combining knowledge probing and Wikidata entity mapping. The method achieved a macro-averaged F1-score of 0.701 across the properties, with the scores varying from 1.00 to 0.328. These r...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We developed a pipeline using LLMs for Knowledge Engineering (LLMKE), combining knowledge probing and Wikidata entity mapping</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">In this work, we explore the use of Large Language Models (LLMs) for knowledge engineering tasks in the context of the ISWC 2023 LM-KBC Challenge. LLMKE won Track 2 of the challenge</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In this work, we explore the use of Large Language Models (LLMs) for knowledge engineering tasks in the context of the ISWC 2023 LM-KBC Challenge</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.08491v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.08491v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. Zhang, I. Reklos, N. Jain, A. M. Pe√±uela, and E. Simperl, &quot;Using Large Language Models for Knowledge Engineering (LLMKE): A Case Study on Wikidata,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.08491v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398404976')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="jais and jais-chat: arabic-centric foundation and instruction-tuned open generative large language models" data-keywords="gpt language model cs.cl cs.ai cs.lg" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2308.16149v2" target="_blank">Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models</a>
                            </h3>
                            <p class="card-authors">Neha Sengupta, Sunil Kumar Sahu, Bokang Jia et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4398405600">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-ce...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs)</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2308.16149v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2308.16149v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. Sengupta et al., &quot;Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2308.16149v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398405600')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="real-world humanoid locomotion with reinforcement learning" data-keywords="reinforcement learning transformer robot humanoid locomotion control simulation imu cs.ro cs.lg" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2303.03381v2" target="_blank">Real-World Humanoid Locomotion with Reinforcement Learning</a>
                            </h3>
                            <p class="card-authors">Ilija Radosavovic, Tete Xiao, Bike Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Transformer</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span></div>

                            <div class="card-details" id="details-4399335120">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots that can autonomously operate in diverse environments have the potential to help address labour shortages in factories, assist elderly at homes, and colonize new planets. While classical controllers for humanoid robots have shown impressive results in a number of settings, they are challenging to generalize and adapt to new environments. Here, we present a fully learning-based approach for real-world humanoid locomotion. Our controller is a causal transformer that takes the history of proprioceptive observations and actions as input and predicts the next action. We hypothesize ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Here, we present a fully learning-based approach for real-world humanoid locomotion</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Here, we present a fully learning-based approach for real-world humanoid locomotion</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2303.03381v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2303.03381v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'I. Radosavovic, T. Xiao, B. Zhang, T. Darrell, J. Malik, and K. Sreenath, &quot;Real-World Humanoid Locomotion with Reinforcement Learning,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2303.03381v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399335120')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="how to raise a robot -- a case for neuro-symbolic ai in constrained task planning for humanoid assistive robots" data-keywords="neural network robot humanoid planning control language model cs.ro cs.cr cs.lg" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.08820v3" target="_blank">How to Raise a Robot -- A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots</a>
                            </h3>
                            <p class="card-authors">Niklas Hemken, Florian Jacob, Fabian Peller-Konrad et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Planning</span></div>

                            <div class="card-details" id="details-4399336032">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots will be able to assist humans in their daily life, in particular due to their versatile action capabilities. However, while these robots need a certain degree of autonomy to learn and explore, they also should respect various constraints, for access control and beyond. We explore the novel field of incorporating privacy, security, and access control constraints with robot task planning approaches. We report preliminary results on the classical symbolic approach, deep-learned neural networks, and modern ideas using large language models as knowledge base. From analyzing their tr...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We explore the novel field of incorporating privacy, security, and access control constraints with robot task planning approaches</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.08820v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.08820v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. Hemken, F. Jacob, F. Peller-Konrad, R. Kartmann, T. Asfour, and H. Hartenstein, &quot;How to Raise a Robot -- A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.08820v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399336032')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="template model inspired task space learning for robust bipedal locomotion" data-keywords="reinforcement learning robot humanoid bipedal locomotion control cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.15442v1" target="_blank">Template Model Inspired Task Space Learning for Robust Bipedal Locomotion</a>
                            </h3>
                            <p class="card-authors">Guillermo A. Castillo, Bowen Weng, Shunpeng Yang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span></div>

                            <div class="card-details" id="details-4399337184">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This work presents a hierarchical framework for bipedal locomotion that combines a Reinforcement Learning (RL)-based high-level (HL) planner policy for the online generation of task space commands with a model-based low-level (LL) controller to track the desired task space trajectories. Different from traditional end-to-end learning approaches, our HL policy takes insights from the angular momentum-based linear inverted pendulum (ALIP) to carefully design the observation and action spaces of the Markov Decision Process (MDP). This simple yet effective design creates an insightful mapping betwe...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This work presents a hierarchical framework for bipedal locomotion that combines a Reinforcement Learning (RL)-based high-level (HL) planner policy for the online generation of task space commands with a model-based low-level (LL) controller to track the desired task space trajectories</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.15442v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.15442v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. A. Castillo, B. Weng, S. Yang, W. Zhang, and A. Hereid, &quot;Template Model Inspired Task Space Learning for Robust Bipedal Locomotion,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.15442v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399337184')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="learning bipedal walking for humanoids with current feedback" data-keywords="reinforcement learning robot humanoid bipedal locomotion control simulation imu cs.ro cs.lg" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2303.03724v2" target="_blank">Learning Bipedal Walking for Humanoids with Current Feedback</a>
                            </h3>
                            <p class="card-authors">Rohan Pratap Singh, Zhaoming Xie, Pierre Gergondet et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span></div>

                            <div class="card-details" id="details-4399347984">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recent advances in deep reinforcement learning (RL) based techniques combined with training in simulation have offered a new approach to developing robust controllers for legged robots. However, the application of such approaches to real hardware has largely been limited to quadrupedal robots with direct-drive actuators and light-weight bipedal robots with low gear-ratio transmission systems. Application to real, life-sized humanoid robots has been less common arguably due to a large sim2real gap. In this paper, we present an approach for effectively overcoming the sim2real gap issue for human...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we present an approach for effectively overcoming the sim2real gap issue for humanoid robots arising from inaccurate torque-tracking at the actuator level</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Application to real, life-sized humanoid robots has been less common arguably due to a large sim2real gap. In this paper, we present an approach for effectively overcoming the sim2real gap issue for humanoid robots arising from inaccurate torque-tracking at the actuator level</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Recent advances in deep reinforcement learning (RL) based techniques combined with training in simulation have offered a new approach to developing robust controllers for legged robots</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2303.03724v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2303.03724v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. P. Singh, Z. Xie, P. Gergondet, and F. Kanehiro, &quot;Learning Bipedal Walking for Humanoids with Current Feedback,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2303.03724v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399347984')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="humanmimic: learning natural locomotion and transitions for humanoid robot via wasserstein adversarial imitation" data-keywords="reinforcement learning robot humanoid locomotion control imu imitation learning cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.14225v4" target="_blank">HumanMimic: Learning Natural Locomotion and Transitions for Humanoid Robot via Wasserstein Adversarial Imitation</a>
                            </h3>
                            <p class="card-authors">Annan Tang, Takuma Hiraoka, Naoki Hiraoka et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399336368">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Transferring human motion skills to humanoid robots remains a significant challenge. In this study, we introduce a Wasserstein adversarial imitation learning system, allowing humanoid robots to replicate natural whole-body locomotion patterns and execute seamless transitions by mimicking human motions. First, we present a unified primitive-skeleton motion retargeting to mitigate morphological differences between arbitrary human demonstrators and humanoid robots. An adversarial critic component is integrated with Reinforcement Learning (RL) to guide the control policy to produce behaviors align...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this study, we introduce a Wasserstein adversarial imitation learning system, allowing humanoid robots to replicate natural whole-body locomotion patterns and execute seamless transitions by mimicking human motions</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Transferring human motion skills to humanoid robots remains a significant challenge</p></div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.14225v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.14225v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Tang et al., &quot;HumanMimic: Learning Natural Locomotion and Transitions for Humanoid Robot via Wasserstein Adversarial Imitation,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.14225v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399336368')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="socially acceptable bipedal navigation: a signal-temporal-logic- driven approach for safe locomotion" data-keywords="robot bipedal locomotion navigation control simulation imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2310.09969v1" target="_blank">Socially Acceptable Bipedal Navigation: A Signal-Temporal-Logic- Driven Approach for Safe Locomotion</a>
                            </h3>
                            <p class="card-authors">Abdulaziz Shamsah, Ye Zhao</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Navigation</span></div>

                            <div class="card-details" id="details-4399348128">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Social navigation for bipedal robots remains relatively unexplored due to the highly complex, nonlinear dynamics of bipedal locomotion. This study presents a preliminary exploration of social navigation for bipedal robots in a human crowded environment. We propose a social path planner that ensures the locomotion safety of the bipedal robot while navigating under a social norm. The proposed planner leverages a conditional variational autoencoder architecture and learns from human crowd datasets to produce a socially acceptable path plan. Robot-specific locomotion safety is formally enforced by...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a social path planner that ensures the locomotion safety of the bipedal robot while navigating under a social norm</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The proposed planner leverages a conditional variational autoencoder architecture and learns from human crowd datasets to produce a socially acceptable path plan</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2310.09969v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2310.09969v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Shamsah, and Y. Zhao, &quot;Socially Acceptable Bipedal Navigation: A Signal-Temporal-Logic- Driven Approach for Safe Locomotion,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2310.09969v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399348128')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="learning vision-based bipedal locomotion for challenging terrain" data-keywords="reinforcement learning robot bipedal locomotion perception control simulation imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.14594v2" target="_blank">Learning Vision-Based Bipedal Locomotion for Challenging Terrain</a>
                            </h3>
                            <p class="card-authors">Helei Duan, Bikram Pandit, Mohitvishnu S. Gadde et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399348560">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Reinforcement learning (RL) for bipedal locomotion has recently demonstrated robust gaits over moderate terrains using only proprioceptive sensing. However, such blind controllers will fail in environments where robots must anticipate and adapt to local terrain, which requires visual perception. In this paper, we propose a fully-learned system that allows bipedal robots to react to local terrain while maintaining commanded travel speed and direction. Our approach first trains a controller in simulation using a heightmap expressed in the robot&#x27;s local frame. Next, data is collected in simulatio...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose a fully-learned system that allows bipedal robots to react to local terrain while maintaining commanded travel speed and direction</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Our approach first trains a controller in simulation using a heightmap expressed in the robot&#x27;s local frame</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.14594v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.14594v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Duan et al., &quot;Learning Vision-Based Bipedal Locomotion for Challenging Terrain,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.14594v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399348560')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="centroidal state estimation and control for hardware-constrained humanoid robots" data-keywords="robot humanoid control cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.11019v1" target="_blank">Centroidal State Estimation and Control for Hardware-constrained Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Grzegorz Ficht, Sven Behnke</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Control</span><span class="keyword-tag">CS.RO</span></div>

                            <div class="card-details" id="details-4399347840">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We introduce novel methods for state estimation, feedforward and feedback control, which specifically target humanoid robots with hardware limitations. Our method combines a five-mass model with approximate dynamics of each mass. It enables acquiring an accurate assessment of the centroidal state and Center of Pressure, even when direct forms of force or contact sensing are unavailable. Upon this, we develop a feedforward scheme that operates on the centroidal state, accounting for insufficient joint tracking capabilities. Finally, we implement feedback mechanisms, which compensate for the lac...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce novel methods for state estimation, feedforward and feedback control, which specifically target humanoid robots with hardware limitations</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">We introduce novel methods for state estimation, feedforward and feedback control, which specifically target humanoid robots with hardware limitations. The whole approach allows for reactive stepping to maintain balance despite these limitations, which was verified on hardware during RoboCup 2023, in Bordeaux, France.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We introduce novel methods for state estimation, feedforward and feedback control, which specifically target humanoid robots with hardware limitations</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.11019v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.11019v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Ficht, and S. Behnke, &quot;Centroidal State Estimation and Control for Hardware-constrained Humanoid Robots,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.11019v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399347840')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="infer and adapt: bipedal locomotion reward learning from demonstrations via inverse reinforcement learning" data-keywords="reinforcement learning robot bipedal locomotion imitation learning cs.ro cs.lg" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.16074v1" target="_blank">Infer and Adapt: Bipedal Locomotion Reward Learning from Demonstrations via Inverse Reinforcement Learning</a>
                            </h3>
                            <p class="card-authors">Feiyang Wu, Zhaoyuan Gu, Hanran Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4399336944">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Enabling bipedal walking robots to learn how to maneuver over highly uneven, dynamically changing terrains is challenging due to the complexity of robot dynamics and interacted environments. Recent advancements in learning from demonstrations have shown promising results for robot learning in complex environments. While imitation learning of expert policies has been well-explored, the study of learning expert reward functions is largely under-explored in legged locomotion. This paper brings state-of-the-art Inverse Reinforcement Learning (IRL) techniques to solving bipedal locomotion problems ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose algorithms for learning expert reward functions, and we subsequently analyze the learned functions</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">This paper brings state-of-the-art Inverse Reinforcement Learning (IRL) techniques to solving bipedal locomotion problems over complex terrains</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper brings state-of-the-art Inverse Reinforcement Learning (IRL) techniques to solving bipedal locomotion problems over complex terrains</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.16074v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.16074v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. Wu, Z. Gu, H. Wu, A. Wu, and Y. Zhao, &quot;Infer and Adapt: Bipedal Locomotion Reward Learning from Demonstrations via Inverse Reinforcement Learning,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.16074v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399336944')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="design and control of a small humanoid equipped with flight unit and wheels for multimodal locomotion" data-keywords="robot humanoid locomotion manipulation control imu cs.ro" data-themes="I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2303.14718v3" target="_blank">Design and Control of a Small Humanoid Equipped with Flight Unit and Wheels for Multimodal Locomotion</a>
                            </h3>
                            <p class="card-authors">Kazuki Sugihara, Moju Zhao, Takuzumi Nishio et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Manipulation</span></div>

                            <div class="card-details" id="details-4399348032">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoids are versatile robotic platforms owing to their limbs with multiple degrees of freedom. Although humanoids can walk like humans, they are relatively slow, and cannot run over large barriers. To address these limitations, we aim to achieve rapid terrestrial locomotion ability and simultaneously expand the locomotion domain to the air by utilizing thrust for propulsion. In this paper, we first describe an optimized construction method for a humanoid robot equipped with wheels and a flight unit to achieve these abilities. Then, we describe the integrated control framework of the proposed...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To address these limitations, we aim to achieve rapid terrestrial locomotion ability and simultaneously expand the locomotion domain to the air by utilizing thrust for propulsion</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In this paper, we first describe an optimized construction method for a humanoid robot equipped with wheels and a flight unit to achieve these abilities</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2303.14718v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2303.14718v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Sugihara, M. Zhao, T. Nishio, T. Makabe, K. Okada, and M. Inaba, &quot;Design and Control of a Small Humanoid Equipped with Flight Unit and Wheels for Multimodal Locomotion,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2303.14718v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399348032')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="learning robust, agile, natural legged locomotion skills in the wild" data-keywords="reinforcement learning robot locomotion perception control simulation imu cs.ro cs.ai" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2304.10888v3" target="_blank">Learning Robust, Agile, Natural Legged Locomotion Skills in the Wild</a>
                            </h3>
                            <p class="card-authors">Yikai Wang, Zheyuan Jiang, Jianyu Chen</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Perception</span></div>

                            <div class="card-details" id="details-4399338000">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recently, reinforcement learning has become a promising and polular solution for robot legged locomotion. Compared to model-based control, reinforcement learning based controllers can achieve better robustness against uncertainties of environments through sim-to-real learning. However, the corresponding learned gaits are in general overly conservative and unatural. In this paper, we propose a new framework for learning robust, agile and natural legged locomotion skills over challenging terrain. We incorporate an adversarial training branch based on real animal locomotion data upon a teacher-st...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose a new framework for learning robust, agile and natural legged locomotion skills over challenging terrain</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Compared to model-based control, reinforcement learning based controllers can achieve better robustness against uncertainties of environments through sim-to-real learning</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2304.10888v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2304.10888v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Wang, Z. Jiang, and J. Chen, &quot;Learning Robust, Agile, Natural Legged Locomotion Skills in the Wild,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2304.10888v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399338000')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="signal temporal logic-guided model predictive control for robust bipedal locomotion resilient to runtime external perturbations" data-keywords="robot bipedal locomotion planning control optimization simulation ros imu cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2310.11290v1" target="_blank">Signal Temporal Logic-Guided Model Predictive Control for Robust Bipedal Locomotion Resilient to Runtime External Perturbations</a>
                            </h3>
                            <p class="card-authors">Zhaoyuan Gu, Rongming Guo, William Yates et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Planning</span></div>

                            <div class="card-details" id="details-4399348272">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This study investigates formal-method-based trajectory optimization (TO) for bipedal locomotion, focusing on scenarios where the robot encounters external perturbations at unforeseen times. Our key research question centers around the assurance of task specification correctness and the maximization of specification robustness for a bipedal robot in the presence of external perturbations.
  Our contribution includes the design of an optimization-based task and motion planning framework that generates optimal control sequences with formal guarantees of external perturbation recovery. As a core c...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Our key research question centers around the assurance of task specification correctness and the maximization of specification robustness for a bipedal robot in the presence of external perturbations.
  Our contribution includes the design of an optimization-based task and motion planning framework that generates optimal control sequences with formal guarantees of external perturbation recovery</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This study investigates formal-method-based trajectory optimization (TO) for bipedal locomotion, focusing on scenarios where the robot encounters external perturbations at unforeseen times</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2310.11290v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2310.11290v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Gu, R. Guo, W. Yates, Y. Chen, and Y. Zhao, &quot;Signal Temporal Logic-Guided Model Predictive Control for Robust Bipedal Locomotion Resilient to Runtime External Perturbations,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2310.11290v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399348272')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="incremental learning of humanoid robot behavior from natural interaction and large language models" data-keywords="robot humanoid perception simulation imu language model cs.ro cs.ai" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.04316v3" target="_blank">Incremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models</a>
                            </h3>
                            <p class="card-authors">Leonard B√§rmann, Rainer Kartmann, Fabian Peller-Konrad et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Perception</span><span class="keyword-tag">Simulation</span></div>

                            <div class="card-details" id="details-4399348800">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Natural-language dialog is key for intuitive human-robot interaction. It can be used not only to express humans&#x27; intents, but also to communicate instructions for improvement if a robot does not understand a command correctly. Of great importance is to endow robots with the ability to learn from such interaction experience in an incremental way to allow them to improve their behaviors or avoid mistakes in the future. In this paper, we propose a system to achieve incremental learning of complex behavior from natural interaction, and demonstrate its implementation on a humanoid robot. Building o...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose a system to achieve incremental learning of complex behavior from natural interaction, and demonstrate its implementation on a humanoid robot</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Building on recent advances, we present a system that deploys Large Language Models (LLMs) for high-level orchestration of the robot&#x27;s behavior, based on the idea of enabling the LLM to generate Python statements in an interactive console to invoke both robot perception and action</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.04316v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.04316v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. B√§rmann, R. Kartmann, F. Peller-Konrad, J. Niehues, A. Waibel, and T. Asfour, &quot;Incremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.04316v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399348800')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="proprioceptive external torque learning for floating base robot and its applications to humanoid locomotion" data-keywords="robot humanoid locomotion control imu cs.ro cs.ai" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.04138v1" target="_blank">Proprioceptive External Torque Learning for Floating Base Robot and its Applications to Humanoid Locomotion</a>
                            </h3>
                            <p class="card-authors">Daegyu Lim, Myeong-Ju Kim, Junhyeok Cha et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Control</span></div>

                            <div class="card-details" id="details-4399349712">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The estimation of external joint torque and contact wrench is essential for achieving stable locomotion of humanoids and safety-oriented robots. Although the contact wrench on the foot of humanoids can be measured using a force-torque sensor (FTS), FTS increases the cost, inertia, complexity, and failure possibility of the system. This paper introduces a method for learning external joint torque solely using proprioceptive sensors (encoders and IMUs) for a floating base robot. For learning, the GRU network is used and random walking data is collected. Real robot experiments demonstrate that th...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper introduces a method for learning external joint torque solely using proprioceptive sensors (encoders and IMUs) for a floating base robot</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.04138v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.04138v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Lim, M. Kim, J. Cha, D. Kim, and J. Park, &quot;Proprioceptive External Torque Learning for Floating Base Robot and its Applications to Humanoid Locomotion,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.04138v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399349712')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="learning perceptive bipedal locomotion over irregular terrain" data-keywords="attention bipedal locomotion control cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2304.07236v1" target="_blank">Learning Perceptive Bipedal Locomotion over Irregular Terrain</a>
                            </h3>
                            <p class="card-authors">Bart van Marum, Matthia Sabatelli, Hamidreza Kasaei</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Control</span></div>

                            <div class="card-details" id="details-4399336512">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In this paper we propose a novel bipedal locomotion controller that uses noisy exteroception to traverse a wide variety of terrains. Building on the cutting-edge advancements in attention based belief encoding for quadrupedal locomotion, our work extends these methods to the bipedal domain, resulting in a robust and reliable internal belief of the terrain ahead despite noisy sensor inputs. Additionally, we present a reward function that allows the controller to successfully traverse irregular terrain. We compare our method with a proprioceptive baseline and show that our method is able to trav...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper we propose a novel bipedal locomotion controller that uses noisy exteroception to traverse a wide variety of terrains</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Building on the cutting-edge advancements in attention based belief encoding for quadrupedal locomotion, our work extends these methods to the bipedal domain, resulting in a robust and reliable internal belief of the terrain ahead despite noisy sensor inputs</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2304.07236v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2304.07236v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. v. Marum, M. Sabatelli, and H. Kasaei, &quot;Learning Perceptive Bipedal Locomotion over Irregular Terrain,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2304.07236v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399336512')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="advantages of multimodal versus verbal-only robot-to-human communication with an anthropomorphic robotic mock driver" data-keywords="attention robot humanoid cs.ro" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2307.00841v1" target="_blank">Advantages of Multimodal versus Verbal-Only Robot-to-Human Communication with an Anthropomorphic Robotic Mock Driver</a>
                            </h3>
                            <p class="card-authors">Tim Schreiter, Lucas Morillo-Mendez, Ravi T. Chadalavada et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">CS.RO</span></div>

                            <div class="card-details" id="details-4399347792">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Robots are increasingly used in shared environments with humans, making effective communication a necessity for successful human-robot interaction. In our work, we study a crucial component: active communication of robot intent. Here, we present an anthropomorphic solution where a humanoid robot communicates the intent of its host robot acting as an &quot;Anthropomorphic Robotic Mock Driver&quot; (ARMoD). We evaluate this approach in two experiments in which participants work alongside a mobile robot on various tasks, while the ARMoD communicates a need for human attention, when required, or gives instr...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Here, we present an anthropomorphic solution where a humanoid robot communicates the intent of its host robot acting as an &quot;Anthropomorphic Robotic Mock Driver&quot; (ARMoD)</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We evaluate this approach in two experiments in which participants work alongside a mobile robot on various tasks, while the ARMoD communicates a need for human attention, when required, or gives instructions to collaborate on a joint task</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2307.00841v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2307.00841v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. Schreiter et al., &quot;Advantages of Multimodal versus Verbal-Only Robot-to-Human Communication with an Anthropomorphic Robotic Mock Driver,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2307.00841v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399347792')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="data-driven latent space representation for robust bipedal locomotion learning" data-keywords="reinforcement learning bipedal locomotion planning simulation imu cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.15740v1" target="_blank">Data-Driven Latent Space Representation for Robust Bipedal Locomotion Learning</a>
                            </h3>
                            <p class="card-authors">Guillermo A. Castillo, Bowen Weng, Wei Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Planning</span></div>

                            <div class="card-details" id="details-4399348368">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents a novel framework for learning robust bipedal walking by combining a data-driven state representation with a Reinforcement Learning (RL) based locomotion policy. The framework utilizes an autoencoder to learn a low-dimensional latent space that captures the complex dynamics of bipedal locomotion from existing locomotion data. This reduced dimensional state representation is then used as states for training a robust RL-based gait policy, eliminating the need for heuristic state selections or the use of template models for gait planning. The results demonstrate that the learn...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents a novel framework for learning robust bipedal walking by combining a data-driven state representation with a Reinforcement Learning (RL) based locomotion policy</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper presents a novel framework for learning robust bipedal walking by combining a data-driven state representation with a Reinforcement Learning (RL) based locomotion policy</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.15740v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.15740v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. A. Castillo, B. Weng, W. Zhang, and A. Hereid, &quot;Data-Driven Latent Space Representation for Robust Bipedal Locomotion Learning,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.15740v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399348368')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="learning agile bipedal motions on a quadrupedal robot" data-keywords="robot bipedal control cs.ro" data-themes="I E L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2311.05818v2" target="_blank">Learning Agile Bipedal Motions on a Quadrupedal Robot</a>
                            </h3>
                            <p class="card-authors">Yunfei Li, Jinhan Li, Wei Fu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Control</span><span class="keyword-tag">CS.RO</span></div>

                            <div class="card-details" id="details-4399335744">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Can a quadrupedal robot perform bipedal motions like humans? Although developing human-like behaviors is more often studied on costly bipedal robot platforms, we present a solution over a lightweight quadrupedal robot that unlocks the agility of the quadruped in an upright standing pose and is capable of a variety of human-like motions. Our framework is with a hierarchical structure. At the low level is a motion-conditioned control policy that allows the quadrupedal robot to track desired base and front limb movements while balancing on two hind feet. The policy is commanded by a high-level mo...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Can a quadrupedal robot perform bipedal motions like humans? Although developing human-like behaviors is more often studied on costly bipedal robot platforms, we present a solution over a lightweight quadrupedal robot that unlocks the agility of the quadruped in an upright standing pose and is capable of a variety of human-like motions</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Our framework is with a hierarchical structure</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2311.05818v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2311.05818v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Li, J. Li, W. Fu, and Y. Wu, &quot;Learning Agile Bipedal Motions on a Quadrupedal Robot,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2311.05818v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4399335744')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="humanoid robot co-design: coupling hardware design with gait generation via hybrid zero dynamics" data-keywords="robot humanoid bipedal locomotion control optimization imu cs.ro math.oc" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2308.10962v1" target="_blank">Humanoid Robot Co-Design: Coupling Hardware Design with Gait Generation via Hybrid Zero Dynamics</a>
                            </h3>
                            <p class="card-authors">Adrian B. Ghansah, Jeeseop Kim, Maegan Tucker et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4398443840">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Selecting robot design parameters can be challenging since these parameters are often coupled with the performance of the controller and, therefore, the resulting capabilities of the robot. This leads to a time-consuming and often expensive process whereby one iterates between designing the robot and manually evaluating its capabilities. This is particularly challenging for bipedal robots, where it can be difficult to evaluate the behavior of the system due to the underlying nonlinear and hybrid dynamics. Thus, in an effort to streamline the design process of bipedal robots, and maximize their...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Thus, in an effort to streamline the design process of bipedal robots, and maximize their performance, this paper presents a systematic framework for the co-design of humanoid robots and their associated walking gaits</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">This is particularly challenging for bipedal robots, where it can be difficult to evaluate the behavior of the system due to the underlying nonlinear and hybrid dynamics. These virtual constraints are combined in an HZD optimization problem which simultaneously determines the design parameters while finding a stable walking gait that minimizes a given cost function</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Thus, in an effort to streamline the design process of bipedal robots, and maximize their performance, this paper presents a systematic framework for the co-design of humanoid robots and their associated walking gaits</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2308.10962v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2308.10962v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. B. Ghansah, J. Kim, M. Tucker, and A. D. Ames, &quot;Humanoid Robot Co-Design: Coupling Hardware Design with Gait Generation via Hybrid Zero Dynamics,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2308.10962v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398443840')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="walking-by-logic: signal temporal logic-guided model predictive control for bipedal locomotion resilient to external perturbations" data-keywords="robot bipedal locomotion planning control optimization simulation ros imu cs.ro" data-themes="I E L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.13172v1" target="_blank">Walking-by-Logic: Signal Temporal Logic-Guided Model Predictive Control for Bipedal Locomotion Resilient to External Perturbations</a>
                            </h3>
                            <p class="card-authors">Zhaoyuan Gu, Rongming Guo, William Yates et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Planning</span></div>

                            <div class="card-details" id="details-4398443600">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This study proposes a novel planning framework based on a model predictive control formulation that incorporates signal temporal logic (STL) specifications for task completion guarantees and robustness quantification. This marks the first-ever study to apply STL-guided trajectory optimization for bipedal locomotion push recovery, where the robot experiences unexpected disturbances. Existing recovery strategies often struggle with complex task logic reasoning and locomotion robustness evaluation, making them susceptible to failures caused by inappropriate recovery strategies or insufficient rob...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To address this issue, the STL-guided framework generates optimal and safe recovery trajectories that simultaneously satisfy the task specification and maximize the locomotion robustness</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This study proposes a novel planning framework based on a model predictive control formulation that incorporates signal temporal logic (STL) specifications for task completion guarantees and robustness quantification</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.13172v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.13172v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Gu, R. Guo, W. Yates, Y. Chen, and Y. Zhao, &quot;Walking-by-Logic: Signal Temporal Logic-Guided Model Predictive Control for Bipedal Locomotion Resilient to External Perturbations,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.13172v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398443600')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="clipswarm: converting text into formations of robots" data-keywords="robot swarm multi-robot optimization cs.ro" data-themes="E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2311.11047v1" target="_blank">CLIPSwarm: Converting text into formations of robots</a>
                            </h3>
                            <p class="card-authors">Pablo Pueyo, Eduardo Montijano, Ana C. Murillo et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Swarm</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Optimization</span></div>

                            <div class="card-details" id="details-4398432416">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We present CLIPSwarm, an algorithm to generate robot swarm formations from natural language descriptions. CLIPSwarm receives an input text and finds the position of the robots to form a shape that corresponds to the given text. To do so, we implement a variation of the Montecarlo particle filter to obtain a matching formation iteratively. In every iteration, we generate a set of new formations and evaluate their Clip Similarity with the given text, selecting the best formations according to this metric. This metric is obtained using Clip, [1], an existing foundation model trained to encode ima...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present CLIPSwarm, an algorithm to generate robot swarm formations from natural language descriptions</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We present CLIPSwarm, an algorithm to generate robot swarm formations from natural language descriptions</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2311.11047v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2311.11047v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Pueyo, E. Montijano, A. C. Murillo, and M. Schwager, &quot;CLIPSwarm: Converting text into formations of robots,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2311.11047v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398432416')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="stoch biro: design and control of a low cost bipedal robot" data-keywords="robot bipedal locomotion navigation control mujoco imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.06512v1" target="_blank">Stoch BiRo: Design and Control of a low cost bipedal robot</a>
                            </h3>
                            <p class="card-authors">GVS Mothish, Karthik Rajgopal, Ravi Kola et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Navigation</span></div>

                            <div class="card-details" id="details-4398445232">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper introduces the Stoch BiRo, a cost-effective bipedal robot designed with a modular mechanical structure having point feet to navigate uneven and unfamiliar terrains. The robot employs proprioceptive actuation in abduction, hips, and knees, leveraging a Raspberry Pi4 for control. Overcoming computational limitations, a Learning-based Linear Policy controller manages balance and locomotion with only 3 degrees of freedom (DoF) per leg, distinct from the typical 5DoF in bipedal systems. Integrated within a modular control architecture, these controllers enable autonomous handling of unfo...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Overcoming computational limitations, a Learning-based Linear Policy controller manages balance and locomotion with only 3 degrees of freedom (DoF) per leg, distinct from the typical 5DoF in bipedal systems</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Integrated within a modular control architecture, these controllers enable autonomous handling of unforeseen terrain disturbances without external sensors or prior environment knowledge</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.06512v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.06512v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Mothish, K. Rajgopal, R. Kola, M. Tayal, and S. Kolathaya, &quot;Stoch BiRo: Design and Control of a low cost bipedal robot,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.06512v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398445232')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="learning quadrupedal locomotion on deformable terrain" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">High Impact</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85146831030&amp;origin=resultslist" target="_blank">Learning quadrupedal locomotion on deformable terrain</a>
                            </h3>
                            <p class="card-authors">Choi S.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398444128">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85146831030&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1126/scirobotics.ade2256" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. S., &quot;Learning quadrupedal locomotion on deformable terrain,&quot; Science Robotics, 2023. doi: 10.1126/scirobotics.ade2256. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85146831030&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398444128')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="vilens: visual, inertial, lidar, and leg odometry for all-terrain legged robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">High Impact</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85136130682&amp;origin=resultslist" target="_blank">VILENS: Visual, Inertial, Lidar, and Leg Odometry for All-Terrain Legged Robots</a>
                            </h3>
                            <p class="card-authors">Wisth D.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398432656">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85136130682&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2022.3193788" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. D., &quot;VILENS: Visual, Inertial, Lidar, and Leg Odometry for All-Terrain Legged Robots,&quot; IEEE Transactions on Robotics, 2023. doi: 10.1109/TRO.2022.3193788. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85136130682&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398432656')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="Scopus" data-year="2023" data-category="other" data-title="intelligent control of multilegged robot smooth motion: a review" data-keywords="" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">High Impact</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168258314&amp;origin=resultslist" target="_blank">Intelligent Control of Multilegged Robot Smooth Motion: A Review</a>
                            </h3>
                            <p class="card-authors">Zhao Y.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398444512">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168258314&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/ACCESS.2023.3304992" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Y., &quot;Intelligent Control of Multilegged Robot Smooth Motion: A Review,&quot; IEEE Access, 2023. doi: 10.1109/ACCESS.2023.3304992. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85168258314&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398444512')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="scientific exploration of challenging planetary analog environments with a team of legged robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">High Impact</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85164541236&amp;origin=resultslist" target="_blank">Scientific exploration of challenging planetary analog environments with a team of legged robots</a>
                            </h3>
                            <p class="card-authors">Arm P.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398445472">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85164541236&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1126/scirobotics.ade9548" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. P., &quot;Scientific exploration of challenging planetary analog environments with a team of legged robots,&quot; Science Robotics, 2023. doi: 10.1126/scirobotics.ade9548. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85164541236&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398445472')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="robot-assisted mobile scanning for automated 3d reconstruction and point cloud semantic segmentation of building interiors" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">High Impact</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85160205515&amp;origin=resultslist" target="_blank">Robot-assisted mobile scanning for automated 3D reconstruction and point cloud semantic segmentation of building interiors</a>
                            </h3>
                            <p class="card-authors">Hu D.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398444560">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85160205515&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1016/j.autcon.2023.104949" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. D., &quot;Robot-assisted mobile scanning for automated 3D reconstruction and point cloud semantic segmentation of building interiors,&quot; Automation in Construction, 2023. doi: 10.1016/j.autcon.2023.104949. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85160205515&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398444560')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="deep whole-body control: learning a unified policy for manipulation and locomotion" data-keywords="" data-themes="I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">High Impact</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85164921897&amp;origin=resultslist" target="_blank">Deep Whole-Body Control: Learning a Unified Policy for Manipulation and Locomotion</a>
                            </h3>
                            <p class="card-authors">Fu Z.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398433520">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85164921897&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. Z., &quot;Deep Whole-Body Control: Learning a Unified Policy for Manipulation and Locomotion,&quot; Proceedings of Machine Learning Research, 2023. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85164921897&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398433520')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="learning robust and agile legged locomotion using adversarial motion priors" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">High Impact</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85163460343&amp;origin=resultslist" target="_blank">Learning Robust and Agile Legged Locomotion Using Adversarial Motion Priors</a>
                            </h3>
                            <p class="card-authors">Wu J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398443312">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85163460343&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3290509" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. J., &quot;Learning Robust and Agile Legged Locomotion Using Adversarial Motion Priors,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3290509. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85163460343&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398443312')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="biconmp: a nonlinear model predictive control framework for whole body motion planning" data-keywords="" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">High Impact</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147306921&amp;origin=resultslist" target="_blank">BiConMP: A Nonlinear Model Predictive Control Framework for Whole Body Motion Planning</a>
                            </h3>
                            <p class="card-authors">Meduri A.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398443456">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147306921&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2022.3228390" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. A., &quot;BiConMP: A Nonlinear Model Predictive Control Framework for Whole Body Motion Planning,&quot; IEEE Transactions on Robotics, 2023. doi: 10.1109/TRO.2022.3228390. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85147306921&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398443456')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="Scopus" data-year="2023" data-category="other" data-title="m3ed: multi-robot, multi-sensor, multi-environment event dataset" data-keywords="" data-themes="R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">High Impact</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85170821836&amp;origin=resultslist" target="_blank">M3ED: Multi-Robot, Multi-Sensor, Multi-Environment Event Dataset</a>
                            </h3>
                            <p class="card-authors">Chaney K.</p>
                            <div class="theme-tags"><span class="theme-badge theme-R">Multi-Robot</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398432368">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85170821836&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/CVPRW59228.2023.00419" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. K., &quot;M3ED: Multi-Robot, Multi-Sensor, Multi-Environment Event Dataset,&quot; IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, 2023. doi: 10.1109/CVPRW59228.2023.00419. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85170821836&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398432368')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="energy-efficient hydraulic pump control for legged robots using model predictive control" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85136037715&amp;origin=resultslist" target="_blank">Energy-Efficient Hydraulic Pump Control for Legged Robots Using Model Predictive Control</a>
                            </h3>
                            <p class="card-authors">Cho B.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398445520">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85136037715&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TMECH.2022.3190506" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. B., &quot;Energy-Efficient Hydraulic Pump Control for Legged Robots Using Model Predictive Control,&quot; IEEE ASME Transactions on Mechatronics, 2023. doi: 10.1109/TMECH.2022.3190506. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85136037715&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398445520')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="Scopus" data-year="2023" data-category="other" data-title="advanced skills through multiple adversarial motion priors in reinforcement learning" data-keywords="" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168698851&amp;origin=resultslist" target="_blank">Advanced Skills through Multiple Adversarial Motion Priors in Reinforcement Learning</a>
                            </h3>
                            <p class="card-authors">Vollenweider E.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398445328">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168698851&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/ICRA48891.2023.10160751" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'V. E., &quot;Advanced Skills through Multiple Adversarial Motion Priors in Reinforcement Learning,&quot; Proceedings IEEE International Conference on Robotics and Automation, 2023. doi: 10.1109/ICRA48891.2023.10160751. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85168698851&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398445328')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="development of the lstm model and universal polynomial equation for all the sub-phases of human gait" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85161564810&amp;origin=resultslist" target="_blank">Development of the LSTM Model and Universal Polynomial Equation for All the Sub-Phases of Human Gait</a>
                            </h3>
                            <p class="card-authors">Semwal V.B.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398445616">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85161564810&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/JSEN.2023.3281401" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. V.B., &quot;Development of the LSTM Model and Universal Polynomial Equation for All the Sub-Phases of Human Gait,&quot; IEEE Sensors Journal, 2023. doi: 10.1109/JSEN.2023.3281401. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85161564810&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398445616')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="genloco: generalized locomotion controllers for quadrupedal robots" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85164969619&amp;origin=resultslist" target="_blank">GenLoco: Generalized Locomotion Controllers for Quadrupedal Robots</a>
                            </h3>
                            <p class="card-authors">Feng G.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398445712">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85164969619&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. G., &quot;GenLoco: Generalized Locomotion Controllers for Quadrupedal Robots,&quot; Proceedings of Machine Learning Research, 2023. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85164969619&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398445712')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="neural volumetric memory for visual locomotion control" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85163788700&amp;origin=resultslist" target="_blank">Neural Volumetric Memory for Visual Locomotion Control</a>
                            </h3>
                            <p class="card-authors">Yang R.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398433568">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85163788700&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/CVPR52729.2023.00144" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. R., &quot;Neural Volumetric Memory for Visual Locomotion Control,&quot; Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2023. doi: 10.1109/CVPR52729.2023.00144. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85163788700&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398433568')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="learning modular robot control policies" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85163756601&amp;origin=resultslist" target="_blank">Learning Modular Robot Control Policies</a>
                            </h3>
                            <p class="card-authors">Whitman J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398443408">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85163756601&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2023.3284362" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. J., &quot;Learning Modular Robot Control Policies,&quot; IEEE Transactions on Robotics, 2023. doi: 10.1109/TRO.2023.3284362. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85163756601&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398443408')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="efficient path planning in narrow passages for robots with ellipsoidal components" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85135748595&amp;origin=resultslist" target="_blank">Efficient Path Planning in Narrow Passages for Robots With Ellipsoidal Components</a>
                            </h3>
                            <p class="card-authors">Ruan S.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398443024">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85135748595&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2022.3187818" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. S., &quot;Efficient Path Planning in Narrow Passages for Robots With Ellipsoidal Components,&quot; IEEE Transactions on Robotics, 2023. doi: 10.1109/TRO.2022.3187818. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85135748595&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398443024')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="reward-adaptive reinforcement learning: dynamic policy gradient optimization for bipedal locomotion" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85144064950&amp;origin=resultslist" target="_blank">Reward-Adaptive Reinforcement Learning: Dynamic Policy Gradient Optimization for Bipedal Locomotion</a>
                            </h3>
                            <p class="card-authors">Huang C.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398443744">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85144064950&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TPAMI.2022.3223407" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. C., &quot;Reward-Adaptive Reinforcement Learning: Dynamic Policy Gradient Optimization for Bipedal Locomotion,&quot; IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. doi: 10.1109/TPAMI.2022.3223407. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85144064950&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398443744')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="robotic monitoring of habitats: the natural intelligence approach" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85164706103&amp;origin=resultslist" target="_blank">Robotic Monitoring of Habitats: The Natural Intelligence Approach</a>
                            </h3>
                            <p class="card-authors">Angelini F.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398433664">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85164706103&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/ACCESS.2023.3294276" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. F., &quot;Robotic Monitoring of Habitats: The Natural Intelligence Approach,&quot; IEEE Access, 2023. doi: 10.1109/ACCESS.2023.3294276. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85164706103&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398433664')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="efficient anytime clf reactive planning system for a bipedal robot on undulating terrain" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147303984&amp;origin=resultslist" target="_blank">Efficient Anytime CLF Reactive Planning System for a Bipedal Robot on Undulating Terrain</a>
                            </h3>
                            <p class="card-authors">Huang J.K.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398432800">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147303984&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2022.3228713" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. J.K., &quot;Efficient Anytime CLF Reactive Planning System for a Bipedal Robot on Undulating Terrain,&quot; IEEE Transactions on Robotics, 2023. doi: 10.1109/TRO.2022.3228713. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85147303984&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398432800')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="inverse-dynamics mpc via nullspace resolution" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85153393919&amp;origin=resultslist" target="_blank">Inverse-Dynamics MPC via Nullspace Resolution</a>
                            </h3>
                            <p class="card-authors">Mastalli C.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398432464">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85153393919&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2023.3262186" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. C., &quot;Inverse-Dynamics MPC via Nullspace Resolution,&quot; IEEE Transactions on Robotics, 2023. doi: 10.1109/TRO.2023.3262186. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85153393919&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398432464')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="cerberus: low-drift visual-inertial-leg odometry for agile locomotion" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168678256&amp;origin=resultslist" target="_blank">Cerberus: Low-Drift Visual-Inertial-Leg Odometry For Agile Locomotion</a>
                            </h3>
                            <p class="card-authors">Yang S.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398433424">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168678256&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/ICRA48891.2023.10160486" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. S., &quot;Cerberus: Low-Drift Visual-Inertial-Leg Odometry For Agile Locomotion,&quot; Proceedings IEEE International Conference on Robotics and Automation, 2023. doi: 10.1109/ICRA48891.2023.10160486. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85168678256&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398433424')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="rl + model-based control: using on-demand optimal control to learn versatile legged locomotion" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168715205&amp;origin=resultslist" target="_blank">RL + Model-Based Control: Using On-Demand Optimal Control to Learn Versatile Legged Locomotion</a>
                            </h3>
                            <p class="card-authors">Kang D.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398445424">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168715205&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3307008" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. D., &quot;RL + Model-Based Control: Using On-Demand Optimal Control to Learn Versatile Legged Locomotion,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3307008. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85168715205&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398445424')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="robust convex model predictive control for quadruped locomotion under uncertainties" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85167778962&amp;origin=resultslist" target="_blank">Robust Convex Model Predictive Control for Quadruped Locomotion Under Uncertainties</a>
                            </h3>
                            <p class="card-authors">Xu S.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398445376">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85167778962&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2023.3299527" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. S., &quot;Robust Convex Model Predictive Control for Quadruped Locomotion Under Uncertainties,&quot; IEEE Transactions on Robotics, 2023. doi: 10.1109/TRO.2023.3299527. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85167778962&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398445376')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="vital: vision-based terrain-aware locomotion for legged robots" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85144013474&amp;origin=resultslist" target="_blank">ViTAL: Vision-Based Terrain-Aware Locomotion for Legged Robots</a>
                            </h3>
                            <p class="card-authors">Fahmi S.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398443216">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85144013474&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2022.3222958" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. S., &quot;ViTAL: Vision-Based Terrain-Aware Locomotion for Legged Robots,&quot; IEEE Transactions on Robotics, 2023. doi: 10.1109/TRO.2022.3222958. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85144013474&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398443216')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="multilegged matter transport: a framework for locomotion on noisy landscapes" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85165369068&amp;origin=resultslist" target="_blank">Multilegged matter transport: A framework for locomotion on noisy landscapes</a>
                            </h3>
                            <p class="card-authors">Chong B.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398433952">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85165369068&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1126/science.ade4985" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. B., &quot;Multilegged matter transport: A framework for locomotion on noisy landscapes,&quot; Science, 2023. doi: 10.1126/science.ade4985. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85165369068&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398433952')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="hybrid ilqr model predictive control for contact implicit stabilization on legged robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85171744736&amp;origin=resultslist" target="_blank">Hybrid iLQR Model Predictive Control for Contact Implicit Stabilization on Legged Robots</a>
                            </h3>
                            <p class="card-authors">Kong N.J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398443984">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85171744736&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2023.3308773" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. N.J., &quot;Hybrid iLQR Model Predictive Control for Contact Implicit Stabilization on Legged Robots,&quot; IEEE Transactions on Robotics, 2023. doi: 10.1109/TRO.2023.3308773. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85171744736&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398443984')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="highly-time-resolved fmcw lidar with synchronously-nonlinearity-corrected acquisition for dynamic locomotion" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85148302116&amp;origin=resultslist" target="_blank">Highly-time-resolved FMCW LiDAR with synchronously-nonlinearity-corrected acquisition for dynamic locomotion</a>
                            </h3>
                            <p class="card-authors">Sun C.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398432560">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85148302116&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1364/OE.480346" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. C., &quot;Highly-time-resolved FMCW LiDAR with synchronously-nonlinearity-corrected acquisition for dynamic locomotion,&quot; Optics Express, 2023. doi: 10.1364/OE.480346. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85148302116&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398432560')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="underwater legged robotics: review and perspectives" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85152616268&amp;origin=resultslist" target="_blank">Underwater legged robotics: review and perspectives</a>
                            </h3>
                            <p class="card-authors">Picardi G.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398444464">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85152616268&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1088/1748-3190/acc0bb" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. G., &quot;Underwater legged robotics: review and perspectives,&quot; Bioinspiration and Biomimetics, 2023. doi: 10.1088/1748-3190/acc0bb. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85152616268&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398444464')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="learning-based design and control for quadrupedal robots with parallel-elastic actuators" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147225048&amp;origin=resultslist" target="_blank">Learning-Based Design and Control for Quadrupedal Robots With Parallel-Elastic Actuators</a>
                            </h3>
                            <p class="card-authors">Bjelonic F.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398444608">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147225048&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3234809" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. F., &quot;Learning-Based Design and Control for Quadrupedal Robots With Parallel-Elastic Actuators,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3234809. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85147225048&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398444608')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="stable flexible-joint floating-base robot balancing and locomotion via variable impedance control" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85129680415&amp;origin=resultslist" target="_blank">Stable Flexible-Joint Floating-Base Robot Balancing and Locomotion via Variable Impedance Control</a>
                            </h3>
                            <p class="card-authors">Spyrakos-Papastavridis E.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398444752">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85129680415&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TIE.2022.3169848" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. E., &quot;Stable Flexible-Joint Floating-Base Robot Balancing and Locomotion via Variable Impedance Control,&quot; IEEE Transactions on Industrial Electronics, 2023. doi: 10.1109/TIE.2022.3169848. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85129680415&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398444752')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="autonomous navigation of underactuated bipedal robots in height-constrained environments" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85165440944&amp;origin=resultslist" target="_blank">Autonomous navigation of underactuated bipedal robots in height-constrained environments</a>
                            </h3>
                            <p class="card-authors">Li Z.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398444800">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85165440944&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1177/02783649231187670" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Z., &quot;Autonomous navigation of underactuated bipedal robots in height-constrained environments,&quot; International Journal of Robotics Research, 2023. doi: 10.1177/02783649231187670. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85165440944&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398444800')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="research on wheel-legged robot based on lqr and adrc" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85171172472&amp;origin=resultslist" target="_blank">Research on wheel-legged robot based on LQR and ADRC</a>
                            </h3>
                            <p class="card-authors">Feng X.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398444848">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85171172472&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1038/s41598-023-41462-1" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. X., &quot;Research on wheel-legged robot based on LQR and ADRC,&quot; Scientific Reports, 2023. doi: 10.1038/s41598-023-41462-1. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85171172472&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398444848')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="towards generation and transition of diverse gaits for quadrupedal robots based on trajectory optimization and whole-body impedance control" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85149405011&amp;origin=resultslist" target="_blank">Towards Generation and Transition of Diverse Gaits for Quadrupedal Robots Based on Trajectory Optimization and Whole-Body Impedance Control</a>
                            </h3>
                            <p class="card-authors">Li Q.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398444944">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85149405011&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3251184" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Q., &quot;Towards Generation and Transition of Diverse Gaits for Quadrupedal Robots Based on Trajectory Optimization and Whole-Body Impedance Control,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3251184. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85149405011&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398444944')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="lateral flexion of a compliant spine improves motor performance in a bioinspired mouse robot" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85179017456&amp;origin=resultslist" target="_blank">Lateral flexion of a compliant spine improves motor performance in a bioinspired mouse robot</a>
                            </h3>
                            <p class="card-authors">Bing Z.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398444992">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85179017456&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1126/scirobotics.adg7165" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. Z., &quot;Lateral flexion of a compliant spine improves motor performance in a bioinspired mouse robot,&quot; Science Robotics, 2023. doi: 10.1126/scirobotics.adg7165. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85179017456&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398444992')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="reinforcement learning-based stable jump control method for asteroid-exploration quadruped robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85175607948&amp;origin=resultslist" target="_blank">Reinforcement learning-based stable jump control method for asteroid-exploration quadruped robots</a>
                            </h3>
                            <p class="card-authors">Qi J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398445184">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85175607948&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1016/j.ast.2023.108689" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Q. J., &quot;Reinforcement learning-based stable jump control method for asteroid-exploration quadruped robots,&quot; Aerospace Science and Technology, 2023. doi: 10.1016/j.ast.2023.108689. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85175607948&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398445184')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="design of clari: a miniature modular origami passive shape-morphing robot" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168902589&amp;origin=resultslist" target="_blank">Design of CLARI: A Miniature Modular Origami Passive Shape-Morphing Robot</a>
                            </h3>
                            <p class="card-authors">Kabutz H.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398444224">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168902589&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1002/aisy.202300181" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. H., &quot;Design of CLARI: A Miniature Modular Origami Passive Shape-Morphing Robot,&quot; Advanced Intelligent Systems, 2023. doi: 10.1002/aisy.202300181. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85168902589&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398444224')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="layered control for cooperative locomotion of two quadrupedal robots: centralized and distributed approaches" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85174812089&amp;origin=resultslist" target="_blank">Layered Control for Cooperative Locomotion of Two Quadrupedal Robots: Centralized and Distributed Approaches</a>
                            </h3>
                            <p class="card-authors">Kim J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398444272">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85174812089&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2023.3319896" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. J., &quot;Layered Control for Cooperative Locomotion of Two Quadrupedal Robots: Centralized and Distributed Approaches,&quot; IEEE Transactions on Robotics, 2023. doi: 10.1109/TRO.2023.3319896. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85174812089&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398444272')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="optimizing bipedal locomotion for the 100m dash with comparison to human running" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168676400&amp;origin=resultslist" target="_blank">Optimizing Bipedal Locomotion for The 100m Dash With Comparison to Human Running</a>
                            </h3>
                            <p class="card-authors">Crowley D.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398445040">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168676400&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/ICRA48891.2023.10160436" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. D., &quot;Optimizing Bipedal Locomotion for The 100m Dash With Comparison to Human Running,&quot; Proceedings IEEE International Conference on Robotics and Automation, 2023. doi: 10.1109/ICRA48891.2023.10160436. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85168676400&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398445040')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="contact optimization for non-prehensile loco-manipulation via hierarchical model predictive control" data-keywords="" data-themes="M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168690385&amp;origin=resultslist" target="_blank">Contact Optimization for Non-Prehensile Loco-Manipulation via Hierarchical Model Predictive Control</a>
                            </h3>
                            <p class="card-authors">Rigo A.</p>
                            <div class="theme-tags"><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398445952">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168690385&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/ICRA48891.2023.10160507" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. A., &quot;Contact Optimization for Non-Prehensile Loco-Manipulation via Hierarchical Model Predictive Control,&quot; Proceedings IEEE International Conference on Robotics and Automation, 2023. doi: 10.1109/ICRA48891.2023.10160507. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85168690385&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398445952')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="Scopus" data-year="2023" data-category="other" data-title="lstp: long short-term motion planning for legged and legged-wheeled systems" data-keywords="" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168661568&amp;origin=resultslist" target="_blank">LSTP: Long Short-Term Motion Planning for Legged and Legged-Wheeled Systems</a>
                            </h3>
                            <p class="card-authors">Jelavic E.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398446048">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168661568&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2023.3302239" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. E., &quot;LSTP: Long Short-Term Motion Planning for Legged and Legged-Wheeled Systems,&quot; IEEE Transactions on Robotics, 2023. doi: 10.1109/TRO.2023.3302239. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85168661568&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398446048')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="modeling and mpc-based pose tracking for wheeled bipedal robot" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85174817902&amp;origin=resultslist" target="_blank">Modeling and MPC-Based Pose Tracking for Wheeled Bipedal Robot</a>
                            </h3>
                            <p class="card-authors">Yu J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398446096">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85174817902&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3322084" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. J., &quot;Modeling and MPC-Based Pose Tracking for Wheeled Bipedal Robot,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3322084. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85174817902&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398446096')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="Scopus" data-year="2023" data-category="other" data-title="synchronized human-humanoid motion imitation" data-keywords="" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85161052925&amp;origin=resultslist" target="_blank">Synchronized Human-Humanoid Motion Imitation</a>
                            </h3>
                            <p class="card-authors">Dallard A.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398446384">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85161052925&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3280807" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. A., &quot;Synchronized Human-Humanoid Motion Imitation,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3280807. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85161052925&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398446384')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="design, modeling, and control of a quadruped robot spidar: spherically vectorable and distributed rotors assisted air-ground quadruped robot" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85159723810&amp;origin=resultslist" target="_blank">Design, Modeling, and Control of a Quadruped Robot SPIDAR: Spherically Vectorable and Distributed Rotors Assisted Air-Ground Quadruped Robot</a>
                            </h3>
                            <p class="card-authors">Zhao M.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398446480">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85159723810&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3272285" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. M., &quot;Design, Modeling, and Control of a Quadruped Robot SPIDAR: Spherically Vectorable and Distributed Rotors Assisted Air-Ground Quadruped Robot,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3272285. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85159723810&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398446480')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="a survey of¬†wheeled-legged robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85138002653&amp;origin=resultslist" target="_blank">A Survey of¬†Wheeled-Legged Robots</a>
                            </h3>
                            <p class="card-authors">Bjelonic M.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398446576">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85138002653&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1007/978-3-031-15226-9_11" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. M., &quot;A Survey of¬†Wheeled-Legged Robots,&quot; Lecture Notes in Networks and Systems, 2023. doi: 10.1007/978-3-031-15226-9_11. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85138002653&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398446576')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="dynamic walking of bipedal robots on uneven stepping stones via adaptive-frequency mpc" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147202313&amp;origin=resultslist" target="_blank">Dynamic Walking of Bipedal Robots on Uneven Stepping Stones via Adaptive-Frequency MPC</a>
                            </h3>
                            <p class="card-authors">Li J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398446624">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147202313&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LCSYS.2023.3234769" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. J., &quot;Dynamic Walking of Bipedal Robots on Uneven Stepping Stones via Adaptive-Frequency MPC,&quot; IEEE Control Systems Letters, 2023. doi: 10.1109/LCSYS.2023.3234769. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85147202313&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398446624')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="seamless reaction strategy for bipedal locomotion exploiting real-time nonlinear model predictive control" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85163504258&amp;origin=resultslist" target="_blank">Seamless Reaction Strategy for Bipedal Locomotion Exploiting Real-Time Nonlinear Model Predictive Control</a>
                            </h3>
                            <p class="card-authors">Choe J.H.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398446720">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85163504258&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3291273" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. J.H., &quot;Seamless Reaction Strategy for Bipedal Locomotion Exploiting Real-Time Nonlinear Model Predictive Control,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3291273. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85163504258&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398446720')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="omnidirectional jump control of a locust-computer hybrid robot" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85148250093&amp;origin=resultslist" target="_blank">Omnidirectional Jump Control of a Locust-Computer Hybrid Robot</a>
                            </h3>
                            <p class="card-authors">Liu P.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398446768">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85148250093&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1089/soro.2021.0137" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. P., &quot;Omnidirectional Jump Control of a Locust-Computer Hybrid Robot,&quot; Soft Robotics, 2023. doi: 10.1089/soro.2021.0137. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85148250093&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398446768')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="footholds optimization for legged robots walking on complex terrain" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85161813406&amp;origin=resultslist" target="_blank">Footholds optimization for legged robots walking on complex terrain</a>
                            </h3>
                            <p class="card-authors">Yin Y.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398447008">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85161813406&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1007/s11465-022-0742-y" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Y., &quot;Footholds optimization for legged robots walking on complex terrain,&quot; Frontiers of Mechanical Engineering, 2023. doi: 10.1007/s11465-022-0742-y. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85161813406&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398447008')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="development of a new robust stable walking algorithm for a humanoid robot using deep reinforcement learning with multi-sensor data fusion" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147877640&amp;origin=resultslist" target="_blank">Development of a New Robust Stable Walking Algorithm for a Humanoid Robot Using Deep Reinforcement Learning with Multi-Sensor Data Fusion</a>
                            </h3>
                            <p class="card-authors">Kaymak √á.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398445808">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147877640&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.3390/electronics12030568" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. √á., &quot;Development of a New Robust Stable Walking Algorithm for a Humanoid Robot Using Deep Reinforcement Learning with Multi-Sensor Data Fusion,&quot; Electronics Switzerland, 2023. doi: 10.3390/electronics12030568. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85147877640&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398445808')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="learning complex motor skills for legged robot fall recovery" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85161045216&amp;origin=resultslist" target="_blank">Learning Complex Motor Skills for Legged Robot Fall Recovery</a>
                            </h3>
                            <p class="card-authors">Yang C.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398446864">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85161045216&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3281290" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. C., &quot;Learning Complex Motor Skills for Legged Robot Fall Recovery,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3281290. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85161045216&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398446864')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="high-density emg, imu, kinetic, and kinematic open-source data for comprehensive locomotion activities" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85176240779&amp;origin=resultslist" target="_blank">High-density EMG, IMU, kinetic, and kinematic open-source data for comprehensive locomotion activities</a>
                            </h3>
                            <p class="card-authors">Dimitrov H.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398446912">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85176240779&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1038/s41597-023-02679-x" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. H., &quot;High-density EMG, IMU, kinetic, and kinematic open-source data for comprehensive locomotion activities,&quot; Scientific Data, 2023. doi: 10.1038/s41597-023-02679-x. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85176240779&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398446912')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="quadruped capturability and push recovery via a switched-systems characterization of dynamic balance" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85151553692&amp;origin=resultslist" target="_blank">Quadruped Capturability and Push Recovery via a Switched-Systems Characterization of Dynamic Balance</a>
                            </h3>
                            <p class="card-authors">Chen H.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398446960">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85151553692&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2023.3240622" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. H., &quot;Quadruped Capturability and Push Recovery via a Switched-Systems Characterization of Dynamic Balance,&quot; IEEE Transactions on Robotics, 2023. doi: 10.1109/TRO.2023.3240622. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85151553692&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398446960')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="barry: a high-payload and agile quadruped robot" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85171567292&amp;origin=resultslist" target="_blank">Barry: A High-Payload and Agile Quadruped Robot</a>
                            </h3>
                            <p class="card-authors">Valsecchi G.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398447104">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85171567292&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3313923" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'V. G., &quot;Barry: A High-Payload and Agile Quadruped Robot,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3313923. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85171567292&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398447104')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="imitating and finetuning model predictive control for robust and symmetric quadrupedal locomotion" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85173012210&amp;origin=resultslist" target="_blank">Imitating and Finetuning Model Predictive Control for Robust and Symmetric Quadrupedal Locomotion</a>
                            </h3>
                            <p class="card-authors">Youm D.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398447152">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85173012210&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3320827" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. D., &quot;Imitating and Finetuning Model Predictive Control for Robust and Symmetric Quadrupedal Locomotion,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3320827. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85173012210&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398447152')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="distributed data-driven predictive control for multi-agent collaborative legged locomotion" data-keywords="" data-themes="I L R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168659479&amp;origin=resultslist" target="_blank">Distributed Data-Driven Predictive Control for Multi-Agent Collaborative Legged Locomotion</a>
                            </h3>
                            <p class="card-authors">Fawcett R.T.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-R">Multi-Robot</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398447200">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168659479&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/ICRA48891.2023.10160914" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. R.T., &quot;Distributed Data-Driven Predictive Control for Multi-Agent Collaborative Legged Locomotion,&quot; Proceedings IEEE International Conference on Robotics and Automation, 2023. doi: 10.1109/ICRA48891.2023.10160914. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85168659479&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398447200')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="design of a novel wheel-legged robot with rim shape changeable wheels" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85180123227&amp;origin=resultslist" target="_blank">Design of A Novel Wheel-Legged Robot with Rim Shape Changeable Wheels</a>
                            </h3>
                            <p class="card-authors">Fu Z.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398447248">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85180123227&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1186/s10033-023-00974-7" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. Z., &quot;Design of A Novel Wheel-Legged Robot with Rim Shape Changeable Wheels,&quot; Chinese Journal of Mechanical Engineering English Edition, 2023. doi: 10.1186/s10033-023-00974-7. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85180123227&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398447248')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="benchmarking potential based rewards for learning humanoid locomotion" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168667660&amp;origin=resultslist" target="_blank">Benchmarking Potential Based Rewards for Learning Humanoid Locomotion</a>
                            </h3>
                            <p class="card-authors">Jeon S.H.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398447440">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168667660&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/ICRA48891.2023.10160885" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. S.H., &quot;Benchmarking Potential Based Rewards for Learning Humanoid Locomotion,&quot; Proceedings IEEE International Conference on Robotics and Automation, 2023. doi: 10.1109/ICRA48891.2023.10160885. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85168667660&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398447440')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="roboshape: using topology patterns to scalably and flexibly deploy accelerators across robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168882746&amp;origin=resultslist" target="_blank">RoboShape: Using Topology Patterns to Scalably and Flexibly Deploy Accelerators Across Robots</a>
                            </h3>
                            <p class="card-authors">Neuman S.M.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398447584">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168882746&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1145/3579371.3589104" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. S.M., &quot;RoboShape: Using Topology Patterns to Scalably and Flexibly Deploy Accelerators Across Robots,&quot; Proceedings International Symposium on Computer Architecture, 2023. doi: 10.1145/3579371.3589104. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85168882746&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398447584')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="Scopus" data-year="2023" data-category="other" data-title="slomo: a general system for legged robot motion imitation from casual videos" data-keywords="" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85171544548&amp;origin=resultslist" target="_blank">SLoMo: A General System for Legged Robot Motion Imitation From Casual Videos</a>
                            </h3>
                            <p class="card-authors">Zhang J.Z.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398447728">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85171544548&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3313937" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. J.Z., &quot;SLoMo: A General System for Legged Robot Motion Imitation From Casual Videos,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3313937. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85171544548&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398447728')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="safe and adaptive 3-d locomotion via constrained task-space imitation learning" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85149357739&amp;origin=resultslist" target="_blank">Safe and Adaptive 3-D Locomotion via Constrained Task-Space Imitation Learning</a>
                            </h3>
                            <p class="card-authors">Ding J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398447776">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85149357739&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TMECH.2023.3239099" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. J., &quot;Safe and Adaptive 3-D Locomotion via Constrained Task-Space Imitation Learning,&quot; IEEE ASME Transactions on Mechatronics, 2023. doi: 10.1109/TMECH.2023.3239099. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85149357739&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398447776')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="humanoid robot path planning using memory-based gravity search algorithm and enhanced differential evolution approach in a complex environment" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85144090926&amp;origin=resultslist" target="_blank">Humanoid robot path planning using memory-based gravity search algorithm and enhanced differential evolution approach in a complex environment</a>
                            </h3>
                            <p class="card-authors">Vikas </p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398447872">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85144090926&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1016/j.eswa.2022.119423" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Vikas, &quot;Humanoid robot path planning using memory-based gravity search algorithm and enhanced differential evolution approach in a complex environment,&quot; Expert Systems with Applications, 2023. doi: 10.1016/j.eswa.2022.119423. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85144090926&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398447872')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="elastic-actuation mechanism for repetitive hopping based on power modulation and cyclic trajectory generation" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85135748349&amp;origin=resultslist" target="_blank">Elastic-Actuation Mechanism for Repetitive Hopping Based on Power Modulation and Cyclic Trajectory Generation</a>
                            </h3>
                            <p class="card-authors">Shin W.D.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398447968">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85135748349&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2022.3189249" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. W.D., &quot;Elastic-Actuation Mechanism for Repetitive Hopping Based on Power Modulation and Cyclic Trajectory Generation,&quot; IEEE Transactions on Robotics, 2023. doi: 10.1109/TRO.2022.3189249. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85135748349&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398447968')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="autonomous navigation with online replanning and recovery behaviors for wheeled-legged robots using behavior trees" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85171539397&amp;origin=resultslist" target="_blank">Autonomous Navigation with Online Replanning and Recovery Behaviors for Wheeled-Legged Robots Using Behavior Trees</a>
                            </h3>
                            <p class="card-authors">De Luca A.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398448016">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85171539397&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3313052" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. L. A., &quot;Autonomous Navigation with Online Replanning and Recovery Behaviors for Wheeled-Legged Robots Using Behavior Trees,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3313052. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85171539397&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398448016')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="sospider: a bio-inspired multimodal untethered soft hexapod robot for planetary lava tube exploration" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85175087874&amp;origin=resultslist" target="_blank">SoSpider: A bio-inspired multimodal untethered soft hexapod robot for planetary lava tube exploration</a>
                            </h3>
                            <p class="card-authors">Niu L.Z.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398444416">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85175087874&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1007/s11431-023-2519-9" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. L.Z., &quot;SoSpider: A bio-inspired multimodal untethered soft hexapod robot for planetary lava tube exploration,&quot; Science China Technological Sciences, 2023. doi: 10.1007/s11431-023-2519-9. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85175087874&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398444416')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="design and control of braver: a bipedal robot actuated via proprioceptive electric motors" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85165443909&amp;origin=resultslist" target="_blank">Design and control of BRAVER: a bipedal robot actuated via proprioceptive electric motors</a>
                            </h3>
                            <p class="card-authors">Zhu Z.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398448064">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85165443909&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1007/s10514-023-10117-5" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Z., &quot;Design and control of BRAVER: a bipedal robot actuated via proprioceptive electric motors,&quot; Autonomous Robots, 2023. doi: 10.1007/s10514-023-10117-5. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85165443909&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398448064')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="orientation control system: enhancing aerial maneuvers for quadruped robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147895546&amp;origin=resultslist" target="_blank">Orientation Control System: Enhancing Aerial Maneuvers for Quadruped Robots</a>
                            </h3>
                            <p class="card-authors">Roscia F.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398448112">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147895546&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.3390/s23031234" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. F., &quot;Orientation Control System: Enhancing Aerial Maneuvers for Quadruped Robots,&quot; Sensors, 2023. doi: 10.3390/s23031234. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85147895546&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398448112')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="do robots outperform humans in human-centered domains?" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85177480613&amp;origin=resultslist" target="_blank">Do robots outperform humans in human-centered domains?</a>
                            </h3>
                            <p class="card-authors">Riener R.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398448160">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85177480613&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.3389/frobt.2023.1223946" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. R., &quot;Do robots outperform humans in human-centered domains?,&quot; Frontiers in Robotics and AI, 2023. doi: 10.3389/frobt.2023.1223946. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85177480613&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398448160')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="torque-based deep reinforcement learning for task-and-robot agnostic learning on bipedal robots using sim-to-real transfer" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85167803521&amp;origin=resultslist" target="_blank">Torque-Based Deep Reinforcement Learning for Task-and-Robot Agnostic Learning on Bipedal Robots Using Sim-to-Real Transfer</a>
                            </h3>
                            <p class="card-authors">Kim D.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398448304">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85167803521&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3304561" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. D., &quot;Torque-Based Deep Reinforcement Learning for Task-and-Robot Agnostic Learning on Bipedal Robots Using Sim-to-Real Transfer,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3304561. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85167803521&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398448304')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="bipedal walking on constrained footholds with mpc footstep control" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85182921448&amp;origin=resultslist" target="_blank">Bipedal Walking on Constrained Footholds with MPC Footstep Control</a>
                            </h3>
                            <p class="card-authors">Acosta B.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398448352">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85182921448&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/Humanoids57100.2023.10375170" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. B., &quot;Bipedal Walking on Constrained Footholds with MPC Footstep Control,&quot; IEEE Ras International Conference on Humanoid Robots, 2023. doi: 10.1109/Humanoids57100.2023.10375170. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85182921448&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398448352')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="real-time neural dense elevation mapping for urban terrain with uncertainty estimations" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85146227296&amp;origin=resultslist" target="_blank">Real-Time Neural Dense Elevation Mapping for Urban Terrain with Uncertainty Estimations</a>
                            </h3>
                            <p class="card-authors">Yang B.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398448448">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85146227296&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2022.3230325" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. B., &quot;Real-Time Neural Dense Elevation Mapping for Urban Terrain with Uncertainty Estimations,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2022.3230325. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85146227296&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398448448')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="a hierarchical framework for quadruped robots gait planning based on ddpg" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85172282632&amp;origin=resultslist" target="_blank">A Hierarchical Framework for Quadruped Robots Gait Planning Based on DDPG</a>
                            </h3>
                            <p class="card-authors">Li Y.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398448496">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85172282632&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.3390/biomimetics8050382" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Y., &quot;A Hierarchical Framework for Quadruped Robots Gait Planning Based on DDPG,&quot; Biomimetics, 2023. doi: 10.3390/biomimetics8050382. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85172282632&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398448496')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="hierarchical generative modelling for autonomous robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85175567595&amp;origin=resultslist" target="_blank">Hierarchical generative modelling for autonomous robots</a>
                            </h3>
                            <p class="card-authors">Yuan K.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398448544">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85175567595&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1038/s42256-023-00752-z" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. K., &quot;Hierarchical generative modelling for autonomous robots,&quot; Nature Machine Intelligence, 2023. doi: 10.1038/s42256-023-00752-z. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85175567595&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398448544')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="reactive landing controller for quadruped robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85171568878&amp;origin=resultslist" target="_blank">Reactive Landing Controller for Quadruped Robots</a>
                            </h3>
                            <p class="card-authors">Roscia F.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398170224">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85171568878&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3313919" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. F., &quot;Reactive Landing Controller for Quadruped Robots,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3313919. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85171568878&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398170224')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="proprioceptive-based whole-body disturbance rejection control for dynamic motions in legged robots" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85174830459&amp;origin=resultslist" target="_blank">Proprioceptive-Based Whole-Body Disturbance Rejection Control for Dynamic Motions in Legged Robots</a>
                            </h3>
                            <p class="card-authors">Zhu Z.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398170176">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85174830459&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3322081" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Z., &quot;Proprioceptive-Based Whole-Body Disturbance Rejection Control for Dynamic Motions in Legged Robots,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3322081. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85174830459&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398170176')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="view: visual-inertial external wrench estimator for legged robot" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85174825415&amp;origin=resultslist" target="_blank">VIEW: Visual-Inertial External Wrench Estimator for Legged Robot</a>
                            </h3>
                            <p class="card-authors">Kang J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398170320">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85174825415&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3322646" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. J., &quot;VIEW: Visual-Inertial External Wrench Estimator for Legged Robot,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3322646. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85174825415&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398170320')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="curiosity-driven learning of joint locomotion and manipulation tasks" data-keywords="" data-themes="I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85184345847&amp;origin=resultslist" target="_blank">Curiosity-Driven Learning of Joint Locomotion and Manipulation Tasks</a>
                            </h3>
                            <p class="card-authors">Schwarke C.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398170416">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85184345847&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. C., &quot;Curiosity-Driven Learning of Joint Locomotion and Manipulation Tasks,&quot; Proceedings of Machine Learning Research, 2023. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85184345847&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398170416')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="predictive control of zero moment point (zmp) for terrain robot kinematics" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85142240663&amp;origin=resultslist" target="_blank">Predictive control of zero moment point (ZMP) for terrain robot kinematics</a>
                            </h3>
                            <p class="card-authors">Haldar A.I.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398170512">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85142240663&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1016/j.matpr.2022.10.286" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. A.I., &quot;Predictive control of zero moment point (ZMP) for terrain robot kinematics,&quot; Materials Today Proceedings, 2023. doi: 10.1016/j.matpr.2022.10.286. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85142240663&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398170512')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="control of¬†wheeled-legged quadrupeds using deep reinforcement learning" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85137985117&amp;origin=resultslist" target="_blank">Control of¬†Wheeled-Legged Quadrupeds Using Deep Reinforcement Learning</a>
                            </h3>
                            <p class="card-authors">Lee J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398170704">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85137985117&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1007/978-3-031-15226-9_14" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. J., &quot;Control of¬†Wheeled-Legged Quadrupeds Using Deep Reinforcement Learning,&quot; Lecture Notes in Networks and Systems, 2023. doi: 10.1007/978-3-031-15226-9_14. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85137985117&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398170704')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="model predictive control of quadruped robot based on reinforcement learning" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85145676804&amp;origin=resultslist" target="_blank">Model Predictive Control of Quadruped Robot Based on Reinforcement Learning</a>
                            </h3>
                            <p class="card-authors">Zhang Z.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398170752">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85145676804&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.3390/app13010154" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Z., &quot;Model Predictive Control of Quadruped Robot Based on Reinforcement Learning,&quot; Applied Sciences Switzerland, 2023. doi: 10.3390/app13010154. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85145676804&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398170752')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="unified motion planner for walking, running, and jumping using the three-dimensional divergent component of motion" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85174857184&amp;origin=resultslist" target="_blank">Unified Motion Planner for Walking, Running, and Jumping Using the Three-Dimensional Divergent Component of Motion</a>
                            </h3>
                            <p class="card-authors">Mesesan G.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398403344">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85174857184&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2023.3321396" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. G., &quot;Unified Motion Planner for Walking, Running, and Jumping Using the Three-Dimensional Divergent Component of Motion,&quot; IEEE Transactions on Robotics, 2023. doi: 10.1109/TRO.2023.3321396. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85174857184&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398403344')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="Scopus" data-year="2023" data-category="other" data-title="multi-robot path planning using a hybrid dynamic window approach and modified chaotic neural oscillator-based hyperbolic gravitational search algorithm in a complex terrain" data-keywords="" data-themes="R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85150738227&amp;origin=resultslist" target="_blank">Multi-robot path planning using a hybrid dynamic window approach and modified chaotic neural oscillator-based hyperbolic gravitational search algorithm in a complex terrain</a>
                            </h3>
                            <p class="card-authors">Vikas </p>
                            <div class="theme-tags"><span class="theme-badge theme-R">Multi-Robot</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398170272">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85150738227&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1007/s11370-023-00460-y" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Vikas, &quot;Multi-robot path planning using a hybrid dynamic window approach and modified chaotic neural oscillator-based hyperbolic gravitational search algorithm in a complex terrain,&quot; Intelligent Service Robotics, 2023. doi: 10.1007/s11370-023-00460-y. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85150738227&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398170272')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="whole body control formulation for humanoid robots with closed/parallel kinematic chains: kangaroo case study" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85180917830&amp;origin=resultslist" target="_blank">Whole Body Control Formulation for Humanoid Robots with Closed/Parallel Kinematic Chains: Kangaroo Case Study</a>
                            </h3>
                            <p class="card-authors">Sovukluk S.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398171088">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85180917830&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/IROS55552.2023.10341391" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. S., &quot;Whole Body Control Formulation for Humanoid Robots with Closed/Parallel Kinematic Chains: Kangaroo Case Study,&quot; IEEE International Conference on Intelligent Robots and Systems, 2023. doi: 10.1109/IROS55552.2023.10341391. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85180917830&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398171088')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="computational design towards energy efficient optimization in overconstrained robotic limbs" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85173230875&amp;origin=resultslist" target="_blank">Computational design towards energy efficient optimization in overconstrained robotic limbs</a>
                            </h3>
                            <p class="card-authors">Gu Y.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398171136">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85173230875&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1093/jcde/qwad083" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Y., &quot;Computational design towards energy efficient optimization in overconstrained robotic limbs,&quot; Journal of Computational Design and Engineering, 2023. doi: 10.1093/jcde/qwad083. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85173230875&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398171136')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="proactive body joint adaptation for energy-efficient locomotion of bio-inspired multi-segmented robots" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147212376&amp;origin=resultslist" target="_blank">Proactive Body Joint Adaptation for Energy-Efficient Locomotion of Bio-Inspired Multi-Segmented Robots</a>
                            </h3>
                            <p class="card-authors">Homchanthanakul J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398171232">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147212376&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3234773" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. J., &quot;Proactive Body Joint Adaptation for Energy-Efficient Locomotion of Bio-Inspired Multi-Segmented Robots,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3234773. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85147212376&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398171232')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="self-organized stick insect-like locomotion under decentralized adaptive neural control: from biological investigation to robot simulation" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85162853092&amp;origin=resultslist" target="_blank">Self-Organized Stick Insect-Like Locomotion under Decentralized Adaptive Neural Control: From Biological Investigation to Robot Simulation</a>
                            </h3>
                            <p class="card-authors">Larsen A.D.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398171280">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85162853092&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1002/adts.202300228" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. A.D., &quot;Self-Organized Stick Insect-Like Locomotion under Decentralized Adaptive Neural Control: From Biological Investigation to Robot Simulation,&quot; Advanced Theory and Simulations, 2023. doi: 10.1002/adts.202300228. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85162853092&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398171280')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="Scopus" data-year="2023" data-category="other" data-title="biomimetic approaches for human arm motion generation: literature review and future directions" data-keywords="" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85153725930&amp;origin=resultslist" target="_blank">Biomimetic Approaches for Human Arm Motion Generation: Literature Review and Future Directions</a>
                            </h3>
                            <p class="card-authors">Trivedi U.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398171376">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85153725930&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.3390/s23083912" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. U., &quot;Biomimetic Approaches for Human Arm Motion Generation: Literature Review and Future Directions,&quot; Sensors, 2023. doi: 10.3390/s23083912. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85153725930&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398171376')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="bio-inspired gait transitions for quadruped locomotion" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85166755794&amp;origin=resultslist" target="_blank">Bio-Inspired Gait Transitions for Quadruped Locomotion</a>
                            </h3>
                            <p class="card-authors">Humphreys J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398171328">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85166755794&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3300249" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. J., &quot;Bio-Inspired Gait Transitions for Quadruped Locomotion,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3300249. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85166755794&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398171328')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="cooperative control strategy of wheel-legged robot based on attitude balance" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147142909&amp;origin=resultslist" target="_blank">Cooperative control strategy of wheel-legged robot based on attitude balance</a>
                            </h3>
                            <p class="card-authors">Shen Y.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398171568">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147142909&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1017/S0263574722001436" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Y., &quot;Cooperative control strategy of wheel-legged robot based on attitude balance,&quot; Robotica, 2023. doi: 10.1017/S0263574722001436. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85147142909&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398171568')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="multimodal bipedal locomotion generation with passive dynamics via deep reinforcement learning" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147445912&amp;origin=resultslist" target="_blank">Multimodal bipedal locomotion generation with passive dynamics via deep reinforcement learning</a>
                            </h3>
                            <p class="card-authors">Koseki S.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398171616">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147445912&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.3389/fnbot.2022.1054239" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. S., &quot;Multimodal bipedal locomotion generation with passive dynamics via deep reinforcement learning,&quot; Frontiers in Neurorobotics, 2023. doi: 10.3389/fnbot.2022.1054239. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85147445912&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398171616')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="hybrid learning mechanisms under a neural control network for various walking speed generation of a quadruped robot" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85170094776&amp;origin=resultslist" target="_blank">Hybrid learning mechanisms under a neural control network for various walking speed generation of a quadruped robot</a>
                            </h3>
                            <p class="card-authors">Zhang Y.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398171760">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85170094776&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1016/j.neunet.2023.08.030" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Y., &quot;Hybrid learning mechanisms under a neural control network for various walking speed generation of a quadruped robot,&quot; Neural Networks, 2023. doi: 10.1016/j.neunet.2023.08.030. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85170094776&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398171760')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="heel-strike and toe-off walking of humanoid robot using quadratic programming considering the foot contact states" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85150016929&amp;origin=resultslist" target="_blank">Heel-strike and toe-off walking of humanoid robot using quadratic programming considering the foot contact states</a>
                            </h3>
                            <p class="card-authors">Park B.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398171808">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85150016929&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1016/j.robot.2023.104396" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. B., &quot;Heel-strike and toe-off walking of humanoid robot using quadratic programming considering the foot contact states,&quot; Robotics and Autonomous Systems, 2023. doi: 10.1016/j.robot.2023.104396. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85150016929&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398171808')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="posture adjustment for a wheel-legged robotic system via leg force control with prescribed transient performance" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85148422235&amp;origin=resultslist" target="_blank">Posture Adjustment for a Wheel-Legged Robotic System Via Leg Force Control With Prescribed Transient Performance</a>
                            </h3>
                            <p class="card-authors">Liu D.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398171904">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85148422235&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TIE.2023.3239859" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. D., &quot;Posture Adjustment for a Wheel-Legged Robotic System Via Leg Force Control With Prescribed Transient Performance,&quot; IEEE Transactions on Industrial Electronics, 2023. doi: 10.1109/TIE.2023.3239859. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85148422235&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398171904')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="learning semantics-aware locomotion skills from human demonstration" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ OTHER</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85164675765&amp;origin=resultslist" target="_blank">Learning Semantics-Aware Locomotion Skills from Human Demonstration</a>
                            </h3>
                            <p class="card-authors">Yang Y.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>

                            <div class="card-details" id="details-4398171952">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85164675765&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Y., &quot;Learning Semantics-Aware Locomotion Skills from Human Demonstration,&quot; Proceedings of Machine Learning Research, 2023. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85164675765&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398171952')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section year-section" data-year="2022">
                <h2 class="section-header">üìÖ 2022 <span class="section-count">(6 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2022" data-category="cs.cl" data-title="self-generated in-context learning: leveraging auto-regressive language models as a demonstration generator" data-keywords="language model cs.cl" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2206.08082v1" target="_blank">Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator</a>
                            </h3>
                            <p class="card-authors">Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398404736">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large-scale pre-trained language models (PLMs) are well-known for being capable of solving a task simply by conditioning a few input-label pairs dubbed demonstrations on a prompt without being explicitly tuned for the desired downstream task. Such a process (i.e., in-context learning), however, naturally leads to high reliance on the demonstrations which are usually selected from external datasets. In this paper, we propose self-generated in-context learning (SG-ICL), which generates demonstrations for in-context learning from PLM itself to minimize the reliance on the external demonstration. ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose self-generated in-context learning (SG-ICL), which generates demonstrations for in-context learning from PLM itself to minimize the reliance on the external demonstration</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large-scale pre-trained language models (PLMs) are well-known for being capable of solving a task simply by conditioning a few input-label pairs dubbed demonstrations on a prompt without being explicitly tuned for the desired downstream task</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2206.08082v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2206.08082v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. J. Kim, H. Cho, J. Kim, T. Kim, K. M. Yoo, and S. Lee, &quot;Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator,&quot; arXiv, 2022. [Online]. Available: http://arxiv.org/abs/2206.08082v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398404736')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2022" data-category="cs.cl" data-title="gpt-neox-20b: an open-source autoregressive language model" data-keywords="gpt language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2204.06745v1" target="_blank">GPT-NeoX-20B: An Open-Source Autoregressive Language Model</a>
                            </h3>
                            <p class="card-authors">Sid Black, Stella Biderman, Eric Hallahan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4398406368">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe \model{}&#x27;s architecture and training and evaluate its performance on a range of language-understanding, mathematics, and knowledge-based tasks. We find that GPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more i...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2204.06745v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2204.06745v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Black et al., &quot;GPT-NeoX-20B: An Open-Source Autoregressive Language Model,&quot; arXiv, 2022. [Online]. Available: http://arxiv.org/abs/2204.06745v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398406368')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2022" data-category="cs.cl" data-title="exploring the value of pre-trained language models for clinical named entity recognition" data-keywords="transformer bert nlp language model cs.cl cs.ai cs.lg" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2210.12770v4" target="_blank">Exploring the Value of Pre-trained Language Models for Clinical Named Entity Recognition</a>
                            </h3>
                            <p class="card-authors">Samuel Belkadi, Lifeng Han, Yuping Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Transformer</span><span class="keyword-tag">Bert</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4398406080">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The practice of fine-tuning Pre-trained Language Models (PLMs) from general or domain-specific data to a specific task with limited resources, has gained popularity within the field of natural language processing (NLP). In this work, we re-visit this assumption and carry out an investigation in clinical NLP, specifically Named Entity Recognition on drugs and their related attributes. We compare Transformer models that are trained from scratch to fine-tuned BERT-based LLMs namely BERT, BioBERT, and ClinicalBERT. Furthermore, we examine the impact of an additional CRF layer on such models to enc...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The practice of fine-tuning Pre-trained Language Models (PLMs) from general or domain-specific data to a specific task with limited resources, has gained popularity within the field of natural language processing (NLP)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2210.12770v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2210.12770v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Belkadi, L. Han, Y. Wu, and G. Nenadic, &quot;Exploring the Value of Pre-trained Language Models for Clinical Named Entity Recognition,&quot; arXiv, 2022. [Online]. Available: http://arxiv.org/abs/2210.12770v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398406080')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2022" data-category="cs.cl" data-title="prompting is programming: a query language for large language models" data-keywords="control language model cs.cl cs.ai" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2212.06094v3" target="_blank">Prompting Is Programming: A Query Language for Large Language Models</a>
                            </h3>
                            <p class="card-authors">Luca Beurer-Kellner, Marc Fischer, Martin Vechev</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Control</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4398405120">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models have demonstrated outstanding performance on a wide range of tasks such as question answering and code generation. On a high level, given an input, a language model can be used to automatically complete the sequence in a statistically-likely way. Based on this, users prompt these models with language instructions or examples, to implement a variety of downstream tasks. Advanced prompting methods can even imply interaction between the language model, a user, and external tools such as calculators. However, to obtain state-of-the-art performance or adapt language models for...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">However, to obtain state-of-the-art performance or adapt language models for specific tasks, complex task- and model-specific programs have to be implemented, which may still require ad-hoc interaction.
  Based on this, we present the novel idea of Language Model Programming (LMP)</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large language models have demonstrated outstanding performance on a wide range of tasks such as question answering and code generation</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2212.06094v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2212.06094v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Beurer-Kellner, M. Fischer, and M. Vechev, &quot;Prompting Is Programming: A Query Language for Large Language Models,&quot; arXiv, 2022. [Online]. Available: http://arxiv.org/abs/2212.06094v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398405120')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2022" data-category="cs.cl" data-title="end-to-end spoken language understanding: performance analyses of a voice command task in a low resource setting" data-keywords="neural network optimization cs.cl cs.sd eess.as" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2207.08179v1" target="_blank">End-to-End Spoken Language Understanding: Performance analyses of a voice command task in a low resource setting</a>
                            </h3>
                            <p class="card-authors">Thierry Desot, Fran√ßois Portet, Michel Vacher</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.SD</span></div>

                            <div class="card-details" id="details-4398406224">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Spoken Language Understanding (SLU) is a core task in most human-machine interaction systems. With the emergence of smart homes, smart phones and smart speakers, SLU has become a key technology for the industry. In a classical SLU approach, an Automatic Speech Recognition (ASR) module transcribes the speech signal into a textual representation from which a Natural Language Understanding (NLU) module extracts semantic information. Recently End-to-End SLU (E2E SLU) based on Deep Neural Networks has gained momentum since it benefits from the joint optimization of the ASR and the NLU parts, hence ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we present a study identifying the signal features and other linguistic properties used by an E2E model to perform the SLU task</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In a classical SLU approach, an Automatic Speech Recognition (ASR) module transcribes the speech signal into a textual representation from which a Natural Language Understanding (NLU) module extracts semantic information</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2207.08179v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2207.08179v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. Desot, F. Portet, and M. Vacher, &quot;End-to-End Spoken Language Understanding: Performance analyses of a voice command task in a low resource setting,&quot; arXiv, 2022. [Online]. Available: http://arxiv.org/abs/2207.08179v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398406224')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2022" data-category="cs.cl" data-title="an overview of indian spoken language recognition from machine learning perspective" data-keywords="neural network cs.cl cs.sd eess.as" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2212.03812v1" target="_blank">An Overview of Indian Spoken Language Recognition from Machine Learning Perspective</a>
                            </h3>
                            <p class="card-authors">Spandan Dey, Md Sahidullah, Goutam Saha</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.SD</span><span class="keyword-tag">EESS.AS</span></div>

                            <div class="card-details" id="details-4398406128">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Automatic spoken language identification (LID) is a very important research field in the era of multilingual voice-command-based human-computer interaction (HCI). A front-end LID module helps to improve the performance of many speech-based applications in the multilingual scenario. India is a populous country with diverse cultures and languages. The majority of the Indian population needs to use their respective native languages for verbal interaction with machines. Therefore, the development of efficient Indian spoken language recognition systems is useful for adapting smart technologies in e...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">In-depth analysis has been presented to emphasize the unique challenges of low-resource and mutual influences for developing LID systems in the Indian contexts</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Several essential aspects of the Indian LID research, such as the detailed description of the available speech corpora, the major research contributions, including the earlier attempts based on statistical modeling to the recent approaches based on different neural network architectures, and the future research trends are discussed</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2212.03812v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2212.03812v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Dey, M. Sahidullah, and G. Saha, &quot;An Overview of Indian Spoken Language Recognition from Machine Learning Perspective,&quot; arXiv, 2022. [Online]. Available: http://arxiv.org/abs/2212.03812v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398406128')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section year-section" data-year="2021">
                <h2 class="section-header">üìÖ 2021 <span class="section-count">(1 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2021" data-category="cs.lg" data-title="differentially private fine-tuning of language models" data-keywords="bert gpt nlp language model cs.lg cs.cl cs.cr" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Machine Learning</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2110.06500v2" target="_blank">Differentially Private Fine-tuning of Language Models</a>
                            </h3>
                            <p class="card-authors">Da Yu, Saurabh Naik, Arturs Backurs et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Bert</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4398405840">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We give simpler, sparser, and faster algorithms for differentially private fine-tuning of large-scale pre-trained language models, which achieve the state-of-the-art privacy versus utility tradeoffs on many standard NLP tasks. We propose a meta-framework for this problem, inspired by the recent success of highly parameter-efficient methods for fine-tuning. Our experiments show that differentially private adaptations of these approaches outperform previous private algorithms in three important dimensions: utility, privacy, and the computational and memory cost of private training. On many commo...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a meta-framework for this problem, inspired by the recent success of highly parameter-efficient methods for fine-tuning</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">We propose a meta-framework for this problem, inspired by the recent success of highly parameter-efficient methods for fine-tuning</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We give simpler, sparser, and faster algorithms for differentially private fine-tuning of large-scale pre-trained language models, which achieve the state-of-the-art privacy versus utility tradeoffs on many standard NLP tasks</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2110.06500v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2110.06500v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Yu et al., &quot;Differentially Private Fine-tuning of Language Models,&quot; arXiv, 2021. [Online]. Available: http://arxiv.org/abs/2110.06500v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4398405840')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

        </div>

        <!-- Category View -->
        <div id="category-view" class="hidden">

            <div class="group-section cat-section" data-category="other">
                <h2 class="section-header">üè∑Ô∏è OTHER <span class="section-count">(142 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="anymal parkour: learning agile navigation for quadrupedal robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">High Impact</span></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85187730333&amp;origin=resultslist" target="_blank">ANYmal parkour: Learning agile navigation for quadrupedal robots</a>
                            </h3>
                            <p class="card-authors">Hoeller D.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398444080">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85187730333&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1126/scirobotics.adi7566" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. D., &quot;ANYmal parkour: Learning agile navigation for quadrupedal robots,&quot; Science Robotics, 2024. doi: 10.1126/scirobotics.adi7566. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85187730333&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398444080')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="learning quadrupedal locomotion on deformable terrain" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">High Impact</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85146831030&amp;origin=resultslist" target="_blank">Learning quadrupedal locomotion on deformable terrain</a>
                            </h3>
                            <p class="card-authors">Choi S.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398444128">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85146831030&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1126/scirobotics.ade2256" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. S., &quot;Learning quadrupedal locomotion on deformable terrain,&quot; Science Robotics, 2023. doi: 10.1126/scirobotics.ade2256. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85146831030&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398444128')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="vilens: visual, inertial, lidar, and leg odometry for all-terrain legged robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">High Impact</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85136130682&amp;origin=resultslist" target="_blank">VILENS: Visual, Inertial, Lidar, and Leg Odometry for All-Terrain Legged Robots</a>
                            </h3>
                            <p class="card-authors">Wisth D.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398432656">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85136130682&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2022.3193788" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. D., &quot;VILENS: Visual, Inertial, Lidar, and Leg Odometry for All-Terrain Legged Robots,&quot; IEEE Transactions on Robotics, 2023. doi: 10.1109/TRO.2022.3193788. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85136130682&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398432656')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="optimization-based control for dynamic legged robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">High Impact</span></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85174809687&amp;origin=resultslist" target="_blank">Optimization-Based Control for Dynamic Legged Robots</a>
                            </h3>
                            <p class="card-authors">Wensing P.M.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398432752">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85174809687&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2023.3324580" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. P.M., &quot;Optimization-Based Control for Dynamic Legged Robots,&quot; IEEE Transactions on Robotics, 2024. doi: 10.1109/TRO.2023.3324580. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85174809687&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398432752')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="Scopus" data-year="2023" data-category="other" data-title="intelligent control of multilegged robot smooth motion: a review" data-keywords="" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">High Impact</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168258314&amp;origin=resultslist" target="_blank">Intelligent Control of Multilegged Robot Smooth Motion: A Review</a>
                            </h3>
                            <p class="card-authors">Zhao Y.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398444512">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168258314&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/ACCESS.2023.3304992" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Y., &quot;Intelligent Control of Multilegged Robot Smooth Motion: A Review,&quot; IEEE Access, 2023. doi: 10.1109/ACCESS.2023.3304992. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85168258314&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398444512')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="scientific exploration of challenging planetary analog environments with a team of legged robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">High Impact</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85164541236&amp;origin=resultslist" target="_blank">Scientific exploration of challenging planetary analog environments with a team of legged robots</a>
                            </h3>
                            <p class="card-authors">Arm P.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398445472">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85164541236&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1126/scirobotics.ade9548" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. P., &quot;Scientific exploration of challenging planetary analog environments with a team of legged robots,&quot; Science Robotics, 2023. doi: 10.1126/scirobotics.ade9548. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85164541236&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398445472')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="robot-assisted mobile scanning for automated 3d reconstruction and point cloud semantic segmentation of building interiors" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">High Impact</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85160205515&amp;origin=resultslist" target="_blank">Robot-assisted mobile scanning for automated 3D reconstruction and point cloud semantic segmentation of building interiors</a>
                            </h3>
                            <p class="card-authors">Hu D.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398444560">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85160205515&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1016/j.autcon.2023.104949" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. D., &quot;Robot-assisted mobile scanning for automated 3D reconstruction and point cloud semantic segmentation of building interiors,&quot; Automation in Construction, 2023. doi: 10.1016/j.autcon.2023.104949. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85160205515&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398444560')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="deep whole-body control: learning a unified policy for manipulation and locomotion" data-keywords="" data-themes="I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">High Impact</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85164921897&amp;origin=resultslist" target="_blank">Deep Whole-Body Control: Learning a Unified Policy for Manipulation and Locomotion</a>
                            </h3>
                            <p class="card-authors">Fu Z.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398433520">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85164921897&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. Z., &quot;Deep Whole-Body Control: Learning a Unified Policy for Manipulation and Locomotion,&quot; Proceedings of Machine Learning Research, 2023. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85164921897&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398433520')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="rapid locomotion via reinforcement learning" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">High Impact</span></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85181503675&amp;origin=resultslist" target="_blank">Rapid locomotion via reinforcement learning</a>
                            </h3>
                            <p class="card-authors">Margolis G.B.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398434000">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85181503675&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1177/02783649231224053" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. G.B., &quot;Rapid locomotion via reinforcement learning,&quot; International Journal of Robotics Research, 2024. doi: 10.1177/02783649231224053. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85181503675&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398434000')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="learning robust and agile legged locomotion using adversarial motion priors" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">High Impact</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85163460343&amp;origin=resultslist" target="_blank">Learning Robust and Agile Legged Locomotion Using Adversarial Motion Priors</a>
                            </h3>
                            <p class="card-authors">Wu J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398443312">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85163460343&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3290509" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. J., &quot;Learning Robust and Agile Legged Locomotion Using Adversarial Motion Priors,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3290509. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85163460343&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398443312')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="biconmp: a nonlinear model predictive control framework for whole body motion planning" data-keywords="" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">High Impact</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147306921&amp;origin=resultslist" target="_blank">BiConMP: A Nonlinear Model Predictive Control Framework for Whole Body Motion Planning</a>
                            </h3>
                            <p class="card-authors">Meduri A.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398443456">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147306921&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2022.3228390" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. A., &quot;BiConMP: A Nonlinear Model Predictive Control Framework for Whole Body Motion Planning,&quot; IEEE Transactions on Robotics, 2023. doi: 10.1109/TRO.2022.3228390. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85147306921&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398443456')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="learning robust autonomous navigation and locomotion for wheeled-legged robots" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">High Impact</span></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85191487739&amp;origin=resultslist" target="_blank">Learning robust autonomous navigation and locomotion for wheeled-legged robots</a>
                            </h3>
                            <p class="card-authors">Lee J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398445568">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85191487739&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1126/scirobotics.adi9641" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. J., &quot;Learning robust autonomous navigation and locomotion for wheeled-legged robots,&quot; Science Robotics, 2024. doi: 10.1126/scirobotics.adi9641. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85191487739&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398445568')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="Scopus" data-year="2023" data-category="other" data-title="m3ed: multi-robot, multi-sensor, multi-environment event dataset" data-keywords="" data-themes="R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">High Impact</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85170821836&amp;origin=resultslist" target="_blank">M3ED: Multi-Robot, Multi-Sensor, Multi-Environment Event Dataset</a>
                            </h3>
                            <p class="card-authors">Chaney K.</p>
                            <div class="theme-tags"><span class="theme-badge theme-R">Multi-Robot</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398432368">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85170821836&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/CVPRW59228.2023.00419" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. K., &quot;M3ED: Multi-Robot, Multi-Sensor, Multi-Environment Event Dataset,&quot; IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, 2023. doi: 10.1109/CVPRW59228.2023.00419. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85170821836&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398432368')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="fast contact-implicit model predictive control" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">High Impact</span></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85182382427&amp;origin=resultslist" target="_blank">Fast Contact-Implicit Model Predictive Control</a>
                            </h3>
                            <p class="card-authors">Le Cleac&#x27;h S.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398445760">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85182382427&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2024.3351554" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. C. S., &quot;Fast Contact-Implicit Model Predictive Control,&quot; IEEE Transactions on Robotics, 2024. doi: 10.1109/TRO.2024.3351554. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85182382427&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398445760')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="energy-efficient hydraulic pump control for legged robots using model predictive control" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85136037715&amp;origin=resultslist" target="_blank">Energy-Efficient Hydraulic Pump Control for Legged Robots Using Model Predictive Control</a>
                            </h3>
                            <p class="card-authors">Cho B.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398445520">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85136037715&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TMECH.2022.3190506" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. B., &quot;Energy-Efficient Hydraulic Pump Control for Legged Robots Using Model Predictive Control,&quot; IEEE ASME Transactions on Mechatronics, 2023. doi: 10.1109/TMECH.2022.3190506. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85136037715&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398445520')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="Scopus" data-year="2023" data-category="other" data-title="advanced skills through multiple adversarial motion priors in reinforcement learning" data-keywords="" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168698851&amp;origin=resultslist" target="_blank">Advanced Skills through Multiple Adversarial Motion Priors in Reinforcement Learning</a>
                            </h3>
                            <p class="card-authors">Vollenweider E.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398445328">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168698851&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/ICRA48891.2023.10160751" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'V. E., &quot;Advanced Skills through Multiple Adversarial Motion Priors in Reinforcement Learning,&quot; Proceedings IEEE International Conference on Robotics and Automation, 2023. doi: 10.1109/ICRA48891.2023.10160751. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85168698851&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398445328')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="icub3 avatar system: enabling remote fully immersive embodiment of humanoid robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85183335405&amp;origin=resultslist" target="_blank">iCub3 avatar system: Enabling remote fully immersive embodiment of humanoid robots</a>
                            </h3>
                            <p class="card-authors">Dafarra S.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398442928">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85183335405&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1126/scirobotics.adh3834" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. S., &quot;iCub3 avatar system: Enabling remote fully immersive embodiment of humanoid robots,&quot; Science Robotics, 2024. doi: 10.1126/scirobotics.adh3834. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85183335405&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398442928')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="development of the lstm model and universal polynomial equation for all the sub-phases of human gait" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85161564810&amp;origin=resultslist" target="_blank">Development of the LSTM Model and Universal Polynomial Equation for All the Sub-Phases of Human Gait</a>
                            </h3>
                            <p class="card-authors">Semwal V.B.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398445616">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85161564810&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/JSEN.2023.3281401" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. V.B., &quot;Development of the LSTM Model and Universal Polynomial Equation for All the Sub-Phases of Human Gait,&quot; IEEE Sensors Journal, 2023. doi: 10.1109/JSEN.2023.3281401. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85161564810&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398445616')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="genloco: generalized locomotion controllers for quadrupedal robots" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85164969619&amp;origin=resultslist" target="_blank">GenLoco: Generalized Locomotion Controllers for Quadrupedal Robots</a>
                            </h3>
                            <p class="card-authors">Feng G.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398445712">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85164969619&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. G., &quot;GenLoco: Generalized Locomotion Controllers for Quadrupedal Robots,&quot; Proceedings of Machine Learning Research, 2023. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85164969619&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398445712')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="electrohydraulic musculoskeletal robotic leg for agile, adaptive, yet energy-efficient locomotion" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85203331069&amp;origin=resultslist" target="_blank">Electrohydraulic musculoskeletal robotic leg for agile, adaptive, yet energy-efficient locomotion</a>
                            </h3>
                            <p class="card-authors">Buchner T.J.K.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398443888">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85203331069&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1038/s41467-024-51568-3" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. T.J.K., &quot;Electrohydraulic musculoskeletal robotic leg for agile, adaptive, yet energy-efficient locomotion,&quot; Nature Communications, 2024. doi: 10.1038/s41467-024-51568-3. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85203331069&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398443888')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="neural volumetric memory for visual locomotion control" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85163788700&amp;origin=resultslist" target="_blank">Neural Volumetric Memory for Visual Locomotion Control</a>
                            </h3>
                            <p class="card-authors">Yang R.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398433568">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85163788700&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/CVPR52729.2023.00144" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. R., &quot;Neural Volumetric Memory for Visual Locomotion Control,&quot; Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2023. doi: 10.1109/CVPR52729.2023.00144. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85163788700&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398433568')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2024" data-category="other" data-title="biohybrid bipedal robot powered by skeletal muscle tissue" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85184781793&amp;origin=resultslist" target="_blank">Biohybrid bipedal robot powered by skeletal muscle tissue</a>
                            </h3>
                            <p class="card-authors">Kinjo R.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398433280">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85184781793&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1016/j.matt.2023.12.035" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. R., &quot;Biohybrid bipedal robot powered by skeletal muscle tissue,&quot; Matter, 2024. doi: 10.1016/j.matt.2023.12.035. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85184781793&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398433280')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="learning modular robot control policies" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85163756601&amp;origin=resultslist" target="_blank">Learning Modular Robot Control Policies</a>
                            </h3>
                            <p class="card-authors">Whitman J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398443408">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85163756601&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2023.3284362" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. J., &quot;Learning Modular Robot Control Policies,&quot; IEEE Transactions on Robotics, 2023. doi: 10.1109/TRO.2023.3284362. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85163756601&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398443408')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="not only rewards but also constraints: applications on legged robot locomotion" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85193256463&amp;origin=resultslist" target="_blank">Not only Rewards but Also Constraints: Applications on Legged Robot Locomotion</a>
                            </h3>
                            <p class="card-authors">Kim Y.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398433184">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85193256463&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2024.3400935" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Y., &quot;Not only Rewards but Also Constraints: Applications on Legged Robot Locomotion,&quot; IEEE Transactions on Robotics, 2024. doi: 10.1109/TRO.2024.3400935. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85193256463&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398433184')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="efficient path planning in narrow passages for robots with ellipsoidal components" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85135748595&amp;origin=resultslist" target="_blank">Efficient Path Planning in Narrow Passages for Robots With Ellipsoidal Components</a>
                            </h3>
                            <p class="card-authors">Ruan S.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398443024">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85135748595&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2022.3187818" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. S., &quot;Efficient Path Planning in Narrow Passages for Robots With Ellipsoidal Components,&quot; IEEE Transactions on Robotics, 2023. doi: 10.1109/TRO.2022.3187818. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85135748595&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398443024')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="reward-adaptive reinforcement learning: dynamic policy gradient optimization for bipedal locomotion" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85144064950&amp;origin=resultslist" target="_blank">Reward-Adaptive Reinforcement Learning: Dynamic Policy Gradient Optimization for Bipedal Locomotion</a>
                            </h3>
                            <p class="card-authors">Huang C.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398443744">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85144064950&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TPAMI.2022.3223407" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. C., &quot;Reward-Adaptive Reinforcement Learning: Dynamic Policy Gradient Optimization for Bipedal Locomotion,&quot; IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. doi: 10.1109/TPAMI.2022.3223407. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85144064950&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398443744')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="robotic monitoring of habitats: the natural intelligence approach" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85164706103&amp;origin=resultslist" target="_blank">Robotic Monitoring of Habitats: The Natural Intelligence Approach</a>
                            </h3>
                            <p class="card-authors">Angelini F.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398433664">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85164706103&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/ACCESS.2023.3294276" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. F., &quot;Robotic Monitoring of Habitats: The Natural Intelligence Approach,&quot; IEEE Access, 2023. doi: 10.1109/ACCESS.2023.3294276. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85164706103&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398433664')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="efficient anytime clf reactive planning system for a bipedal robot on undulating terrain" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147303984&amp;origin=resultslist" target="_blank">Efficient Anytime CLF Reactive Planning System for a Bipedal Robot on Undulating Terrain</a>
                            </h3>
                            <p class="card-authors">Huang J.K.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398432800">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147303984&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2022.3228713" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. J.K., &quot;Efficient Anytime CLF Reactive Planning System for a Bipedal Robot on Undulating Terrain,&quot; IEEE Transactions on Robotics, 2023. doi: 10.1109/TRO.2022.3228713. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85147303984&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398432800')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="development of bioinspired multimodal underwater robot ‚Äúhero-blue‚Äù for walking, swimming, and crawling" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85182952443&amp;origin=resultslist" target="_blank">Development of Bioinspired Multimodal Underwater Robot ‚ÄúHERO-BLUE‚Äù for Walking, Swimming, and Crawling</a>
                            </h3>
                            <p class="card-authors">Kim T.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398444176">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85182952443&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2024.3353040" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. T., &quot;Development of Bioinspired Multimodal Underwater Robot ‚ÄúHERO-BLUE‚Äù for Walking, Swimming, and Crawling,&quot; IEEE Transactions on Robotics, 2024. doi: 10.1109/TRO.2024.3353040. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85182952443&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398444176')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="inverse-dynamics mpc via nullspace resolution" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85153393919&amp;origin=resultslist" target="_blank">Inverse-Dynamics MPC via Nullspace Resolution</a>
                            </h3>
                            <p class="card-authors">Mastalli C.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398432464">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85153393919&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2023.3262186" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. C., &quot;Inverse-Dynamics MPC via Nullspace Resolution,&quot; IEEE Transactions on Robotics, 2023. doi: 10.1109/TRO.2023.3262186. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85153393919&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398432464')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="cerberus: low-drift visual-inertial-leg odometry for agile locomotion" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168678256&amp;origin=resultslist" target="_blank">Cerberus: Low-Drift Visual-Inertial-Leg Odometry For Agile Locomotion</a>
                            </h3>
                            <p class="card-authors">Yang S.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398433424">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168678256&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/ICRA48891.2023.10160486" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. S., &quot;Cerberus: Low-Drift Visual-Inertial-Leg Odometry For Agile Locomotion,&quot; Proceedings IEEE International Conference on Robotics and Automation, 2023. doi: 10.1109/ICRA48891.2023.10160486. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85168678256&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398433424')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="lifelike agility and play in quadrupedal robots using reinforcement learning and generative pre-trained models" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85197508387&amp;origin=resultslist" target="_blank">Lifelike agility and play in quadrupedal robots using reinforcement learning and generative pre-trained models</a>
                            </h3>
                            <p class="card-authors">Han L.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398444320">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85197508387&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1038/s42256-024-00861-3" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. L., &quot;Lifelike agility and play in quadrupedal robots using reinforcement learning and generative pre-trained models,&quot; Nature Machine Intelligence, 2024. doi: 10.1038/s42256-024-00861-3. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85197508387&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398444320')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="rl + model-based control: using on-demand optimal control to learn versatile legged locomotion" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168715205&amp;origin=resultslist" target="_blank">RL + Model-Based Control: Using On-Demand Optimal Control to Learn Versatile Legged Locomotion</a>
                            </h3>
                            <p class="card-authors">Kang D.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398445424">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168715205&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3307008" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. D., &quot;RL + Model-Based Control: Using On-Demand Optimal Control to Learn Versatile Legged Locomotion,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3307008. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85168715205&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398445424')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="robust convex model predictive control for quadruped locomotion under uncertainties" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85167778962&amp;origin=resultslist" target="_blank">Robust Convex Model Predictive Control for Quadruped Locomotion Under Uncertainties</a>
                            </h3>
                            <p class="card-authors">Xu S.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398445376">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85167778962&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2023.3299527" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. S., &quot;Robust Convex Model Predictive Control for Quadruped Locomotion Under Uncertainties,&quot; IEEE Transactions on Robotics, 2023. doi: 10.1109/TRO.2023.3299527. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85167778962&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398445376')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="vital: vision-based terrain-aware locomotion for legged robots" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85144013474&amp;origin=resultslist" target="_blank">ViTAL: Vision-Based Terrain-Aware Locomotion for Legged Robots</a>
                            </h3>
                            <p class="card-authors">Fahmi S.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398443216">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85144013474&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2022.3222958" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. S., &quot;ViTAL: Vision-Based Terrain-Aware Locomotion for Legged Robots,&quot; IEEE Transactions on Robotics, 2023. doi: 10.1109/TRO.2022.3222958. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85144013474&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398443216')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="multilegged matter transport: a framework for locomotion on noisy landscapes" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85165369068&amp;origin=resultslist" target="_blank">Multilegged matter transport: A framework for locomotion on noisy landscapes</a>
                            </h3>
                            <p class="card-authors">Chong B.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398433952">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85165369068&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1126/science.ade4985" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. B., &quot;Multilegged matter transport: A framework for locomotion on noisy landscapes,&quot; Science, 2023. doi: 10.1126/science.ade4985. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85165369068&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398433952')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="hybrid ilqr model predictive control for contact implicit stabilization on legged robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85171744736&amp;origin=resultslist" target="_blank">Hybrid iLQR Model Predictive Control for Contact Implicit Stabilization on Legged Robots</a>
                            </h3>
                            <p class="card-authors">Kong N.J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398443984">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85171744736&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2023.3308773" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. N.J., &quot;Hybrid iLQR Model Predictive Control for Contact Implicit Stabilization on Legged Robots,&quot; IEEE Transactions on Robotics, 2023. doi: 10.1109/TRO.2023.3308773. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85171744736&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398443984')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="highly-time-resolved fmcw lidar with synchronously-nonlinearity-corrected acquisition for dynamic locomotion" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85148302116&amp;origin=resultslist" target="_blank">Highly-time-resolved FMCW LiDAR with synchronously-nonlinearity-corrected acquisition for dynamic locomotion</a>
                            </h3>
                            <p class="card-authors">Sun C.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398432560">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85148302116&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1364/OE.480346" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. C., &quot;Highly-time-resolved FMCW LiDAR with synchronously-nonlinearity-corrected acquisition for dynamic locomotion,&quot; Optics Express, 2023. doi: 10.1364/OE.480346. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85148302116&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398432560')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="underwater legged robotics: review and perspectives" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85152616268&amp;origin=resultslist" target="_blank">Underwater legged robotics: review and perspectives</a>
                            </h3>
                            <p class="card-authors">Picardi G.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398444464">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85152616268&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1088/1748-3190/acc0bb" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. G., &quot;Underwater legged robotics: review and perspectives,&quot; Bioinspiration and Biomimetics, 2023. doi: 10.1088/1748-3190/acc0bb. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85152616268&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398444464')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="learning-based design and control for quadrupedal robots with parallel-elastic actuators" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147225048&amp;origin=resultslist" target="_blank">Learning-Based Design and Control for Quadrupedal Robots With Parallel-Elastic Actuators</a>
                            </h3>
                            <p class="card-authors">Bjelonic F.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398444608">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147225048&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3234809" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. F., &quot;Learning-Based Design and Control for Quadrupedal Robots With Parallel-Elastic Actuators,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3234809. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85147225048&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398444608')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2024" data-category="other" data-title="adaptive-force-based control of dynamic legged locomotion over uneven terrain" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85189350789&amp;origin=resultslist" target="_blank">Adaptive-Force-Based Control of Dynamic Legged Locomotion Over Uneven Terrain</a>
                            </h3>
                            <p class="card-authors">Sombolestan M.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398444656">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85189350789&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2024.3381554" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. M., &quot;Adaptive-Force-Based Control of Dynamic Legged Locomotion Over Uneven Terrain,&quot; IEEE Transactions on Robotics, 2024. doi: 10.1109/TRO.2024.3381554. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85189350789&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398444656')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2024" data-category="other" data-title="max: a wheeled-legged quadruped robot for multimodal agile locomotion" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85181580701&amp;origin=resultslist" target="_blank">Max: A Wheeled-Legged Quadruped Robot for Multimodal Agile Locomotion</a>
                            </h3>
                            <p class="card-authors">Zhou Q.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398444704">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85181580701&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TASE.2023.3345876" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Q., &quot;Max: A Wheeled-Legged Quadruped Robot for Multimodal Agile Locomotion,&quot; IEEE Transactions on Automation Science and Engineering, 2024. doi: 10.1109/TASE.2023.3345876. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85181580701&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398444704')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="stable flexible-joint floating-base robot balancing and locomotion via variable impedance control" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85129680415&amp;origin=resultslist" target="_blank">Stable Flexible-Joint Floating-Base Robot Balancing and Locomotion via Variable Impedance Control</a>
                            </h3>
                            <p class="card-authors">Spyrakos-Papastavridis E.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398444752">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85129680415&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TIE.2022.3169848" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. E., &quot;Stable Flexible-Joint Floating-Base Robot Balancing and Locomotion via Variable Impedance Control,&quot; IEEE Transactions on Industrial Electronics, 2023. doi: 10.1109/TIE.2022.3169848. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85129680415&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398444752')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="autonomous navigation of underactuated bipedal robots in height-constrained environments" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85165440944&amp;origin=resultslist" target="_blank">Autonomous navigation of underactuated bipedal robots in height-constrained environments</a>
                            </h3>
                            <p class="card-authors">Li Z.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398444800">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85165440944&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1177/02783649231187670" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Z., &quot;Autonomous navigation of underactuated bipedal robots in height-constrained environments,&quot; International Journal of Robotics Research, 2023. doi: 10.1177/02783649231187670. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85165440944&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398444800')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="research on wheel-legged robot based on lqr and adrc" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85171172472&amp;origin=resultslist" target="_blank">Research on wheel-legged robot based on LQR and ADRC</a>
                            </h3>
                            <p class="card-authors">Feng X.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398444848">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85171172472&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1038/s41598-023-41462-1" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. X., &quot;Research on wheel-legged robot based on LQR and ADRC,&quot; Scientific Reports, 2023. doi: 10.1038/s41598-023-41462-1. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85171172472&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398444848')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="Scopus" data-year="2024" data-category="other" data-title="compliant motion control of wheel-legged humanoid robot on rough terrains" data-keywords="" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85174819735&amp;origin=resultslist" target="_blank">Compliant Motion Control of Wheel-Legged Humanoid Robot on Rough Terrains</a>
                            </h3>
                            <p class="card-authors">Zhao L.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398444896">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85174819735&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TMECH.2023.3320762" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. L., &quot;Compliant Motion Control of Wheel-Legged Humanoid Robot on Rough Terrains,&quot; IEEE ASME Transactions on Mechatronics, 2024. doi: 10.1109/TMECH.2023.3320762. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85174819735&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398444896')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="towards generation and transition of diverse gaits for quadrupedal robots based on trajectory optimization and whole-body impedance control" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85149405011&amp;origin=resultslist" target="_blank">Towards Generation and Transition of Diverse Gaits for Quadrupedal Robots Based on Trajectory Optimization and Whole-Body Impedance Control</a>
                            </h3>
                            <p class="card-authors">Li Q.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398444944">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85149405011&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3251184" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Q., &quot;Towards Generation and Transition of Diverse Gaits for Quadrupedal Robots Based on Trajectory Optimization and Whole-Body Impedance Control,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3251184. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85149405011&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398444944')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="lateral flexion of a compliant spine improves motor performance in a bioinspired mouse robot" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85179017456&amp;origin=resultslist" target="_blank">Lateral flexion of a compliant spine improves motor performance in a bioinspired mouse robot</a>
                            </h3>
                            <p class="card-authors">Bing Z.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398444992">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85179017456&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1126/scirobotics.adg7165" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. Z., &quot;Lateral flexion of a compliant spine improves motor performance in a bioinspired mouse robot,&quot; Science Robotics, 2023. doi: 10.1126/scirobotics.adg7165. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85179017456&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398444992')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="reinforcement learning-based stable jump control method for asteroid-exploration quadruped robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85175607948&amp;origin=resultslist" target="_blank">Reinforcement learning-based stable jump control method for asteroid-exploration quadruped robots</a>
                            </h3>
                            <p class="card-authors">Qi J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398445184">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85175607948&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1016/j.ast.2023.108689" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Q. J., &quot;Reinforcement learning-based stable jump control method for asteroid-exploration quadruped robots,&quot; Aerospace Science and Technology, 2023. doi: 10.1016/j.ast.2023.108689. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85175607948&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398445184')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="design of clari: a miniature modular origami passive shape-morphing robot" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168902589&amp;origin=resultslist" target="_blank">Design of CLARI: A Miniature Modular Origami Passive Shape-Morphing Robot</a>
                            </h3>
                            <p class="card-authors">Kabutz H.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398444224">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168902589&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1002/aisy.202300181" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. H., &quot;Design of CLARI: A Miniature Modular Origami Passive Shape-Morphing Robot,&quot; Advanced Intelligent Systems, 2023. doi: 10.1002/aisy.202300181. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85168902589&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398444224')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="layered control for cooperative locomotion of two quadrupedal robots: centralized and distributed approaches" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85174812089&amp;origin=resultslist" target="_blank">Layered Control for Cooperative Locomotion of Two Quadrupedal Robots: Centralized and Distributed Approaches</a>
                            </h3>
                            <p class="card-authors">Kim J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398444272">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85174812089&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2023.3319896" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. J., &quot;Layered Control for Cooperative Locomotion of Two Quadrupedal Robots: Centralized and Distributed Approaches,&quot; IEEE Transactions on Robotics, 2023. doi: 10.1109/TRO.2023.3319896. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85174812089&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398444272')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="optimizing bipedal locomotion for the 100m dash with comparison to human running" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168676400&amp;origin=resultslist" target="_blank">Optimizing Bipedal Locomotion for The 100m Dash With Comparison to Human Running</a>
                            </h3>
                            <p class="card-authors">Crowley D.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398445040">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168676400&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/ICRA48891.2023.10160436" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. D., &quot;Optimizing Bipedal Locomotion for The 100m Dash With Comparison to Human Running,&quot; Proceedings IEEE International Conference on Robotics and Automation, 2023. doi: 10.1109/ICRA48891.2023.10160436. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85168676400&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398445040')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="deep ensemble learning approach for lower limb movement recognition from multichannel semg signals" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85185144957&amp;origin=resultslist" target="_blank">Deep ensemble learning approach for lower limb movement recognition from multichannel sEMG signals</a>
                            </h3>
                            <p class="card-authors">Tokas P.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398445088">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85185144957&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1007/s00521-024-09465-9" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. P., &quot;Deep ensemble learning approach for lower limb movement recognition from multichannel sEMG signals,&quot; Neural Computing and Applications, 2024. doi: 10.1007/s00521-024-09465-9. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85185144957&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398445088')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="liquid-metal soft electronics coupled with multi-legged robots for targeted delivery in the gastrointestinal tract" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85187259087&amp;origin=resultslist" target="_blank">Liquid-metal soft electronics coupled with multi-legged robots for targeted delivery in the gastrointestinal tract</a>
                            </h3>
                            <p class="card-authors">Ye Z.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398444368">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85187259087&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1016/j.device.2023.100181" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Z., &quot;Liquid-metal soft electronics coupled with multi-legged robots for targeted delivery in the gastrointestinal tract,&quot; Device, 2024. doi: 10.1016/j.device.2023.100181. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85187259087&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398444368')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="contact optimization for non-prehensile loco-manipulation via hierarchical model predictive control" data-keywords="" data-themes="M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168690385&amp;origin=resultslist" target="_blank">Contact Optimization for Non-Prehensile Loco-Manipulation via Hierarchical Model Predictive Control</a>
                            </h3>
                            <p class="card-authors">Rigo A.</p>
                            <div class="theme-tags"><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398445952">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168690385&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/ICRA48891.2023.10160507" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. A., &quot;Contact Optimization for Non-Prehensile Loco-Manipulation via Hierarchical Model Predictive Control,&quot; Proceedings IEEE International Conference on Robotics and Automation, 2023. doi: 10.1109/ICRA48891.2023.10160507. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85168690385&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398445952')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2024" data-category="other" data-title="ai-cpg: adaptive imitated central pattern generators for bipedal locomotion learned through reinforced reflex neural networks" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85190737819&amp;origin=resultslist" target="_blank">AI-CPG: Adaptive Imitated Central Pattern Generators for Bipedal Locomotion Learned Through Reinforced Reflex Neural Networks</a>
                            </h3>
                            <p class="card-authors">Li G.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398446000">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85190737819&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2024.3388842" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. G., &quot;AI-CPG: Adaptive Imitated Central Pattern Generators for Bipedal Locomotion Learned Through Reinforced Reflex Neural Networks,&quot; IEEE Robotics and Automation Letters, 2024. doi: 10.1109/LRA.2024.3388842. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85190737819&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398446000')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="Scopus" data-year="2023" data-category="other" data-title="lstp: long short-term motion planning for legged and legged-wheeled systems" data-keywords="" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168661568&amp;origin=resultslist" target="_blank">LSTP: Long Short-Term Motion Planning for Legged and Legged-Wheeled Systems</a>
                            </h3>
                            <p class="card-authors">Jelavic E.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398446048">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168661568&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2023.3302239" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. E., &quot;LSTP: Long Short-Term Motion Planning for Legged and Legged-Wheeled Systems,&quot; IEEE Transactions on Robotics, 2023. doi: 10.1109/TRO.2023.3302239. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85168661568&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398446048')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="modeling and mpc-based pose tracking for wheeled bipedal robot" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-citations">Cited</span></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85174817902&amp;origin=resultslist" target="_blank">Modeling and MPC-Based Pose Tracking for Wheeled Bipedal Robot</a>
                            </h3>
                            <p class="card-authors">Yu J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398446096">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85174817902&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3322084" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. J., &quot;Modeling and MPC-Based Pose Tracking for Wheeled Bipedal Robot,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3322084. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85174817902&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398446096')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2024" data-category="other" data-title="cdm-mpc: an integrated dynamic planning and control framework for bipedal robots jumping" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85195411305&amp;origin=resultslist" target="_blank">CDM-MPC: An Integrated Dynamic Planning and Control Framework for Bipedal Robots Jumping</a>
                            </h3>
                            <p class="card-authors">He Z.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398446288">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85195411305&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2024.3408487" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Z., &quot;CDM-MPC: An Integrated Dynamic Planning and Control Framework for Bipedal Robots Jumping,&quot; IEEE Robotics and Automation Letters, 2024. doi: 10.1109/LRA.2024.3408487. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85195411305&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398446288')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2024" data-category="other" data-title="hybrid internal model: learning agile legged locomotion with simulated robot response" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85198305369&amp;origin=resultslist" target="_blank">HYBRID INTERNAL MODEL: LEARNING AGILE LEGGED LOCOMOTION WITH SIMULATED ROBOT RESPONSE</a>
                            </h3>
                            <p class="card-authors">Long J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398446144">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85198305369&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. J., &quot;HYBRID INTERNAL MODEL: LEARNING AGILE LEGGED LOCOMOTION WITH SIMULATED ROBOT RESPONSE,&quot; 12th International Conference on Learning Representations Iclr 2024, 2024. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85198305369&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398446144')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="dipper: diffusion-based 2d path planner applied on legged robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85196223831&amp;origin=resultslist" target="_blank">DiPPeR: Diffusion-based 2D Path Planner applied on Legged Robots</a>
                            </h3>
                            <p class="card-authors">Liu J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398446240">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85196223831&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/ICRA57147.2024.10610013" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. J., &quot;DiPPeR: Diffusion-based 2D Path Planner applied on Legged Robots,&quot; Proceedings IEEE International Conference on Robotics and Automation, 2024. doi: 10.1109/ICRA57147.2024.10610013. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85196223831&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398446240')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="Scopus" data-year="2023" data-category="other" data-title="synchronized human-humanoid motion imitation" data-keywords="" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85161052925&amp;origin=resultslist" target="_blank">Synchronized Human-Humanoid Motion Imitation</a>
                            </h3>
                            <p class="card-authors">Dallard A.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398446384">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85161052925&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3280807" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. A., &quot;Synchronized Human-Humanoid Motion Imitation,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3280807. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85161052925&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398446384')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2025" data-category="other" data-title="leg state estimation for quadruped robot by using probabilistic model with proprioceptive feedback" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2025</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-105008431336&amp;origin=resultslist" target="_blank">Leg State Estimation for Quadruped Robot by Using Probabilistic Model With Proprioceptive Feedback</a>
                            </h3>
                            <p class="card-authors">Sun J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398446432">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-105008431336&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TMECH.2024.3421251" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. J., &quot;Leg State Estimation for Quadruped Robot by Using Probabilistic Model With Proprioceptive Feedback,&quot; IEEE ASME Transactions on Mechatronics, 2025. doi: 10.1109/TMECH.2024.3421251. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-105008431336&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398446432')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="design, modeling, and control of a quadruped robot spidar: spherically vectorable and distributed rotors assisted air-ground quadruped robot" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85159723810&amp;origin=resultslist" target="_blank">Design, Modeling, and Control of a Quadruped Robot SPIDAR: Spherically Vectorable and Distributed Rotors Assisted Air-Ground Quadruped Robot</a>
                            </h3>
                            <p class="card-authors">Zhao M.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398446480">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85159723810&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3272285" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. M., &quot;Design, Modeling, and Control of a Quadruped Robot SPIDAR: Spherically Vectorable and Distributed Rotors Assisted Air-Ground Quadruped Robot,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3272285. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85159723810&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398446480')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="revisiting reward design and evaluation for robust humanoid standing and walking" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85216463921&amp;origin=resultslist" target="_blank">Revisiting Reward Design and Evaluation for Robust Humanoid Standing and Walking</a>
                            </h3>
                            <p class="card-authors">Van Marum B.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398446528">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85216463921&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/IROS58592.2024.10802680" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'V. M. B., &quot;Revisiting Reward Design and Evaluation for Robust Humanoid Standing and Walking,&quot; IEEE International Conference on Intelligent Robots and Systems, 2024. doi: 10.1109/IROS58592.2024.10802680. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85216463921&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398446528')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="a survey of¬†wheeled-legged robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85138002653&amp;origin=resultslist" target="_blank">A Survey of¬†Wheeled-Legged Robots</a>
                            </h3>
                            <p class="card-authors">Bjelonic M.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398446576">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85138002653&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1007/978-3-031-15226-9_11" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. M., &quot;A Survey of¬†Wheeled-Legged Robots,&quot; Lecture Notes in Networks and Systems, 2023. doi: 10.1007/978-3-031-15226-9_11. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85138002653&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398446576')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="dynamic walking of bipedal robots on uneven stepping stones via adaptive-frequency mpc" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147202313&amp;origin=resultslist" target="_blank">Dynamic Walking of Bipedal Robots on Uneven Stepping Stones via Adaptive-Frequency MPC</a>
                            </h3>
                            <p class="card-authors">Li J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398446624">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147202313&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LCSYS.2023.3234769" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. J., &quot;Dynamic Walking of Bipedal Robots on Uneven Stepping Stones via Adaptive-Frequency MPC,&quot; IEEE Control Systems Letters, 2023. doi: 10.1109/LCSYS.2023.3234769. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85147202313&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398446624')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="moral: learning morphologically adaptive locomotion controller for quadrupedal robots on challenging terrains" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85187977237&amp;origin=resultslist" target="_blank">MorAL: Learning Morphologically Adaptive Locomotion Controller for Quadrupedal Robots on Challenging Terrains</a>
                            </h3>
                            <p class="card-authors">Luo Z.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398446672">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85187977237&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2024.3375086" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Z., &quot;MorAL: Learning Morphologically Adaptive Locomotion Controller for Quadrupedal Robots on Challenging Terrains,&quot; IEEE Robotics and Automation Letters, 2024. doi: 10.1109/LRA.2024.3375086. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85187977237&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398446672')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="seamless reaction strategy for bipedal locomotion exploiting real-time nonlinear model predictive control" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85163504258&amp;origin=resultslist" target="_blank">Seamless Reaction Strategy for Bipedal Locomotion Exploiting Real-Time Nonlinear Model Predictive Control</a>
                            </h3>
                            <p class="card-authors">Choe J.H.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398446720">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85163504258&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3291273" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. J.H., &quot;Seamless Reaction Strategy for Bipedal Locomotion Exploiting Real-Time Nonlinear Model Predictive Control,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3291273. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85163504258&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398446720')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="omnidirectional jump control of a locust-computer hybrid robot" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85148250093&amp;origin=resultslist" target="_blank">Omnidirectional Jump Control of a Locust-Computer Hybrid Robot</a>
                            </h3>
                            <p class="card-authors">Liu P.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398446768">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85148250093&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1089/soro.2021.0137" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. P., &quot;Omnidirectional Jump Control of a Locust-Computer Hybrid Robot,&quot; Soft Robotics, 2023. doi: 10.1089/soro.2021.0137. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85148250093&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398446768')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="footholds optimization for legged robots walking on complex terrain" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85161813406&amp;origin=resultslist" target="_blank">Footholds optimization for legged robots walking on complex terrain</a>
                            </h3>
                            <p class="card-authors">Yin Y.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398447008">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85161813406&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1007/s11465-022-0742-y" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Y., &quot;Footholds optimization for legged robots walking on complex terrain,&quot; Frontiers of Mechanical Engineering, 2023. doi: 10.1007/s11465-022-0742-y. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85161813406&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398447008')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="development of a new robust stable walking algorithm for a humanoid robot using deep reinforcement learning with multi-sensor data fusion" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147877640&amp;origin=resultslist" target="_blank">Development of a New Robust Stable Walking Algorithm for a Humanoid Robot Using Deep Reinforcement Learning with Multi-Sensor Data Fusion</a>
                            </h3>
                            <p class="card-authors">Kaymak √á.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398445808">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147877640&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.3390/electronics12030568" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. √á., &quot;Development of a New Robust Stable Walking Algorithm for a Humanoid Robot Using Deep Reinforcement Learning with Multi-Sensor Data Fusion,&quot; Electronics Switzerland, 2023. doi: 10.3390/electronics12030568. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85147877640&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398445808')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2024" data-category="other" data-title="humanoid loco-manipulations using combined fast dense 3d tracking and slam with wide-angle depth-images" data-keywords="" data-themes="I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85162718807&amp;origin=resultslist" target="_blank">Humanoid Loco-Manipulations Using Combined Fast Dense 3D Tracking and SLAM with Wide-Angle Depth-Images</a>
                            </h3>
                            <p class="card-authors">Chappellet K.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398446336">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85162718807&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TASE.2023.3283497" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. K., &quot;Humanoid Loco-Manipulations Using Combined Fast Dense 3D Tracking and SLAM with Wide-Angle Depth-Images,&quot; IEEE Transactions on Automation Science and Engineering, 2024. doi: 10.1109/TASE.2023.3283497. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85162718807&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398446336')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="learning complex motor skills for legged robot fall recovery" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85161045216&amp;origin=resultslist" target="_blank">Learning Complex Motor Skills for Legged Robot Fall Recovery</a>
                            </h3>
                            <p class="card-authors">Yang C.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398446864">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85161045216&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3281290" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. C., &quot;Learning Complex Motor Skills for Legged Robot Fall Recovery,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3281290. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85161045216&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398446864')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="high-density emg, imu, kinetic, and kinematic open-source data for comprehensive locomotion activities" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85176240779&amp;origin=resultslist" target="_blank">High-density EMG, IMU, kinetic, and kinematic open-source data for comprehensive locomotion activities</a>
                            </h3>
                            <p class="card-authors">Dimitrov H.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398446912">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85176240779&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1038/s41597-023-02679-x" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. H., &quot;High-density EMG, IMU, kinetic, and kinematic open-source data for comprehensive locomotion activities,&quot; Scientific Data, 2023. doi: 10.1038/s41597-023-02679-x. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85176240779&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398446912')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="quadruped capturability and push recovery via a switched-systems characterization of dynamic balance" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85151553692&amp;origin=resultslist" target="_blank">Quadruped Capturability and Push Recovery via a Switched-Systems Characterization of Dynamic Balance</a>
                            </h3>
                            <p class="card-authors">Chen H.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398446960">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85151553692&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2023.3240622" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. H., &quot;Quadruped Capturability and Push Recovery via a Switched-Systems Characterization of Dynamic Balance,&quot; IEEE Transactions on Robotics, 2023. doi: 10.1109/TRO.2023.3240622. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85151553692&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398446960')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="barry: a high-payload and agile quadruped robot" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85171567292&amp;origin=resultslist" target="_blank">Barry: A High-Payload and Agile Quadruped Robot</a>
                            </h3>
                            <p class="card-authors">Valsecchi G.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398447104">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85171567292&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3313923" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'V. G., &quot;Barry: A High-Payload and Agile Quadruped Robot,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3313923. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85171567292&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398447104')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="imitating and finetuning model predictive control for robust and symmetric quadrupedal locomotion" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85173012210&amp;origin=resultslist" target="_blank">Imitating and Finetuning Model Predictive Control for Robust and Symmetric Quadrupedal Locomotion</a>
                            </h3>
                            <p class="card-authors">Youm D.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398447152">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85173012210&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3320827" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. D., &quot;Imitating and Finetuning Model Predictive Control for Robust and Symmetric Quadrupedal Locomotion,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3320827. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85173012210&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398447152')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="distributed data-driven predictive control for multi-agent collaborative legged locomotion" data-keywords="" data-themes="I L R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168659479&amp;origin=resultslist" target="_blank">Distributed Data-Driven Predictive Control for Multi-Agent Collaborative Legged Locomotion</a>
                            </h3>
                            <p class="card-authors">Fawcett R.T.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-R">Multi-Robot</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398447200">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168659479&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/ICRA48891.2023.10160914" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. R.T., &quot;Distributed Data-Driven Predictive Control for Multi-Agent Collaborative Legged Locomotion,&quot; Proceedings IEEE International Conference on Robotics and Automation, 2023. doi: 10.1109/ICRA48891.2023.10160914. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85168659479&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398447200')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="design of a novel wheel-legged robot with rim shape changeable wheels" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85180123227&amp;origin=resultslist" target="_blank">Design of A Novel Wheel-Legged Robot with Rim Shape Changeable Wheels</a>
                            </h3>
                            <p class="card-authors">Fu Z.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398447248">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85180123227&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1186/s10033-023-00974-7" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. Z., &quot;Design of A Novel Wheel-Legged Robot with Rim Shape Changeable Wheels,&quot; Chinese Journal of Mechanical Engineering English Edition, 2023. doi: 10.1186/s10033-023-00974-7. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85180123227&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398447248')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2024" data-category="other" data-title="highly maneuverable humanoid running via 3d slip+foot dynamics" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85179809683&amp;origin=resultslist" target="_blank">Highly Maneuverable Humanoid Running via 3D SLIP+Foot Dynamics</a>
                            </h3>
                            <p class="card-authors">Sovukluk S.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398447296">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85179809683&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3342668" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. S., &quot;Highly Maneuverable Humanoid Running via 3D SLIP+Foot Dynamics,&quot; IEEE Robotics and Automation Letters, 2024. doi: 10.1109/LRA.2023.3342668. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85179809683&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398447296')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="Scopus" data-year="2024" data-category="other" data-title="marine sediment sampling with an underwater legged robot: a user-driven sampling approach for microplastic analysis" data-keywords="" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85182359859&amp;origin=resultslist" target="_blank">Marine Sediment Sampling With an Underwater Legged Robot: A User-Driven Sampling Approach for Microplastic Analysis</a>
                            </h3>
                            <p class="card-authors">Astolfi A.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398447488">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85182359859&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/MRA.2023.3341288" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. A., &quot;Marine Sediment Sampling With an Underwater Legged Robot: A User-Driven Sampling Approach for Microplastic Analysis,&quot; IEEE Robotics and Automation Magazine, 2024. doi: 10.1109/MRA.2023.3341288. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85182359859&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398447488')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2024" data-category="other" data-title="a new method for finding the proper initial conditions in passive locomotion of bipedal robotic systems" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85178144535&amp;origin=resultslist" target="_blank">A new method for finding the proper initial conditions in passive locomotion of bipedal robotic systems</a>
                            </h3>
                            <p class="card-authors">Fazel R.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398447344">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85178144535&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1016/j.cnsns.2023.107693" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. R., &quot;A new method for finding the proper initial conditions in passive locomotion of bipedal robotic systems,&quot; Communications in Nonlinear Science and Numerical Simulation, 2024. doi: 10.1016/j.cnsns.2023.107693. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85178144535&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398447344')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="benchmarking potential based rewards for learning humanoid locomotion" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168667660&amp;origin=resultslist" target="_blank">Benchmarking Potential Based Rewards for Learning Humanoid Locomotion</a>
                            </h3>
                            <p class="card-authors">Jeon S.H.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398447440">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168667660&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/ICRA48891.2023.10160885" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. S.H., &quot;Benchmarking Potential Based Rewards for Learning Humanoid Locomotion,&quot; Proceedings IEEE International Conference on Robotics and Automation, 2023. doi: 10.1109/ICRA48891.2023.10160885. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85168667660&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398447440')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="roboshape: using topology patterns to scalably and flexibly deploy accelerators across robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168882746&amp;origin=resultslist" target="_blank">RoboShape: Using Topology Patterns to Scalably and Flexibly Deploy Accelerators Across Robots</a>
                            </h3>
                            <p class="card-authors">Neuman S.M.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398447584">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85168882746&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1145/3579371.3589104" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. S.M., &quot;RoboShape: Using Topology Patterns to Scalably and Flexibly Deploy Accelerators Across Robots,&quot; Proceedings International Symposium on Computer Architecture, 2023. doi: 10.1145/3579371.3589104. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85168882746&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398447584')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="optimization-based flocking control and mpc-based gait synchronization control for multiple quadruped robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85182362384&amp;origin=resultslist" target="_blank">Optimization-Based Flocking Control and MPC-Based Gait Synchronization Control for Multiple Quadruped Robots</a>
                            </h3>
                            <p class="card-authors">Liu K.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398447632">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85182362384&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2024.3350372" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. K., &quot;Optimization-Based Flocking Control and MPC-Based Gait Synchronization Control for Multiple Quadruped Robots,&quot; IEEE Robotics and Automation Letters, 2024. doi: 10.1109/LRA.2024.3350372. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85182362384&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398447632')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2025" data-category="other" data-title="learning-based legged locomotion: state of the art and future perspectives" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2025</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85216854259&amp;origin=resultslist" target="_blank">Learning-based legged locomotion: State of the art and future perspectives</a>
                            </h3>
                            <p class="card-authors">Ha S.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398447680">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85216854259&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1177/02783649241312698" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. S., &quot;Learning-based legged locomotion: State of the art and future perspectives,&quot; International Journal of Robotics Research, 2025. doi: 10.1177/02783649241312698. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85216854259&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398447680')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="Scopus" data-year="2023" data-category="other" data-title="slomo: a general system for legged robot motion imitation from casual videos" data-keywords="" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85171544548&amp;origin=resultslist" target="_blank">SLoMo: A General System for Legged Robot Motion Imitation From Casual Videos</a>
                            </h3>
                            <p class="card-authors">Zhang J.Z.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398447728">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85171544548&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3313937" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. J.Z., &quot;SLoMo: A General System for Legged Robot Motion Imitation From Casual Videos,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3313937. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85171544548&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398447728')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="safe and adaptive 3-d locomotion via constrained task-space imitation learning" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85149357739&amp;origin=resultslist" target="_blank">Safe and Adaptive 3-D Locomotion via Constrained Task-Space Imitation Learning</a>
                            </h3>
                            <p class="card-authors">Ding J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398447776">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85149357739&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TMECH.2023.3239099" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. J., &quot;Safe and Adaptive 3-D Locomotion via Constrained Task-Space Imitation Learning,&quot; IEEE ASME Transactions on Mechatronics, 2023. doi: 10.1109/TMECH.2023.3239099. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85149357739&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398447776')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="robust jumping with an articulated soft quadruped via trajectory optimization and iterative learning" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85177025212&amp;origin=resultslist" target="_blank">Robust Jumping with an Articulated Soft Quadruped Via Trajectory Optimization and Iterative Learning</a>
                            </h3>
                            <p class="card-authors">Ding J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398447824">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85177025212&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3331288" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. J., &quot;Robust Jumping with an Articulated Soft Quadruped Via Trajectory Optimization and Iterative Learning,&quot; IEEE Robotics and Automation Letters, 2024. doi: 10.1109/LRA.2023.3331288. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85177025212&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398447824')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="humanoid robot path planning using memory-based gravity search algorithm and enhanced differential evolution approach in a complex environment" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85144090926&amp;origin=resultslist" target="_blank">Humanoid robot path planning using memory-based gravity search algorithm and enhanced differential evolution approach in a complex environment</a>
                            </h3>
                            <p class="card-authors">Vikas </p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398447872">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85144090926&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1016/j.eswa.2022.119423" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Vikas, &quot;Humanoid robot path planning using memory-based gravity search algorithm and enhanced differential evolution approach in a complex environment,&quot; Expert Systems with Applications, 2023. doi: 10.1016/j.eswa.2022.119423. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85144090926&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398447872')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2024" data-category="other" data-title="learning agile locomotion and adaptive behaviors via rl-augmented mpc" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85202431823&amp;origin=resultslist" target="_blank">Learning Agile Locomotion and Adaptive Behaviors via RL-augmented MPC</a>
                            </h3>
                            <p class="card-authors">Chen Y.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398447920">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85202431823&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/ICRA57147.2024.10610453" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Y., &quot;Learning Agile Locomotion and Adaptive Behaviors via RL-augmented MPC,&quot; Proceedings IEEE International Conference on Robotics and Automation, 2024. doi: 10.1109/ICRA57147.2024.10610453. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85202431823&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398447920')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="elastic-actuation mechanism for repetitive hopping based on power modulation and cyclic trajectory generation" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85135748349&amp;origin=resultslist" target="_blank">Elastic-Actuation Mechanism for Repetitive Hopping Based on Power Modulation and Cyclic Trajectory Generation</a>
                            </h3>
                            <p class="card-authors">Shin W.D.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398447968">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85135748349&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2022.3189249" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. W.D., &quot;Elastic-Actuation Mechanism for Repetitive Hopping Based on Power Modulation and Cyclic Trajectory Generation,&quot; IEEE Transactions on Robotics, 2023. doi: 10.1109/TRO.2022.3189249. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85135748349&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398447968')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="autonomous navigation with online replanning and recovery behaviors for wheeled-legged robots using behavior trees" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85171539397&amp;origin=resultslist" target="_blank">Autonomous Navigation with Online Replanning and Recovery Behaviors for Wheeled-Legged Robots Using Behavior Trees</a>
                            </h3>
                            <p class="card-authors">De Luca A.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398448016">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85171539397&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3313052" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. L. A., &quot;Autonomous Navigation with Online Replanning and Recovery Behaviors for Wheeled-Legged Robots Using Behavior Trees,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3313052. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85171539397&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398448016')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="learning to walk in confined spaces using 3d representation" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85202444899&amp;origin=resultslist" target="_blank">Learning to walk in confined spaces using 3D representation</a>
                            </h3>
                            <p class="card-authors">Miki T.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398447536">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85202444899&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/ICRA57147.2024.10610271" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. T., &quot;Learning to walk in confined spaces using 3D representation,&quot; Proceedings IEEE International Conference on Robotics and Automation, 2024. doi: 10.1109/ICRA57147.2024.10610271. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85202444899&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398447536')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="sospider: a bio-inspired multimodal untethered soft hexapod robot for planetary lava tube exploration" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85175087874&amp;origin=resultslist" target="_blank">SoSpider: A bio-inspired multimodal untethered soft hexapod robot for planetary lava tube exploration</a>
                            </h3>
                            <p class="card-authors">Niu L.Z.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398444416">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85175087874&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1007/s11431-023-2519-9" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. L.Z., &quot;SoSpider: A bio-inspired multimodal untethered soft hexapod robot for planetary lava tube exploration,&quot; Science China Technological Sciences, 2023. doi: 10.1007/s11431-023-2519-9. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85175087874&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398444416')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="design and control of braver: a bipedal robot actuated via proprioceptive electric motors" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85165443909&amp;origin=resultslist" target="_blank">Design and control of BRAVER: a bipedal robot actuated via proprioceptive electric motors</a>
                            </h3>
                            <p class="card-authors">Zhu Z.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398448064">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85165443909&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1007/s10514-023-10117-5" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Z., &quot;Design and control of BRAVER: a bipedal robot actuated via proprioceptive electric motors,&quot; Autonomous Robots, 2023. doi: 10.1007/s10514-023-10117-5. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85165443909&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398448064')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="orientation control system: enhancing aerial maneuvers for quadruped robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147895546&amp;origin=resultslist" target="_blank">Orientation Control System: Enhancing Aerial Maneuvers for Quadruped Robots</a>
                            </h3>
                            <p class="card-authors">Roscia F.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398448112">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147895546&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.3390/s23031234" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. F., &quot;Orientation Control System: Enhancing Aerial Maneuvers for Quadruped Robots,&quot; Sensors, 2023. doi: 10.3390/s23031234. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85147895546&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398448112')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="do robots outperform humans in human-centered domains?" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85177480613&amp;origin=resultslist" target="_blank">Do robots outperform humans in human-centered domains?</a>
                            </h3>
                            <p class="card-authors">Riener R.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398448160">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85177480613&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.3389/frobt.2023.1223946" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. R., &quot;Do robots outperform humans in human-centered domains?,&quot; Frontiers in Robotics and AI, 2023. doi: 10.3389/frobt.2023.1223946. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85177480613&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398448160')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="torque-based deep reinforcement learning for task-and-robot agnostic learning on bipedal robots using sim-to-real transfer" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85167803521&amp;origin=resultslist" target="_blank">Torque-Based Deep Reinforcement Learning for Task-and-Robot Agnostic Learning on Bipedal Robots Using Sim-to-Real Transfer</a>
                            </h3>
                            <p class="card-authors">Kim D.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398448304">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85167803521&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3304561" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. D., &quot;Torque-Based Deep Reinforcement Learning for Task-and-Robot Agnostic Learning on Bipedal Robots Using Sim-to-Real Transfer,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3304561. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85167803521&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398448304')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="bipedal walking on constrained footholds with mpc footstep control" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85182921448&amp;origin=resultslist" target="_blank">Bipedal Walking on Constrained Footholds with MPC Footstep Control</a>
                            </h3>
                            <p class="card-authors">Acosta B.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398448352">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85182921448&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/Humanoids57100.2023.10375170" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. B., &quot;Bipedal Walking on Constrained Footholds with MPC Footstep Control,&quot; IEEE Ras International Conference on Humanoid Robots, 2023. doi: 10.1109/Humanoids57100.2023.10375170. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85182921448&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398448352')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2024" data-category="other" data-title="cts: concurrent teacher-student reinforcement learning for legged locomotion" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85204230435&amp;origin=resultslist" target="_blank">CTS: Concurrent Teacher-Student Reinforcement Learning for Legged Locomotion</a>
                            </h3>
                            <p class="card-authors">Wang H.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398448400">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85204230435&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2024.3457379" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. H., &quot;CTS: Concurrent Teacher-Student Reinforcement Learning for Legged Locomotion,&quot; IEEE Robotics and Automation Letters, 2024. doi: 10.1109/LRA.2024.3457379. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85204230435&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398448400')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="real-time neural dense elevation mapping for urban terrain with uncertainty estimations" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85146227296&amp;origin=resultslist" target="_blank">Real-Time Neural Dense Elevation Mapping for Urban Terrain with Uncertainty Estimations</a>
                            </h3>
                            <p class="card-authors">Yang B.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398448448">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85146227296&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2022.3230325" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. B., &quot;Real-Time Neural Dense Elevation Mapping for Urban Terrain with Uncertainty Estimations,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2022.3230325. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85146227296&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398448448')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="a hierarchical framework for quadruped robots gait planning based on ddpg" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85172282632&amp;origin=resultslist" target="_blank">A Hierarchical Framework for Quadruped Robots Gait Planning Based on DDPG</a>
                            </h3>
                            <p class="card-authors">Li Y.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398448496">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85172282632&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.3390/biomimetics8050382" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Y., &quot;A Hierarchical Framework for Quadruped Robots Gait Planning Based on DDPG,&quot; Biomimetics, 2023. doi: 10.3390/biomimetics8050382. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85172282632&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398448496')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="hierarchical generative modelling for autonomous robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85175567595&amp;origin=resultslist" target="_blank">Hierarchical generative modelling for autonomous robots</a>
                            </h3>
                            <p class="card-authors">Yuan K.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398448544">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85175567595&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1038/s42256-023-00752-z" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. K., &quot;Hierarchical generative modelling for autonomous robots,&quot; Nature Machine Intelligence, 2023. doi: 10.1038/s42256-023-00752-z. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85175567595&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398448544')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2025" data-category="other" data-title="a novel adaptive dynamic optimal balance control method for wheel-legged robot" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2025</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85205929380&amp;origin=resultslist" target="_blank">A novel adaptive dynamic optimal balance control method for wheel-legged robot</a>
                            </h3>
                            <p class="card-authors">Liu X.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398448592">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85205929380&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1016/j.apm.2024.115737" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. X., &quot;A novel adaptive dynamic optimal balance control method for wheel-legged robot,&quot; Applied Mathematical Modelling, 2025. doi: 10.1016/j.apm.2024.115737. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85205929380&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398448592')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="reactive landing controller for quadruped robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85171568878&amp;origin=resultslist" target="_blank">Reactive Landing Controller for Quadruped Robots</a>
                            </h3>
                            <p class="card-authors">Roscia F.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398170224">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85171568878&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3313919" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. F., &quot;Reactive Landing Controller for Quadruped Robots,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3313919. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85171568878&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398170224')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="proprioceptive-based whole-body disturbance rejection control for dynamic motions in legged robots" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85174830459&amp;origin=resultslist" target="_blank">Proprioceptive-Based Whole-Body Disturbance Rejection Control for Dynamic Motions in Legged Robots</a>
                            </h3>
                            <p class="card-authors">Zhu Z.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398170176">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85174830459&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3322081" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Z., &quot;Proprioceptive-Based Whole-Body Disturbance Rejection Control for Dynamic Motions in Legged Robots,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3322081. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85174830459&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398170176')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="view: visual-inertial external wrench estimator for legged robot" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85174825415&amp;origin=resultslist" target="_blank">VIEW: Visual-Inertial External Wrench Estimator for Legged Robot</a>
                            </h3>
                            <p class="card-authors">Kang J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398170320">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85174825415&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3322646" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. J., &quot;VIEW: Visual-Inertial External Wrench Estimator for Legged Robot,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3322646. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85174825415&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398170320')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="Scopus" data-year="2024" data-category="other" data-title="design, motions, capabilities, and applications of quadruped robots: a comprehensive review" data-keywords="" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85201620126&amp;origin=resultslist" target="_blank">Design, motions, capabilities, and applications of quadruped robots: a comprehensive review</a>
                            </h3>
                            <p class="card-authors">Majithia A.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398170368">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85201620126&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.3389/fmech.2024.1448681" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. A., &quot;Design, motions, capabilities, and applications of quadruped robots: a comprehensive review,&quot; Frontiers in Mechanical Engineering, 2024. doi: 10.3389/fmech.2024.1448681. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85201620126&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398170368')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="curiosity-driven learning of joint locomotion and manipulation tasks" data-keywords="" data-themes="I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85184345847&amp;origin=resultslist" target="_blank">Curiosity-Driven Learning of Joint Locomotion and Manipulation Tasks</a>
                            </h3>
                            <p class="card-authors">Schwarke C.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398170416">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85184345847&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. C., &quot;Curiosity-Driven Learning of Joint Locomotion and Manipulation Tasks,&quot; Proceedings of Machine Learning Research, 2023. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85184345847&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398170416')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="leg-kilo: robust kinematic-inertial-lidar odometry for dynamic legged robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85200800452&amp;origin=resultslist" target="_blank">Leg-KILO: Robust Kinematic-Inertial-Lidar Odometry for Dynamic Legged Robots</a>
                            </h3>
                            <p class="card-authors">Ou G.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398170464">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85200800452&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2024.3440730" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'O. G., &quot;Leg-KILO: Robust Kinematic-Inertial-Lidar Odometry for Dynamic Legged Robots,&quot; IEEE Robotics and Automation Letters, 2024. doi: 10.1109/LRA.2024.3440730. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85200800452&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398170464')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="predictive control of zero moment point (zmp) for terrain robot kinematics" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85142240663&amp;origin=resultslist" target="_blank">Predictive control of zero moment point (ZMP) for terrain robot kinematics</a>
                            </h3>
                            <p class="card-authors">Haldar A.I.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398170512">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85142240663&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1016/j.matpr.2022.10.286" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. A.I., &quot;Predictive control of zero moment point (ZMP) for terrain robot kinematics,&quot; Materials Today Proceedings, 2023. doi: 10.1016/j.matpr.2022.10.286. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85142240663&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398170512')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="leveraging large language models for comprehensive locomotion control in humanoid robots design" data-keywords="" data-themes="I E L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85207356903&amp;origin=resultslist" target="_blank">Leveraging large language models for comprehensive locomotion control in humanoid robots design</a>
                            </h3>
                            <p class="card-authors">Sun S.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398170560">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85207356903&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1016/j.birob.2024.100187" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. S., &quot;Leveraging large language models for comprehensive locomotion control in humanoid robots design,&quot; Biomimetic Intelligence and Robotics, 2024. doi: 10.1016/j.birob.2024.100187. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85207356903&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398170560')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2025" data-category="other" data-title="design and control of skater: a wheeled- bipedal robot with high-speed turning robustness and terrain adaptability" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2025</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-105003137460&amp;origin=resultslist" target="_blank">Design and Control of SKATER: A Wheeled- Bipedal Robot With High-Speed Turning Robustness and Terrain Adaptability</a>
                            </h3>
                            <p class="card-authors">Wang Y.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398170608">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-105003137460&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TMECH.2024.3420390" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. Y., &quot;Design and Control of SKATER: A Wheeled- Bipedal Robot With High-Speed Turning Robustness and Terrain Adaptability,&quot; IEEE ASME Transactions on Mechatronics, 2025. doi: 10.1109/TMECH.2024.3420390. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-105003137460&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398170608')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="staf: interaction-based design and evaluation of sensorized terrain-adaptive foot for legged robot traversing on soft slopes" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85183993768&amp;origin=resultslist" target="_blank">STAF: Interaction-Based Design and Evaluation of Sensorized Terrain-Adaptive Foot for Legged Robot Traversing on Soft Slopes</a>
                            </h3>
                            <p class="card-authors">Yao C.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398170656">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85183993768&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TMECH.2024.3350183" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. C., &quot;STAF: Interaction-Based Design and Evaluation of Sensorized Terrain-Adaptive Foot for Legged Robot Traversing on Soft Slopes,&quot; IEEE ASME Transactions on Mechatronics, 2024. doi: 10.1109/TMECH.2024.3350183. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85183993768&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398170656')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="control of¬†wheeled-legged quadrupeds using deep reinforcement learning" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85137985117&amp;origin=resultslist" target="_blank">Control of¬†Wheeled-Legged Quadrupeds Using Deep Reinforcement Learning</a>
                            </h3>
                            <p class="card-authors">Lee J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398170704">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85137985117&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1007/978-3-031-15226-9_14" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. J., &quot;Control of¬†Wheeled-Legged Quadrupeds Using Deep Reinforcement Learning,&quot; Lecture Notes in Networks and Systems, 2023. doi: 10.1007/978-3-031-15226-9_14. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85137985117&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398170704')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="model predictive control of quadruped robot based on reinforcement learning" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85145676804&amp;origin=resultslist" target="_blank">Model Predictive Control of Quadruped Robot Based on Reinforcement Learning</a>
                            </h3>
                            <p class="card-authors">Zhang Z.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398170752">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85145676804&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.3390/app13010154" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Z., &quot;Model Predictive Control of Quadruped Robot Based on Reinforcement Learning,&quot; Applied Sciences Switzerland, 2023. doi: 10.3390/app13010154. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85145676804&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398170752')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="unified motion planner for walking, running, and jumping using the three-dimensional divergent component of motion" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85174857184&amp;origin=resultslist" target="_blank">Unified Motion Planner for Walking, Running, and Jumping Using the Three-Dimensional Divergent Component of Motion</a>
                            </h3>
                            <p class="card-authors">Mesesan G.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398403344">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85174857184&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2023.3321396" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. G., &quot;Unified Motion Planner for Walking, Running, and Jumping Using the Three-Dimensional Divergent Component of Motion,&quot; IEEE Transactions on Robotics, 2023. doi: 10.1109/TRO.2023.3321396. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85174857184&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398403344')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="Scopus" data-year="2023" data-category="other" data-title="multi-robot path planning using a hybrid dynamic window approach and modified chaotic neural oscillator-based hyperbolic gravitational search algorithm in a complex terrain" data-keywords="" data-themes="R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85150738227&amp;origin=resultslist" target="_blank">Multi-robot path planning using a hybrid dynamic window approach and modified chaotic neural oscillator-based hyperbolic gravitational search algorithm in a complex terrain</a>
                            </h3>
                            <p class="card-authors">Vikas </p>
                            <div class="theme-tags"><span class="theme-badge theme-R">Multi-Robot</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398170272">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85150738227&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1007/s11370-023-00460-y" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Vikas, &quot;Multi-robot path planning using a hybrid dynamic window approach and modified chaotic neural oscillator-based hyperbolic gravitational search algorithm in a complex terrain,&quot; Intelligent Service Robotics, 2023. doi: 10.1007/s11370-023-00460-y. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85150738227&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398170272')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="bioinspired soft spine enables small-scale robotic rat to conquer challenging environments" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85166578780&amp;origin=resultslist" target="_blank">Bioinspired Soft Spine Enables Small-Scale Robotic Rat to Conquer Challenging Environments</a>
                            </h3>
                            <p class="card-authors">Wang R.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398170800">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85166578780&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1089/soro.2022.0220" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. R., &quot;Bioinspired Soft Spine Enables Small-Scale Robotic Rat to Conquer Challenging Environments,&quot; Soft Robotics, 2024. doi: 10.1089/soro.2022.0220. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85166578780&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398170800')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="on second-order derivatives of rigid-body dynamics: theory and implementation" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85186988695&amp;origin=resultslist" target="_blank">On Second-Order Derivatives of Rigid-Body Dynamics: Theory and Implementation</a>
                            </h3>
                            <p class="card-authors">Singh S.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398170848">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85186988695&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2024.3370002" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. S., &quot;On Second-Order Derivatives of Rigid-Body Dynamics: Theory and Implementation,&quot; IEEE Transactions on Robotics, 2024. doi: 10.1109/TRO.2024.3370002. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85186988695&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398170848')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2024" data-category="other" data-title="adaptive bipedal robot walking on industrial pipes under neural multimodal locomotion control: toward robotic out-pipe inspection" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85165403812&amp;origin=resultslist" target="_blank">Adaptive Bipedal Robot Walking on Industrial Pipes Under Neural Multimodal Locomotion Control: Toward Robotic Out-Pipe Inspection</a>
                            </h3>
                            <p class="card-authors">Srisuchinnawong A.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398170896">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85165403812&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TMECH.2023.3293950" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. A., &quot;Adaptive Bipedal Robot Walking on Industrial Pipes Under Neural Multimodal Locomotion Control: Toward Robotic Out-Pipe Inspection,&quot; IEEE ASME Transactions on Mechatronics, 2024. doi: 10.1109/TMECH.2023.3293950. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85165403812&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398170896')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="biomimetic soft-legged robotic locomotion, interactions and transitions in terrestrial, aquatic and multiple environments" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85190800381&amp;origin=resultslist" target="_blank">Biomimetic soft-legged robotic locomotion, interactions and transitions in terrestrial, aquatic and multiple environments</a>
                            </h3>
                            <p class="card-authors">Yu Z.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398171040">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85190800381&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1016/j.susmat.2024.e00930" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Z., &quot;Biomimetic soft-legged robotic locomotion, interactions and transitions in terrestrial, aquatic and multiple environments,&quot; Sustainable Materials and Technologies, 2024. doi: 10.1016/j.susmat.2024.e00930. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85190800381&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398171040')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="whole body control formulation for humanoid robots with closed/parallel kinematic chains: kangaroo case study" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85180917830&amp;origin=resultslist" target="_blank">Whole Body Control Formulation for Humanoid Robots with Closed/Parallel Kinematic Chains: Kangaroo Case Study</a>
                            </h3>
                            <p class="card-authors">Sovukluk S.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398171088">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85180917830&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/IROS55552.2023.10341391" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. S., &quot;Whole Body Control Formulation for Humanoid Robots with Closed/Parallel Kinematic Chains: Kangaroo Case Study,&quot; IEEE International Conference on Intelligent Robots and Systems, 2023. doi: 10.1109/IROS55552.2023.10341391. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85180917830&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398171088')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="computational design towards energy efficient optimization in overconstrained robotic limbs" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85173230875&amp;origin=resultslist" target="_blank">Computational design towards energy efficient optimization in overconstrained robotic limbs</a>
                            </h3>
                            <p class="card-authors">Gu Y.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398171136">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85173230875&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1093/jcde/qwad083" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Y., &quot;Computational design towards energy efficient optimization in overconstrained robotic limbs,&quot; Journal of Computational Design and Engineering, 2023. doi: 10.1093/jcde/qwad083. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85173230875&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398171136')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="pie: parkour with implicit-explicit learning framework for legged robots" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85203996501&amp;origin=resultslist" target="_blank">PIE: Parkour With Implicit-Explicit Learning Framework for Legged Robots</a>
                            </h3>
                            <p class="card-authors">Luo S.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398171184">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85203996501&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2024.3459797" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. S., &quot;PIE: Parkour With Implicit-Explicit Learning Framework for Legged Robots,&quot; IEEE Robotics and Automation Letters, 2024. doi: 10.1109/LRA.2024.3459797. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85203996501&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398171184')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="proactive body joint adaptation for energy-efficient locomotion of bio-inspired multi-segmented robots" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147212376&amp;origin=resultslist" target="_blank">Proactive Body Joint Adaptation for Energy-Efficient Locomotion of Bio-Inspired Multi-Segmented Robots</a>
                            </h3>
                            <p class="card-authors">Homchanthanakul J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398171232">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147212376&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3234773" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. J., &quot;Proactive Body Joint Adaptation for Energy-Efficient Locomotion of Bio-Inspired Multi-Segmented Robots,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3234773. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85147212376&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398171232')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="learning risk-aware quadrupedal locomotion using distributional reinforcement learning" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85200564118&amp;origin=resultslist" target="_blank">Learning Risk-Aware Quadrupedal Locomotion using Distributional Reinforcement Learning</a>
                            </h3>
                            <p class="card-authors">Schneider L.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398171424">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85200564118&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/ICRA57147.2024.10610137" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. L., &quot;Learning Risk-Aware Quadrupedal Locomotion using Distributional Reinforcement Learning,&quot; Proceedings IEEE International Conference on Robotics and Automation, 2024. doi: 10.1109/ICRA57147.2024.10610137. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85200564118&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398171424')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="self-organized stick insect-like locomotion under decentralized adaptive neural control: from biological investigation to robot simulation" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85162853092&amp;origin=resultslist" target="_blank">Self-Organized Stick Insect-Like Locomotion under Decentralized Adaptive Neural Control: From Biological Investigation to Robot Simulation</a>
                            </h3>
                            <p class="card-authors">Larsen A.D.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398171280">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85162853092&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1002/adts.202300228" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. A.D., &quot;Self-Organized Stick Insect-Like Locomotion under Decentralized Adaptive Neural Control: From Biological Investigation to Robot Simulation,&quot; Advanced Theory and Simulations, 2023. doi: 10.1002/adts.202300228. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85162853092&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398171280')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="Scopus" data-year="2023" data-category="other" data-title="biomimetic approaches for human arm motion generation: literature review and future directions" data-keywords="" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85153725930&amp;origin=resultslist" target="_blank">Biomimetic Approaches for Human Arm Motion Generation: Literature Review and Future Directions</a>
                            </h3>
                            <p class="card-authors">Trivedi U.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398171376">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85153725930&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.3390/s23083912" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. U., &quot;Biomimetic Approaches for Human Arm Motion Generation: Literature Review and Future Directions,&quot; Sensors, 2023. doi: 10.3390/s23083912. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85153725930&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398171376')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="bio-inspired gait transitions for quadruped locomotion" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85166755794&amp;origin=resultslist" target="_blank">Bio-Inspired Gait Transitions for Quadruped Locomotion</a>
                            </h3>
                            <p class="card-authors">Humphreys J.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398171328">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85166755794&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/LRA.2023.3300249" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. J., &quot;Bio-Inspired Gait Transitions for Quadruped Locomotion,&quot; IEEE Robotics and Automation Letters, 2023. doi: 10.1109/LRA.2023.3300249. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85166755794&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398171328')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="a survey on legged robots: advances, technologies and applications" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85206246595&amp;origin=resultslist" target="_blank">A survey on legged robots: Advances, technologies and applications</a>
                            </h3>
                            <p class="card-authors">Wu Z.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398171520">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85206246595&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1016/j.engappai.2024.109418" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. Z., &quot;A survey on legged robots: Advances, technologies and applications,&quot; Engineering Applications of Artificial Intelligence, 2024. doi: 10.1016/j.engappai.2024.109418. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85206246595&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398171520')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="cooperative control strategy of wheel-legged robot based on attitude balance" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147142909&amp;origin=resultslist" target="_blank">Cooperative control strategy of wheel-legged robot based on attitude balance</a>
                            </h3>
                            <p class="card-authors">Shen Y.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398171568">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147142909&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1017/S0263574722001436" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Y., &quot;Cooperative control strategy of wheel-legged robot based on attitude balance,&quot; Robotica, 2023. doi: 10.1017/S0263574722001436. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85147142909&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398171568')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2023" data-category="other" data-title="multimodal bipedal locomotion generation with passive dynamics via deep reinforcement learning" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147445912&amp;origin=resultslist" target="_blank">Multimodal bipedal locomotion generation with passive dynamics via deep reinforcement learning</a>
                            </h3>
                            <p class="card-authors">Koseki S.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398171616">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85147445912&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.3389/fnbot.2022.1054239" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. S., &quot;Multimodal bipedal locomotion generation with passive dynamics via deep reinforcement learning,&quot; Frontiers in Neurorobotics, 2023. doi: 10.3389/fnbot.2022.1054239. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85147445912&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398171616')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2025" data-category="other" data-title="nonsmooth trajectory optimization for wheeled balancing robots with contact switches and impacts" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2025</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-86000429293&amp;origin=resultslist" target="_blank">Nonsmooth Trajectory Optimization for Wheeled Balancing Robots With Contact Switches and Impacts</a>
                            </h3>
                            <p class="card-authors">Klemm V.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398171664">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-86000429293&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TRO.2023.3326334" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. V., &quot;Nonsmooth Trajectory Optimization for Wheeled Balancing Robots With Contact Switches and Impacts,&quot; IEEE Transactions on Robotics, 2025. doi: 10.1109/TRO.2023.3326334. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-86000429293&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398171664')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="Scopus" data-year="2024" data-category="other" data-title="comparative analysis of reinforcement learning algorithms for bipedal robot locomotion" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85181844587&amp;origin=resultslist" target="_blank">Comparative Analysis of Reinforcement Learning Algorithms for Bipedal Robot Locomotion</a>
                            </h3>
                            <p class="card-authors">Aydogmus O.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398171712">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85181844587&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/ACCESS.2023.3344393" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. O., &quot;Comparative Analysis of Reinforcement Learning Algorithms for Bipedal Robot Locomotion,&quot; IEEE Access, 2024. doi: 10.1109/ACCESS.2023.3344393. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85181844587&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398171712')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="hybrid learning mechanisms under a neural control network for various walking speed generation of a quadruped robot" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85170094776&amp;origin=resultslist" target="_blank">Hybrid learning mechanisms under a neural control network for various walking speed generation of a quadruped robot</a>
                            </h3>
                            <p class="card-authors">Zhang Y.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398171760">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85170094776&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1016/j.neunet.2023.08.030" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Y., &quot;Hybrid learning mechanisms under a neural control network for various walking speed generation of a quadruped robot,&quot; Neural Networks, 2023. doi: 10.1016/j.neunet.2023.08.030. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85170094776&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398171760')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="heel-strike and toe-off walking of humanoid robot using quadratic programming considering the foot contact states" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85150016929&amp;origin=resultslist" target="_blank">Heel-strike and toe-off walking of humanoid robot using quadratic programming considering the foot contact states</a>
                            </h3>
                            <p class="card-authors">Park B.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398171808">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85150016929&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1016/j.robot.2023.104396" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. B., &quot;Heel-strike and toe-off walking of humanoid robot using quadratic programming considering the foot contact states,&quot; Robotics and Autonomous Systems, 2023. doi: 10.1016/j.robot.2023.104396. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85150016929&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398171808')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2024" data-category="other" data-title="design and multimodal locomotion plan of a hexapod robot with improved knee joints" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85189527908&amp;origin=resultslist" target="_blank">Design and multimodal locomotion plan of a hexapod robot with improved knee joints</a>
                            </h3>
                            <p class="card-authors">Xu K.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398171856">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85189527908&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1002/rob.22324" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. K., &quot;Design and multimodal locomotion plan of a hexapod robot with improved knee joints,&quot; Journal of Field Robotics, 2024. doi: 10.1002/rob.22324. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85189527908&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398171856')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="posture adjustment for a wheel-legged robotic system via leg force control with prescribed transient performance" data-keywords="" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85148422235&amp;origin=resultslist" target="_blank">Posture Adjustment for a Wheel-Legged Robotic System Via Leg Force Control With Prescribed Transient Performance</a>
                            </h3>
                            <p class="card-authors">Liu D.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398171904">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85148422235&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://doi.org/10.1109/TIE.2023.3239859" class="card-link link-doi" target="_blank">üîó DOI</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. D., &quot;Posture Adjustment for a Wheel-Legged Robotic System Via Leg Force Control With Prescribed Transient Performance,&quot; IEEE Transactions on Industrial Electronics, 2023. doi: 10.1109/TIE.2023.3239859. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85148422235&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398171904')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="Scopus" data-year="2023" data-category="other" data-title="learning semantics-aware locomotion skills from human demonstration" data-keywords="" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">Scopus ¬∑ 2023</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85164675765&amp;origin=resultslist" target="_blank">Learning Semantics-Aware Locomotion Skills from Human Demonstration</a>
                            </h3>
                            <p class="card-authors">Yang Y.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"></div>
                            <div class="card-details" id="cat-details-4398171952">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text"></p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="https://www.scopus.com/record/display.uri?eid=2-s2.0-85164675765&amp;origin=resultslist" class="card-link link-paper" target="_blank">üìÑ Paper</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Y., &quot;Learning Semantics-Aware Locomotion Skills from Human Demonstration,&quot; Proceedings of Machine Learning Research, 2023. [Online]. Available: https://www.scopus.com/record/display.uri?eid=2-s2.0-85164675765&amp;origin=resultslist')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398171952')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="cs.ro">
                <h2 class="section-header">üè∑Ô∏è Robotics <span class="section-count">(119 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="a mini-review on mobile manipulators with variable autonomy" data-keywords="robot language model cs.ro cs.hc" data-themes="E L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.10887v1" target="_blank">A Mini-Review on Mobile Manipulators with Variable Autonomy</a>
                            </h3>
                            <p class="card-authors">Cesar Alan Contreras, Alireza Rastegarpanah, Rustam Stolkin et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.RO</span><span class="keyword-tag">CS.HC</span></div>
                            <div class="card-details" id="cat-details-4397314544">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents a mini-review of the current state of research in mobile manipulators with variable levels of autonomy, emphasizing their associated challenges and application environments. The need for mobile manipulators in different environments is evident due to the unique challenges and risks each presents. Many systems deployed in these environments are not fully autonomous, requiring human-robot teaming to ensure safe and reliable operations under uncertainties. Through this analysis, we identify gaps and challenges in the literature on Variable Autonomy, including cognitive workloa...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents a mini-review of the current state of research in mobile manipulators with variable levels of autonomy, emphasizing their associated challenges and application environments</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.10887v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.10887v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. A. Contreras, A. Rastegarpanah, R. Stolkin, and M. Chiou, &quot;A Mini-Review on Mobile Manipulators with Variable Autonomy,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.10887v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397314544')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="tidybot: personalized robot assistance with large language models" data-keywords="robot perception planning language model cs.ro cs.ai cs.cl" data-themes="S I M E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2305.05658v2" target="_blank">TidyBot: Personalized Robot Assistance with Large Language Models</a>
                            </h3>
                            <p class="card-authors">Jimmy Wu, Rika Antonova, Adam Kan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Perception</span><span class="keyword-tag">Planning</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4398403440">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people&#x27;s preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We also demonstrate our approach on a real-world mobile manipulator called TidyBot, which successfully puts away 85.0% of objects in real-world test scenarios.</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2305.05658v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2305.05658v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Wu et al., &quot;TidyBot: Personalized Robot Assistance with Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2305.05658v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398403440')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="a variable autonomy approach for an automated weeding platform" data-keywords="robot imu cs.ro" data-themes="S">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2303.05461v1" target="_blank">A Variable Autonomy approach for an Automated Weeding Platform</a>
                            </h3>
                            <p class="card-authors">Ionut Moraru, Tsvetan Zhivkov, Shaun Coutts et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Imu</span><span class="keyword-tag">CS.RO</span></div>
                            <div class="card-details" id="cat-details-4398402144">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Climate change, increase in world population and the war in Ukraine have led nations such as the UK to put a larger focus on food security, while simultaneously trying to halt declines in biodiversity and reduce risks to human health posed by chemically-reliant farming practices. Achieving these goals simultaneously will require novel approaches and accelerating the deployment of Agri-Robotics from the lab and into the field. In this paper we describe the ARWAC robot platform for mechanical weeding. We explain why the mechanical weeding approach is beneficial compared to the use of pesticides ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Thereafter, we present the system design and processing pipeline for generating a course of action for the robot to follow, such that it removes as many weeds as possible</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2303.05461v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2303.05461v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'I. Moraru, T. Zhivkov, S. Coutts, D. Li, and E. I. Sklar, &quot;A Variable Autonomy approach for an Automated Weeding Platform,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2303.05461v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398402144')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="learning and autonomy for extraterrestrial terrain sampling: an experience report from owlat deployment" data-keywords="control cs.ro" data-themes="S I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2311.17405v2" target="_blank">Learning and Autonomy for Extraterrestrial Terrain Sampling: An Experience Report from OWLAT Deployment</a>
                            </h3>
                            <p class="card-authors">Pranay Thangeda, Ashish Goel, Erica Tevere et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Control</span><span class="keyword-tag">CS.RO</span></div>
                            <div class="card-details" id="cat-details-4398406896">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Extraterrestrial autonomous lander missions increasingly demand adaptive capabilities to handle the unpredictable and diverse nature of the terrain. This paper discusses the deployment of a Deep Meta-Learning with Controlled Deployment Gaps (CoDeGa) trained model for terrain scooping tasks in Ocean Worlds Lander Autonomy Testbed (OWLAT) at NASA Jet Propulsion Laboratory. The CoDeGa-powered scooping strategy is designed to adapt to novel terrains, selecting scooping actions based on the available RGB-D image data and limited experience. The paper presents our experiences with transferring the s...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2311.17405v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2311.17405v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Thangeda et al., &quot;Learning and Autonomy for Extraterrestrial Terrain Sampling: An Experience Report from OWLAT Deployment,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2311.17405v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398406896')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="sim-to-real transfer in deep reinforcement learning for bipedal locomotion" data-keywords="reinforcement learning robot bipedal locomotion control simulation imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2511.06465v1" target="_blank">Sim-to-Real Transfer in Deep Reinforcement Learning for Bipedal Locomotion</a>
                            </h3>
                            <p class="card-authors">Lingfan Bao, Tianhu Peng, Chengxu Zhou</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399336320">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This chapter addresses the critical challenge of simulation-to-reality (sim-to-real) transfer for deep reinforcement learning (DRL) in bipedal locomotion. After contextualizing the problem within various control architectures, we dissect the ``curse of simulation&#x27;&#x27; by analyzing the primary sources of sim-to-real gap: robot dynamics, contact modeling, state estimation, and numerical solvers. Building on this diagnosis, we structure the solutions around two complementary philosophies. The first is to shrink the gap through model-centric strategies that systematically improve the simulator&#x27;s phys...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2511.06465v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2511.06465v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Bao, T. Peng, and C. Zhou, &quot;Sim-to-Real Transfer in Deep Reinforcement Learning for Bipedal Locomotion,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2511.06465v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399336320')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="learning bipedal locomotion on gear-driven humanoid robot using foot-mounted imus" data-keywords="reinforcement learning robot humanoid bipedal locomotion imu cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.00614v2" target="_blank">Learning Bipedal Locomotion on Gear-Driven Humanoid Robot Using Foot-Mounted IMUs</a>
                            </h3>
                            <p class="card-authors">Sotaro Katayama, Yuta Koda, Norio Nagatsuka et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span></div>
                            <div class="card-details" id="cat-details-4399335264">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Sim-to-real reinforcement learning (RL) for humanoid robots with high-gear ratio actuators remains challenging due to complex actuator dynamics and the absence of torque sensors. To address this, we propose a novel RL framework leveraging foot-mounted inertial measurement units (IMUs). Instead of pursuing detailed actuator modeling and system identification, we utilize foot-mounted IMU measurements to enhance rapid stabilization capabilities over challenging terrains. Additionally, we propose symmetric data augmentation dedicated to the proposed observation space and random network distillatio...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address this, we propose a novel RL framework leveraging foot-mounted inertial measurement units (IMUs)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.00614v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.00614v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Katayama, Y. Koda, N. Nagatsuka, and M. Kinoshita, &quot;Learning Bipedal Locomotion on Gear-Driven Humanoid Robot Using Foot-Mounted IMUs,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.00614v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399335264')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="hierarchical reduced-order model predictive control for robust locomotion on humanoid robots" data-keywords="robot humanoid locomotion planning control simulation ros imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.04722v1" target="_blank">Hierarchical Reduced-Order Model Predictive Control for Robust Locomotion on Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Adrian B. Ghansah, Sergio A. Esteban, Aaron D. Ames</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Planning</span></div>
                            <div class="card-details" id="cat-details-4399336080">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">As humanoid robots enter real-world environments, ensuring robust locomotion across diverse environments is crucial. This paper presents a computationally efficient hierarchical control framework for humanoid robot locomotion based on reduced-order models -- enabling versatile step planning and incorporating arm and torso dynamics to better stabilize the walking. At the high level, we use the step-to-step dynamics of the ALIP model to simultaneously optimize over step periods, step lengths, and ankle torques via nonlinear MPC. The ALIP trajectories are used as references to a linear MPC framew...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents a computationally efficient hierarchical control framework for humanoid robot locomotion based on reduced-order models -- enabling versatile step planning and incorporating arm and torso dynamics to better stabilize the walking</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.04722v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.04722v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. B. Ghansah, S. A. Esteban, and A. D. Ames, &quot;Hierarchical Reduced-Order Model Predictive Control for Robust Locomotion on Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.04722v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399336080')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="learning sim-to-real humanoid locomotion in 15 minutes" data-keywords="reinforcement learning robot humanoid locomotion control simulation imu cs.ro cs.ai cs.lg" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2512.01996v1" target="_blank">Learning Sim-to-Real Humanoid Locomotion in 15 Minutes</a>
                            </h3>
                            <p class="card-authors">Younggyo Seo, Carmelo Sferrazza, Juyue Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399336224">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Massively parallel simulation has reduced reinforcement learning (RL) training time for robots from days to minutes. However, achieving fast and reliable sim-to-real RL for humanoid control remains difficult due to the challenges introduced by factors such as high dimensionality and domain randomization. In this work, we introduce a simple and practical recipe based on off-policy RL algorithms, i.e., FastSAC and FastTD3, that enables rapid training of humanoid locomotion policies in just 15 minutes with a single RTX 4090 GPU. Our simple recipe stabilizes off-policy RL algorithms at massive sca...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we introduce a simple and practical recipe based on off-policy RL algorithms, i.e., FastSAC and FastTD3, that enables rapid training of humanoid locomotion policies in just 15 minutes with a single RTX 4090 GPU</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2512.01996v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2512.01996v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Seo, C. Sferrazza, J. Chen, G. Shi, R. Duan, and P. Abbeel, &quot;Learning Sim-to-Real Humanoid Locomotion in 15 Minutes,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2512.01996v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399336224')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="learning humanoid locomotion over challenging terrain" data-keywords="reinforcement learning transformer robot humanoid locomotion control ros cs.ro cs.lg" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.03654v1" target="_blank">Learning Humanoid Locomotion over Challenging Terrain</a>
                            </h3>
                            <p class="card-authors">Ilija Radosavovic, Sarthak Kamat, Trevor Darrell et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Transformer</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span></div>
                            <div class="card-details" id="cat-details-4399337328">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots can, in principle, use their legs to go almost anywhere. Developing controllers capable of traversing diverse terrains, however, remains a considerable challenge. Classical controllers are hard to generalize broadly while the learning-based methods have primarily focused on gentle terrains. Here, we present a learning-based approach for blind humanoid locomotion capable of traversing challenging natural and man-made terrain. Our method uses a transformer model to predict the next action based on the history of proprioceptive observations and actions. The model is first pre-trai...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Here, we present a learning-based approach for blind humanoid locomotion capable of traversing challenging natural and man-made terrain</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.03654v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.03654v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'I. Radosavovic, S. Kamat, T. Darrell, and J. Malik, &quot;Learning Humanoid Locomotion over Challenging Terrain,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.03654v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399337328')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="romoco: robotic motion control toolbox for reduced-order model-based locomotion on bipedal and humanoid robots" data-keywords="robot humanoid bipedal locomotion control simulation ros imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.19545v1" target="_blank">RoMoCo: Robotic Motion Control Toolbox for Reduced-Order Model-Based Locomotion on Bipedal and Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Min Dai, Aaron D. Ames</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399337040">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We present RoMoCo, an open-source C++ toolbox for the synthesis and evaluation of reduced-order model-based planners and whole-body controllers for bipedal and humanoid robots. RoMoCo&#x27;s modular architecture unifies state-of-the-art planners and whole-body locomotion controllers under a consistent API, enabling rapid prototyping and reproducible benchmarking. By leveraging reduced-order models for platform-agnostic gait generation, RoMoCo enables flexible controller design across diverse robots. We demonstrate its versatility and performance through extensive simulations on the Cassie, Unitree ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present RoMoCo, an open-source C++ toolbox for the synthesis and evaluation of reduced-order model-based planners and whole-body controllers for bipedal and humanoid robots</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.19545v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.19545v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Dai, and A. D. Ames, &quot;RoMoCo: Robotic Motion Control Toolbox for Reduced-Order Model-Based Locomotion on Bipedal and Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.19545v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399337040')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="real-world humanoid locomotion with reinforcement learning" data-keywords="reinforcement learning transformer robot humanoid locomotion control simulation imu cs.ro cs.lg" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2303.03381v2" target="_blank">Real-World Humanoid Locomotion with Reinforcement Learning</a>
                            </h3>
                            <p class="card-authors">Ilija Radosavovic, Tete Xiao, Bike Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Transformer</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span></div>
                            <div class="card-details" id="cat-details-4399335120">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots that can autonomously operate in diverse environments have the potential to help address labour shortages in factories, assist elderly at homes, and colonize new planets. While classical controllers for humanoid robots have shown impressive results in a number of settings, they are challenging to generalize and adapt to new environments. Here, we present a fully learning-based approach for real-world humanoid locomotion. Our controller is a causal transformer that takes the history of proprioceptive observations and actions as input and predicts the next action. We hypothesize ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Here, we present a fully learning-based approach for real-world humanoid locomotion</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2303.03381v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2303.03381v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'I. Radosavovic, T. Xiao, B. Zhang, T. Darrell, J. Malik, and K. Sreenath, &quot;Real-World Humanoid Locomotion with Reinforcement Learning,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2303.03381v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399335120')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="learning bipedal walking for humanoid robots in challenging environments with obstacle avoidance" data-keywords="reinforcement learning robot humanoid bipedal locomotion cs.ro cs.lg" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.08212v1" target="_blank">Learning Bipedal Walking for Humanoid Robots in Challenging Environments with Obstacle Avoidance</a>
                            </h3>
                            <p class="card-authors">Marwan Hamze, Mitsuharu Morisawa, Eiichi Yoshida</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span></div>
                            <div class="card-details" id="cat-details-4399335168">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Deep reinforcement learning has seen successful implementations on humanoid robots to achieve dynamic walking. However, these implementations have been so far successful in simple environments void of obstacles. In this paper, we aim to achieve bipedal locomotion in an environment where obstacles are present using a policy-based reinforcement learning. By adding simple distance reward terms to a state of art reward function that can achieve basic bipedal locomotion, the trained policy succeeds in navigating the robot towards the desired destination without colliding with the obstacles along th...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.08212v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.08212v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Hamze, M. Morisawa, and E. Yoshida, &quot;Learning Bipedal Walking for Humanoid Robots in Challenging Environments with Obstacle Avoidance,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.08212v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399335168')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="learning getting-up policies for real-world humanoid robots" data-keywords="robot humanoid locomotion control cs.ro cs.lg" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.12152v2" target="_blank">Learning Getting-Up Policies for Real-World Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Xialin He, Runpei Dong, Zixuan Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Control</span></div>
                            <div class="card-details" id="cat-details-4399334688">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Automatic fall recovery is a crucial prerequisite before humanoid robots can be reliably deployed. Hand-designing controllers for getting up is difficult because of the varied configurations a humanoid can end up in after a fall and the challenging terrains humanoid robots are expected to operate on. This paper develops a learning framework to produce controllers that enable humanoid robots to get up from varying configurations on varying terrains. Unlike previous successful applications of learning to humanoid locomotion, the getting-up task involves complex contact patterns (which necessitat...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.12152v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.12152v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. He, R. Dong, Z. Chen, and S. Gupta, &quot;Learning Getting-Up Policies for Real-World Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.12152v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399334688')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="robot trains robot: automatic real-world policy adaptation and learning for humanoids" data-keywords="reinforcement learning robot humanoid locomotion simulation imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.12252v2" target="_blank">Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids</a>
                            </h3>
                            <p class="card-authors">Kaizhe Hu, Haochen Shi, Yao He et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399335504">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Simulation-based reinforcement learning (RL) has significantly advanced humanoid locomotion tasks, yet direct real-world RL from scratch or adapting from pretrained policies remains rare, limiting the full potential of humanoid robots. Real-world learning, despite being crucial for overcoming the sim-to-real gap, faces substantial challenges related to safety, reward design, and learning efficiency. To address these limitations, we propose Robot-Trains-Robot (RTR), a novel framework where a robotic arm teacher actively supports and guides a humanoid robot student. The RTR system provides prote...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address these limitations, we propose Robot-Trains-Robot (RTR), a novel framework where a robotic arm teacher actively supports and guides a humanoid robot student</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.12252v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.12252v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Hu, H. Shi, Y. He, W. Wang, C. K. Liu, and S. Song, &quot;Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.12252v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399335504')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="robust humanoid walking on compliant and uneven terrain with deep reinforcement learning" data-keywords="reinforcement learning robot humanoid bipedal locomotion control simulation imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.13619v1" target="_blank">Robust Humanoid Walking on Compliant and Uneven Terrain with Deep Reinforcement Learning</a>
                            </h3>
                            <p class="card-authors">Rohan P. Singh, Mitsuharu Morisawa, Mehdi Benallegue et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span></div>
                            <div class="card-details" id="cat-details-4399334928">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">For the deployment of legged robots in real-world environments, it is essential to develop robust locomotion control methods for challenging terrains that may exhibit unexpected deformability and irregularity. In this paper, we explore the application of sim-to-real deep reinforcement learning (RL) for the design of bipedal locomotion controllers for humanoid robots on compliant and uneven terrains. Our key contribution is to show that a simple training curriculum for exposing the RL agent to randomized terrains in simulation can achieve robust walking on a real humanoid robot using only propr...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a new control policy to enable modification of the observed clock signal, leading to adaptive gait frequencies depending on the terrain and command velocity</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.13619v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.13619v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. P. Singh, M. Morisawa, M. Benallegue, Z. Xie, and F. Kanehiro, &quot;Robust Humanoid Walking on Compliant and Uneven Terrain with Deep Reinforcement Learning,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.13619v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399334928')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="whole-body humanoid robot locomotion with human reference" data-keywords="reinforcement learning robot humanoid locomotion imitation learning cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.18294v4" target="_blank">Whole-body Humanoid Robot Locomotion with Human Reference</a>
                            </h3>
                            <p class="card-authors">Qiang Zhang, Peter Cui, David Yan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399336464">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recently, humanoid robots have made significant advances in their ability to perform challenging tasks due to the deployment of Reinforcement Learning (RL), however, the inherent complexity of humanoid robots, including the difficulty of designing complicated reward functions and training entire sophisticated systems, still poses a notable challenge. To conquer these challenges, after many iterations and in-depth investigations, we have meticulously developed a full-size humanoid robot, &quot;Adam&quot;, whose innovative structural design greatly improves the efficiency and effectiveness of the imitatio...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.18294v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.18294v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Q. Zhang et al., &quot;Whole-body Humanoid Robot Locomotion with Human Reference,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.18294v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399336464')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="mash: cooperative-heterogeneous multi-agent reinforcement learning for single humanoid robot locomotion" data-keywords="reinforcement learning robot humanoid locomotion multi-agent multi-robot cooperative control cs.ro cs.ai" data-themes="I L R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.10423v1" target="_blank">MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion</a>
                            </h3>
                            <p class="card-authors">Qi Liu, Xiaopeng Zhang, Mingshan Tan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-R">Multi-Robot</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399338768">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper proposes a novel method to enhance locomotion for a single humanoid robot through cooperative-heterogeneous multi-agent deep reinforcement learning (MARL). While most existing methods typically employ single-agent reinforcement learning algorithms for a single humanoid robot or MARL algorithms for multi-robot system tasks, we propose a distinct paradigm: applying cooperative-heterogeneous MARL to optimize locomotion for a single humanoid robot. The proposed method, multi-agent reinforcement learning for single humanoid locomotion (MASH), treats each limb (legs and arms) as an indepe...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">While most existing methods typically employ single-agent reinforcement learning algorithms for a single humanoid robot or MARL algorithms for multi-robot system tasks, we propose a distinct paradigm: applying cooperative-heterogeneous MARL to optimize locomotion for a single humanoid robot</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.10423v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.10423v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Q. Liu, X. Zhang, M. Tan, S. Ma, J. Ding, and Y. Li, &quot;MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.10423v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399338768')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="birodiff: diffusion policies for bipedal robot locomotion on unseen terrains" data-keywords="diffusion robot bipedal locomotion control simulation imu cs.ro cs.ai eess.sy" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.05424v1" target="_blank">BiRoDiff: Diffusion policies for bipedal robot locomotion on unseen terrains</a>
                            </h3>
                            <p class="card-authors">GVS Mothish, Manan Tayal, Shishir Kolathaya</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Diffusion</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399336656">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Locomotion on unknown terrains is essential for bipedal robots to handle novel real-world challenges, thus expanding their utility in disaster response and exploration. In this work, we introduce a lightweight framework that learns a single walking controller that yields locomotion on multiple terrains. We have designed a real-time robot controller based on diffusion models, which not only captures multiple behaviours with different velocities in a single policy but also generalizes well for unseen terrains. Our controller learns with offline data, which is better than online learning in aspec...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we introduce a lightweight framework that learns a single walking controller that yields locomotion on multiple terrains</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.05424v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.05424v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Mothish, M. Tayal, and S. Kolathaya, &quot;BiRoDiff: Diffusion policies for bipedal robot locomotion on unseen terrains,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.05424v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399336656')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2026" data-category="cs.ro" data-title="locomotion beyond feet" data-keywords="reinforcement learning robot humanoid locomotion planning simulation ros imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-new">New</span></div>
                            <span class="card-source">arXiv ¬∑ 2026</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2601.03607v1" target="_blank">Locomotion Beyond Feet</a>
                            </h3>
                            <p class="card-authors">Tae Hoon Yang, Haochen Shi, Jiacheng Hu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399347648">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Most locomotion methods for humanoid robots focus on leg-based gaits, yet natural bipeds frequently rely on hands, knees, and elbows to establish additional contacts for stability and support in complex environments. This paper introduces Locomotion Beyond Feet, a comprehensive system for whole-body humanoid locomotion across extremely challenging terrains, including low-clearance spaces under chairs, knee-high walls, knee-high platforms, and steep ascending and descending stairs. Our approach addresses two key challenges: contact-rich motion planning and generalization across diverse terrains...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Our approach addresses two key challenges: contact-rich motion planning and generalization across diverse terrains</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2601.03607v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2601.03607v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. H. Yang et al., &quot;Locomotion Beyond Feet,&quot; arXiv, 2026. [Online]. Available: http://arxiv.org/abs/2601.03607v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399347648')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="how to raise a robot -- a case for neuro-symbolic ai in constrained task planning for humanoid assistive robots" data-keywords="neural network robot humanoid planning control language model cs.ro cs.cr cs.lg" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.08820v3" target="_blank">How to Raise a Robot -- A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots</a>
                            </h3>
                            <p class="card-authors">Niklas Hemken, Florian Jacob, Fabian Peller-Konrad et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Planning</span></div>
                            <div class="card-details" id="cat-details-4399336032">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots will be able to assist humans in their daily life, in particular due to their versatile action capabilities. However, while these robots need a certain degree of autonomy to learn and explore, they also should respect various constraints, for access control and beyond. We explore the novel field of incorporating privacy, security, and access control constraints with robot task planning approaches. We report preliminary results on the classical symbolic approach, deep-learned neural networks, and modern ideas using large language models as knowledge base. From analyzing their tr...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.08820v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.08820v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. Hemken, F. Jacob, F. Peller-Konrad, R. Kartmann, T. Asfour, and H. Hartenstein, &quot;How to Raise a Robot -- A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.08820v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399336032')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="periodic bipedal gait learning using reward composition based on a novel gait planner for humanoid robots" data-keywords="reinforcement learning robot humanoid bipedal locomotion planning cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.08416v1" target="_blank">Periodic Bipedal Gait Learning Using Reward Composition Based on a Novel Gait Planner for Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Bolin Li, Linwei Sun, Xuecong Huang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span></div>
                            <div class="card-details" id="cat-details-4399335840">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents a periodic bipedal gait learning method using reward composition, integrated with a real-time gait planner for humanoid robots. First, we introduce a novel gait planner that incorporates dynamics to design the desired joint trajectory. In the gait design process, the 3D robot model is decoupled into two 2D models, which are then approximated as hybrid inverted pendulums (H-LIP) for trajectory planning. The gait planner operates in parallel in real time within the robot&#x27;s learning environment. Second, based on this gait planner, we design three effective reward functions wit...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents a periodic bipedal gait learning method using reward composition, integrated with a real-time gait planner for humanoid robots</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.08416v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.08416v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. Li, L. Sun, X. Huang, Y. Jiang, and L. Zhu, &quot;Periodic Bipedal Gait Learning Using Reward Composition Based on a Novel Gait Planner for Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.08416v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399335840')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="constrained reinforcement learning for unstable point-feet bipedal locomotion applied to the bolt robot" data-keywords="reinforcement learning robot bipedal locomotion control cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.02194v1" target="_blank">Constrained Reinforcement Learning for Unstable Point-Feet Bipedal Locomotion Applied to the Bolt Robot</a>
                            </h3>
                            <p class="card-authors">Constant Roux, Elliot Chane-Sane, Ludovic De Matte√Øs et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399347504">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Bipedal locomotion is a key challenge in robotics, particularly for robots like Bolt, which have a point-foot design. This study explores the control of such underactuated robots using constrained reinforcement learning, addressing their inherent instability, lack of arms, and limited foot actuation. We present a methodology that leverages Constraints-as-Terminations and domain randomization techniques to enable sim-to-real transfer. Through a series of qualitative and quantitative experiments, we evaluate our approach in terms of balance maintenance, velocity control, and responses to slip an...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present a methodology that leverages Constraints-as-Terminations and domain randomization techniques to enable sim-to-real transfer</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.02194v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.02194v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Roux et al., &quot;Constrained Reinforcement Learning for Unstable Point-Feet Bipedal Locomotion Applied to the Bolt Robot,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.02194v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399347504')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="lipm-guided reinforcement learning for stable and perceptive locomotion in bipedal robots" data-keywords="robot bipedal locomotion simulation ros camera imu cs.ro" data-themes="S I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.09106v2" target="_blank">LIPM-Guided Reinforcement Learning for Stable and Perceptive Locomotion in Bipedal Robots</a>
                            </h3>
                            <p class="card-authors">Haokai Su, Haoxiang Luo, Shunpeng Yang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Simulation</span></div>
                            <div class="card-details" id="cat-details-4399337088">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Achieving stable and robust perceptive locomotion for bipedal robots in unstructured outdoor environments remains a critical challenge due to complex terrain geometry and susceptibility to external disturbances. In this work, we propose a novel reward design inspired by the Linear Inverted Pendulum Model (LIPM) to enable perceptive and stable locomotion in the wild. The LIPM provides theoretical guidance for dynamic balance by regulating the center of mass (CoM) height and the torso orientation. These are key factors for terrain-aware locomotion, as they help ensure a stable viewpoint for the ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose a novel reward design inspired by the Linear Inverted Pendulum Model (LIPM) to enable perceptive and stable locomotion in the wild</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.09106v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.09106v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Su, H. Luo, S. Yang, K. Jiang, W. Zhang, and H. Chen, &quot;LIPM-Guided Reinforcement Learning for Stable and Perceptive Locomotion in Bipedal Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.09106v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399337088')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="natural humanoid robot locomotion with generative motion prior" data-keywords="robot humanoid locomotion simulation imu cs.ro cs.lg" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.09015v1" target="_blank">Natural Humanoid Robot Locomotion with Generative Motion Prior</a>
                            </h3>
                            <p class="card-authors">Haodong Zhang, Liang Zhang, Zhenghan Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Simulation</span></div>
                            <div class="card-details" id="cat-details-4399348464">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Natural and lifelike locomotion remains a fundamental challenge for humanoid robots to interact with human society. However, previous methods either neglect motion naturalness or rely on unstable and ambiguous style rewards. In this paper, we propose a novel Generative Motion Prior (GMP) that provides fine-grained motion-level supervision for the task of natural humanoid robot locomotion. To leverage natural human motions, we first employ whole-body motion retargeting to effectively transfer them to the robot. Subsequently, we train a generative model offline to predict future natural referenc...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose a novel Generative Motion Prior (GMP) that provides fine-grained motion-level supervision for the task of natural humanoid robot locomotion</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.09015v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.09015v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Zhang, L. Zhang, Z. Chen, L. Chen, Y. Wang, and R. Xiong, &quot;Natural Humanoid Robot Locomotion with Generative Motion Prior,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.09015v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399348464')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="humanoid agent via embodied chain-of-action reasoning with multimodal foundation models for zero-shot loco-manipulation" data-keywords="robot humanoid locomotion manipulation coordination perception ros cs.ro cs.ai" data-themes="E I M L R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.09532v3" target="_blank">Humanoid Agent via Embodied Chain-of-Action Reasoning with Multimodal Foundation Models for Zero-Shot Loco-Manipulation</a>
                            </h3>
                            <p class="card-authors">Congcong Wen, Geeta Chandra Raju Bethala, Yu Hao et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Manipulation</span></div>
                            <div class="card-details" id="cat-details-4399334976">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid loco-manipulation, which integrates whole-body locomotion with dexterous manipulation, remains a fundamental challenge in robotics. Beyond whole-body coordination and balance, a central difficulty lies in understanding human instructions and translating them into coherent sequences of embodied actions. Recent advances in foundation models provide transferable multimodal representations and reasoning capabilities, yet existing efforts remain largely restricted to either locomotion or manipulation in isolation, with limited applicability to humanoid settings. In this paper, we propose H...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose Humanoid-COA, the first humanoid agent framework that integrates foundation model reasoning with an Embodied Chain-of-Action (CoA) mechanism for zero-shot loco-manipulation</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.09532v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.09532v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Wen et al., &quot;Humanoid Agent via Embodied Chain-of-Action Reasoning with Multimodal Foundation Models for Zero-Shot Loco-Manipulation,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.09532v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399334976')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="template model inspired task space learning for robust bipedal locomotion" data-keywords="reinforcement learning robot humanoid bipedal locomotion control cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.15442v1" target="_blank">Template Model Inspired Task Space Learning for Robust Bipedal Locomotion</a>
                            </h3>
                            <p class="card-authors">Guillermo A. Castillo, Bowen Weng, Shunpeng Yang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span></div>
                            <div class="card-details" id="cat-details-4399337184">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This work presents a hierarchical framework for bipedal locomotion that combines a Reinforcement Learning (RL)-based high-level (HL) planner policy for the online generation of task space commands with a model-based low-level (LL) controller to track the desired task space trajectories. Different from traditional end-to-end learning approaches, our HL policy takes insights from the angular momentum-based linear inverted pendulum (ALIP) to carefully design the observation and action spaces of the Markov Decision Process (MDP). This simple yet effective design creates an insightful mapping betwe...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.15442v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.15442v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. A. Castillo, B. Weng, S. Yang, W. Zhang, and A. Hereid, &quot;Template Model Inspired Task Space Learning for Robust Bipedal Locomotion,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.15442v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399337184')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="learning bipedal walking for humanoids with current feedback" data-keywords="reinforcement learning robot humanoid bipedal locomotion control simulation imu cs.ro cs.lg" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2303.03724v2" target="_blank">Learning Bipedal Walking for Humanoids with Current Feedback</a>
                            </h3>
                            <p class="card-authors">Rohan Pratap Singh, Zhaoming Xie, Pierre Gergondet et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span></div>
                            <div class="card-details" id="cat-details-4399347984">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recent advances in deep reinforcement learning (RL) based techniques combined with training in simulation have offered a new approach to developing robust controllers for legged robots. However, the application of such approaches to real hardware has largely been limited to quadrupedal robots with direct-drive actuators and light-weight bipedal robots with low gear-ratio transmission systems. Application to real, life-sized humanoid robots has been less common arguably due to a large sim2real gap. In this paper, we present an approach for effectively overcoming the sim2real gap issue for human...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we present an approach for effectively overcoming the sim2real gap issue for humanoid robots arising from inaccurate torque-tracking at the actuator level</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2303.03724v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2303.03724v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. P. Singh, Z. Xie, P. Gergondet, and F. Kanehiro, &quot;Learning Bipedal Walking for Humanoids with Current Feedback,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2303.03724v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399347984')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="polygmap: a perceptive locomotion framework for humanoid robot stair climbing" data-keywords="robot humanoid locomotion perception planning lidar camera imu sensor fusion cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2510.12346v1" target="_blank">PolygMap: A Perceptive Locomotion Framework for Humanoid Robot Stair Climbing</a>
                            </h3>
                            <p class="card-authors">Bingquan Li, Ning Wang, Tianwei Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Perception</span></div>
                            <div class="card-details" id="cat-details-4399334784">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recently, biped robot walking technology has been significantly developed, mainly in the context of a bland walking scheme. To emulate human walking, robots need to step on the positions they see in unknown spaces accurately. In this paper, we present PolyMap, a perception-based locomotion planning framework for humanoid robots to climb stairs. Our core idea is to build a real-time polygonal staircase plane semantic map, followed by a footstep planar using these polygonal plane segments. These plane segmentation and visual odometry are done by multi-sensor fusion(LiDAR, RGB-D camera and IMUs)....</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we present PolyMap, a perception-based locomotion planning framework for humanoid robots to climb stairs</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2510.12346v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2510.12346v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. Li, N. Wang, T. Zhang, Z. He, and Y. Wu, &quot;PolygMap: A Perceptive Locomotion Framework for Humanoid Robot Stair Climbing,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2510.12346v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399334784')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="humanplus: humanoid shadowing and imitation from humans" data-keywords="reinforcement learning robot humanoid perception control simulation camera imu cs.ro cs.ai" data-themes="I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2406.10454v1" target="_blank">HumanPlus: Humanoid Shadowing and Imitation from Humans</a>
                            </h3>
                            <p class="card-authors">Zipeng Fu, Qingqing Zhao, Qi Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Perception</span></div>
                            <div class="card-details" id="cat-details-4399335936">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">One of the key arguments for building robots that have similar form factors to human beings is that we can leverage the massive human data for training. Yet, doing so has remained challenging in practice due to the complexities in humanoid perception and control, lingering physical gaps between humanoids and humans in morphologies and actuation, and lack of a data pipeline for humanoids to learn autonomous skills from egocentric vision. In this paper, we introduce a full-stack system for humanoids to learn motion and autonomous skills from human data. We first train a low-level policy in simul...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we introduce a full-stack system for humanoids to learn motion and autonomous skills from human data</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2406.10454v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2406.10454v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Fu, Q. Zhao, Q. Wu, G. Wetzstein, and C. Finn, &quot;HumanPlus: Humanoid Shadowing and Imitation from Humans,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2406.10454v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399335936')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="unified multi-rate model predictive control for a jet-powered humanoid robot" data-keywords="robot humanoid control simulation mujoco imu cs.ro eess.sy" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2505.16478v2" target="_blank">Unified Multi-Rate Model Predictive Control for a Jet-Powered Humanoid Robot</a>
                            </h3>
                            <p class="card-authors">Davide Gorbani, Giuseppe L&#x27;Erario, Hosameldin Awadalla Omer Mohamed et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Control</span><span class="keyword-tag">Simulation</span></div>
                            <div class="card-details" id="cat-details-4399337280">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We propose a novel Model Predictive Control (MPC) framework for a jet-powered flying humanoid robot. The controller is based on a linearised centroidal momentum model to represent the flight dynamics, augmented with a second-order nonlinear model to explicitly account for the slow and nonlinear dynamics of jet propulsion. A key contribution is the introduction of a multi-rate MPC formulation that handles the different actuation rates of the robot&#x27;s joints and jet engines while embedding the jet dynamics directly into the predictive model. We validated the framework using the jet-powered humano...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a novel Model Predictive Control (MPC) framework for a jet-powered flying humanoid robot</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2505.16478v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2505.16478v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Gorbani, G. L&#x27;Erario, H. A. O. Mohamed, and D. Pucci, &quot;Unified Multi-Rate Model Predictive Control for a Jet-Powered Humanoid Robot,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2505.16478v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399337280')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="generalizable humanoid manipulation with 3d diffusion policies" data-keywords="diffusion robot humanoid manipulation lidar cs.ro cs.cv cs.lg" data-themes="S M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.10803v3" target="_blank">Generalizable Humanoid Manipulation with 3D Diffusion Policies</a>
                            </h3>
                            <p class="card-authors">Yanjie Ze, Zixuan Chen, Wenhao Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Diffusion</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Manipulation</span></div>
                            <div class="card-details" id="cat-details-4399348080">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots capable of autonomous operation in diverse environments have long been a goal for roboticists. However, autonomous manipulation by humanoid robots has largely been restricted to one specific scene, primarily due to the difficulty of acquiring generalizable skills and the expensiveness of in-the-wild humanoid robot data. In this work, we build a real-world robotic system to address this challenging problem. Our system is mainly an integration of 1) a whole-upper-body robotic teleoperation system to acquire human-like robot data, 2) a 25-DoF humanoid robot platform with a height-...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.10803v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.10803v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Ze et al., &quot;Generalizable Humanoid Manipulation with 3D Diffusion Policies,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.10803v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399348080')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="stride: automating reward design, deep reinforcement learning training and feedback optimization in humanoid robotics locomotion" data-keywords="reinforcement learning robot humanoid locomotion coordination control optimization ros imu language model" data-themes="I E L R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.04692v3" target="_blank">STRIDE: Automating Reward Design, Deep Reinforcement Learning Training and Feedback Optimization in Humanoid Robotics Locomotion</a>
                            </h3>
                            <p class="card-authors">Zhenwei Wu, Jinxiong Lu, Yuxiao Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399335792">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robotics presents significant challenges in artificial intelligence, requiring precise coordination and control of high-degree-of-freedom systems. Designing effective reward functions for deep reinforcement learning (DRL) in this domain remains a critical bottleneck, demanding extensive manual effort, domain expertise, and iterative refinement. To overcome these challenges, we introduce STRIDE, a novel framework built on agentic engineering to automate reward design, DRL training, and feedback optimization for humanoid robot locomotion tasks. By combining the structured principles of ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To overcome these challenges, we introduce STRIDE, a novel framework built on agentic engineering to automate reward design, DRL training, and feedback optimization for humanoid robot locomotion tasks</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.04692v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.04692v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Wu, J. Lu, Y. Chen, Y. Liu, Y. Zhuang, and L. Hu, &quot;STRIDE: Automating Reward Design, Deep Reinforcement Learning Training and Feedback Optimization in Humanoid Robotics Locomotion,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.04692v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399335792')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="styleloco: generative adversarial distillation for natural humanoid robot locomotion" data-keywords="reinforcement learning robot humanoid locomotion simulation ros imu imitation learning cs.ro cs.ai" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.15082v1" target="_blank">StyleLoco: Generative Adversarial Distillation for Natural Humanoid Robot Locomotion</a>
                            </h3>
                            <p class="card-authors">Le Ma, Ziyu Meng, Tengyu Liu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399336848">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots are anticipated to acquire a wide range of locomotion capabilities while ensuring natural movement across varying speeds and terrains. Existing methods encounter a fundamental dilemma in learning humanoid locomotion: reinforcement learning with handcrafted rewards can achieve agile locomotion but produces unnatural gaits, while Generative Adversarial Imitation Learning (GAIL) with motion capture data yields natural movements but suffers from unstable training processes and restricted agility. Integrating these approaches proves challenging due to the inherent heterogeneity betw...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address this, we introduce StyleLoco, a novel two-stage framework that bridges this gap through a Generative Adversarial Distillation (GAD) process</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.15082v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.15082v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Ma et al., &quot;StyleLoco: Generative Adversarial Distillation for Natural Humanoid Robot Locomotion,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.15082v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399336848')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="rhino: learning real-time humanoid-human-object interaction from human demonstrations" data-keywords="robot humanoid locomotion manipulation control cs.ro cs.hc cs.lg" data-themes="S E I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.13134v1" target="_blank">RHINO: Learning Real-Time Humanoid-Human-Object Interaction from Human Demonstrations</a>
                            </h3>
                            <p class="card-authors">Jingxiao Chen, Xinyao Li, Jiahang Cao et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Manipulation</span></div>
                            <div class="card-details" id="cat-details-4399337520">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots have shown success in locomotion and manipulation. Despite these basic abilities, humanoids are still required to quickly understand human instructions and react based on human interaction signals to become valuable assistants in human daily life. Unfortunately, most existing works only focus on multi-stage interactions, treating each task separately, and neglecting real-time feedback. In this work, we aim to empower humanoid robots with real-time reaction abilities to achieve various tasks, allowing human to interrupt robots at any time, and making robots respond to humans imm...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To support such abilities, we propose a general humanoid-human-object interaction framework, named RHINO, i.e., Real-time Humanoid-human Interaction and Object manipulation</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.13134v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.13134v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Chen et al., &quot;RHINO: Learning Real-Time Humanoid-Human-Object Interaction from Human Demonstrations,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.13134v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399337520')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="humanmimic: learning natural locomotion and transitions for humanoid robot via wasserstein adversarial imitation" data-keywords="reinforcement learning robot humanoid locomotion control imu imitation learning cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.14225v4" target="_blank">HumanMimic: Learning Natural Locomotion and Transitions for Humanoid Robot via Wasserstein Adversarial Imitation</a>
                            </h3>
                            <p class="card-authors">Annan Tang, Takuma Hiraoka, Naoki Hiraoka et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399336368">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Transferring human motion skills to humanoid robots remains a significant challenge. In this study, we introduce a Wasserstein adversarial imitation learning system, allowing humanoid robots to replicate natural whole-body locomotion patterns and execute seamless transitions by mimicking human motions. First, we present a unified primitive-skeleton motion retargeting to mitigate morphological differences between arbitrary human demonstrators and humanoid robots. An adversarial critic component is integrated with Reinforcement Learning (RL) to guide the control policy to produce behaviors align...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this study, we introduce a Wasserstein adversarial imitation learning system, allowing humanoid robots to replicate natural whole-body locomotion patterns and execute seamless transitions by mimicking human motions</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.14225v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.14225v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Tang et al., &quot;HumanMimic: Learning Natural Locomotion and Transitions for Humanoid Robot via Wasserstein Adversarial Imitation,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.14225v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399336368')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="learning vision-driven reactive soccer skills for humanoid robots" data-keywords="reinforcement learning robot humanoid coordination perception control ros cs.ro" data-themes="S I R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2511.03996v1" target="_blank">Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Yushi Wang, Changsheng Luo, Penghui Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-R">Multi-Robot</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Coordination</span></div>
                            <div class="card-details" id="cat-details-4399337424">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid soccer poses a representative challenge for embodied intelligence, requiring robots to operate within a tightly coupled perception-action loop. However, existing systems typically rely on decoupled modules, resulting in delayed responses and incoherent behaviors in dynamic environments, while real-world perceptual limitations further exacerbate these issues. In this work, we present a unified reinforcement learning-based controller that enables humanoid robots to acquire reactive soccer skills through the direct integration of visual perception and motion control. Our approach extends...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we present a unified reinforcement learning-based controller that enables humanoid robots to acquire reactive soccer skills through the direct integration of visual perception and motion control</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2511.03996v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2511.03996v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Wang et al., &quot;Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2511.03996v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399337424')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="socially acceptable bipedal navigation: a signal-temporal-logic- driven approach for safe locomotion" data-keywords="robot bipedal locomotion navigation control simulation imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2310.09969v1" target="_blank">Socially Acceptable Bipedal Navigation: A Signal-Temporal-Logic- Driven Approach for Safe Locomotion</a>
                            </h3>
                            <p class="card-authors">Abdulaziz Shamsah, Ye Zhao</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Navigation</span></div>
                            <div class="card-details" id="cat-details-4399348128">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Social navigation for bipedal robots remains relatively unexplored due to the highly complex, nonlinear dynamics of bipedal locomotion. This study presents a preliminary exploration of social navigation for bipedal robots in a human crowded environment. We propose a social path planner that ensures the locomotion safety of the bipedal robot while navigating under a social norm. The proposed planner leverages a conditional variational autoencoder architecture and learns from human crowd datasets to produce a socially acceptable path plan. Robot-specific locomotion safety is formally enforced by...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a social path planner that ensures the locomotion safety of the bipedal robot while navigating under a social norm</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2310.09969v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2310.09969v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Shamsah, and Y. Zhao, &quot;Socially Acceptable Bipedal Navigation: A Signal-Temporal-Logic- Driven Approach for Safe Locomotion,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2310.09969v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399348128')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="end-to-end humanoid robot safe and comfortable locomotion policy" data-keywords="reinforcement learning robot humanoid locomotion navigation perception control optimization lidar cs.ro" data-themes="S I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.07611v1" target="_blank">End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy</a>
                            </h3>
                            <p class="card-authors">Zifan Wang, Xun Yang, Jianzhuang Zhao et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399336752">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The deployment of humanoid robots in unstructured, human-centric environments requires navigation capabilities that extend beyond simple locomotion to include robust perception, provable safety, and socially aware behavior. Current reinforcement learning approaches are often limited by blind controllers that lack environmental awareness or by vision-based systems that fail to perceive complex 3D obstacles. In this work, we present an end-to-end locomotion policy that directly maps raw, spatio-temporal LiDAR point clouds to motor commands, enabling robust navigation in cluttered dynamic scenes....</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we present an end-to-end locomotion policy that directly maps raw, spatio-temporal LiDAR point clouds to motor commands, enabling robust navigation in cluttered dynamic scenes</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.07611v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.07611v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Wang et al., &quot;End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.07611v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399336752')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="child (controller for humanoid imitation and live demonstration): a whole-body humanoid teleoperation system" data-keywords="robot humanoid manipulation control cs.ro" data-themes="I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.00162v2" target="_blank">CHILD (Controller for Humanoid Imitation and Live Demonstration): a Whole-Body Humanoid Teleoperation System</a>
                            </h3>
                            <p class="card-authors">Noboru Myers, Obin Kwon, Sankalp Yamsani et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Manipulation</span><span class="keyword-tag">Control</span></div>
                            <div class="card-details" id="cat-details-4399348656">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recent advances in teleoperation have demonstrated robots performing complex manipulation tasks. However, existing works rarely support whole-body joint-level teleoperation for humanoid robots, limiting the diversity of tasks that can be accomplished. This work presents Controller for Humanoid Imitation and Live Demonstration (CHILD), a compact reconfigurable teleoperation system that enables joint level control over humanoid robots. CHILD fits within a standard baby carrier, allowing the operator control over all four limbs, and supports both direct joint mapping for full-body control and loc...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.00162v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.00162v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. Myers, O. Kwon, S. Yamsani, and J. Kim, &quot;CHILD (Controller for Humanoid Imitation and Live Demonstration): a Whole-Body Humanoid Teleoperation System,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.00162v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399348656')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="real-time polygonal semantic mapping for humanoid robot stair climbing" data-keywords="diffusion robot humanoid planning cs.ro cs.cv" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2411.01919v1" target="_blank">Real-Time Polygonal Semantic Mapping for Humanoid Robot Stair Climbing</a>
                            </h3>
                            <p class="card-authors">Teng Bin, Jianming Yao, Tin Lun Lam et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Diffusion</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Planning</span></div>
                            <div class="card-details" id="cat-details-4399347696">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We present a novel algorithm for real-time planar semantic mapping tailored for humanoid robots navigating complex terrains such as staircases. Our method is adaptable to any odometry input and leverages GPU-accelerated processes for planar extraction, enabling the rapid generation of globally consistent semantic maps. We utilize an anisotropic diffusion filter on depth images to effectively minimize noise from gradient jumps while preserving essential edge details, enhancing normal vector images&#x27; accuracy and smoothness. Both the anisotropic diffusion and the RANSAC-based plane extraction pro...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present a novel algorithm for real-time planar semantic mapping tailored for humanoid robots navigating complex terrains such as staircases</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2411.01919v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2411.01919v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. Bin, J. Yao, T. L. Lam, and T. Zhang, &quot;Real-Time Polygonal Semantic Mapping for Humanoid Robot Stair Climbing,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2411.01919v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399347696')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="rl-augmented adaptive model predictive control for bipedal locomotion over challenging terrain" data-keywords="reinforcement learning robot humanoid bipedal locomotion control simulation ros isaac imu" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.18466v1" target="_blank">RL-augmented Adaptive Model Predictive Control for Bipedal Locomotion over Challenging Terrain</a>
                            </h3>
                            <p class="card-authors">Junnosuke Kamohara, Feiyang Wu, Chinmayee Wamorkar et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span></div>
                            <div class="card-details" id="cat-details-4399339200">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Model predictive control (MPC) has demonstrated effectiveness for humanoid bipedal locomotion; however, its applicability in challenging environments, such as rough and slippery terrain, is limited by the difficulty of modeling terrain interactions. In contrast, reinforcement learning (RL) has achieved notable success in training robust locomotion policies over diverse terrain, yet it lacks guarantees of constraint satisfaction and often requires substantial reward shaping. Recent efforts in combining MPC and RL have shown promise of taking the best of both worlds, but they are primarily restr...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose an RL-augmented MPC framework tailored for bipedal locomotion over rough and slippery terrain</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.18466v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.18466v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Kamohara, F. Wu, C. Wamorkar, S. Hutchinson, and Y. Zhao, &quot;RL-augmented Adaptive Model Predictive Control for Bipedal Locomotion over Challenging Terrain,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.18466v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399339200')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="vmts: vision-assisted teacher-student reinforcement learning for multi-terrain locomotion in bipedal robots" data-keywords="reinforcement learning robot bipedal locomotion perception control ros cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.07049v2" target="_blank">VMTS: Vision-Assisted Teacher-Student Reinforcement Learning for Multi-Terrain Locomotion in Bipedal Robots</a>
                            </h3>
                            <p class="card-authors">Fu Chen, Rui Wan, Peidong Liu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399349232">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Bipedal robots, due to their anthropomorphic design, offer substantial potential across various applications, yet their control is hindered by the complexity of their structure. Currently, most research focuses on proprioception-based methods, which lack the capability to overcome complex terrain. While visual perception is vital for operation in human-centric environments, its integration complicates control further. Recent reinforcement learning (RL) approaches have shown promise in enhancing legged robot locomotion, particularly with proprioception-based methods. However, terrain adaptabili...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we introduce a novel mixture of experts teacher-student network RL strategy, which enhances the performance of teacher-student policies based on visual inputs through a simple yet effective approach</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.07049v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.07049v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. Chen, R. Wan, P. Liu, N. Zheng, and B. Zhou, &quot;VMTS: Vision-Assisted Teacher-Student Reinforcement Learning for Multi-Terrain Locomotion in Bipedal Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.07049v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399349232')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="booster gym: an end-to-end reinforcement learning framework for humanoid robot locomotion" data-keywords="reinforcement learning robot humanoid locomotion simulation imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.15132v1" target="_blank">Booster Gym: An End-to-End Reinforcement Learning Framework for Humanoid Robot Locomotion</a>
                            </h3>
                            <p class="card-authors">Yushi Wang, Penghui Chen, Xinyu Han et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399347888">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recent advancements in reinforcement learning (RL) have led to significant progress in humanoid robot locomotion, simplifying the design and training of motion policies in simulation. However, the numerous implementation details make transferring these policies to real-world robots a challenging task. To address this, we have developed a comprehensive code framework that covers the entire process from training to deployment, incorporating common RL training methods, domain randomization, reward function design, and solutions for handling parallel structures. This library is made available as a...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.15132v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.15132v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Wang, P. Chen, X. Han, F. Wu, and M. Zhao, &quot;Booster Gym: An End-to-End Reinforcement Learning Framework for Humanoid Robot Locomotion,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.15132v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399347888')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="deep reinforcement learning for bipedal locomotion: a brief survey" data-keywords="reinforcement learning robot bipedal locomotion control cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2404.17070v7" target="_blank">Deep Reinforcement Learning for Bipedal Locomotion: A Brief Survey</a>
                            </h3>
                            <p class="card-authors">Lingfan Bao, Joseph Humphreys, Tianhu Peng et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399349424">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Bipedal robots are gaining global recognition due to their potential applications and advancements in artificial intelligence, particularly through Deep Reinforcement Learning (DRL). While DRL has significantly advanced bipedal locomotion, the development of a unified framework capable of handling a wide range of tasks remains an ongoing challenge. This survey systematically categorises, compares, and analyses existing DRL frameworks for bipedal locomotion, organising them into end-to-end and hierarchical control schemes. End-to-end frameworks are evaluated based on their learning approaches, ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2404.17070v7" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2404.17070v7" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Bao, J. Humphreys, T. Peng, and C. Zhou, &quot;Deep Reinforcement Learning for Bipedal Locomotion: A Brief Survey,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2404.17070v7')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399349424')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="mechanical intelligence-aware curriculum reinforcement learning for humanoids with parallel actuation" data-keywords="reinforcement learning robot humanoid locomotion control simulation mujoco imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.00273v3" target="_blank">Mechanical Intelligence-Aware Curriculum Reinforcement Learning for Humanoids with Parallel Actuation</a>
                            </h3>
                            <p class="card-authors">Yusuke Tanaka, Alvin Zhu, Quanyou Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399348704">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Reinforcement learning (RL) has enabled advances in humanoid robot locomotion, yet most learning frameworks do not account for mechanical intelligence embedded in parallel actuation mechanisms due to limitations in simulator support for closed kinematic chains. This omission can lead to inaccurate motion modeling and suboptimal policies, particularly for robots with high actuation complexity. This paper presents general formulations and simulation methods for three types of parallel mechanisms: a differential pulley, a five-bar linkage, and a four-bar linkage, and trains a parallel-mechanism a...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents general formulations and simulation methods for three types of parallel mechanisms: a differential pulley, a five-bar linkage, and a four-bar linkage, and trains a parallel-mechanism aware policy through an end-to-end curriculum RL framework for BRUCE, a kid-sized humanoid robot</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.00273v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.00273v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Tanaka, A. Zhu, Q. Wang, Y. Liu, and D. Hong, &quot;Mechanical Intelligence-Aware Curriculum Reinforcement Learning for Humanoids with Parallel Actuation,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.00273v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399348704')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="capture point control in thruster-assisted bipedal locomotion" data-keywords="robot bipedal locomotion manipulation control simulation imu cs.ro eess.sy" data-themes="I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2406.14799v1" target="_blank">Capture Point Control in Thruster-Assisted Bipedal Locomotion</a>
                            </h3>
                            <p class="card-authors">Shreyansh Pitroda, Aditya Bondada, Kaushik Venkatesh Krishnamurthy et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Manipulation</span></div>
                            <div class="card-details" id="cat-details-4399336992">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Despite major advancements in control design that are robust to unplanned disturbances, bipedal robots are still susceptible to falling over and struggle to negotiate rough terrains. By utilizing thrusters in our bipedal robot, we can perform additional posture manipulation and expand the modes of locomotion to enhance the robot&#x27;s stability and ability to negotiate rough and difficult-to-navigate terrains. In this paper, we present our efforts in designing a controller based on capture point control for our thruster-assisted walking model named Harpy and explore its control design possibilitie...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we present our efforts in designing a controller based on capture point control for our thruster-assisted walking model named Harpy and explore its control design possibilities</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2406.14799v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2406.14799v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Pitroda et al., &quot;Capture Point Control in Thruster-Assisted Bipedal Locomotion,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2406.14799v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399336992')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="a framework for optimal ankle design of humanoid robots" data-keywords="robot humanoid optimization ros cs.ro" data-themes="M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.16469v1" target="_blank">A Framework for Optimal Ankle Design of Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Guglielmo Cervettini, Roberto Mauceri, Alex Coppola et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">Ros</span></div>
                            <div class="card-details" id="cat-details-4399347456">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The design of the humanoid ankle is critical for safe and efficient ground interaction. Key factors such as mechanical compliance and motor mass distribution have driven the adoption of parallel mechanism architectures. However, selecting the optimal configuration depends on both actuator availability and task requirements. We propose a unified methodology for the design and evaluation of parallel ankle mechanisms. A multi-objective optimization synthesizes the mechanism geometry, the resulting solutions are evaluated using a scalar cost function that aggregates key performance metrics for cro...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a unified methodology for the design and evaluation of parallel ankle mechanisms</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.16469v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.16469v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Cervettini et al., &quot;A Framework for Optimal Ankle Design of Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.16469v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399347456')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="learning humanoid standing-up control across diverse postures" data-keywords="reinforcement learning robot humanoid locomotion manipulation control simulation ros imu cs.ro" data-themes="S I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.08378v2" target="_blank">Learning Humanoid Standing-up Control across Diverse Postures</a>
                            </h3>
                            <p class="card-authors">Tao Huang, Junli Ren, Huayi Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399349136">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Standing-up control is crucial for humanoid robots, with the potential for integration into current locomotion and loco-manipulation systems, such as fall recovery. Existing approaches are either limited to simulations that overlook hardware constraints or rely on predefined ground-specific motion trajectories, failing to enable standing up across postures in real-world scenes. To bridge this gap, we present HoST (Humanoid Standing-up Control), a reinforcement learning framework that learns standing-up control from scratch, enabling robust sim-to-real transfer across diverse postures. HoST eff...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To bridge this gap, we present HoST (Humanoid Standing-up Control), a reinforcement learning framework that learns standing-up control from scratch, enabling robust sim-to-real transfer across diverse postures</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.08378v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.08378v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. Huang et al., &quot;Learning Humanoid Standing-up Control across Diverse Postures,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.08378v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399349136')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="learning vision-based bipedal locomotion for challenging terrain" data-keywords="reinforcement learning robot bipedal locomotion perception control simulation imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.14594v2" target="_blank">Learning Vision-Based Bipedal Locomotion for Challenging Terrain</a>
                            </h3>
                            <p class="card-authors">Helei Duan, Bikram Pandit, Mohitvishnu S. Gadde et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399348560">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Reinforcement learning (RL) for bipedal locomotion has recently demonstrated robust gaits over moderate terrains using only proprioceptive sensing. However, such blind controllers will fail in environments where robots must anticipate and adapt to local terrain, which requires visual perception. In this paper, we propose a fully-learned system that allows bipedal robots to react to local terrain while maintaining commanded travel speed and direction. Our approach first trains a controller in simulation using a heightmap expressed in the robot&#x27;s local frame. Next, data is collected in simulatio...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose a fully-learned system that allows bipedal robots to react to local terrain while maintaining commanded travel speed and direction</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.14594v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.14594v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Duan et al., &quot;Learning Vision-Based Bipedal Locomotion for Challenging Terrain,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.14594v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399348560')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="centroidal state estimation and control for hardware-constrained humanoid robots" data-keywords="robot humanoid control cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.11019v1" target="_blank">Centroidal State Estimation and Control for Hardware-constrained Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Grzegorz Ficht, Sven Behnke</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Control</span><span class="keyword-tag">CS.RO</span></div>
                            <div class="card-details" id="cat-details-4399347840">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We introduce novel methods for state estimation, feedforward and feedback control, which specifically target humanoid robots with hardware limitations. Our method combines a five-mass model with approximate dynamics of each mass. It enables acquiring an accurate assessment of the centroidal state and Center of Pressure, even when direct forms of force or contact sensing are unavailable. Upon this, we develop a feedforward scheme that operates on the centroidal state, accounting for insufficient joint tracking capabilities. Finally, we implement feedback mechanisms, which compensate for the lac...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce novel methods for state estimation, feedforward and feedback control, which specifically target humanoid robots with hardware limitations</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.11019v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.11019v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Ficht, and S. Behnke, &quot;Centroidal State Estimation and Control for Hardware-constrained Humanoid Robots,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.11019v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399347840')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="a unified and general humanoid whole-body controller for versatile locomotion" data-keywords="robot humanoid locomotion manipulation control simulation imu cs.ro cs.ai" data-themes="S I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.03206v3" target="_blank">A Unified and General Humanoid Whole-Body Controller for Versatile Locomotion</a>
                            </h3>
                            <p class="card-authors">Yufei Xue, Wentao Dong, Minghuan Liu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Manipulation</span></div>
                            <div class="card-details" id="cat-details-4399336896">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Locomotion is a fundamental skill for humanoid robots. However, most existing works make locomotion a single, tedious, unextendable, and unconstrained movement. This limits the kinematic capabilities of humanoid robots. In contrast, humans possess versatile athletic abilities-running, jumping, hopping, and finely adjusting gait parameters such as frequency and foot height. In this paper, we investigate solutions to bring such versatility into humanoid locomotion and thereby propose HugWBC: a unified and general humanoid whole-body controller for versatile locomotion. By designing a general com...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.03206v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.03206v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Xue, W. Dong, M. Liu, W. Zhang, and J. Pang, &quot;A Unified and General Humanoid Whole-Body Controller for Versatile Locomotion,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.03206v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399336896')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="reinforcement learning for versatile, dynamic, and robust bipedal locomotion control" data-keywords="reinforcement learning robot bipedal locomotion control simulation ros imu cs.ro cs.ai" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.16889v2" target="_blank">Reinforcement Learning for Versatile, Dynamic, and Robust Bipedal Locomotion Control</a>
                            </h3>
                            <p class="card-authors">Zhongyu Li, Xue Bin Peng, Pieter Abbeel et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399335552">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents a comprehensive study on using deep reinforcement learning (RL) to create dynamic locomotion controllers for bipedal robots. Going beyond focusing on a single locomotion skill, we develop a general control solution that can be used for a range of dynamic bipedal skills, from periodic walking and running to aperiodic jumping and standing. Our RL-based controller incorporates a novel dual-history architecture, utilizing both a long-term and short-term input/output (I/O) history of the robot. This control architecture, when trained through the proposed end-to-end RL approach, ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents a comprehensive study on using deep reinforcement learning (RL) to create dynamic locomotion controllers for bipedal robots</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.16889v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.16889v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Li, X. B. Peng, P. Abbeel, S. Levine, G. Berseth, and K. Sreenath, &quot;Reinforcement Learning for Versatile, Dynamic, and Robust Bipedal Locomotion Control,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.16889v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399335552')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="infer and adapt: bipedal locomotion reward learning from demonstrations via inverse reinforcement learning" data-keywords="reinforcement learning robot bipedal locomotion imitation learning cs.ro cs.lg" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.16074v1" target="_blank">Infer and Adapt: Bipedal Locomotion Reward Learning from Demonstrations via Inverse Reinforcement Learning</a>
                            </h3>
                            <p class="card-authors">Feiyang Wu, Zhaoyuan Gu, Hanran Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399336944">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Enabling bipedal walking robots to learn how to maneuver over highly uneven, dynamically changing terrains is challenging due to the complexity of robot dynamics and interacted environments. Recent advancements in learning from demonstrations have shown promising results for robot learning in complex environments. While imitation learning of expert policies has been well-explored, the study of learning expert reward functions is largely under-explored in legged locomotion. This paper brings state-of-the-art Inverse Reinforcement Learning (IRL) techniques to solving bipedal locomotion problems ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose algorithms for learning expert reward functions, and we subsequently analyze the learned functions</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.16074v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.16074v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. Wu, Z. Gu, H. Wu, A. Wu, and Y. Zhao, &quot;Infer and Adapt: Bipedal Locomotion Reward Learning from Demonstrations via Inverse Reinforcement Learning,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.16074v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399336944')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="autotuning bipedal locomotion mpc with grfm-net for efficient sim-to-real transfer" data-keywords="robot humanoid bipedal locomotion control optimization simulation imu cs.ro cs.ai" data-themes="I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.15710v1" target="_blank">Autotuning Bipedal Locomotion MPC with GRFM-Net for Efficient Sim-to-Real Transfer</a>
                            </h3>
                            <p class="card-authors">Qianzhong Chen, Junheng Li, Sheng Cheng et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399347936">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Bipedal locomotion control is essential for humanoid robots to navigate complex, human-centric environments. While optimization-based control designs are popular for integrating sophisticated models of humanoid robots, they often require labor-intensive manual tuning. In this work, we address the challenges of parameter selection in bipedal locomotion control using DiffTune, a model-based autotuning method that leverages differential programming for efficient parameter learning. A major difficulty lies in balancing model fidelity with differentiability. We address this difficulty using a low-f...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.15710v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.15710v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Q. Chen, J. Li, S. Cheng, N. Hovakimyan, and Q. Nguyen, &quot;Autotuning Bipedal Locomotion MPC with GRFM-Net for Efficient Sim-to-Real Transfer,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.15710v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399347936')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="design and control of a small humanoid equipped with flight unit and wheels for multimodal locomotion" data-keywords="robot humanoid locomotion manipulation control imu cs.ro" data-themes="I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2303.14718v3" target="_blank">Design and Control of a Small Humanoid Equipped with Flight Unit and Wheels for Multimodal Locomotion</a>
                            </h3>
                            <p class="card-authors">Kazuki Sugihara, Moju Zhao, Takuzumi Nishio et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Manipulation</span></div>
                            <div class="card-details" id="cat-details-4399348032">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoids are versatile robotic platforms owing to their limbs with multiple degrees of freedom. Although humanoids can walk like humans, they are relatively slow, and cannot run over large barriers. To address these limitations, we aim to achieve rapid terrestrial locomotion ability and simultaneously expand the locomotion domain to the air by utilizing thrust for propulsion. In this paper, we first describe an optimized construction method for a humanoid robot equipped with wheels and a flight unit to achieve these abilities. Then, we describe the integrated control framework of the proposed...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2303.14718v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2303.14718v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Sugihara, M. Zhao, T. Nishio, T. Makabe, K. Okada, and M. Inaba, &quot;Design and Control of a Small Humanoid Equipped with Flight Unit and Wheels for Multimodal Locomotion,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2303.14718v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399348032')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="achieving stable high-speed locomotion for humanoid robots with deep reinforcement learning" data-keywords="reinforcement learning robot humanoid locomotion control simulation imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.16611v1" target="_blank">Achieving Stable High-Speed Locomotion for Humanoid Robots with Deep Reinforcement Learning</a>
                            </h3>
                            <p class="card-authors">Xinming Zhang, Xianghui Wang, Lerong Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399334832">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots offer significant versatility for performing a wide range of tasks, yet their basic ability to walk and run, especially at high velocities, remains a challenge. This letter presents a novel method that combines deep reinforcement learning with kinodynamic priors to achieve stable locomotion control (KSLC). KSLC promotes coordinated arm movements to counteract destabilizing forces, enhancing overall stability. Compared to the baseline method, KSLC provides more accurate tracking of commanded velocities and better generalization in velocity control. In simulation tests, the KSLC-...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.16611v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.16611v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. Zhang, X. Wang, L. Zhang, G. Guo, X. Shen, and W. Zhang, &quot;Achieving Stable High-Speed Locomotion for Humanoid Robots with Deep Reinforcement Learning,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.16611v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399334832')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="unitracker: learning universal whole-body motion tracker for humanoid robots" data-keywords="robot humanoid control simulation ros imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.07356v3" target="_blank">UniTracker: Learning Universal Whole-Body Motion Tracker for Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Kangning Yin, Weishuai Zeng, Ke Fan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Control</span><span class="keyword-tag">Simulation</span></div>
                            <div class="card-details" id="cat-details-4399349184">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Achieving expressive and generalizable whole-body motion control is essential for deploying humanoid robots in real-world environments. In this work, we propose UniTracker, a three-stage training framework that enables robust and scalable motion tracking across a wide range of human behaviors. In the first stage, we train a teacher policy with privileged observations to generate high-quality actions. In the second stage, we introduce a Conditional Variational Autoencoder (CVAE) to model a universal student policy that can be deployed directly on real hardware. The CVAE structure allows the pol...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose UniTracker, a three-stage training framework that enables robust and scalable motion tracking across a wide range of human behaviors</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.07356v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.07356v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Yin et al., &quot;UniTracker: Learning Universal Whole-Body Motion Tracker for Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.07356v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399349184')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="online dnn-driven nonlinear mpc for stylistic humanoid robot walking with step adjustment" data-keywords="neural network robot humanoid locomotion control optimization cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.07849v1" target="_blank">Online DNN-driven Nonlinear MPC for Stylistic Humanoid Robot Walking with Step Adjustment</a>
                            </h3>
                            <p class="card-authors">Giulio Romualdi, Paolo Maria Viceconte, Lorenzo Moretti et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399348608">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents a three-layered architecture that enables stylistic locomotion with online contact location adjustment. Our method combines an autoregressive Deep Neural Network (DNN) acting as a trajectory generation layer with a model-based trajectory adjustment and trajectory control layers. The DNN produces centroidal and postural references serving as an initial guess and regularizer for the other layers. Being the DNN trained on human motion capture data, the resulting robot motion exhibits locomotion patterns, resembling a human walking style. The trajectory adjustment layer utilize...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents a three-layered architecture that enables stylistic locomotion with online contact location adjustment</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.07849v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.07849v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Romualdi et al., &quot;Online DNN-driven Nonlinear MPC for Stylistic Humanoid Robot Walking with Step Adjustment,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.07849v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399348608')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="learning robust, agile, natural legged locomotion skills in the wild" data-keywords="reinforcement learning robot locomotion perception control simulation imu cs.ro cs.ai" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2304.10888v3" target="_blank">Learning Robust, Agile, Natural Legged Locomotion Skills in the Wild</a>
                            </h3>
                            <p class="card-authors">Yikai Wang, Zheyuan Jiang, Jianyu Chen</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Perception</span></div>
                            <div class="card-details" id="cat-details-4399338000">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recently, reinforcement learning has become a promising and polular solution for robot legged locomotion. Compared to model-based control, reinforcement learning based controllers can achieve better robustness against uncertainties of environments through sim-to-real learning. However, the corresponding learned gaits are in general overly conservative and unatural. In this paper, we propose a new framework for learning robust, agile and natural legged locomotion skills over challenging terrain. We incorporate an adversarial training branch based on real animal locomotion data upon a teacher-st...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose a new framework for learning robust, agile and natural legged locomotion skills over challenging terrain</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2304.10888v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2304.10888v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Wang, Z. Jiang, and J. Chen, &quot;Learning Robust, Agile, Natural Legged Locomotion Skills in the Wild,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2304.10888v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399338000')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="rl-augmented mpc framework for agile and robust bipedal footstep locomotion planning and control" data-keywords="reinforcement learning robot humanoid bipedal locomotion planning control cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.17683v1" target="_blank">RL-augmented MPC Framework for Agile and Robust Bipedal Footstep Locomotion Planning and Control</a>
                            </h3>
                            <p class="card-authors">Seung Hyeon Bang, Carlos Arribalzaga Jov√©, Luis Sentis</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span></div>
                            <div class="card-details" id="cat-details-4399348848">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper proposes an online bipedal footstep planning strategy that combines model predictive control (MPC) and reinforcement learning (RL) to achieve agile and robust bipedal maneuvers. While MPC-based foot placement controllers have demonstrated their effectiveness in achieving dynamic locomotion, their performance is often limited by the use of simplified models and assumptions. To address this challenge, we develop a novel foot placement controller that leverages a learned policy to bridge the gap between the use of a simplified model and the more complex full-order robot system. Specifi...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address this challenge, we develop a novel foot placement controller that leverages a learned policy to bridge the gap between the use of a simplified model and the more complex full-order robot system</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.17683v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.17683v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. H. Bang, C. A. Jov√©, and L. Sentis, &quot;RL-augmented MPC Framework for Agile and Robust Bipedal Footstep Locomotion Planning and Control,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.17683v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399348848')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="the duke humanoid: design and control for energy efficient bipedal locomotion using passive dynamics" data-keywords="reinforcement learning robot humanoid locomotion simulation imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.19795v2" target="_blank">The Duke Humanoid: Design and Control For Energy Efficient Bipedal Locomotion Using Passive Dynamics</a>
                            </h3>
                            <p class="card-authors">Boxi Xia, Bokuan Li, Jacob Lee et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399335696">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We present the Duke Humanoid, an open-source 10-degrees-of-freedom humanoid, as an extensible platform for locomotion research. The design mimics human physiology, with symmetrical body alignment in the frontal plane to maintain static balance with straight knees. We develop a reinforcement learning policy that can be deployed zero-shot on the hardware for velocity-tracking walking tasks. Additionally, to enhance energy efficiency in locomotion, we propose an end-to-end reinforcement learning algorithm that encourages the robot to leverage passive dynamics. Our experimental results show that o...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present the Duke Humanoid, an open-source 10-degrees-of-freedom humanoid, as an extensible platform for locomotion research</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.19795v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.19795v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. Xia, B. Li, J. Lee, M. Scutari, and B. Chen, &quot;The Duke Humanoid: Design and Control For Energy Efficient Bipedal Locomotion Using Passive Dynamics,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.19795v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399335696')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="harmon: whole-body motion generation of humanoid robots from language descriptions" data-keywords="robot humanoid imu language model cs.ro cs.ai" data-themes="S I E L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.12773v1" target="_blank">Harmon: Whole-Body Motion Generation of Humanoid Robots from Language Descriptions</a>
                            </h3>
                            <p class="card-authors">Zhenyu Jiang, Yuqi Xie, Jinhan Li et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Imu</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4399349520">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots, with their human-like embodiment, have the potential to integrate seamlessly into human environments. Critical to their coexistence and cooperation with humans is the ability to understand natural language communications and exhibit human-like behaviors. This work focuses on generating diverse whole-body motions for humanoid robots from language descriptions. We leverage human motion priors from extensive human motion datasets to initialize humanoid motions and employ the commonsense reasoning capabilities of Vision Language Models (VLMs) to edit and refine these motions. Our ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Our approach demonstrates the capability to produce natural, expressive, and text-aligned humanoid motions, validated through both simulated and real-world experiments</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.12773v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.12773v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Jiang, Y. Xie, J. Li, Y. Yuan, Y. Zhu, and Y. Zhu, &quot;Harmon: Whole-Body Motion Generation of Humanoid Robots from Language Descriptions,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.12773v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399349520')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="signal temporal logic-guided model predictive control for robust bipedal locomotion resilient to runtime external perturbations" data-keywords="robot bipedal locomotion planning control optimization simulation ros imu cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2310.11290v1" target="_blank">Signal Temporal Logic-Guided Model Predictive Control for Robust Bipedal Locomotion Resilient to Runtime External Perturbations</a>
                            </h3>
                            <p class="card-authors">Zhaoyuan Gu, Rongming Guo, William Yates et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Planning</span></div>
                            <div class="card-details" id="cat-details-4399348272">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This study investigates formal-method-based trajectory optimization (TO) for bipedal locomotion, focusing on scenarios where the robot encounters external perturbations at unforeseen times. Our key research question centers around the assurance of task specification correctness and the maximization of specification robustness for a bipedal robot in the presence of external perturbations.
  Our contribution includes the design of an optimization-based task and motion planning framework that generates optimal control sequences with formal guarantees of external perturbation recovery. As a core c...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Our key research question centers around the assurance of task specification correctness and the maximization of specification robustness for a bipedal robot in the presence of external perturbations.
  Our contribution includes the design of an optimization-based task and motion planning framework that generates optimal control sequences with formal guarantees of external perturbation recovery</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2310.11290v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2310.11290v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Gu, R. Guo, W. Yates, Y. Chen, and Y. Zhao, &quot;Signal Temporal Logic-Guided Model Predictive Control for Robust Bipedal Locomotion Resilient to Runtime External Perturbations,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2310.11290v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399348272')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="learning humanoid locomotion with world model reconstruction" data-keywords="robot humanoid locomotion control ros cs.ro cs.lg" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.16230v1" target="_blank">Learning Humanoid Locomotion with World Model Reconstruction</a>
                            </h3>
                            <p class="card-authors">Wandong Sun, Long Chen, Yongbo Su et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Control</span></div>
                            <div class="card-details" id="cat-details-4399335984">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots are designed to navigate environments accessible to humans using their legs. However, classical research has primarily focused on controlled laboratory settings, resulting in a gap in developing controllers for navigating complex real-world terrains. This challenge mainly arises from the limitations and noise in sensor data, which hinder the robot&#x27;s understanding of itself and the environment. In this study, we introduce World Model Reconstruction (WMR), an end-to-end learning-based approach for blind humanoid locomotion across challenging terrains. We propose training an estim...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this study, we introduce World Model Reconstruction (WMR), an end-to-end learning-based approach for blind humanoid locomotion across challenging terrains</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.16230v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.16230v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. Sun, L. Chen, Y. Su, B. Cao, Y. Liu, and Z. Xie, &quot;Learning Humanoid Locomotion with World Model Reconstruction,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.16230v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399335984')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="incremental learning of humanoid robot behavior from natural interaction and large language models" data-keywords="robot humanoid perception simulation imu language model cs.ro cs.ai" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.04316v3" target="_blank">Incremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models</a>
                            </h3>
                            <p class="card-authors">Leonard B√§rmann, Rainer Kartmann, Fabian Peller-Konrad et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Perception</span><span class="keyword-tag">Simulation</span></div>
                            <div class="card-details" id="cat-details-4399348800">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Natural-language dialog is key for intuitive human-robot interaction. It can be used not only to express humans&#x27; intents, but also to communicate instructions for improvement if a robot does not understand a command correctly. Of great importance is to endow robots with the ability to learn from such interaction experience in an incremental way to allow them to improve their behaviors or avoid mistakes in the future. In this paper, we propose a system to achieve incremental learning of complex behavior from natural interaction, and demonstrate its implementation on a humanoid robot. Building o...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose a system to achieve incremental learning of complex behavior from natural interaction, and demonstrate its implementation on a humanoid robot</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.04316v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.04316v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. B√§rmann, R. Kartmann, F. Peller-Konrad, J. Niehues, A. Waibel, and T. Asfour, &quot;Incremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.04316v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399348800')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2026" data-category="cs.ro" data-title="walk the planc: physics-guided rl for agile humanoid locomotion on constrained footholds" data-keywords="reinforcement learning robot humanoid bipedal locomotion perception planning control optimization cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-new">New</span></div>
                            <span class="card-source">arXiv ¬∑ 2026</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2601.06286v1" target="_blank">Walk the PLANC: Physics-Guided RL for Agile Humanoid Locomotion on Constrained Footholds</a>
                            </h3>
                            <p class="card-authors">Min Dai, William D. Compton, Junheng Li et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span></div>
                            <div class="card-details" id="cat-details-4399347744">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Bipedal humanoid robots must precisely coordinate balance, timing, and contact decisions when locomoting on constrained footholds such as stepping stones, beams, and planks -- even minor errors can lead to catastrophic failure. Classical optimization and control pipelines handle these constraints well but depend on highly accurate mathematical representations of terrain geometry, making them prone to error when perception is noisy or incomplete. Meanwhile, reinforcement learning has shown strong resilience to disturbances and modeling errors, yet end-to-end policies rarely discover the precise...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we introduce a locomotion framework in which a reduced-order stepping planner supplies dynamically consistent motion targets that steer the RL training process via Control Lyapunov Function (CLF) rewards</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2601.06286v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2601.06286v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Dai, W. D. Compton, J. Li, L. Yang, and A. D. Ames, &quot;Walk the PLANC: Physics-Guided RL for Agile Humanoid Locomotion on Constrained Footholds,&quot; arXiv, 2026. [Online]. Available: http://arxiv.org/abs/2601.06286v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399347744')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="proprioceptive external torque learning for floating base robot and its applications to humanoid locomotion" data-keywords="robot humanoid locomotion control imu cs.ro cs.ai" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.04138v1" target="_blank">Proprioceptive External Torque Learning for Floating Base Robot and its Applications to Humanoid Locomotion</a>
                            </h3>
                            <p class="card-authors">Daegyu Lim, Myeong-Ju Kim, Junhyeok Cha et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Control</span></div>
                            <div class="card-details" id="cat-details-4399349712">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The estimation of external joint torque and contact wrench is essential for achieving stable locomotion of humanoids and safety-oriented robots. Although the contact wrench on the foot of humanoids can be measured using a force-torque sensor (FTS), FTS increases the cost, inertia, complexity, and failure possibility of the system. This paper introduces a method for learning external joint torque solely using proprioceptive sensors (encoders and IMUs) for a floating base robot. For learning, the GRU network is used and random walking data is collected. Real robot experiments demonstrate that th...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.04138v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.04138v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Lim, M. Kim, J. Cha, D. Kim, and J. Park, &quot;Proprioceptive External Torque Learning for Floating Base Robot and its Applications to Humanoid Locomotion,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.04138v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399349712')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="a hierarchical framework for humanoid locomotion with supernumerary limbs" data-keywords="robot humanoid locomotion control simulation imu imitation learning cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2512.00077v1" target="_blank">A Hierarchical Framework for Humanoid Locomotion with Supernumerary Limbs</a>
                            </h3>
                            <p class="card-authors">Bowen Zhi</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Control</span></div>
                            <div class="card-details" id="cat-details-4399349472">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The integration of Supernumerary Limbs (SLs) on humanoid robots poses a significant stability challenge due to the dynamic perturbations they introduce. This thesis addresses this issue by designing a novel hierarchical control architecture to improve humanoid locomotion stability with SLs. The core of this framework is a decoupled strategy that combines learning-based locomotion with model-based balancing. The low-level component consists of a walking gait for a Unitree H1 humanoid through imitation learning and curriculum learning. The high-level component actively utilizes the SLs for dynam...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2512.00077v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2512.00077v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. Zhi, &quot;A Hierarchical Framework for Humanoid Locomotion with Supernumerary Limbs,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2512.00077v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399349472')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="robust-locomotion-by-logic: perturbation-resilient bipedal locomotion via signal temporal logic guided model predictive control" data-keywords="robot bipedal locomotion planning control optimization simulation ros imu cs.ro" data-themes="I E L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.15993v2" target="_blank">Robust-Locomotion-by-Logic: Perturbation-Resilient Bipedal Locomotion via Signal Temporal Logic Guided Model Predictive Control</a>
                            </h3>
                            <p class="card-authors">Zhaoyuan Gu, Yuntian Zhao, Yipu Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Planning</span></div>
                            <div class="card-details" id="cat-details-4399348512">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This study introduces a robust planning framework that utilizes a model predictive control (MPC) approach, enhanced by incorporating signal temporal logic (STL) specifications. This marks the first-ever study to apply STL-guided trajectory optimization for bipedal locomotion, specifically designed to handle both translational and orientational perturbations. Existing recovery strategies often struggle with reasoning complex task logic and evaluating locomotion robustness systematically, making them susceptible to failures caused by inappropriate recovery strategies or lack of robustness. To ad...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address these issues, we design an analytical stability metric for bipedal locomotion and quantify this metric using STL specifications, which guide the generation of recovery trajectories to achieve maximum robustness degree</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.15993v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.15993v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Gu et al., &quot;Robust-Locomotion-by-Logic: Perturbation-Resilient Bipedal Locomotion via Signal Temporal Logic Guided Model Predictive Control,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.15993v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399348512')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="humanoidbench: simulated humanoid benchmark for whole-body locomotion and manipulation" data-keywords="reinforcement learning robot humanoid locomotion manipulation imu cs.ro cs.ai cs.lg" data-themes="I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.10506v2" target="_blank">HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation</a>
                            </h3>
                            <p class="card-authors">Carmelo Sferrazza, Dun-Ming Huang, Xingyu Lin et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399349376">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots hold great promise in assisting humans in diverse environments and tasks, due to their flexibility and adaptability leveraging human-like morphology. However, research in humanoid robots is often bottlenecked by the costly and fragile hardware setups. To accelerate algorithmic research in humanoid robots, we present a high-dimensional, simulated robot learning benchmark, HumanoidBench, featuring a humanoid robot equipped with dexterous hands and a variety of challenging whole-body manipulation and locomotion tasks. Our findings reveal that state-of-the-art reinforcement learnin...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To accelerate algorithmic research in humanoid robots, we present a high-dimensional, simulated robot learning benchmark, HumanoidBench, featuring a humanoid robot equipped with dexterous hands and a variety of challenging whole-body manipulation and locomotion tasks</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.10506v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.10506v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Sferrazza, D. Huang, X. Lin, Y. Lee, and P. Abbeel, &quot;HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.10506v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399349376')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="guiding collision-free humanoid multi-contact locomotion using convex kinematic relaxations and dynamic optimization" data-keywords="robot humanoid navigation planning optimization simulation imu cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.08335v1" target="_blank">Guiding Collision-Free Humanoid Multi-Contact Locomotion using Convex Kinematic Relaxations and Dynamic Optimization</a>
                            </h3>
                            <p class="card-authors">Carlos Gonzalez, Luis Sentis</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Navigation</span><span class="keyword-tag">Planning</span></div>
                            <div class="card-details" id="cat-details-4399348176">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots rely on multi-contact planners to navigate a diverse set of environments, including those that are unstructured and highly constrained. To synthesize stable multi-contact plans within a reasonable time frame, most planners assume statically stable motions or rely on reduced order models. However, these approaches can also render the problem infeasible in the presence of large obstacles or when operating near kinematic and dynamic limits. To that end, we propose a new multi-contact framework that leverages recent advancements in relaxing collision-free path planning into a conve...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To that end, we propose a new multi-contact framework that leverages recent advancements in relaxing collision-free path planning into a convex optimization problem, extending it to be applicable to humanoid multi-contact navigation</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.08335v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.08335v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Gonzalez, and L. Sentis, &quot;Guiding Collision-Free Humanoid Multi-Contact Locomotion using Convex Kinematic Relaxations and Dynamic Optimization,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.08335v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399348176')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="descriptive model-based learning and control for bipedal locomotion" data-keywords="robot bipedal locomotion planning control imu cs.ro eess.sy" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2511.00512v1" target="_blank">Descriptive Model-based Learning and Control for Bipedal Locomotion</a>
                            </h3>
                            <p class="card-authors">Suraj Kumar, Andy Ruina</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Planning</span></div>
                            <div class="card-details" id="cat-details-4399337376">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Bipedal balance is challenging due to its multi-phase, hybrid nature and high-dimensional state space. Traditional balance control approaches for bipedal robots rely on low-dimensional models for locomotion planning and reactive control, constraining the full robot to behave like these simplified models. This involves tracking preset reference paths for the Center of Mass and upper body obtained through low-dimensional models, often resulting in inefficient walking patterns with bent knees. However, we observe that bipedal balance is inherently low-dimensional and can be effectively described ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose a novel control approach that avoids prescribing a low-dimensional model to the full model</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2511.00512v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2511.00512v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Kumar, and A. Ruina, &quot;Descriptive Model-based Learning and Control for Bipedal Locomotion,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2511.00512v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399337376')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="quadratic programming optimization for bio-inspired thruster-assisted bipedal locomotion on inclined slopes" data-keywords="robot bipedal locomotion manipulation control optimization simulation imu cs.ro" data-themes="I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2411.12968v1" target="_blank">Quadratic Programming Optimization for Bio-Inspired Thruster-Assisted Bipedal Locomotion on Inclined Slopes</a>
                            </h3>
                            <p class="card-authors">Shreyansh Pitroda, Eric Sihite, Kaushik Venkatesh Krishnamurthy et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Manipulation</span></div>
                            <div class="card-details" id="cat-details-4399349328">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Our work aims to make significant strides in understanding unexplored locomotion control paradigms based on the integration of posture manipulation and thrust vectoring. These techniques are commonly seen in nature, such as Chukar birds using their wings to run on a nearly vertical wall. In this work, we show quadratic programming with contact constraints which is then given to the whole body controller to map on robot states to produce a thruster-assisted slope walking controller for our state-of-the-art Harpy platform. Harpy is a bipedal robot capable of legged-aerial locomotion using its le...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2411.12968v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2411.12968v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Pitroda et al., &quot;Quadratic Programming Optimization for Bio-Inspired Thruster-Assisted Bipedal Locomotion on Inclined Slopes,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2411.12968v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399349328')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="preference-conditioned multi-objective rl for integrated command tracking and force compliance in humanoid locomotion" data-keywords="robot humanoid locomotion navigation optimization simulation imu cs.ro" data-themes="S I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2510.10851v1" target="_blank">Preference-Conditioned Multi-Objective RL for Integrated Command Tracking and Force Compliance in Humanoid Locomotion</a>
                            </h3>
                            <p class="card-authors">Tingxuan Leng, Yushi Wang, Tinglong Zheng et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Navigation</span></div>
                            <div class="card-details" id="cat-details-4399349568">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid locomotion requires not only accurate command tracking for navigation but also compliant responses to external forces during human interaction. Despite significant progress, existing RL approaches mainly emphasize robustness, yielding policies that resist external forces but lack compliance-particularly challenging for inherently unstable humanoids. In this work, we address this by formulating humanoid locomotion as a multi-objective optimization problem that balances command tracking and external force compliance. We introduce a preference-conditioned multi-objective RL (MORL) framew...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce a preference-conditioned multi-objective RL (MORL) framework that integrates rigid command following and compliant behaviors within a single omnidirectional locomotion policy</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2510.10851v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2510.10851v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. Leng, Y. Wang, T. Zheng, C. Luo, and M. Zhao, &quot;Preference-Conditioned Multi-Objective RL for Integrated Command Tracking and Force Compliance in Humanoid Locomotion,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2510.10851v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399349568')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="learning velocity-based humanoid locomotion: massively parallel learning with brax and mjx" data-keywords="reinforcement learning robot humanoid locomotion simulation imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.05148v1" target="_blank">Learning Velocity-based Humanoid Locomotion: Massively Parallel Learning with Brax and MJX</a>
                            </h3>
                            <p class="card-authors">William Thibault, William Melek, Katja Mombaur</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399348896">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid locomotion is a key skill to bring humanoids out of the lab and into the real-world. Many motion generation methods for locomotion have been proposed including reinforcement learning (RL). RL locomotion policies offer great versatility and generalizability along with the ability to experience new knowledge to improve over time. This work presents a velocity-based RL locomotion policy for the REEM-C robot. The policy uses a periodic reward formulation and is implemented in Brax/MJX for fast training. Simulation results for the policy are demonstrated with future experimental results in...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.05148v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.05148v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. Thibault, W. Melek, and K. Mombaur, &quot;Learning Velocity-based Humanoid Locomotion: Massively Parallel Learning with Brax and MJX,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.05148v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399348896')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="learning perceptive bipedal locomotion over irregular terrain" data-keywords="attention bipedal locomotion control cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2304.07236v1" target="_blank">Learning Perceptive Bipedal Locomotion over Irregular Terrain</a>
                            </h3>
                            <p class="card-authors">Bart van Marum, Matthia Sabatelli, Hamidreza Kasaei</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Control</span></div>
                            <div class="card-details" id="cat-details-4399336512">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In this paper we propose a novel bipedal locomotion controller that uses noisy exteroception to traverse a wide variety of terrains. Building on the cutting-edge advancements in attention based belief encoding for quadrupedal locomotion, our work extends these methods to the bipedal domain, resulting in a robust and reliable internal belief of the terrain ahead despite noisy sensor inputs. Additionally, we present a reward function that allows the controller to successfully traverse irregular terrain. We compare our method with a proprioceptive baseline and show that our method is able to trav...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper we propose a novel bipedal locomotion controller that uses noisy exteroception to traverse a wide variety of terrains</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2304.07236v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2304.07236v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. v. Marum, M. Sabatelli, and H. Kasaei, &quot;Learning Perceptive Bipedal Locomotion over Irregular Terrain,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2304.07236v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399336512')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="inekformer: a hybrid state estimator for humanoid robots" data-keywords="deep learning transformer robot humanoid bipedal locomotion control cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2511.16306v1" target="_blank">InEKFormer: A Hybrid State Estimator for Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Lasse Hohmeyer, Mihaela Popescu, Ivan Bergonzani et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Deep Learning</span><span class="keyword-tag">Transformer</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span></div>
                            <div class="card-details" id="cat-details-4399336128">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots have great potential for a wide range of applications, including industrial and domestic use, healthcare, and search and rescue missions. However, bipedal locomotion in different environments is still a challenge when it comes to performing stable and dynamic movements. This is where state estimation plays a crucial role, providing fast and accurate feedback of the robot&#x27;s floating base state to the motion controller. Although classical state estimation methods such as Kalman filters are widely used in robotics, they require expert knowledge to fine-tune the noise parameters. D...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose the InEKFormer, a novel hybrid state estimation method that incorporates an invariant extended Kalman filter (InEKF) and a Transformer network</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2511.16306v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2511.16306v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Hohmeyer, M. Popescu, I. Bergonzani, D. Mronga, and F. Kirchner, &quot;InEKFormer: A Hybrid State Estimator for Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2511.16306v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399336128')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="advantages of multimodal versus verbal-only robot-to-human communication with an anthropomorphic robotic mock driver" data-keywords="attention robot humanoid cs.ro" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2307.00841v1" target="_blank">Advantages of Multimodal versus Verbal-Only Robot-to-Human Communication with an Anthropomorphic Robotic Mock Driver</a>
                            </h3>
                            <p class="card-authors">Tim Schreiter, Lucas Morillo-Mendez, Ravi T. Chadalavada et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">CS.RO</span></div>
                            <div class="card-details" id="cat-details-4399347792">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Robots are increasingly used in shared environments with humans, making effective communication a necessity for successful human-robot interaction. In our work, we study a crucial component: active communication of robot intent. Here, we present an anthropomorphic solution where a humanoid robot communicates the intent of its host robot acting as an &quot;Anthropomorphic Robotic Mock Driver&quot; (ARMoD). We evaluate this approach in two experiments in which participants work alongside a mobile robot on various tasks, while the ARMoD communicates a need for human attention, when required, or gives instr...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Here, we present an anthropomorphic solution where a humanoid robot communicates the intent of its host robot acting as an &quot;Anthropomorphic Robotic Mock Driver&quot; (ARMoD)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2307.00841v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2307.00841v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. Schreiter et al., &quot;Advantages of Multimodal versus Verbal-Only Robot-to-Human Communication with an Anthropomorphic Robotic Mock Driver,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2307.00841v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399347792')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="h-zero: cross-humanoid locomotion pretraining enables few-shot novel embodiment transfer" data-keywords="robot humanoid locomotion control simulation ros imu cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2512.00971v1" target="_blank">H-Zero: Cross-Humanoid Locomotion Pretraining Enables Few-shot Novel Embodiment Transfer</a>
                            </h3>
                            <p class="card-authors">Yunfeng Lin, Minghuan Liu, Yufei Xue et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Control</span></div>
                            <div class="card-details" id="cat-details-4399348320">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The rapid advancement of humanoid robotics has intensified the need for robust and adaptable controllers to enable stable and efficient locomotion across diverse platforms. However, developing such controllers remains a significant challenge because existing solutions are tailored to specific robot designs, requiring extensive tuning of reward functions, physical parameters, and training hyperparameters for each embodiment. To address this challenge, we introduce H-Zero, a cross-humanoid locomotion pretraining pipeline that learns a generalizable humanoid base policy. We show that pretraining ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address this challenge, we introduce H-Zero, a cross-humanoid locomotion pretraining pipeline that learns a generalizable humanoid base policy</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2512.00971v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2512.00971v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Lin et al., &quot;H-Zero: Cross-Humanoid Locomotion Pretraining Enables Few-shot Novel Embodiment Transfer,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2512.00971v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399348320')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="learning to walk in costume: adversarial motion priors for aesthetically constrained humanoids" data-keywords="reinforcement learning robot humanoid locomotion cs.ro cs.ai eess.sy" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.05581v1" target="_blank">Learning to Walk in Costume: Adversarial Motion Priors for Aesthetically Constrained Humanoids</a>
                            </h3>
                            <p class="card-authors">Arturo Flores Alvarez, Fatemeh Zargarbashi, Havel Liu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399349280">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We present a Reinforcement Learning (RL)-based locomotion system for Cosmo, a custom-built humanoid robot designed for entertainment applications. Unlike traditional humanoids, entertainment robots present unique challenges due to aesthetic-driven design choices. Cosmo embodies these with a disproportionately large head (16% of total mass), limited sensing, and protective shells that considerably restrict movement. To address these challenges, we apply Adversarial Motion Priors (AMP) to enable the robot to learn natural-looking movements while maintaining physical stability. We develop tailore...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present a Reinforcement Learning (RL)-based locomotion system for Cosmo, a custom-built humanoid robot designed for entertainment applications</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.05581v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.05581v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. F. Alvarez et al., &quot;Learning to Walk in Costume: Adversarial Motion Priors for Aesthetically Constrained Humanoids,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.05581v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399349280')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="data-driven latent space representation for robust bipedal locomotion learning" data-keywords="reinforcement learning bipedal locomotion planning simulation imu cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.15740v1" target="_blank">Data-Driven Latent Space Representation for Robust Bipedal Locomotion Learning</a>
                            </h3>
                            <p class="card-authors">Guillermo A. Castillo, Bowen Weng, Wei Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Planning</span></div>
                            <div class="card-details" id="cat-details-4399348368">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents a novel framework for learning robust bipedal walking by combining a data-driven state representation with a Reinforcement Learning (RL) based locomotion policy. The framework utilizes an autoencoder to learn a low-dimensional latent space that captures the complex dynamics of bipedal locomotion from existing locomotion data. This reduced dimensional state representation is then used as states for training a robust RL-based gait policy, eliminating the need for heuristic state selections or the use of template models for gait planning. The results demonstrate that the learn...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents a novel framework for learning robust bipedal walking by combining a data-driven state representation with a Reinforcement Learning (RL) based locomotion policy</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.15740v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.15740v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. A. Castillo, B. Weng, W. Zhang, and A. Hereid, &quot;Data-Driven Latent Space Representation for Robust Bipedal Locomotion Learning,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.15740v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399348368')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="bipedal safe navigation over uncertain rough terrain: unifying terrain mapping and locomotion stability" data-keywords="robot bipedal locomotion navigation planning simulation mujoco imu cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.16356v2" target="_blank">Bipedal Safe Navigation over Uncertain Rough Terrain: Unifying Terrain Mapping and Locomotion Stability</a>
                            </h3>
                            <p class="card-authors">Kasidit Muenprasitivej, Jesse Jiang, Abdulaziz Shamsah et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Navigation</span></div>
                            <div class="card-details" id="cat-details-4399335024">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We study the problem of bipedal robot navigation in complex environments with uncertain and rough terrain. In particular, we consider a scenario in which the robot is expected to reach a desired goal location by traversing an environment with uncertain terrain elevation. Such terrain uncertainties induce not only untraversable regions but also robot motion perturbations. Thus, the problems of terrain mapping and locomotion stability are intertwined. We evaluate three different kernels for Gaussian process (GP) regression to learn the terrain elevation. We also learn the motion deviation result...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a hierarchical locomotion-dynamics-aware sampling-based navigation planner</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.16356v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.16356v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Muenprasitivej, J. Jiang, A. Shamsah, S. Coogan, and Y. Zhao, &quot;Bipedal Safe Navigation over Uncertain Rough Terrain: Unifying Terrain Mapping and Locomotion Stability,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.16356v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399335024')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="beamdojo: learning agile humanoid locomotion on sparse footholds" data-keywords="reinforcement learning robot humanoid locomotion simulation lidar imu cs.ro cs.ai cs.lg" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.10363v3" target="_blank">BeamDojo: Learning Agile Humanoid Locomotion on Sparse Footholds</a>
                            </h3>
                            <p class="card-authors">Huayi Wang, Zirui Wang, Junli Ren et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399349088">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Traversing risky terrains with sparse footholds poses a significant challenge for humanoid robots, requiring precise foot placements and stable locomotion. Existing learning-based approaches often struggle on such complex terrains due to sparse foothold rewards and inefficient learning processes. To address these challenges, we introduce BeamDojo, a reinforcement learning (RL) framework designed for enabling agile humanoid locomotion on sparse footholds. BeamDojo begins by introducing a sampling-based foothold reward tailored for polygonal feet, along with a double critic to balancing the lear...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address these challenges, we introduce BeamDojo, a reinforcement learning (RL) framework designed for enabling agile humanoid locomotion on sparse footholds</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.10363v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.10363v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Wang et al., &quot;BeamDojo: Learning Agile Humanoid Locomotion on Sparse Footholds,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.10363v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399349088')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="humanoid whole-body badminton via multi-stage reinforcement learning" data-keywords="robot humanoid locomotion manipulation control simulation ros imu cs.ro" data-themes="S I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2511.11218v2" target="_blank">Humanoid Whole-Body Badminton via Multi-Stage Reinforcement Learning</a>
                            </h3>
                            <p class="card-authors">Chenhao Liu, Leyun Jiang, Yibo Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Manipulation</span></div>
                            <div class="card-details" id="cat-details-4399348224">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots have demonstrated strong capabilities for interacting with static scenes across locomotion, manipulation, and more challenging loco-manipulation tasks. Yet the real world is dynamic, and quasi-static interactions are insufficient to cope with diverse environmental conditions. As a step toward more dynamic interaction scenarios, we present a reinforcement-learning-based training pipeline that produces a unified whole-body controller for humanoid badminton, enabling coordinated lower-body footwork and upper-body striking without motion priors or expert demonstrations. Training fo...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">As a step toward more dynamic interaction scenarios, we present a reinforcement-learning-based training pipeline that produces a unified whole-body controller for humanoid badminton, enabling coordinated lower-body footwork and upper-body striking without motion priors or expert demonstrations</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2511.11218v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2511.11218v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Liu, L. Jiang, Y. Wang, K. Yao, J. Fu, and X. Ren, &quot;Humanoid Whole-Body Badminton via Multi-Stage Reinforcement Learning,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2511.11218v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399348224')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="learning generic and dynamic locomotion of humanoids across discrete terrains" data-keywords="reinforcement learning neural network robot humanoid locomotion control optimization simulation imu cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.17227v2" target="_blank">Learning Generic and Dynamic Locomotion of Humanoids Across Discrete Terrains</a>
                            </h3>
                            <p class="card-authors">Shangqun Yu, Nisal Perera, Daniel Marew et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span></div>
                            <div class="card-details" id="cat-details-4399336560">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper addresses the challenge of terrain-adaptive dynamic locomotion in humanoid robots, a problem traditionally tackled by optimization-based methods or reinforcement learning (RL). Optimization-based methods, such as model-predictive control, excel in finding optimal reaction forces and achieving agile locomotion, especially in quadruped, but struggle with the nonlinear hybrid dynamics of legged systems and the real-time computation of step location, timing, and reaction forces. Conversely, RL-based methods show promise in navigating dynamic and rough terrains but are limited by their e...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce a novel locomotion architecture that integrates a neural network policy, trained through RL in simplified environments, with a state-of-the-art motion controller combining model-predictive control (MPC) and whole-body impulse control (WBIC)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.17227v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.17227v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Yu, N. Perera, D. Marew, and D. Kim, &quot;Learning Generic and Dynamic Locomotion of Humanoids Across Discrete Terrains,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.17227v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399336560')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="towards embodiment scaling laws in robot locomotion" data-keywords="robot locomotion control simulation ros imu cs.ro cs.ai cs.lg" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2505.05753v2" target="_blank">Towards Embodiment Scaling Laws in Robot Locomotion</a>
                            </h3>
                            <p class="card-authors">Bo Ai, Liu Dai, Nico Bohlinger et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Control</span><span class="keyword-tag">Simulation</span></div>
                            <div class="card-details" id="cat-details-4399335072">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Cross-embodiment generalization underpins the vision of building generalist embodied agents for any robot, yet its enabling factors remain poorly understood. We investigate embodiment scaling laws, the hypothesis that increasing the number of training embodiments improves generalization to unseen ones, using robot locomotion as a test bed. We procedurally generate ~1,000 embodiments with topological, geometric, and joint-level kinematic variations, and train policies on random subsets. We observe positive scaling trends supporting the hypothesis, and find that embodiment scaling enables substa...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2505.05753v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2505.05753v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. Ai et al., &quot;Towards Embodiment Scaling Laws in Robot Locomotion,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2505.05753v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399335072')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="bracing for impact: robust humanoid push recovery and locomotion with reduced order models" data-keywords="robot humanoid locomotion control simulation imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2505.11495v1" target="_blank">Bracing for Impact: Robust Humanoid Push Recovery and Locomotion with Reduced Order Models</a>
                            </h3>
                            <p class="card-authors">Lizhi Yang, Blake Werner, Adrian B. Ghansah et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Control</span></div>
                            <div class="card-details" id="cat-details-4399336704">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Push recovery during locomotion will facilitate the deployment of humanoid robots in human-centered environments. In this paper, we present a unified framework for walking control and push recovery for humanoid robots, leveraging the arms for push recovery while dynamically walking. The key innovation is to use the environment, such as walls, to facilitate push recovery by combining Single Rigid Body model predictive control (SRB-MPC) with Hybrid Linear Inverted Pendulum (HLIP) dynamics to enable robust locomotion, push detection, and recovery by utilizing the robot&#x27;s arms to brace against suc...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we present a unified framework for walking control and push recovery for humanoid robots, leveraging the arms for push recovery while dynamically walking</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2505.11495v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2505.11495v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Yang, B. Werner, A. B. Ghansah, and A. D. Ames, &quot;Bracing for Impact: Robust Humanoid Push Recovery and Locomotion with Reduced Order Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2505.11495v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399336704')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="synthetic data pipelines for adaptive, mission-ready militarized humanoids" data-keywords="humanoid navigation perception imu cs.ro" data-themes="S">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2512.14411v1" target="_blank">Synthetic Data Pipelines for Adaptive, Mission-Ready Militarized Humanoids</a>
                            </h3>
                            <p class="card-authors">Mohammed Ayman Habib, Aldo Petruzzelli</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Navigation</span><span class="keyword-tag">Perception</span><span class="keyword-tag">Imu</span></div>
                            <div class="card-details" id="cat-details-4399349040">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Omnia presents a synthetic data driven pipeline to accelerate the training, validation, and deployment readiness of militarized humanoids. The approach converts first-person spatial observations captured from point-of-view recordings, smart glasses, augmented reality headsets, and spatial browsing workflows into scalable, mission-specific synthetic datasets for humanoid autonomy. By generating large volumes of high-fidelity simulated scenarios and pairing them with automated labeling and model training, the pipeline enables rapid iteration on perception, navigation, and decision-making capabil...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2512.14411v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2512.14411v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. A. Habib, and A. Petruzzelli, &quot;Synthetic Data Pipelines for Adaptive, Mission-Ready Militarized Humanoids,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2512.14411v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399349040')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="extended hybrid zero dynamics for bipedal walking of the knee-less robot slider" data-keywords="reinforcement learning robot humanoid bipedal locomotion control cs.ro eess.sy" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.01165v2" target="_blank">Extended Hybrid Zero Dynamics for Bipedal Walking of the Knee-less Robot SLIDER</a>
                            </h3>
                            <p class="card-authors">Rui Zong, Martin Liang, Yuntian Fang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span></div>
                            <div class="card-details" id="cat-details-4399336800">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Knee-less bipedal robots like SLIDER have the advantage of ultra-lightweight legs and improved walking energy efficiency compared to traditional humanoid robots. In this paper, we firstly introduce an improved hardware design of the SLIDER bipedal robot with new line-feet and more optimized mass distribution that enables higher locomotion speeds. Secondly, we propose an extended Hybrid Zero Dynamics (eHZD) method, which can be applied to prismatic joint robots like SLIDER. The eHZD method is then used to generate a library of gaits with varying reference velocities in an offline way. Thirdly, ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Secondly, we propose an extended Hybrid Zero Dynamics (eHZD) method, which can be applied to prismatic joint robots like SLIDER</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.01165v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.01165v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Zong et al., &quot;Extended Hybrid Zero Dynamics for Bipedal Walking of the Knee-less Robot SLIDER,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.01165v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399336800')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="flam: foundation model-based body stabilization for humanoid locomotion and manipulation" data-keywords="reinforcement learning attention robot humanoid locomotion manipulation control cs.ro cs.lg" data-themes="E I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.22249v1" target="_blank">FLAM: Foundation Model-Based Body Stabilization for Humanoid Locomotion and Manipulation</a>
                            </h3>
                            <p class="card-authors">Xianqi Zhang, Hongliang Wei, Wenrui Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Attention</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span></div>
                            <div class="card-details" id="cat-details-4399336272">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots have attracted significant attention in recent years. Reinforcement Learning (RL) is one of the main ways to control the whole body of humanoid robots. RL enables agents to complete tasks by learning from environment interactions, guided by task rewards. However, existing RL methods rarely explicitly consider the impact of body stability on humanoid locomotion and manipulation. Achieving high performance in whole-body control remains a challenge for RL methods that rely solely on task rewards. In this paper, we propose a Foundation model-based method for humanoid Locomotion And...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose a Foundation model-based method for humanoid Locomotion And Manipulation (FLAM for short)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.22249v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.22249v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. Zhang, H. Wei, W. Wang, X. Wang, X. Fan, and D. Zhao, &quot;FLAM: Foundation Model-Based Body Stabilization for Humanoid Locomotion and Manipulation,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.22249v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399336272')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="advancing humanoid locomotion: mastering challenging terrains with denoising world model learning" data-keywords="reinforcement learning neural network robot humanoid locomotion control cs.ro cs.ai eess.sy" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.14472v1" target="_blank">Advancing Humanoid Locomotion: Mastering Challenging Terrains with Denoising World Model Learning</a>
                            </h3>
                            <p class="card-authors">Xinyang Gu, Yen-Jen Wang, Xiang Zhu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span></div>
                            <div class="card-details" id="cat-details-4399336176">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots, with their human-like skeletal structure, are especially suited for tasks in human-centric environments. However, this structure is accompanied by additional challenges in locomotion controller design, especially in complex real-world environments. As a result, existing humanoid robots are limited to relatively simple terrains, either with model-based control or model-free reinforcement learning. In this work, we introduce Denoising World Model Learning (DWL), an end-to-end reinforcement learning framework for humanoid locomotion control, which demonstrates the world&#x27;s first h...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we introduce Denoising World Model Learning (DWL), an end-to-end reinforcement learning framework for humanoid locomotion control, which demonstrates the world&#x27;s first humanoid robot to master real-world challenging terrains such as snowy and inclined land in the wild, up and down stairs, and extremely uneven terrains</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.14472v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.14472v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. Gu et al., &quot;Advancing Humanoid Locomotion: Mastering Challenging Terrains with Denoising World Model Learning,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.14472v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399336176')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="learning humanoid locomotion with perceptive internal model" data-keywords="robot humanoid locomotion perception control simulation ros camera imu cs.ro" data-themes="I E L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2411.14386v1" target="_blank">Learning Humanoid Locomotion with Perceptive Internal Model</a>
                            </h3>
                            <p class="card-authors">Junfeng Long, Junli Ren, Moji Shi et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Perception</span></div>
                            <div class="card-details" id="cat-details-4399347600">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In contrast to quadruped robots that can navigate diverse terrains using a &quot;blind&quot; policy, humanoid robots require accurate perception for stable locomotion due to their high degrees of freedom and inherently unstable morphology. However, incorporating perceptual signals often introduces additional disturbances to the system, potentially reducing its robustness, generalizability, and efficiency. This paper presents the Perceptive Internal Model (PIM), which relies on onboard, continuously updated elevation maps centered around the robot to perceive its surroundings. We train the policy using g...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents the Perceptive Internal Model (PIM), which relies on onboard, continuously updated elevation maps centered around the robot to perceive its surroundings</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2411.14386v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2411.14386v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Long et al., &quot;Learning Humanoid Locomotion with Perceptive Internal Model,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2411.14386v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399347600')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="stride: an open-source, low-cost, and versatile bipedal robot platform for research and education" data-keywords="robot bipedal locomotion control cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.02648v1" target="_blank">STRIDE: An Open-Source, Low-Cost, and Versatile Bipedal Robot Platform for Research and Education</a>
                            </h3>
                            <p class="card-authors">Yuhao Huang, Yicheng Zeng, Xiaobin Xiong</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Control</span></div>
                            <div class="card-details" id="cat-details-4399348944">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In this paper, we present STRIDE, a Simple, Terrestrial, Reconfigurable, Intelligent, Dynamic, and Educational bipedal platform. STRIDE aims to propel bipedal robotics research and education by providing a cost-effective implementation with step-by-step instructions for building a bipedal robotic platform while providing flexible customizations via a modular and durable design. Moreover, a versatile terrain setup and a quantitative disturbance injection system are augmented to the robot platform to replicate natural terrains and push forces that can be used to evaluate legged locomotion in pra...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we present STRIDE, a Simple, Terrestrial, Reconfigurable, Intelligent, Dynamic, and Educational bipedal platform</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.02648v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.02648v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Huang, Y. Zeng, and X. Xiong, &quot;STRIDE: An Open-Source, Low-Cost, and Versatile Bipedal Robot Platform for Research and Education,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.02648v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399348944')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="no more blind spots: learning vision-based omnidirectional bipedal locomotion for challenging terrain" data-keywords="reinforcement learning bipedal locomotion control simulation imu cs.ro cs.ai" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.11929v1" target="_blank">No More Blind Spots: Learning Vision-Based Omnidirectional Bipedal Locomotion for Challenging Terrain</a>
                            </h3>
                            <p class="card-authors">Mohitvishnu S. Gadde, Pranay Dugar, Ashish Malik et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Control</span></div>
                            <div class="card-details" id="cat-details-4399349664">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Effective bipedal locomotion in dynamic environments, such as cluttered indoor spaces or uneven terrain, requires agile and adaptive movement in all directions. This necessitates omnidirectional terrain sensing and a controller capable of processing such input. We present a learning framework for vision-based omnidirectional bipedal locomotion, enabling seamless movement using depth images. A key challenge is the high computational cost of rendering omnidirectional depth images in simulation, making traditional sim-to-real reinforcement learning (RL) impractical. Our method combines a robust b...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present a learning framework for vision-based omnidirectional bipedal locomotion, enabling seamless movement using depth images</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.11929v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.11929v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. S. Gadde, P. Dugar, A. Malik, and A. Fern, &quot;No More Blind Spots: Learning Vision-Based Omnidirectional Bipedal Locomotion for Challenging Terrain,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.11929v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399349664')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="humanoid-gym: reinforcement learning for humanoid robot with zero-shot sim2real transfer" data-keywords="reinforcement learning robot humanoid locomotion simulation mujoco isaac imu cs.ro cs.ai" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2404.05695v2" target="_blank">Humanoid-Gym: Reinforcement Learning for Humanoid Robot with Zero-Shot Sim2Real Transfer</a>
                            </h3>
                            <p class="card-authors">Xinyang Gu, Yen-Jen Wang, Jianyu Chen</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399336608">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid-Gym is an easy-to-use reinforcement learning (RL) framework based on Nvidia Isaac Gym, designed to train locomotion skills for humanoid robots, emphasizing zero-shot transfer from simulation to the real-world environment. Humanoid-Gym also integrates a sim-to-sim framework from Isaac Gym to Mujoco that allows users to verify the trained policies in different physical simulations to ensure the robustness and generalization of the policies. This framework is verified by RobotEra&#x27;s XBot-S (1.2-meter tall humanoid robot) and XBot-L (1.65-meter tall humanoid robot) in a real-world environm...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2404.05695v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2404.05695v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. Gu, Y. Wang, and J. Chen, &quot;Humanoid-Gym: Reinforcement Learning for Humanoid Robot with Zero-Shot Sim2Real Transfer,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2404.05695v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399336608')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="robust rl control for bipedal locomotion with closed kinematic chains" data-keywords="reinforcement learning robot bipedal locomotion control ros cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.10164v1" target="_blank">Robust RL Control for Bipedal Locomotion with Closed Kinematic Chains</a>
                            </h3>
                            <p class="card-authors">Egor Maslennikov, Eduard Zaliaev, Nikita Dudorov et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4399335600">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Developing robust locomotion controllers for bipedal robots with closed kinematic chains presents unique challenges, particularly since most reinforcement learning (RL) approaches simplify these parallel mechanisms into serial models during training. We demonstrate that this simplification significantly impairs sim-to-real transfer by failing to capture essential aspects such as joint coupling, friction dynamics, and motor-space control characteristics. In this work, we present an RL framework that explicitly incorporates closed-chain dynamics and validate it on our custom-built robot TopA. Ou...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We demonstrate that this simplification significantly impairs sim-to-real transfer by failing to capture essential aspects such as joint coupling, friction dynamics, and motor-space control characteristics</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.10164v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.10164v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'E. Maslennikov et al., &quot;Robust RL Control for Bipedal Locomotion with Closed Kinematic Chains,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.10164v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399335600')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="learning agile bipedal motions on a quadrupedal robot" data-keywords="robot bipedal control cs.ro" data-themes="I E L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2311.05818v2" target="_blank">Learning Agile Bipedal Motions on a Quadrupedal Robot</a>
                            </h3>
                            <p class="card-authors">Yunfei Li, Jinhan Li, Wei Fu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Control</span><span class="keyword-tag">CS.RO</span></div>
                            <div class="card-details" id="cat-details-4399335744">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Can a quadrupedal robot perform bipedal motions like humans? Although developing human-like behaviors is more often studied on costly bipedal robot platforms, we present a solution over a lightweight quadrupedal robot that unlocks the agility of the quadruped in an upright standing pose and is capable of a variety of human-like motions. Our framework is with a hierarchical structure. At the low level is a motion-conditioned control policy that allows the quadrupedal robot to track desired base and front limb movements while balancing on two hind feet. The policy is commanded by a high-level mo...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Can a quadrupedal robot perform bipedal motions like humans? Although developing human-like behaviors is more often studied on costly bipedal robot platforms, we present a solution over a lightweight quadrupedal robot that unlocks the agility of the quadruped in an upright standing pose and is capable of a variety of human-like motions</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2311.05818v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2311.05818v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Li, J. Li, W. Fu, and Y. Wu, &quot;Learning Agile Bipedal Motions on a Quadrupedal Robot,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2311.05818v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399335744')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="robot vulnerability and the elicitation of user empathy" data-keywords="robot cs.ro cs.hc" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.02684v1" target="_blank">Robot Vulnerability and the Elicitation of User Empathy</a>
                            </h3>
                            <p class="card-authors">Morten Roed Frederiksen, Katrin Fischer, Maja Matariƒá</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">CS.RO</span><span class="keyword-tag">CS.HC</span></div>
                            <div class="card-details" id="cat-details-4398432704">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper describes a between-subjects Amazon Mechanical Turk study (n = 220) that investigated how a robot&#x27;s affective narrative influences its ability to elicit empathy in human observers. We first conducted a pilot study to develop and validate the robot&#x27;s affective narratives. Then, in the full study, the robot used one of three different affective narrative strategies (funny, sad, neutral) while becoming less functional at its shopping task over the course of the interaction. As the functionality of the robot degraded, participants were repeatedly asked if they were willing to help the r...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.02684v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.02684v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. R. Frederiksen, K. Fischer, and M. Matariƒá, &quot;Robot Vulnerability and the Elicitation of User Empathy,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.02684v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398432704')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="socially acceptable bipedal robot navigation via social zonotope network model predictive control" data-keywords="neural network robot bipedal locomotion navigation planning control simulation imu cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2406.17151v1" target="_blank">Socially Acceptable Bipedal Robot Navigation via Social Zonotope Network Model Predictive Control</a>
                            </h3>
                            <p class="card-authors">Abdulaziz Shamsah, Krishanu Agarwal, Nigam Katta et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4398443168">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This study addresses the challenge of social bipedal navigation in a dynamic, human-crowded environment, a research area largely underexplored in legged robot navigation. We present a zonotope-based framework that couples prediction and motion planning for a bipedal ego-agent to account for bidirectional influence with the surrounding pedestrians. This framework incorporates a Social Zonotope Network (SZN), a neural network that predicts future pedestrian reachable sets and plans future socially acceptable reachable set for the ego-agent. SZN generates the reachable sets as zonotopes for effic...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present a zonotope-based framework that couples prediction and motion planning for a bipedal ego-agent to account for bidirectional influence with the surrounding pedestrians</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2406.17151v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2406.17151v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Shamsah, K. Agarwal, N. Katta, A. Raju, S. Kousik, and Y. Zhao, &quot;Socially Acceptable Bipedal Robot Navigation via Social Zonotope Network Model Predictive Control,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2406.17151v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398443168')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="humanoid robot co-design: coupling hardware design with gait generation via hybrid zero dynamics" data-keywords="robot humanoid bipedal locomotion control optimization imu cs.ro math.oc" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2308.10962v1" target="_blank">Humanoid Robot Co-Design: Coupling Hardware Design with Gait Generation via Hybrid Zero Dynamics</a>
                            </h3>
                            <p class="card-authors">Adrian B. Ghansah, Jeeseop Kim, Maegan Tucker et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4398443840">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Selecting robot design parameters can be challenging since these parameters are often coupled with the performance of the controller and, therefore, the resulting capabilities of the robot. This leads to a time-consuming and often expensive process whereby one iterates between designing the robot and manually evaluating its capabilities. This is particularly challenging for bipedal robots, where it can be difficult to evaluate the behavior of the system due to the underlying nonlinear and hybrid dynamics. Thus, in an effort to streamline the design process of bipedal robots, and maximize their...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Thus, in an effort to streamline the design process of bipedal robots, and maximize their performance, this paper presents a systematic framework for the co-design of humanoid robots and their associated walking gaits</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2308.10962v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2308.10962v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. B. Ghansah, J. Kim, M. Tucker, and A. D. Ames, &quot;Humanoid Robot Co-Design: Coupling Hardware Design with Gait Generation via Hybrid Zero Dynamics,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2308.10962v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398443840')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="play everywhere: a temporal logic based game environment independent approach for playing soccer with robots" data-keywords="robot cs.ro cs.ai" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.12628v1" target="_blank">Play Everywhere: A Temporal Logic based Game Environment Independent Approach for Playing Soccer with Robots</a>
                            </h3>
                            <p class="card-authors">Vincenzo Suriani, Emanuele Musumeci, Daniele Nardi et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">CS.RO</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4398443072">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Robots playing soccer often rely on hard-coded behaviors that struggle to generalize when the game environment change. In this paper, we propose a temporal logic based approach that allows robots&#x27; behaviors and goals to adapt to the semantics of the environment. In particular, we present a hierarchical representation of soccer in which the robot selects the level of operation based on the perceived semantic characteristics of the environment, thus modifying dynamically the set of rules and goals to apply. The proposed approach enables the robot to operate in unstructured environments, just as ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose a temporal logic based approach that allows robots&#x27; behaviors and goals to adapt to the semantics of the environment</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.12628v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.12628v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'V. Suriani, E. Musumeci, D. Nardi, and D. D. Bloisi, &quot;Play Everywhere: A Temporal Logic based Game Environment Independent Approach for Playing Soccer with Robots,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.12628v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398443072')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="humanoid everyday: a comprehensive robotic dataset for open-world humanoid manipulation" data-keywords="robot humanoid locomotion manipulation control ros lidar cs.ro cs.lg" data-themes="S E I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2510.08807v1" target="_blank">Humanoid Everyday: A Comprehensive Robotic Dataset for Open-World Humanoid Manipulation</a>
                            </h3>
                            <p class="card-authors">Zhenyu Zhao, Hongyi Jing, Xiawei Liu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Manipulation</span></div>
                            <div class="card-details" id="cat-details-4398443696">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">From loco-motion to dextrous manipulation, humanoid robots have made remarkable strides in demonstrating complex full-body capabilities. However, the majority of current robot learning datasets and benchmarks mainly focus on stationary robot arms, and the few existing humanoid datasets are either confined to fixed environments or limited in task diversity, often lacking human-humanoid interaction and lower-body locomotion. Moreover, there are a few standardized evaluation platforms for benchmarking learning-based policies on humanoid data. In this work, we present Humanoid Everyday, a large-sc...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we present Humanoid Everyday, a large-scale and diverse humanoid manipulation dataset characterized by extensive task variety involving dextrous object manipulation, human-humanoid interaction, locomotion-integrated actions, and more</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2510.08807v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2510.08807v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Zhao et al., &quot;Humanoid Everyday: A Comprehensive Robotic Dataset for Open-World Humanoid Manipulation,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2510.08807v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398443696')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="reinforcement learning with data bootstrapping for dynamic subgoal pursuit in humanoid robot navigation" data-keywords="reinforcement learning robot humanoid bipedal locomotion navigation control simulation ros imu" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.02206v1" target="_blank">Reinforcement Learning with Data Bootstrapping for Dynamic Subgoal Pursuit in Humanoid Robot Navigation</a>
                            </h3>
                            <p class="card-authors">Chengyang Peng, Zhihao Zhang, Shiting Gong et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span></div>
                            <div class="card-details" id="cat-details-4398443120">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Safe and real-time navigation is fundamental for humanoid robot applications. However, existing bipedal robot navigation frameworks often struggle to balance computational efficiency with the precision required for stable locomotion. We propose a novel hierarchical framework that continuously generates dynamic subgoals to guide the robot through cluttered environments. Our method comprises a high-level reinforcement learning (RL) planner for subgoal selection in a robot-centric coordinate system and a low-level Model Predictive Control (MPC) based planner which produces robust walking gaits to...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a novel hierarchical framework that continuously generates dynamic subgoals to guide the robot through cluttered environments</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.02206v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.02206v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Peng, Z. Zhang, S. Gong, S. Agrawal, K. A. Redmill, and A. Hereid, &quot;Reinforcement Learning with Data Bootstrapping for Dynamic Subgoal Pursuit in Humanoid Robot Navigation,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.02206v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398443120')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="walking-by-logic: signal temporal logic-guided model predictive control for bipedal locomotion resilient to external perturbations" data-keywords="robot bipedal locomotion planning control optimization simulation ros imu cs.ro" data-themes="I E L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.13172v1" target="_blank">Walking-by-Logic: Signal Temporal Logic-Guided Model Predictive Control for Bipedal Locomotion Resilient to External Perturbations</a>
                            </h3>
                            <p class="card-authors">Zhaoyuan Gu, Rongming Guo, William Yates et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Planning</span></div>
                            <div class="card-details" id="cat-details-4398443600">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This study proposes a novel planning framework based on a model predictive control formulation that incorporates signal temporal logic (STL) specifications for task completion guarantees and robustness quantification. This marks the first-ever study to apply STL-guided trajectory optimization for bipedal locomotion push recovery, where the robot experiences unexpected disturbances. Existing recovery strategies often struggle with complex task logic reasoning and locomotion robustness evaluation, making them susceptible to failures caused by inappropriate recovery strategies or insufficient rob...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.13172v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.13172v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Gu, R. Guo, W. Yates, Y. Chen, and Y. Zhao, &quot;Walking-by-Logic: Signal Temporal Logic-Guided Model Predictive Control for Bipedal Locomotion Resilient to External Perturbations,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.13172v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398443600')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="human impression of humanoid robots mirroring social cues" data-keywords="robot humanoid perception control ros imu cs.ro cs.hc" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.12076v1" target="_blank">Human Impression of Humanoid Robots Mirroring Social Cues</a>
                            </h3>
                            <p class="card-authors">Di Fu, Fares Abawi, Philipp Allgeuer et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Perception</span><span class="keyword-tag">Control</span></div>
                            <div class="card-details" id="cat-details-4398443264">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Mirroring non-verbal social cues such as affect or movement can enhance human-human and human-robot interactions in the real world. The robotic platforms and control methods also impact people&#x27;s perception of human-robot interaction. However, limited studies have compared robot imitation across different platforms and control methods. Our research addresses this gap by conducting two experiments comparing people&#x27;s perception of affective mirroring between the iCub and Pepper robots and movement mirroring between vision-based iCub control and Inertial Measurement Unit (IMU)-based iCub control. ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.12076v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.12076v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Fu, F. Abawi, P. Allgeuer, and S. Wermter, &quot;Human Impression of Humanoid Robots Mirroring Social Cues,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.12076v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398443264')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="hwc-loco: a hierarchical whole-body control approach to robust humanoid locomotion" data-keywords="robot humanoid locomotion control optimization ros imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.00923v3" target="_blank">HWC-Loco: A Hierarchical Whole-Body Control Approach to Robust Humanoid Locomotion</a>
                            </h3>
                            <p class="card-authors">Sixu Lin, Guanren Qiao, Yunxin Tai et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Control</span></div>
                            <div class="card-details" id="cat-details-4398442976">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots, capable of assuming human roles in various workplaces, have become essential to embodied intelligence. However, as robots with complex physical structures, learning a control model that can operate robustly across diverse environments remains inherently challenging, particularly under the discrepancies between training and deployment environments. In this study, we propose HWC-Loco, a robust whole-body control algorithm tailored for humanoid locomotion tasks. By reformulating policy learning as a robust optimization problem, HWC-Loco explicitly learns to recover from safety-cr...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this study, we propose HWC-Loco, a robust whole-body control algorithm tailored for humanoid locomotion tasks</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.00923v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.00923v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Lin, G. Qiao, Y. Tai, A. Li, K. Jia, and G. Liu, &quot;HWC-Loco: A Hierarchical Whole-Body Control Approach to Robust Humanoid Locomotion,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.00923v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398442976')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="doublyaware: dual planning and policy awareness for temporal difference learning in humanoid locomotion" data-keywords="reinforcement learning robot humanoid locomotion planning control simulation imu cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.12095v1" target="_blank">DoublyAware: Dual Planning and Policy Awareness for Temporal Difference Learning in Humanoid Locomotion</a>
                            </h3>
                            <p class="card-authors">Khang Nguyen, An T. Le, Jan Peters et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4398432320">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Achieving robust robot learning for humanoid locomotion is a fundamental challenge in model-based reinforcement learning (MBRL), where environmental stochasticity and randomness can hinder efficient exploration and learning stability. The environmental, so-called aleatoric, uncertainty can be amplified in high-dimensional action spaces with complex contact dynamics, and further entangled with epistemic uncertainty in the models during learning phases. In this work, we propose DoublyAware, an uncertainty-aware extension of Temporal Difference Model Predictive Control (TD-MPC) that explicitly de...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose DoublyAware, an uncertainty-aware extension of Temporal Difference Model Predictive Control (TD-MPC) that explicitly decomposes uncertainty into two disjoint interpretable components, i.e., planning and policy uncertainties</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.12095v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.12095v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Nguyen, A. T. Le, J. Peters, and M. N. Vu, &quot;DoublyAware: Dual Planning and Policy Awareness for Temporal Difference Learning in Humanoid Locomotion,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.12095v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398432320')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="anticipatory and adaptive footstep streaming for teleoperated bipedal robots" data-keywords="robot humanoid locomotion cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.11802v1" target="_blank">Anticipatory and Adaptive Footstep Streaming for Teleoperated Bipedal Robots</a>
                            </h3>
                            <p class="card-authors">Luigi Penco, Beomyeong Park, Stefan Fasano et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">CS.RO</span></div>
                            <div class="card-details" id="cat-details-4398443552">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Achieving seamless synchronization between user and robot motion in teleoperation, particularly during high-speed tasks, remains a significant challenge. In this work, we propose a novel approach for transferring stepping motions from the user to the robot in real-time. Instead of directly replicating user foot poses, we retarget user steps to robot footstep locations, allowing the robot to utilize its own dynamics for locomotion, ensuring better balance and stability. Our method anticipates user footsteps to minimize delays between when the user initiates and completes a step and when the rob...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose a novel approach for transferring stepping motions from the user to the robot in real-time</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.11802v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.11802v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Penco et al., &quot;Anticipatory and Adaptive Footstep Streaming for Teleoperated Bipedal Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.11802v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398443552')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="humanoid locomotion and manipulation: current progress and challenges in control, planning, and learning" data-keywords="robot humanoid locomotion manipulation planning control imitation learning cs.ro" data-themes="S E I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2501.02116v2" target="_blank">Humanoid Locomotion and Manipulation: Current Progress and Challenges in Control, Planning, and Learning</a>
                            </h3>
                            <p class="card-authors">Zhaoyuan Gu, Junheng Li, Wenlan Shen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Manipulation</span></div>
                            <div class="card-details" id="cat-details-4398433328">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots hold great potential to perform various human-level skills, involving unified locomotion and manipulation in real-world settings. Driven by advances in machine learning and the strength of existing model-based approaches, these capabilities have progressed rapidly, but often separately. This survey offers a comprehensive overview of the state-of-the-art in humanoid locomotion and manipulation (HLM), with a focus on control, planning, and learning methods. We first review the model-based methods that have been the backbone of humanoid robotics for the past three decades. We disc...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2501.02116v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2501.02116v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Gu et al., &quot;Humanoid Locomotion and Manipulation: Current Progress and Challenges in Control, Planning, and Learning,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2501.02116v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398433328')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="distillation-ppo: a novel two-stage reinforcement learning framework for humanoid robot perceptive locomotion" data-keywords="reinforcement learning attention robot humanoid locomotion control imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.08299v1" target="_blank">Distillation-PPO: A Novel Two-Stage Reinforcement Learning Framework for Humanoid Robot Perceptive Locomotion</a>
                            </h3>
                            <p class="card-authors">Qiang Zhang, Gang Han, Jingkai Sun et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Attention</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span></div>
                            <div class="card-details" id="cat-details-4398443360">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In recent years, humanoid robots have garnered significant attention from both academia and industry due to their high adaptability to environments and human-like characteristics. With the rapid advancement of reinforcement learning, substantial progress has been made in the walking control of humanoid robots. However, existing methods still face challenges when dealing with complex environments and irregular terrains. In the field of perceptive locomotion, existing approaches are generally divided into two-stage methods and end-to-end methods. Two-stage methods first train a teacher policy in...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.08299v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.08299v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Q. Zhang et al., &quot;Distillation-PPO: A Novel Two-Stage Reinforcement Learning Framework for Humanoid Robot Perceptive Locomotion,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.08299v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398443360')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="clipswarm: converting text into formations of robots" data-keywords="robot swarm multi-robot optimization cs.ro" data-themes="E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2311.11047v1" target="_blank">CLIPSwarm: Converting text into formations of robots</a>
                            </h3>
                            <p class="card-authors">Pablo Pueyo, Eduardo Montijano, Ana C. Murillo et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Swarm</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Optimization</span></div>
                            <div class="card-details" id="cat-details-4398432416">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We present CLIPSwarm, an algorithm to generate robot swarm formations from natural language descriptions. CLIPSwarm receives an input text and finds the position of the robots to form a shape that corresponds to the given text. To do so, we implement a variation of the Montecarlo particle filter to obtain a matching formation iteratively. In every iteration, we generate a set of new formations and evaluate their Clip Similarity with the given text, selecting the best formations according to this metric. This metric is obtained using Clip, [1], an existing foundation model trained to encode ima...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present CLIPSwarm, an algorithm to generate robot swarm formations from natural language descriptions</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2311.11047v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2311.11047v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Pueyo, E. Montijano, A. C. Murillo, and M. Schwager, &quot;CLIPSwarm: Converting text into formations of robots,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2311.11047v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398432416')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="what can you say to a robot? capability communication leads to more natural conversations" data-keywords="robot cs.ro cs.hc" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.01448v1" target="_blank">What Can You Say to a Robot? Capability Communication Leads to More Natural Conversations</a>
                            </h3>
                            <p class="card-authors">Merle M. Reimann, Koen V. Hindriks, Florian A. Kunneman et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">CS.RO</span><span class="keyword-tag">CS.HC</span></div>
                            <div class="card-details" id="cat-details-4398445136">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">When encountering a robot in the wild, it is not inherently clear to human users what the robot&#x27;s capabilities are. When encountering misunderstandings or problems in spoken interaction, robots often just apologize and move on, without additional effort to make sure the user understands what happened. We set out to compare the effect of two speech based capability communication strategies (proactive, reactive) to a robot without such a strategy, in regard to the user&#x27;s rating of and their behavior during the interaction. For this, we conducted an in-person user study with 120 participants who ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.01448v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.01448v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. M. Reimann, K. V. Hindriks, F. A. Kunneman, C. Oertel, G. Skantze, and I. Leite, &quot;What Can You Say to a Robot? Capability Communication Leads to More Natural Conversations,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.01448v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398445136')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="learning differentiable reachability maps for optimization-based humanoid motion generation" data-keywords="neural network robot humanoid manipulation planning optimization cs.ro" data-themes="I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.11275v1" target="_blank">Learning Differentiable Reachability Maps for Optimization-based Humanoid Motion Generation</a>
                            </h3>
                            <p class="card-authors">Masaki Murooka, Iori Kumagai, Mitsuharu Morisawa et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Manipulation</span></div>
                            <div class="card-details" id="cat-details-4398445664">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">To reduce the computational cost of humanoid motion generation, we introduce a new approach to representing robot kinematic reachability: the differentiable reachability map. This map is a scalar-valued function defined in the task space that takes positive values only in regions reachable by the robot&#x27;s end-effector. A key feature of this representation is that it is continuous and differentiable with respect to task-space coordinates, enabling its direct use as constraints in continuous optimization for humanoid motion planning. We describe a method to learn such differentiable reachability ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To reduce the computational cost of humanoid motion generation, we introduce a new approach to representing robot kinematic reachability: the differentiable reachability map</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.11275v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.11275v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Murooka, I. Kumagai, M. Morisawa, and F. Kanehiro, &quot;Learning Differentiable Reachability Maps for Optimization-based Humanoid Motion Generation,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.11275v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398445664')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="llm-based ambiguity detection in natural language instructions for collaborative surgical robots" data-keywords="robot language model cs.ro cs.hc" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.11525v1" target="_blank">LLM-based ambiguity detection in natural language instructions for collaborative surgical robots</a>
                            </h3>
                            <p class="card-authors">Ana Davila, Jacinto Colan, Yasuhisa Hasegawa</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.RO</span><span class="keyword-tag">CS.HC</span></div>
                            <div class="card-details" id="cat-details-4398433472">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Ambiguity in natural language instructions poses significant risks in safety-critical human-robot interaction, particularly in domains such as surgery. To address this, we propose a framework that uses Large Language Models (LLMs) for ambiguity detection specifically designed for collaborative surgical scenarios. Our method employs an ensemble of LLM evaluators, each configured with distinct prompting techniques to identify linguistic, contextual, procedural, and critical ambiguities. A chain-of-thought evaluator is included to systematically analyze instruction structure for potential issues....</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address this, we propose a framework that uses Large Language Models (LLMs) for ambiguity detection specifically designed for collaborative surgical scenarios</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.11525v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.11525v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Davila, J. Colan, and Y. Hasegawa, &quot;LLM-based ambiguity detection in natural language instructions for collaborative surgical robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.11525v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398433472')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="what am i? evaluating the effect of language fluency and task competency on the perception of a social robot" data-keywords="robot perception cs.ro" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.11085v1" target="_blank">What Am I? Evaluating the Effect of Language Fluency and Task Competency on the Perception of a Social Robot</a>
                            </h3>
                            <p class="card-authors">Shahira Ali, Haley N. Green, Tariq Iqbal</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Perception</span><span class="keyword-tag">CS.RO</span></div>
                            <div class="card-details" id="cat-details-4398445280">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recent advancements in robot capabilities have enabled them to interact with people in various human-social environments (HSEs). In many of these environments, the perception of the robot often depends on its capabilities, e.g., task competency, language fluency, etc. To enable fluent human-robot interaction (HRI) in HSEs, it is crucial to understand the impact of these capabilities on the perception of the robot. Although many works have investigated the effects of various robot capabilities on the robot&#x27;s perception separately, in this paper, we present a large-scale HRI study (n = 60) to in...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Although many works have investigated the effects of various robot capabilities on the robot&#x27;s perception separately, in this paper, we present a large-scale HRI study (n = 60) to investigate the combined impact of both language fluency and task competency on the perception of a robot</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.11085v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.11085v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Ali, H. N. Green, and T. Iqbal, &quot;What Am I? Evaluating the Effect of Language Fluency and Task Competency on the Perception of a Social Robot,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.11085v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398445280')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="ppf: pre-training and preservative fine-tuning of humanoid locomotion via model-assumption-based regularization" data-keywords="reinforcement learning robot humanoid locomotion control simulation ros imu cs.ro" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.09833v2" target="_blank">PPF: Pre-training and Preservative Fine-tuning of Humanoid Locomotion via Model-Assumption-based Regularization</a>
                            </h3>
                            <p class="card-authors">Hyunyoung Jung, Zhaoyuan Gu, Ye Zhao et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4398443504">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid locomotion is a challenging task due to its inherent complexity and high-dimensional dynamics, as well as the need to adapt to diverse and unpredictable environments. In this work, we introduce a novel learning framework for effectively training a humanoid locomotion policy that imitates the behavior of a model-based controller while extending its capabilities to handle more complex locomotion tasks, such as more challenging terrain and higher velocity commands. Our framework consists of three key components: pre-training through imitation of the model-based controller, fine-tuning vi...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we introduce a novel learning framework for effectively training a humanoid locomotion policy that imitates the behavior of a model-based controller while extending its capabilities to handle more complex locomotion tasks, such as more challenging terrain and higher velocity commands</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.09833v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.09833v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Jung, Z. Gu, Y. Zhao, H. Park, and S. Ha, &quot;PPF: Pre-training and Preservative Fine-tuning of Humanoid Locomotion via Model-Assumption-based Regularization,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.09833v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398443504')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="stoch biro: design and control of a low cost bipedal robot" data-keywords="robot bipedal locomotion navigation control mujoco imu cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.06512v1" target="_blank">Stoch BiRo: Design and Control of a low cost bipedal robot</a>
                            </h3>
                            <p class="card-authors">GVS Mothish, Karthik Rajgopal, Ravi Kola et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Navigation</span></div>
                            <div class="card-details" id="cat-details-4398445232">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper introduces the Stoch BiRo, a cost-effective bipedal robot designed with a modular mechanical structure having point feet to navigate uneven and unfamiliar terrains. The robot employs proprioceptive actuation in abduction, hips, and knees, leveraging a Raspberry Pi4 for control. Overcoming computational limitations, a Learning-based Linear Policy controller manages balance and locomotion with only 3 degrees of freedom (DoF) per leg, distinct from the typical 5DoF in bipedal systems. Integrated within a modular control architecture, these controllers enable autonomous handling of unfo...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.06512v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.06512v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Mothish, K. Rajgopal, R. Kola, M. Tayal, and S. Kolathaya, &quot;Stoch BiRo: Design and Control of a low cost bipedal robot,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.06512v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398445232')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="dribble master: learning agile humanoid dribbling through legged locomotion" data-keywords="reinforcement learning robot humanoid locomotion manipulation perception control simulation ros camera" data-themes="I M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2505.12679v2" target="_blank">Dribble Master: Learning Agile Humanoid Dribbling Through Legged Locomotion</a>
                            </h3>
                            <p class="card-authors">Zhuoheng Wang, Jinyin Zhou, Qi Wu</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4398443648">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid soccer dribbling is a highly challenging task that demands dexterous ball manipulation while maintaining dynamic balance. Traditional rule-based methods often struggle to achieve accurate ball control due to their reliance on fixed walking patterns and limited adaptability to real-time ball dynamics. To address these challenges, we propose a two-stage curriculum learning framework that enables a humanoid robot to acquire dribbling skills without explicit dynamics or predefined trajectories. In the first stage, the robot learns basic locomotion skills; in the second stage, we fine-tune...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address these challenges, we propose a two-stage curriculum learning framework that enables a humanoid robot to acquire dribbling skills without explicit dynamics or predefined trajectories</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2505.12679v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2505.12679v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Wang, J. Zhou, and Q. Wu, &quot;Dribble Master: Learning Agile Humanoid Dribbling Through Legged Locomotion,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2505.12679v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398443648')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="ctbc: contact-triggered blind climbing for wheeled bipedal robots with instruction learning and reinforcement learning" data-keywords="attention robot bipedal locomotion cs.ro" data-themes="S I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.02986v2" target="_blank">CTBC: Contact-Triggered Blind Climbing for Wheeled Bipedal Robots with Instruction Learning and Reinforcement Learning</a>
                            </h3>
                            <p class="card-authors">Rankun Li, Hao Wang, Qi Li et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4398445856">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In recent years, wheeled bipedal robots have gained increasing attention due to their advantages in mobility, such as high-speed locomotion on flat terrain. However, their performance on complex environments (e.g., staircases) remains inferior to that of traditional legged robots. To overcome this limitation, we propose a general contact-triggered blind climbing (CTBC) framework for wheeled bipedal robots. Upon detecting wheel-obstacle contact, the robot triggers a leg-lifting motion to overcome the obstacle. By leveraging a strongly-guided feedforward trajectory, our method enables the robot ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To overcome this limitation, we propose a general contact-triggered blind climbing (CTBC) framework for wheeled bipedal robots</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.02986v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.02986v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Li et al., &quot;CTBC: Contact-Triggered Blind Climbing for Wheeled Bipedal Robots with Instruction Learning and Reinforcement Learning,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.02986v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398445856')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="cs.cl">
                <h2 class="section-header">üè∑Ô∏è Computational Linguistics <span class="section-count">(95 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="is self-knowledge and action consistent or not: investigating large language model&#x27;s personality" data-keywords="language model cs.cl cs.cy" data-themes="S M E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.14679v2" target="_blank">Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model&#x27;s Personality</a>
                            </h3>
                            <p class="card-authors">Yiming Ai, Zhiwei He, Ziyin Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.CY</span></div>
                            <div class="card-details" id="cat-details-4397313248">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In this study, we delve into the validity of conventional personality questionnaires in capturing the human-like personality traits of Large Language Models (LLMs). Our objective is to assess the congruence between the personality traits LLMs claim to possess and their demonstrated tendencies in real-world scenarios. By conducting an extensive examination of LLM outputs against observed human response patterns, we aim to understand the disjunction between self-knowledge and action in LLMs.</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.14679v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.14679v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Ai et al., &quot;Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model&#x27;s Personality,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.14679v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397313248')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="large language models lack understanding of character composition of words" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.11357v3" target="_blank">Large Language Models Lack Understanding of Character Composition of Words</a>
                            </h3>
                            <p class="card-authors">Andrew Shin, Kunitake Kaneko</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4397308688">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) have demonstrated remarkable performances on a wide range of natural language tasks. Yet, LLMs&#x27; successes have been largely restricted to tasks concerning words, sentences, or documents, and it remains questionable how much they understand the minimal units of text, namely characters. In this paper, we examine contemporary LLMs regarding their ability to understand character composition of words, and show that most of them fail to reliably carry out even the simple tasks that can be handled by humans with perfection. We analyze their behaviors with comparison to to...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.11357v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.11357v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Shin, and K. Kaneko, &quot;Large Language Models Lack Understanding of Character Composition of Words,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.11357v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397308688')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="unmasking the shadows of ai: investigating deceptive capabilities in large language models" data-keywords="language model cs.cl cs.ai" data-themes="S I M E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.09676v1" target="_blank">Unmasking the Shadows of AI: Investigating Deceptive Capabilities in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Linge Guo</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4397304800">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This research critically navigates the intricate landscape of AI deception, concentrating on deceptive behaviours of Large Language Models (LLMs). My objective is to elucidate this issue, examine the discourse surrounding it, and subsequently delve into its categorization and ramifications. The essay initiates with an evaluation of the AI Safety Summit 2023 (ASS) and introduction of LLMs, emphasising multidimensional biases that underlie their deceptive behaviours.The literature review covers four types of deception categorised: Strategic deception, Imitation, Sycophancy, and Unfaithful Reason...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.09676v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.09676v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Guo, &quot;Unmasking the Shadows of AI: Investigating Deceptive Capabilities in Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.09676v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397304800')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="self-cognition in large language models: an exploratory study" data-keywords="ros language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.01505v1" target="_blank">Self-Cognition in Large Language Models: An Exploratory Study</a>
                            </h3>
                            <p class="card-authors">Dongping Chen, Jiawen Shi, Yao Wan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4397308016">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">While Large Language Models (LLMs) have achieved remarkable success across various applications, they also raise concerns regarding self-cognition. In this paper, we perform a pioneering study to explore self-cognition in LLMs. Specifically, we first construct a pool of self-cognition instruction prompts to evaluate where an LLM exhibits self-cognition and four well-designed principles to quantify LLMs&#x27; self-cognition. Our study reveals that 4 of the 48 models on Chatbot Arena--specifically Command R, Claude3-Opus, Llama-3-70b-Instruct, and Reka-core--demonstrate some level of detectable self-...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.01505v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.01505v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Chen, J. Shi, Y. Wan, P. Zhou, N. Z. Gong, and L. Sun, &quot;Self-Cognition in Large Language Models: An Exploratory Study,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.01505v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397308016')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="making large language models better reasoners with alignment" data-keywords="language model cs.cl cs.ai cs.lg" data-themes="I M E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.02144v1" target="_blank">Making Large Language Models Better Reasoners with Alignment</a>
                            </h3>
                            <p class="card-authors">Peiyi Wang, Lei Li, Liang Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.LG</span></div>
                            <div class="card-details" id="cat-details-4397307344">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Reasoning is a cognitive process of using evidence to reach a sound conclusion. The reasoning capability is essential for large language models (LLMs) to serve as the brain of the artificial general intelligence agent. Recent studies reveal that fine-tuning LLMs on data with the chain of thought (COT) reasoning process can significantly enhance their reasoning capabilities. However, we find that the fine-tuned LLMs suffer from an \textit{Assessment Misalignment} problem, i.e., they frequently assign higher scores to subpar COTs, leading to potential limitations in their reasoning abilities. To...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address this problem, we introduce an \textit{Alignment Fine-Tuning (AFT)} paradigm, which involves three steps: 1) fine-tuning LLMs with COT training data; 2) generating multiple COT responses for each question, and categorizing them into positive and negative ones based on whether they achieve the correct answer; 3) calibrating the scores of positive and negative responses given by LLMs with a novel constraint alignment loss</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.02144v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.02144v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Wang et al., &quot;Making Large Language Models Better Reasoners with Alignment,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.02144v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397307344')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="beneath the surface: unveiling harmful memes with multimodal reasoning distilled from large language models" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.05434v1" target="_blank">Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models</a>
                            </h3>
                            <p class="card-authors">Hongzhan Lin, Ziyang Luo, Jing Ma et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4397305568">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The age of social media is rife with memes. Understanding and detecting harmful memes pose a significant challenge due to their implicit meaning that is not explicitly conveyed through the surface text and image. However, existing harmful meme detection approaches only recognize superficial harm-indicative signals in an end-to-end classification manner but ignore in-depth cognition of the meme text and image. In this paper, we attempt to detect harmful memes based on advanced reasoning over the interplay of multimodal information in memes. Inspired by the success of Large Language Models (LLMs...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Then we propose a novel generative framework to learn reasonable thoughts from LLMs for better multimodal fusion and lightweight fine-tuning, which consists of two training stages: 1) Distill multimodal reasoning knowledge from LLMs; and 2) Fine-tune the generative framework to infer harmfulness</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.05434v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.05434v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Lin, Z. Luo, J. Ma, and L. Chen, &quot;Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.05434v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397305568')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="all languages matter: on the multilingual safety of large language models" data-keywords="gpt ros language model cs.cl cs.ai" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2310.00905v2" target="_blank">All Languages Matter: On the Multilingual Safety of Large Language Models</a>
                            </h3>
                            <p class="card-authors">Wenxuan Wang, Zhaopeng Tu, Chang Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4397308064">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Safety lies at the core of developing and deploying large language models (LLMs). However, previous safety benchmarks only concern the safety in one language, e.g. the majority language in the pretraining data such as English. In this work, we build the first multilingual safety benchmark for LLMs, XSafety, in response to the global deployment of LLMs in practice. XSafety covers 14 kinds of commonly used safety issues across 10 languages that span several language families. We utilize XSafety to empirically study the multilingual safety for 4 widely-used LLMs, including both close-API and open...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In addition, we propose several simple and effective prompting methods to improve the multilingual safety of ChatGPT by evoking safety knowledge and improving cross-lingual generalization of safety alignment</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2310.00905v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2310.00905v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. Wang et al., &quot;All Languages Matter: On the Multilingual Safety of Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2310.00905v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397308064')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="classifying german language proficiency levels using large language models" data-keywords="language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2512.06483v1" target="_blank">Classifying German Language Proficiency Levels Using Large Language Models</a>
                            </h3>
                            <p class="card-authors">Elias-Leander Ahlers, Witold Brunsmann, Malte Schilling</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4397313488">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Assessing language proficiency is essential for education, as it enables instruction tailored to learners needs. This paper investigates the use of Large Language Models (LLMs) for automatically classifying German texts according to the Common European Framework of Reference for Languages (CEFR) into different proficiency levels. To support robust training and evaluation, we construct a diverse dataset by combining multiple existing CEFR-annotated corpora with synthetic data. We then evaluate prompt-engineering strategies, fine-tuning of a LLaMA-3-8B-Instruct model and a probing-based approach...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2512.06483v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2512.06483v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'E. Ahlers, W. Brunsmann, and M. Schilling, &quot;Classifying German Language Proficiency Levels Using Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2512.06483v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397313488')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="not all experts are equal: efficient expert pruning and skipping for mixture-of-experts large language models" data-keywords="ros imu language model cs.cl cs.ai cs.lg" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.14800v2" target="_blank">Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models</a>
                            </h3>
                            <p class="card-authors">Xudong Lu, Qi Liu, Yuhui Xu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Imu</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4397307200">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">A pivotal advancement in the progress of large language models (LLMs) is the emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer parameters, but it is still hard to deploy them due to their immense parameter sizes. Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques. Specifically, we propose, for the first time to our best knowledge, post-tr...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.14800v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.14800v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. Lu et al., &quot;Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.14800v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397307200')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="teal: tokenize and embed all for multi-modal large language models" data-keywords="language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2311.04589v3" target="_blank">TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models</a>
                            </h3>
                            <p class="card-authors">Zhen Yang, Yingxue Zhang, Fandong Meng et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4397304512">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Despite Multi-modal Large Language Models (MM-LLMs) have made exciting strides recently, they are still struggling to efficiently model the interactions among multi-modal inputs and the generation in non-textual modalities. In this work, we propose TEAL (Tokenize and Embed ALl)}, an approach to treat the input from any modality as a token sequence and learn a joint embedding space for all modalities. Specifically, for the input from any modality, TEAL first discretizes it into a token sequence with the off-the-shelf tokenizer and embeds the token sequence into a joint embedding space with a le...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose TEAL (Tokenize and Embed ALl)}, an approach to treat the input from any modality as a token sequence and learn a joint embedding space for all modalities</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2311.04589v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2311.04589v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Yang, Y. Zhang, F. Meng, and J. Zhou, &quot;TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2311.04589v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397304512')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="exploring large language models to generate easy to read content" data-keywords="ros nlp language model cs.cl cs.hc" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.20046v1" target="_blank">Exploring Large Language Models to generate Easy to Read content</a>
                            </h3>
                            <p class="card-authors">Paloma Mart√≠nez, Lourdes Moreno, Alberto Ramos</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4397085024">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Ensuring text accessibility and understandability are essential goals, particularly for individuals with cognitive impairments and intellectual disabilities, who encounter challenges in accessing information across various mediums such as web pages, newspapers, administrative tasks, or health documents. Initiatives like Easy to Read and Plain Language guidelines aim to simplify complex texts; however, standardizing these guidelines remains challenging and often involves manual processes. This work presents an exploratory investigation into leveraging Artificial Intelligence (AI) and Natural La...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.20046v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.20046v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Mart√≠nez, L. Moreno, and A. Ramos, &quot;Exploring Large Language Models to generate Easy to Read content,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.20046v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397085024')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="large language models in ambulatory devices for home health diagnostics: a case study of sickle cell anemia management" data-keywords="language model cs.cl" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2305.03715v1" target="_blank">Large Language Models in Ambulatory Devices for Home Health Diagnostics: A case study of Sickle Cell Anemia Management</a>
                            </h3>
                            <p class="card-authors">Oluwatosin Ogundare, Subuola Sofolahan</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4397304704">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This study investigates the potential of an ambulatory device that incorporates Large Language Models (LLMs) in cadence with other specialized ML models to assess anemia severity in sickle cell patients in real time. The device would rely on sensor data that measures angiogenic material levels to assess anemia severity, providing real-time information to patients and clinicians to reduce the frequency of vaso-occlusive crises because of the early detection of anemia severity, allowing for timely interventions and potentially reducing the likelihood of serious complications. The main challenges...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2305.03715v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2305.03715v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'O. Ogundare, and S. Sofolahan, &quot;Large Language Models in Ambulatory Devices for Home Health Diagnostics: A case study of Sickle Cell Anemia Management,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2305.03715v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397304704')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="reinforcement learning meets large language models: a survey of advancements and applications across the llm lifecycle" data-keywords="reinforcement learning ros language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.16679v1" target="_blank">Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle</a>
                            </h3>
                            <p class="card-authors">Keliang Liu, Dingkang Yang, Ziyun Qian et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4397308784">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In recent years, training methods centered on Reinforcement Learning (RL) have markedly enhanced the reasoning and alignment performance of Large Language Models (LLMs), particularly in understanding human intents, following user instructions, and bolstering inferential strength. Although existing surveys offer overviews of RL augmented LLMs, their scope is often limited, failing to provide a comprehensive summary of how RL operates across the full lifecycle of LLMs. We systematically review the theoretical and practical advancements whereby RL empowers LLMs, especially Reinforcement Learning ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.16679v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.16679v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Liu et al., &quot;Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.16679v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397308784')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="larabench: benchmarking arabic ai with large language models" data-keywords="gpt ros nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2305.14982v2" target="_blank">LAraBench: Benchmarking Arabic AI with Large Language Models</a>
                            </h3>
                            <p class="card-authors">Ahmed Abdelali, Hamdy Mubarak, Shammur Absar Chowdhury et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4397312192">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recent advancements in Large Language Models (LLMs) have significantly influenced the landscape of language and speech research. Despite this progress, these models lack specific benchmarking against state-of-the-art (SOTA) models tailored to particular languages and tasks. LAraBench addresses this gap for Arabic Natural Language Processing (NLP) and Speech Processing tasks, including sequence tagging and content classification across different domains. We utilized models such as GPT-3.5-turbo, GPT-4, BLOOMZ, Jais-13b-chat, Whisper, and USM, employing zero and few-shot learning techniques to t...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2305.14982v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2305.14982v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Abdelali et al., &quot;LAraBench: Benchmarking Arabic AI with Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2305.14982v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397312192')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="scaling behavior of machine translation with large language models under prompt injection attacks" data-keywords="language model cs.cl" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.09832v1" target="_blank">Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks</a>
                            </h3>
                            <p class="card-authors">Zhifan Sun, Antonio Valerio Miceli-Barone</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4397304320">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Language Models (LLMs) are increasingly becoming the preferred foundation platforms for many Natural Language Processing tasks such as Machine Translation, owing to their quality often comparable to or better than task-specific models, and the simplicity of specifying the task through natural language instructions or in-context examples. Their generality, however, opens them up to subversion by end users who may embed into their requests instructions that cause the model to behave in unauthorized and possibly unsafe ways. In this work we study these Prompt Injection Attacks (PIAs) on mul...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce a new benchmark data set and we discover that on multiple language pairs and injected prompts written in English, larger models under certain conditions may become more susceptible to successful attacks, an instance of the Inverse Scaling phenomenon (McKenzie et al., 2023)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.09832v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.09832v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Sun, and A. V. Miceli-Barone, &quot;Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.09832v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397304320')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="emissions and performance trade-off between small and large language models" data-keywords="ros language model cs.cl cs.ai cs.cy" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2601.08844v1" target="_blank">Emissions and Performance Trade-off Between Small and Large Language Models</a>
                            </h3>
                            <p class="card-authors">Anandita Garg, Uma Gaba, Deepan Muthirayan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4397306912">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The advent of Large Language Models (LLMs) has raised concerns about their enormous carbon footprint, starting with energy-intensive training and continuing through repeated inference. This study investigates the potential of using fine-tuned Small Language Models (SLMs) as a sustainable alternative for predefined tasks. Here, we present a comparative analysis of the performance-emissions trade-off between LLMs and fine-tuned SLMs across selected tasks under Natural Language Processing, Reasoning and Programming. Our results show that in four out of the six selected tasks, SLMs maintained comp...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Here, we present a comparative analysis of the performance-emissions trade-off between LLMs and fine-tuned SLMs across selected tasks under Natural Language Processing, Reasoning and Programming</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2601.08844v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2601.08844v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Garg, U. Gaba, D. Muthirayan, and A. R. Chowdhury, &quot;Emissions and Performance Trade-off Between Small and Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2601.08844v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397306912')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="babysit a language model from scratch: interactive language learning by trials and demonstrations" data-keywords="control language model cs.cl cs.ai" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.13828v2" target="_blank">Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations</a>
                            </h3>
                            <p class="card-authors">Ziqiao Ma, Zekun Wang, Joyce Chai</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Control</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4397314400">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humans are efficient language learners and inherently social creatures. Our language development is largely shaped by our social interactions, for example, the demonstration and feedback from caregivers. Contrary to human language learning, recent advancements in large language models have primarily adopted a non-interactive training paradigm, and refined pre-trained models through feedback afterward. In this work, we explore how corrective feedback from interactions influences neural language acquisition from scratch through systematically controlled experiments, assessing whether it contribu...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce a trial-and-demonstration (TnD) learning framework that incorporates three distinct components: student trials, teacher demonstrations, and a reward conditioned on language competence at various developmental stages</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.13828v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.13828v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Ma, Z. Wang, and J. Chai, &quot;Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.13828v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397314400')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="neuron-level knowledge attribution in large language models" data-keywords="attention ros language model cs.cl cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.12141v4" target="_blank">Neuron-Level Knowledge Attribution in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Zeping Yu, Sophia Ananiadou</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4397304656">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Identifying important neurons for final predictions is essential for understanding the mechanisms of large language models. Due to computational constraints, current attribution techniques struggle to operate at neuron level. In this paper, we propose a static method for pinpointing significant neurons. Compared to seven other methods, our approach demonstrates superior performance across three metrics. Additionally, since most static methods typically only identify &quot;value neurons&quot; directly contributing to the final prediction, we propose a method for identifying &quot;query neurons&quot; which activate...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose a static method for pinpointing significant neurons</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.12141v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.12141v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Yu, and S. Ananiadou, &quot;Neuron-Level Knowledge Attribution in Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.12141v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397304656')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="how do language models learn facts? dynamics, curricula and hallucinations" data-keywords="neural network attention imu language model cs.cl cs.lg" data-themes="E L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.21676v2" target="_blank">How do language models learn facts? Dynamics, curricula and hallucinations</a>
                            </h3>
                            <p class="card-authors">Nicolas Zucchet, J√∂rg Bornschein, Stephanie Chan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Attention</span><span class="keyword-tag">Imu</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4397303216">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models accumulate vast knowledge during pre-training, yet the dynamics governing this acquisition remain poorly understood. This work investigates the learning dynamics of language models on a synthetic factual recall task, uncovering three key findings: First, language models learn in three phases, exhibiting a performance plateau before acquiring precise factual knowledge. Mechanistically, this plateau coincides with the formation of attention-based circuits that support recall. Second, the training data distribution significantly impacts learning dynamics, as imbalanced distr...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.21676v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.21676v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. Zucchet, J. Bornschein, S. Chan, A. Lampinen, R. Pascanu, and S. De, &quot;How do language models learn facts? Dynamics, curricula and hallucinations,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.21676v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397303216')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="behavioral bias of vision-language models: a behavioral finance view" data-keywords="gpt language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.15256v1" target="_blank">Behavioral Bias of Vision-Language Models: A Behavioral Finance View</a>
                            </h3>
                            <p class="card-authors">Yuhang Xiao, Yudi Lin, Ming-Chang Chiu</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4397306144">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Vision-Language Models (LVLMs) evolve rapidly as Large Language Models (LLMs) was equipped with vision modules to create more human-like models. However, we should carefully evaluate their applications in different domains, as they may possess undesired biases. Our work studies the potential behavioral biases of LVLMs from a behavioral finance perspective, an interdisciplinary subject that jointly considers finance and psychology. We propose an end-to-end framework, from data collection to new evaluation metrics, to assess LVLMs&#x27; reasoning capabilities and the dynamic behaviors manifeste...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose an end-to-end framework, from data collection to new evaluation metrics, to assess LVLMs&#x27; reasoning capabilities and the dynamic behaviors manifested in two established human financial behavioral biases: recency bias and authority bias</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.15256v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.15256v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Xiao, Y. Lin, and M. Chiu, &quot;Behavioral Bias of Vision-Language Models: A Behavioral Finance View,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.15256v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397306144')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="factuality challenges in the era of large language models" data-keywords="attention gpt ros language model cs.cl cs.ai cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2310.05189v2" target="_blank">Factuality Challenges in the Era of Large Language Models</a>
                            </h3>
                            <p class="card-authors">Isabelle Augenstein, Timothy Baldwin, Meeyoung Cha et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4397306480">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The emergence of tools based on Large Language Models (LLMs), such as OpenAI&#x27;s ChatGPT, Microsoft&#x27;s Bing Chat, and Google&#x27;s Bard, has garnered immense public attention. These incredibly useful, natural-sounding tools mark significant advances in natural language generation, yet they exhibit a propensity to generate false, erroneous, or misleading content -- commonly referred to as &quot;hallucinations.&quot; Moreover, LLMs can be exploited for malicious applications, such as generating false but credible-sounding content and profiles at scale. This poses a significant challenge to society in terms of th...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2310.05189v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2310.05189v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'I. Augenstein et al., &quot;Factuality Challenges in the Era of Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2310.05189v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397306480')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="can chatgpt be your personal medical assistant?" data-keywords="gpt language model cs.cl cs.si" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.12006v1" target="_blank">Can ChatGPT be Your Personal Medical Assistant?</a>
                            </h3>
                            <p class="card-authors">Md. Rafiul Biswas, Ashhadul Islam, Zubair Shah et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.SI</span></div>
                            <div class="card-details" id="cat-details-4397314448">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The advanced large language model (LLM) ChatGPT has shown its potential in different domains and remains unbeaten due to its characteristics compared to other LLMs. This study aims to evaluate the potential of using a fine-tuned ChatGPT model as a personal medical assistant in the Arabic language. To do so, this study uses publicly available online questions and answering datasets in Arabic language. There are almost 430K questions and answers for 20 disease-specific categories. GPT-3.5-turbo model was fine-tuned with a portion of this dataset. The performance of this fine-tuned model was eval...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.12006v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.12006v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. R. Biswas, A. Islam, Z. Shah, W. Zaghouani, and S. B. Belhaouari, &quot;Can ChatGPT be Your Personal Medical Assistant?,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.12006v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397314448')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="exploring advanced large language models with llmsuite" data-keywords="reinforcement learning transformer gpt language model cs.cl cs.cv" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.12036v2" target="_blank">Exploring Advanced Large Language Models with LLMsuite</a>
                            </h3>
                            <p class="card-authors">Giorgio Roffo</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Transformer</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4397304944">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This tutorial explores the advancements and challenges in the development of Large Language Models (LLMs) such as ChatGPT and Gemini. It addresses inherent limitations like temporal knowledge cutoffs, mathematical inaccuracies, and the generation of incorrect information, proposing solutions like Retrieval Augmented Generation (RAG), Program-Aided Language Models (PAL), and frameworks such as ReAct and LangChain. The integration of these techniques enhances LLM performance and reliability, especially in multi-step reasoning and complex task execution. The paper also covers fine-tuning strategi...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.12036v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.12036v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Roffo, &quot;Exploring Advanced Large Language Models with LLMsuite,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.12036v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397304944')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="large language models and multimodal retrieval for visual word sense disambiguation" data-keywords="transformer language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2310.14025v1" target="_blank">Large Language Models and Multimodal Retrieval for Visual Word Sense Disambiguation</a>
                            </h3>
                            <p class="card-authors">Anastasia Kritharoula, Maria Lymperaiou, Giorgos Stamou</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Transformer</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4397313200">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Visual Word Sense Disambiguation (VWSD) is a novel challenging task with the goal of retrieving an image among a set of candidates, which better represents the meaning of an ambiguous word within a given context. In this paper, we make a substantial step towards unveiling this interesting task by applying a varying set of approaches. Since VWSD is primarily a text-image retrieval task, we explore the latest transformer-based methods for multimodal retrieval. Additionally, we utilize Large Language Models (LLMs) as knowledge bases to enhance the given phrases and resolve ambiguity related to th...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2310.14025v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2310.14025v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Kritharoula, M. Lymperaiou, and G. Stamou, &quot;Large Language Models and Multimodal Retrieval for Visual Word Sense Disambiguation,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2310.14025v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397313200')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="open sesame! universal black box jailbreaking of large language models" data-keywords="language model cs.cl cs.cv cs.ne" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.01446v4" target="_blank">Open Sesame! Universal Black Box Jailbreaking of Large Language Models</a>
                            </h3>
                            <p class="card-authors">Raz Lapid, Ron Langberg, Moshe Sipper</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.CV</span><span class="keyword-tag">CS.NE</span></div>
                            <div class="card-details" id="cat-details-4397304608">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs), designed to provide helpful and safe responses, often rely on alignment techniques to align with user intent and social guidelines. Unfortunately, this alignment can be exploited by malicious actors seeking to manipulate an LLM&#x27;s outputs for unintended purposes. In this paper we introduce a novel approach that employs a genetic algorithm (GA) to manipulate LLMs when model architecture and parameters are inaccessible. The GA attack works by optimizing a universal adversarial prompt that -- when combined with a user&#x27;s query -- disrupts the attacked model&#x27;s alignment...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper we introduce a novel approach that employs a genetic algorithm (GA) to manipulate LLMs when model architecture and parameters are inaccessible</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.01446v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.01446v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Lapid, R. Langberg, and M. Sipper, &quot;Open Sesame! Universal Black Box Jailbreaking of Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.01446v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397304608')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="fs-rag: a frame semantics based approach for improved factual accuracy in large language models" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2406.16167v1" target="_blank">FS-RAG: A Frame Semantics Based Approach for Improved Factual Accuracy in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Harish Tayyar Madabushi</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4397307296">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We present a novel extension to Retrieval Augmented Generation with the goal of mitigating factual inaccuracies in the output of large language models. Specifically, our method draws on the cognitive linguistic theory of frame semantics for the indexing and retrieval of factual information relevant to helping large language models answer queries. We conduct experiments to demonstrate the effectiveness of this method both in terms of retrieval effectiveness and in terms of the relevance of the frames and frame relations automatically generated. Our results show that this novel mechanism of Fram...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present a novel extension to Retrieval Augmented Generation with the goal of mitigating factual inaccuracies in the output of large language models</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2406.16167v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2406.16167v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. T. Madabushi, &quot;FS-RAG: A Frame Semantics Based Approach for Improved Factual Accuracy in Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2406.16167v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397307296')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="large language models have learned to use language" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2512.12447v1" target="_blank">Large language models have learned to use language</a>
                            </h3>
                            <p class="card-authors">Gary Lupyan</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4397315264">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Acknowledging that large language models have learned to use language can open doors to breakthrough language science. Achieving these breakthroughs may require abandoning some long-held ideas about how language knowledge is evaluated and reckoning with the difficult fact that we have entered a post-Turing test era.</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2512.12447v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2512.12447v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Lupyan, &quot;Large language models have learned to use language,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2512.12447v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397315264')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="demystifying instruction mixing for fine-tuning large language models" data-keywords="ros nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.10793v3" target="_blank">Demystifying Instruction Mixing for Fine-tuning Large Language Models</a>
                            </h3>
                            <p class="card-authors">Renxi Wang, Haonan Li, Minghao Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4397306192">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Instruction tuning significantly enhances the performance of large language models (LLMs) across various tasks. However, the procedure to optimizing the mixing of instruction datasets for LLM fine-tuning is still poorly understood. This study categorizes instructions into three primary types: NLP downstream tasks, coding, and general chat. We explore the effects of instruction tuning on different combinations of datasets on LLM performance, and find that certain instruction types are more advantageous for specific applications but can negatively impact other areas. This work provides insights ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.10793v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.10793v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Wang et al., &quot;Demystifying Instruction Mixing for Fine-tuning Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.10793v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397306192')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="precise length control in large language models" data-keywords="control language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.11937v1" target="_blank">Precise Length Control in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Bradley Butcher, Michael O&#x27;Keefe, James Titchener</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Control</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4397313536">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Language Models (LLMs) are increasingly used in production systems, powering applications such as chatbots, summarization, and question answering. Despite their success, controlling the length of their response remains a significant challenge, particularly for tasks requiring structured outputs or specific levels of detail. In this work, we propose a method to adapt pre-trained decoder-only LLMs for precise control of response length. Our approach incorporates a secondary length-difference positional encoding (LDPE) into the input embeddings, which counts down to a user-set response term...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose a method to adapt pre-trained decoder-only LLMs for precise control of response length</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.11937v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.11937v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. Butcher, M. O&#x27;Keefe, and J. Titchener, &quot;Precise Length Control in Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.11937v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397313536')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="response: emergent analogical reasoning in large language models" data-keywords="gpt ros language model cs.cl cs.ai" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2308.16118v2" target="_blank">Response: Emergent analogical reasoning in large language models</a>
                            </h3>
                            <p class="card-authors">Damian Hodel, Jevin West</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4397307104">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In their recent Nature Human Behaviour paper, &quot;Emergent analogical reasoning in large language models,&quot; (Webb, Holyoak, and Lu, 2023) the authors argue that &quot;large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems.&quot; In this response, we provide counterexamples of the letter string analogies. In our tests, GPT-3 fails to solve simplest variations of the original tasks, whereas human performance remains consistently high across all modified versions. Zero-shot reasoning is an extraordinary claim that requires extraord...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2308.16118v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2308.16118v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Hodel, and J. West, &quot;Response: Emergent analogical reasoning in large language models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2308.16118v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397307104')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="using large language models to provide explanatory feedback to human tutors" data-keywords="language model cs.cl cs.ai cs.hc" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2306.15498v1" target="_blank">Using Large Language Models to Provide Explanatory Feedback to Human Tutors</a>
                            </h3>
                            <p class="card-authors">Jionghao Lin, Danielle R. Thomas, Feifei Han et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.HC</span></div>
                            <div class="card-details" id="cat-details-4397304992">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Research demonstrates learners engaging in the process of producing explanations to support their reasoning, can have a positive impact on learning. However, providing learners real-time explanatory feedback often presents challenges related to classification accuracy, particularly in domain-specific environments, containing situationally complex and nuanced responses. We present two approaches for supplying tutors real-time feedback within an online lesson on how to give students effective praise. This work-in-progress demonstrates considerable accuracy in binary classification for corrective...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present two approaches for supplying tutors real-time feedback within an online lesson on how to give students effective praise</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2306.15498v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2306.15498v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Lin et al., &quot;Using Large Language Models to Provide Explanatory Feedback to Human Tutors,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2306.15498v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397304992')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="instruction-tuned large language models for machine translation in the medical domain" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.16440v2" target="_blank">Instruction-tuned Large Language Models for Machine Translation in the Medical Domain</a>
                            </h3>
                            <p class="card-authors">Miguel Rios</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4397314304">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Language Models (LLMs) have shown promising results on machine translation for high resource language pairs and domains. However, in specialised domains (e.g. medical) LLMs have shown lower performance compared to standard neural machine translation models. The consistency in the machine translation of terminology is crucial for users, researchers, and translators in specialised domains. In this study, we compare the performance between baseline LLMs and instruction-tuned LLMs in the medical domain. In addition, we introduce terminology from specialised medical dictionaries into the inst...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In addition, we introduce terminology from specialised medical dictionaries into the instruction formatted datasets for fine-tuning LLMs</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.16440v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.16440v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Rios, &quot;Instruction-tuned Large Language Models for Machine Translation in the Medical Domain,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.16440v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397314304')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="learning from failure: integrating negative examples when fine-tuning large language models as agents" data-keywords="control optimization language model cs.cl" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.11651v2" target="_blank">Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents</a>
                            </h3>
                            <p class="card-authors">Renxi Wang, Haonan Li, Xudong Han et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Control</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4397313584">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools such as search engines. However, LLMs are optimized for language generation instead of tool use during training or alignment, limiting their effectiveness as agents. To resolve this problem, previous work has first collected interaction trajectories between LLMs and environments, using only trajectories that successfully finished the task to fine-tune smaller models, making fine-tuning data scarce and acquiring it both difficult and costly. Discarding failed trajectories also ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We further analyze the inference results and find that our method provides a better trade-off between valuable information and errors in unsuccessful trajectories</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.11651v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.11651v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Wang, H. Li, X. Han, Y. Zhang, and T. Baldwin, &quot;Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.11651v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397313584')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="soft inductive bias approach via explicit reasoning perspectives in inappropriate utterance detection using large language models" data-keywords="attention ros language model cs.cl" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2512.08480v1" target="_blank">Soft Inductive Bias Approach via Explicit Reasoning Perspectives in Inappropriate Utterance Detection Using Large Language Models</a>
                            </h3>
                            <p class="card-authors">Ju-Young Kim, Ji-Hong Park, Se-Yeon Lee et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398402240">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recent incidents in certain online games and communities, where anonymity is guaranteed, show that unchecked inappropriate remarks frequently escalate into verbal abuse and even criminal behavior, raising significant social concerns. Consequently, there is a growing need for research on techniques that can detect inappropriate utterances within conversational texts to help build a safer communication environment. Although large-scale language models trained on Korean corpora and chain-of-thought reasoning have recently gained attention, research applying these approaches to inappropriate utter...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this study, we propose a soft inductive bias approach that explicitly defines reasoning perspectives to guide the inference process, thereby promoting rational decision-making and preventing errors that may arise during reasoning</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2512.08480v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2512.08480v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Kim, J. Park, S. Lee, S. Park, and G. Kim, &quot;Soft Inductive Bias Approach via Explicit Reasoning Perspectives in Inappropriate Utterance Detection Using Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2512.08480v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398402240')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="detecting mode collapse in language models via narration" data-keywords="reinforcement learning gpt simulation imu language model cs.cl cs.ai" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.04477v1" target="_blank">Detecting Mode Collapse in Language Models via Narration</a>
                            </h3>
                            <p class="card-authors">Sil Hamilton</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Simulation</span><span class="keyword-tag">Imu</span></div>
                            <div class="card-details" id="cat-details-4398403392">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">No two authors write alike. Personal flourishes invoked in written narratives, from lexicon to rhetorical devices, imply a particular author--what literary theorists label the implied or virtual author; distinct from the real author or narrator of a text. Early large language models trained on unfiltered training sets drawn from a variety of discordant sources yielded incoherent personalities, problematic for conversational tasks but proving useful for sampling literature from multiple perspectives. Successes in alignment research in recent years have allowed researchers to impose subjectively...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Our method and results are significant for researchers seeking to employ language models in sociological simulations.</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.04477v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.04477v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Hamilton, &quot;Detecting Mode Collapse in Language Models via Narration,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.04477v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398403392')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="investigating retrieval-augmented generation in quranic studies: a study of 13 open-source large language models" data-keywords="language model cs.cl cs.ai cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.16581v1" target="_blank">Investigating Retrieval-Augmented Generation in Quranic Studies: A Study of 13 Open-Source Large Language Models</a>
                            </h3>
                            <p class="card-authors">Zahra Khalila, Arbi Haza Nasution, Winda Monika et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.LG</span></div>
                            <div class="card-details" id="cat-details-4398401904">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Accurate and contextually faithful responses are critical when applying large language models (LLMs) to sensitive and domain-specific tasks, such as answering queries related to quranic studies. General-purpose LLMs often struggle with hallucinations, where generated responses deviate from authoritative sources, raising concerns about their reliability in religious contexts. This challenge highlights the need for systems that can integrate domain-specific knowledge while maintaining response accuracy, relevance, and faithfulness. In this study, we investigate 13 open-source LLMs categorized in...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.16581v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.16581v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Khalila et al., &quot;Investigating Retrieval-Augmented Generation in Quranic Studies: A Study of 13 Open-Source Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.16581v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398401904')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="knowledge-driven agentic scientific corpus distillation framework for biomedical large language models training" data-keywords="gpt multi-agent language model cs.cl cs.ai q-bio.qm" data-themes="E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Multi-Agent Systems</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.19565v3" target="_blank">Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training</a>
                            </h3>
                            <p class="card-authors">Meng Xiao, Xunxin Cai, Qingqing Long et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Multi-Agent</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4397313296">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Corpus distillation for biomedical large language models (LLMs) seeks to address the pressing challenge of insufficient quantity and quality in open-source annotated scientific corpora, which remains a bottleneck for effective LLM training in biomedical research. This paper proposes a knowledge-driven, agentic framework for scientific corpus distillation, tailored explicitly for LLM training in the biomedical domain, addressing the challenge posed by the complex hierarchy of biomedical knowledge. Central to our approach is a collaborative multi-agent architecture, where specialized agents, eac...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Central to our approach is a collaborative multi-agent architecture, where specialized agents, each guided by the Medical Subject Headings (MeSH) hierarchy, work in concert to autonomously extract, synthesize, and self-evaluate high-quality textual data from vast scientific literature</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.19565v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.19565v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Xiao, X. Cai, Q. Long, C. Wang, Y. Zhou, and H. Zhu, &quot;Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.19565v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397313296')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="wizardlm: empowering large pre-trained language models to follow complex instructions" data-keywords="gpt nlp language model cs.cl cs.ai" data-themes="E L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2304.12244v3" target="_blank">WizardLM: Empowering large pre-trained language models to follow complex instructions</a>
                            </h3>
                            <p class="card-authors">Can Xu, Qingfeng Sun, Kai Zheng et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398401664">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data t...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2304.12244v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2304.12244v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Xu et al., &quot;WizardLM: Empowering large pre-trained language models to follow complex instructions,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2304.12244v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398401664')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="pallm: evaluating and enhancing palliative care conversations with large language models" data-keywords="gpt imu nlp language model cs.cl cs.hc" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.15188v2" target="_blank">PALLM: Evaluating and Enhancing PALLiative Care Conversations with Large Language Models</a>
                            </h3>
                            <p class="card-authors">Zhiyuan Wang, Fangxu Yuan, Virginia LeBaron et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Imu</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4398402768">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Effective patient-provider communication is crucial in clinical care, directly impacting patient outcomes and quality of life. Traditional evaluation methods, such as human ratings, patient feedback, and provider self-assessments, are often limited by high costs and scalability issues. Although existing natural language processing (NLP) techniques show promise, they struggle with the nuances of clinical communication and require sensitive clinical data for training, reducing their effectiveness in real-world applications. Emerging large language models (LLMs) offer a new approach to assessing ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.15188v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.15188v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Wang, F. Yuan, V. LeBaron, T. Flickinger, and L. E. Barnes, &quot;PALLM: Evaluating and Enhancing PALLiative Care Conversations with Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.15188v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398402768')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="efficacy of large language models in systematic reviews" data-keywords="gpt language model cs.cl cs.lg" data-themes="M E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.04646v2" target="_blank">Efficacy of Large Language Models in Systematic Reviews</a>
                            </h3>
                            <p class="card-authors">Aaditya Shah, Shridhar Mehendale, Siddha Kanthi</p>
                            <div class="theme-tags"><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.LG</span></div>
                            <div class="card-details" id="cat-details-4398402384">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This study investigates the effectiveness of Large Language Models (LLMs) in interpreting existing literature through a systematic review of the relationship between Environmental, Social, and Governance (ESG) factors and financial performance. The primary objective is to assess how LLMs can replicate a systematic review on a corpus of ESG-focused papers. We compiled and hand-coded a database of 88 relevant papers published from March 2020 to May 2024. Additionally, we used a set of 238 papers from a previous systematic review of ESG literature from January 2015 to February 2020. We evaluated ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.04646v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.04646v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Shah, S. Mehendale, and S. Kanthi, &quot;Efficacy of Large Language Models in Systematic Reviews,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.04646v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398402384')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="thames: an end-to-end tool for hallucination mitigation and evaluation in large language models" data-keywords="gpt ros language model cs.cl" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.11353v3" target="_blank">THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Mengfei Liang, Archish Arun, Zekun Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398401856">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Hallucination, the generation of factually incorrect content, is a growing challenge in Large Language Models (LLMs). Existing detection and mitigation methods are often isolated and insufficient for domain-specific needs, lacking a standardized pipeline. This paper introduces THaMES (Tool for Hallucination Mitigations and EvaluationS), an integrated framework and library addressing this gap. THaMES offers an end-to-end solution for evaluating and mitigating hallucinations in LLMs, featuring automated test set generation, multifaceted benchmarking, and adaptable mitigation strategies. It autom...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.11353v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.11353v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Liang et al., &quot;THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.11353v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398401856')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="head-specific intervention can induce misaligned ai coordination in large language models" data-keywords="attention coordination control language model cs.cl cs.ai" data-themes="S I E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.05945v3" target="_blank">Head-Specific Intervention Can Induce Misaligned AI Coordination in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Paul Darm, Annalisa Riccardi</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Coordination</span><span class="keyword-tag">Control</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4398403056">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Robust alignment guardrails for large language models (LLMs) are becoming increasingly important with their widespread application. In contrast to previous studies, we demonstrate that inference-time activation interventions can bypass safety alignments and effectively steer model generations towards harmful AI coordination. Our method applies fine-grained interventions at specific attention heads, which we identify by probing each head in a simple binary choice task. We then show that interventions on these heads generalise to the open-ended generation setting, effectively circumventing safet...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In contrast to previous studies, we demonstrate that inference-time activation interventions can bypass safety alignments and effectively steer model generations towards harmful AI coordination</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.05945v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.05945v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Darm, and A. Riccardi, &quot;Head-Specific Intervention Can Induce Misaligned AI Coordination in Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.05945v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398403056')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="understanding network behaviors through natural language question-answering" data-keywords="ros language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2510.21894v1" target="_blank">Understanding Network Behaviors through Natural Language Question-Answering</a>
                            </h3>
                            <p class="card-authors">Mingzhe Xing, Chang Tian, Jianan Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4398402288">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Modern large-scale networks introduce significant complexity in understanding network behaviors, increasing the risk of misconfiguration. Prior work proposed to understand network behaviors by mining network configurations, typically relying on domain-specific languages interfaced with formal models. While effective, they suffer from a steep learning curve and limited flexibility. In contrast, natural language (NL) offers a more accessible and interpretable interface, motivating recent research on NL-guided network behavior understanding. Recent advances in large language models (LLMs) further...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To tackle the above challenges, we propose NetMind, a novel framework for querying networks using NL</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2510.21894v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2510.21894v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Xing et al., &quot;Understanding Network Behaviors through Natural Language Question-Answering,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2510.21894v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398402288')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="prompting and fine-tuning open-sourced large language models for stance classification" data-keywords="ros language model cs.cl cs.ai" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.13734v2" target="_blank">Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification</a>
                            </h3>
                            <p class="card-authors">Iain J. Cruickshank, Lynnette Hui Xian Ng</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4398402432">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Stance classification, the task of predicting the viewpoint of an author on a subject of interest, has long been a focal point of research in domains ranging from social science to machine learning. Current stance detection methods rely predominantly on manual annotation of sentences, followed by training a supervised machine learning model. However, this manual annotation process requires laborious annotation effort, and thus hampers its potential to generalize across different contexts. In this work, we investigate the use of Large Language Models (LLMs) as a stance detection methodology tha...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.13734v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.13734v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'I. J. Cruickshank, and L. H. X. Ng, &quot;Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.13734v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398402432')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="can large language models (or humans) disentangle text?" data-keywords="language model cs.cl" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.16584v2" target="_blank">Can Large Language Models (or Humans) Disentangle Text?</a>
                            </h3>
                            <p class="card-authors">Nicolas Audinet de Pieuchon, Adel Daoud, Connor Thomas Jerzak et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398404160">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We investigate the potential of large language models (LLMs) to disentangle text variables--to remove the textual traces of an undesired forbidden variable in a task sometimes known as text distillation and closely related to the fairness in AI and causal inference literature. We employ a range of various LLM approaches in an attempt to disentangle text by identifying and removing information about a target variable while preserving other relevant signals. We show that in the strong test of removing sentiment, the statistical association between the processed text and sentiment is still detect...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.16584v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.16584v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. A. d. Pieuchon, A. Daoud, C. T. Jerzak, M. Johansson, and R. Johansson, &quot;Can Large Language Models (or Humans) Disentangle Text?,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.16584v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398404160')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="speaker attribution in german parliamentary debates with qlora-adapted large language models" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.09902v2" target="_blank">Speaker attribution in German parliamentary debates with QLoRA-adapted large language models</a>
                            </h3>
                            <p class="card-authors">Tobias Bornheim, Niklas Grieger, Patrick Gustav Blaneck et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398403008">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The growing body of political texts opens up new opportunities for rich insights into political dynamics and ideologies but also increases the workload for manual analysis. Automated speaker attribution, which detects who said what to whom in a speech event and is closely related to semantic role labeling, is an important processing step for computational text analysis. We study the potential of the large language model family Llama 2 to automate speaker attribution in German parliamentary debates from 2017-2021. We fine-tune Llama 2 with QLoRA, an efficient training strategy, and observe our ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We fine-tune Llama 2 with QLoRA, an efficient training strategy, and observe our approach to achieve competitive performance in the GermEval 2023 Shared Task On Speaker Attribution in German News Articles and Parliamentary Debates</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.09902v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.09902v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. Bornheim, N. Grieger, P. G. Blaneck, and S. Bialonski, &quot;Speaker attribution in German parliamentary debates with QLoRA-adapted large language models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.09902v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398403008')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="fanal -- financial activity news alerting language modeling framework" data-keywords="bert gpt optimization language model cs.cl cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.03527v1" target="_blank">FANAL -- Financial Activity News Alerting Language Modeling Framework</a>
                            </h3>
                            <p class="card-authors">Urjitkumar Patel, Fang-Chun Yeh, Chinmay Gondhalekar et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Bert</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4398402624">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In the rapidly evolving financial sector, the accurate and timely interpretation of market news is essential for stakeholders needing to navigate unpredictable events. This paper introduces FANAL (Financial Activity News Alerting Language Modeling Framework), a specialized BERT-based framework engineered for real-time financial event detection and analysis, categorizing news into twelve distinct financial categories. FANAL leverages silver-labeled data processed through XGBoost and employs advanced fine-tuning techniques, alongside ORBERT (Odds Ratio BERT), a novel variant of BERT fine-tuned w...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.03527v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.03527v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'U. Patel, F. Yeh, C. Gondhalekar, and H. Nalluri, &quot;FANAL -- Financial Activity News Alerting Language Modeling Framework,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.03527v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398402624')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="a zero-shot and few-shot study of instruction-finetuned large language models applied to clinical and biomedical tasks" data-keywords="bert gpt nlp language model cs.cl cs.ai cs.lg" data-themes="S I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2307.12114v3" target="_blank">A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks</a>
                            </h3>
                            <p class="card-authors">Yanis Labrak, Mickael Rouvier, Richard Dufour</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Bert</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4398404016">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We evaluate four state-of-the-art instruction-tuned large language models (LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13 real-world clinical and biomedical natural language processing (NLP) tasks in English, such as named-entity recognition (NER), question-answering (QA), relation extraction (RE), etc. Our overall results demonstrate that the evaluated LLMs begin to approach performance of state-of-the-art models in zero- and few-shot scenarios for most tasks, and particularly well for the QA task, even though they have never seen examples from these tasks before. Ho...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2307.12114v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2307.12114v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Labrak, M. Rouvier, and R. Dufour, &quot;A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2307.12114v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398404016')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="measuring the inconsistency of large language models in preferential ranking" data-keywords="language model cs.cl" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.08851v1" target="_blank">Measuring the Inconsistency of Large Language Models in Preferential Ranking</a>
                            </h3>
                            <p class="card-authors">Xiutian Zhao, Ke Wang, Wei Peng</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398402096">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Despite large language models&#x27; (LLMs) recent advancements, their bias and hallucination issues persist, and their ability to offer consistent preferential rankings remains underexplored. This study investigates the capacity of LLMs to provide consistent ordinal preferences, a crucial aspect in scenarios with dense decision space or lacking absolute answers. We introduce a formalization of consistency based on order theory, outlining criteria such as transitivity, asymmetry, reversibility, and independence from irrelevant alternatives. Our diagnostic experiments on selected state-of-the-art LLM...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce a formalization of consistency based on order theory, outlining criteria such as transitivity, asymmetry, reversibility, and independence from irrelevant alternatives</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.08851v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.08851v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. Zhao, K. Wang, and W. Peng, &quot;Measuring the Inconsistency of Large Language Models in Preferential Ranking,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.08851v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398402096')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="overview of the first workshop on language models for low-resource languages (loreslm 2025)" data-keywords="nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.16365v1" target="_blank">Overview of the First Workshop on Language Models for Low-Resource Languages (LoResLM 2025)</a>
                            </h3>
                            <p class="card-authors">Hansi Hettiarachchi, Tharindu Ranasinghe, Paul Rayson et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4398403632">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The first Workshop on Language Models for Low-Resource Languages (LoResLM 2025) was held in conjunction with the 31st International Conference on Computational Linguistics (COLING 2025) in Abu Dhabi, United Arab Emirates. This workshop mainly aimed to provide a forum for researchers to share and discuss their ongoing work on language models (LMs) focusing on low-resource languages, following the recent advancements in neural language models and their linguistic biases towards high-resource languages. LoResLM 2025 attracted notable interest from the natural language processing (NLP) community, ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.16365v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.16365v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Hettiarachchi et al., &quot;Overview of the First Workshop on Language Models for Low-Resource Languages (LoResLM 2025),&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.16365v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398403632')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="commander-gpt: fully unleashing the sarcasm detection capability of multi-modal large language models" data-keywords="attention gpt nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.18681v3" target="_blank">Commander-GPT: Fully Unleashing the Sarcasm Detection Capability of Multi-Modal Large Language Models</a>
                            </h3>
                            <p class="card-authors">Yazhou Zhang, Chunwang Zou, Bo Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4398403152">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Sarcasm detection, as a crucial research direction in the field of Natural Language Processing (NLP), has attracted widespread attention. Traditional sarcasm detection tasks have typically focused on single-modal approaches (e.g., text), but due to the implicit and subtle nature of sarcasm, such methods often fail to yield satisfactory results. In recent years, researchers have shifted the focus of sarcasm detection to multi-modal approaches. However, effectively leveraging multi-modal information to accurately identify sarcastic content remains a challenge that warrants further exploration. L...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Leveraging the powerful integrated processing capabilities of Multi-Modal Large Language Models (MLLMs) for various information sources, we propose an innovative multi-modal Commander-GPT framework</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.18681v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.18681v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Zhang, C. Zou, B. Wang, and J. Qin, &quot;Commander-GPT: Fully Unleashing the Sarcasm Detection Capability of Multi-Modal Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.18681v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398403152')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="how important is tokenization in french medical masked language models?" data-keywords="ros nlp language model cs.cl cs.ai cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.15010v2" target="_blank">How Important Is Tokenization in French Medical Masked Language Models?</a>
                            </h3>
                            <p class="card-authors">Yanis Labrak, Adrien Bazoge, Beatrice Daille et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398404400">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Subword tokenization has become the prevailing standard in the field of natural language processing (NLP) over recent years, primarily due to the widespread utilization of pre-trained language models. This shift began with Byte-Pair Encoding (BPE) and was later followed by the adoption of SentencePiece and WordPiece. While subword tokenization consistently outperforms character and word-level tokenization, the precise factors contributing to its success remain unclear. Key aspects such as the optimal segmentation granularity for diverse tasks and languages, the influence of data sources on tok...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.15010v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.15010v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Labrak, A. Bazoge, B. Daille, M. Rouvier, and R. Dufour, &quot;How Important Is Tokenization in French Medical Masked Language Models?,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.15010v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398404400')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="evaluating class membership relations in knowledge graphs using large language models" data-keywords="gpt language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2404.17000v1" target="_blank">Evaluating Class Membership Relations in Knowledge Graphs using Large Language Models</a>
                            </h3>
                            <p class="card-authors">Bradley P. Allen, Paul T. Groth</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4398401952">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">A backbone of knowledge graphs are their class membership relations, which assign entities to a given class. As part of the knowledge engineering process, we propose a new method for evaluating the quality of these relations by processing descriptions of a given entity and class using a zero-shot chain-of-thought classifier that uses a natural language intensional definition of a class. We evaluate the method using two publicly available knowledge graphs, Wikidata and CaLiGraph, and 7 large language models. Using the gpt-4-0125-preview large language model, the method&#x27;s classification performa...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">As part of the knowledge engineering process, we propose a new method for evaluating the quality of these relations by processing descriptions of a given entity and class using a zero-shot chain-of-thought classifier that uses a natural language intensional definition of a class</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2404.17000v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2404.17000v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. P. Allen, and P. T. Groth, &quot;Evaluating Class Membership Relations in Knowledge Graphs using Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2404.17000v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398401952')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="social bias in large language models for bangla: an empirical study on gender and religious bias" data-keywords="nlp language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.03536v3" target="_blank">Social Bias in Large Language Models For Bangla: An Empirical Study on Gender and Religious Bias</a>
                            </h3>
                            <p class="card-authors">Jayanta Sadhu, Maneesha Rani Saha, Rifat Shahriyar</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398404352">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The rapid growth of Large Language Models (LLMs) has put forward the study of biases as a crucial field. It is important to assess the influence of different types of biases embedded in LLMs to ensure fair use in sensitive fields. Although there have been extensive works on bias assessment in English, such efforts are rare and scarce for a major language like Bangla. In this work, we examine two types of social biases in LLM generated outputs for Bangla language. Our main contributions in this work are: (1) bias studies on two different social biases for Bangla, (2) a curated dataset for bias ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.03536v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.03536v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Sadhu, M. R. Saha, and R. Shahriyar, &quot;Social Bias in Large Language Models For Bangla: An Empirical Study on Gender and Religious Bias,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.03536v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398404352')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="llm-as-a-judge: rapid evaluation of legal document recommendation for retrieval-augmented generation" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.12382v1" target="_blank">LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for Retrieval-Augmented Generation</a>
                            </h3>
                            <p class="card-authors">Anu Pradhan, Alexandra Ortan, Apurv Verma et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398402912">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The evaluation bottleneck in recommendation systems has become particularly acute with the rise of Generative AI, where traditional metrics fall short of capturing nuanced quality dimensions that matter in specialized domains like legal research. Can we trust Large Language Models to serve as reliable judges of their own kind? This paper investigates LLM-as-a-Judge as a principled approach to evaluating Retrieval-Augmented Generation systems in legal contexts, where the stakes of recommendation quality are exceptionally high.
  We tackle two fundamental questions that determine practical viabi...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.12382v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.12382v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Pradhan, A. Ortan, A. Verma, and M. Seshadri, &quot;LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for Retrieval-Augmented Generation,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.12382v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398402912')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2022" data-category="cs.cl" data-title="self-generated in-context learning: leveraging auto-regressive language models as a demonstration generator" data-keywords="language model cs.cl" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2022</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2206.08082v1" target="_blank">Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator</a>
                            </h3>
                            <p class="card-authors">Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398404736">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large-scale pre-trained language models (PLMs) are well-known for being capable of solving a task simply by conditioning a few input-label pairs dubbed demonstrations on a prompt without being explicitly tuned for the desired downstream task. Such a process (i.e., in-context learning), however, naturally leads to high reliance on the demonstrations which are usually selected from external datasets. In this paper, we propose self-generated in-context learning (SG-ICL), which generates demonstrations for in-context learning from PLM itself to minimize the reliance on the external demonstration. ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose self-generated in-context learning (SG-ICL), which generates demonstrations for in-context learning from PLM itself to minimize the reliance on the external demonstration</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2206.08082v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2206.08082v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. J. Kim, H. Cho, J. Kim, T. Kim, K. M. Yoo, and S. Lee, &quot;Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator,&quot; arXiv, 2022. [Online]. Available: http://arxiv.org/abs/2206.08082v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398404736')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="multi-model synthetic training for mission-critical small language models" data-keywords="gpt ros language model cs.cl cs.ai cs.lg" data-themes="S I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.13047v1" target="_blank">Multi-Model Synthetic Training for Mission-Critical Small Language Models</a>
                            </h3>
                            <p class="card-authors">Nolan Platt, Pragyansmita Nayak</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4397311472">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Language Models (LLMs) have demonstrated remarkable capabilities across many domains, yet their appli- cation to specialized fields remains constrained by the scarcity and complexity of domain-specific training data. We present a novel approach that achieves a 261x cost reduction for maritime intelligence by using LLMs as one-time teachers rather than using them directly for inference. Our method transforms 3.2 billion Automatic Identification System (AIS) vessel tracking records into 21,543 synthetic question and answer pairs through multi-model generation (GPT-4o and o3-mini), preventi...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present a novel approach that achieves a 261x cost reduction for maritime intelligence by using LLMs as one-time teachers rather than using them directly for inference</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.13047v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.13047v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. Platt, and P. Nayak, &quot;Multi-Model Synthetic Training for Mission-Critical Small Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.13047v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397311472')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="clinical information extraction for low-resource languages with few-shot learning using pre-trained language models and prompting" data-keywords="language model cs.cl cs.ai cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.13369v2" target="_blank">Clinical information extraction for Low-resource languages with Few-shot learning using Pre-trained language models and Prompting</a>
                            </h3>
                            <p class="card-authors">Phillip Richter-Pechanski, Philipp Wiesenbach, Dominic M. Schwab et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.LG</span></div>
                            <div class="card-details" id="cat-details-4398404112">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Automatic extraction of medical information from clinical documents poses several challenges: high costs of required clinical expertise, limited interpretability of model predictions, restricted computational resources and privacy regulations. Recent advances in domain-adaptation and prompting methods showed promising results with minimal training data using lightweight masked language models, which are suited for well-established interpretability methods. We are first to present a systematic evaluation of these methods in a low-resource setting, by performing multi-class section classificatio...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We demonstrate that a lightweight, domain-adapted pretrained model, prompted with just 20 shots, outperforms a traditional classification model by 30.5% accuracy</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.13369v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.13369v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Richter-Pechanski et al., &quot;Clinical information extraction for Low-resource languages with Few-shot learning using Pre-trained language models and Prompting,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.13369v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398404112')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="towards typologically aware rescoring to mitigate unfaithfulness in lower-resource languages" data-keywords="bert ros language model cs.cl" data-themes="M E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.17664v2" target="_blank">Towards Typologically Aware Rescoring to Mitigate Unfaithfulness in Lower-Resource Languages</a>
                            </h3>
                            <p class="card-authors">Tsan Tsai Chan, Xin Tong, Thi Thu Uyen Hoang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Bert</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398401472">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Multilingual large language models (LLMs) are known to more frequently generate non-faithful output in resource-constrained languages (Guerreiro et al., 2023 - arXiv:2303.16104), potentially because these typologically diverse languages are underrepresented in their training data. To mitigate unfaithfulness in such settings, we propose using computationally light auxiliary models to rescore the outputs of larger architectures. As proof of the feasibility of such an approach, we show that monolingual 4-layer BERT models pretrained from scratch on less than 700 MB of data without fine-tuning are...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To mitigate unfaithfulness in such settings, we propose using computationally light auxiliary models to rescore the outputs of larger architectures</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.17664v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.17664v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. T. Chan, X. Tong, T. T. U. Hoang, B. Tepnadze, and W. Stempniak, &quot;Towards Typologically Aware Rescoring to Mitigate Unfaithfulness in Lower-Resource Languages,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.17664v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398401472')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="wizardcoder: empowering code large language models with evol-instruct" data-keywords="nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2306.08568v2" target="_blank">WizardCoder: Empowering Code Large Language Models with Evol-Instruct</a>
                            </h3>
                            <p class="card-authors">Ziyang Luo, Can Xu, Pu Zhao et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4398404928">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2306.08568v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2306.08568v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Luo et al., &quot;WizardCoder: Empowering Code Large Language Models with Evol-Instruct,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2306.08568v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398404928')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2026" data-category="cs.cl" data-title="quantifying non deterministic drift in large language models" data-keywords="gpt control ros language model cs.cl cs.ai" data-themes="S I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-new">New</span></div>
                            <span class="card-source">arXiv ¬∑ 2026</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2601.19934v1" target="_blank">Quantifying non deterministic drift in large language models</a>
                            </h3>
                            <p class="card-authors">Claire Nicholson</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Control</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4398405456">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) are widely used for tasks ranging from summarisation to decision support. In practice, identical prompts do not always produce identical outputs, even when temperature and other decoding parameters are fixed. In this work, we conduct repeated-run experiments to empirically quantify baseline behavioural drift, defined as output variability observed when the same prompt is issued multiple times under operator-free conditions. We evaluate two publicly accessible models, gpt-4o-mini and llama3.1-8b, across five prompt categories using exact repeats, perturbed inputs, a...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2601.19934v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2601.19934v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Nicholson, &quot;Quantifying non deterministic drift in large language models,&quot; arXiv, 2026. [Online]. Available: http://arxiv.org/abs/2601.19934v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398405456')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="causal reasoning in large language models: a knowledge graph approach" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.11588v1" target="_blank">Causal Reasoning in Large Language Models: A Knowledge Graph Approach</a>
                            </h3>
                            <p class="card-authors">Yejin Kim, Eojin Kang, Juae Kim et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398405552">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) typically improve performance by either retrieving semantically similar information, or enhancing reasoning abilities through structured prompts like chain-of-thought. While both strategies are considered crucial, it remains unclear which has a greater impact on model performance or whether a combination of both is necessary. This paper answers this question by proposing a knowledge graph (KG)-based random-walk reasoning approach that leverages causal relationships. We conduct experiments on the commonsense question answering task that is based on a KG. The KG inhe...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.11588v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.11588v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Kim, E. Kang, J. Kim, and H. H. Huang, &quot;Causal Reasoning in Large Language Models: A Knowledge Graph Approach,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.11588v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398405552')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="pediatricsgpt: large language models as chinese medical assistants for pediatric applications" data-keywords="gpt optimization ros language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.19266v4" target="_blank">PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications</a>
                            </h3>
                            <p class="card-authors">Dingkang Yang, Jinjie Wei, Dongling Xiao et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4398405888">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Developing intelligent pediatric consultation systems offers promising prospects for improving diagnostic efficiency, especially in China, where healthcare resources are scarce. Despite recent advances in Large Language Models (LLMs) for Chinese medicine, their performance is sub-optimal in pediatric applications due to inadequate instruction data and vulnerable training procedures. To address the above issues, this paper builds PedCorpus, a high-quality dataset of over 300,000 multi-task instructions from pediatric textbooks, guidelines, and knowledge graph resources to fulfil diverse diagnos...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Upon well-designed PedCorpus, we propose PediatricsGPT, the first Chinese pediatric LLM assistant built on a systematic and robust training pipeline</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.19266v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.19266v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Yang et al., &quot;PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.19266v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398405888')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2026" data-category="cs.cl" data-title="sharp: social harm analysis via risk profiles for measuring inequities in large language models" data-keywords="ros language model cs.cl cs.ai" data-themes="E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-new">New</span></div>
                            <span class="card-source">arXiv ¬∑ 2026</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2601.21235v1" target="_blank">SHARP: Social Harm Analysis via Risk Profiles for Measuring Inequities in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Alok Abhishek, Tushar Bandopadhyay, Lisa Erickson</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4398402048">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) are increasingly deployed in high-stakes domains, where rare but severe failures can result in irreversible harm. However, prevailing evaluation benchmarks often reduce complex social risk to mean-centered scalar scores, thereby obscuring distributional structure, cross-dimensional interactions, and worst-case behavior. This paper introduces Social Harm Analysis via Risk Profiles (SHARP), a framework for multidimensional, distribution-aware evaluation of social harm. SHARP models harm as a multivariate random variable and integrates explicit decomposition into bias...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2601.21235v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2601.21235v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Abhishek, T. Bandopadhyay, and L. Erickson, &quot;SHARP: Social Harm Analysis via Risk Profiles for Measuring Inequities in Large Language Models,&quot; arXiv, 2026. [Online]. Available: http://arxiv.org/abs/2601.21235v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398402048')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2022" data-category="cs.cl" data-title="gpt-neox-20b: an open-source autoregressive language model" data-keywords="gpt language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2022</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2204.06745v1" target="_blank">GPT-NeoX-20B: An Open-Source Autoregressive Language Model</a>
                            </h3>
                            <p class="card-authors">Sid Black, Stella Biderman, Eric Hallahan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398406368">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe \model{}&#x27;s architecture and training and evaluate its performance on a range of language-understanding, mathematics, and knowledge-based tasks. We find that GPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more i...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2204.06745v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2204.06745v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Black et al., &quot;GPT-NeoX-20B: An Open-Source Autoregressive Language Model,&quot; arXiv, 2022. [Online]. Available: http://arxiv.org/abs/2204.06745v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398406368')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="beyond data quantity: key factors driving performance in multilingual language models" data-keywords="ros language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.12500v1" target="_blank">Beyond Data Quantity: Key Factors Driving Performance in Multilingual Language Models</a>
                            </h3>
                            <p class="card-authors">Sina Bagheri Nezhad, Ameeta Agrawal, Rhitabrat Pokharel</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4398403488">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Multilingual language models (MLLMs) are crucial for handling text across various languages, yet they often show performance disparities due to differences in resource availability and linguistic characteristics. While the impact of pre-train data percentage and model size on performance is well-known, our study reveals additional critical factors that significantly influence MLLM effectiveness. Analyzing a wide range of features, including geographical, linguistic, and resource-related aspects, we focus on the SIB-200 dataset for classification and the Flores-200 dataset for machine translati...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.12500v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.12500v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. B. Nezhad, A. Agrawal, and R. Pokharel, &quot;Beyond Data Quantity: Key Factors Driving Performance in Multilingual Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.12500v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398403488')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="detection of personal data in structured datasets using a large language model" data-keywords="gpt ros language model cs.cl" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.22305v1" target="_blank">Detection of Personal Data in Structured Datasets Using a Large Language Model</a>
                            </h3>
                            <p class="card-authors">Albert Agisha Ntwali, Luca R√ºck, Martin Heckmann</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398406272">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We propose a novel approach for detecting personal data in structured datasets, leveraging GPT-4o, a state-of-the-art Large Language Model. A key innovation of our method is the incorporation of contextual information: in addition to a feature&#x27;s name and values, we utilize information from other feature names within the dataset as well as the dataset description. We compare our approach to alternative methods, including Microsoft Presidio and CASSED, evaluating them on multiple datasets: DeSSI, a large synthetic dataset, datasets we collected from Kaggle and OpenML as well as MIMIC-Demo-Ext, a...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a novel approach for detecting personal data in structured datasets, leveraging GPT-4o, a state-of-the-art Large Language Model</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.22305v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.22305v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. A. Ntwali, L. R√ºck, and M. Heckmann, &quot;Detection of Personal Data in Structured Datasets Using a Large Language Model,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.22305v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398406272')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="a comprehensive survey of scientific large language models and their applications in scientific discovery" data-keywords="ros language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2406.10833v3" target="_blank">A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery</a>
                            </h3>
                            <p class="card-authors">Yu Zhang, Xiusi Chen, Bowen Jin et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398406800">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In many scientific fields, large language models (LLMs) have revolutionized the way text and other modalities of data (e.g., molecules and proteins) are handled, achieving superior performance in various applications and augmenting the scientific discovery process. Nevertheless, previous surveys on scientific LLMs often concentrate on one or two fields or a single modality. In this paper, we aim to provide a more holistic view of the research landscape by unveiling cross-field and cross-modal connections between scientific LLMs regarding their architectures and pre-training techniques. To this...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2406.10833v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2406.10833v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Zhang et al., &quot;A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2406.10833v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398406800')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2022" data-category="cs.cl" data-title="exploring the value of pre-trained language models for clinical named entity recognition" data-keywords="transformer bert nlp language model cs.cl cs.ai cs.lg" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2022</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2210.12770v4" target="_blank">Exploring the Value of Pre-trained Language Models for Clinical Named Entity Recognition</a>
                            </h3>
                            <p class="card-authors">Samuel Belkadi, Lifeng Han, Yuping Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Transformer</span><span class="keyword-tag">Bert</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4398406080">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The practice of fine-tuning Pre-trained Language Models (PLMs) from general or domain-specific data to a specific task with limited resources, has gained popularity within the field of natural language processing (NLP). In this work, we re-visit this assumption and carry out an investigation in clinical NLP, specifically Named Entity Recognition on drugs and their related attributes. We compare Transformer models that are trained from scratch to fine-tuned BERT-based LLMs namely BERT, BioBERT, and ClinicalBERT. Furthermore, we examine the impact of an additional CRF layer on such models to enc...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2210.12770v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2210.12770v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Belkadi, L. Han, Y. Wu, and G. Nenadic, &quot;Exploring the Value of Pre-trained Language Models for Clinical Named Entity Recognition,&quot; arXiv, 2022. [Online]. Available: http://arxiv.org/abs/2210.12770v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398406080')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="a survey of large language models for arabic language and its dialects" data-keywords="ros language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.20238v2" target="_blank">A Survey of Large Language Models for Arabic Language and its Dialects</a>
                            </h3>
                            <p class="card-authors">Malak Mashaabi, Shahad Al-Khalifa, Hend Al-Khalifa</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4398405072">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This survey offers a comprehensive overview of Large Language Models (LLMs) designed for Arabic language and its dialects. It covers key architectures, including encoder-only, decoder-only, and encoder-decoder models, along with the datasets used for pre-training, spanning Classical Arabic, Modern Standard Arabic, and Dialectal Arabic. The study also explores monolingual, bilingual, and multilingual LLMs, analyzing their architectures and performance across downstream tasks, such as sentiment analysis, named entity recognition, and question answering. Furthermore, it assesses the openness of A...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.20238v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.20238v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Mashaabi, S. Al-Khalifa, and H. Al-Khalifa, &quot;A Survey of Large Language Models for Arabic Language and its Dialects,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.20238v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398405072')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="turkish native language identification v2" data-keywords="cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2307.14850v6" target="_blank">Turkish Native Language Identification V2</a>
                            </h3>
                            <p class="card-authors">Ahmet Yavuz Uluslu, Gerold Schneider</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398404784">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents the first application of Native Language Identification (NLI) for the Turkish language. NLI is the task of automatically identifying an individual&#x27;s native language (L1) based on their writing or speech in a non-native language (L2). While most NLI research has focused on L2 English, our study extends this scope to L2 Turkish by analyzing a corpus of texts written by native speakers of Albanian, Arabic and Persian. We leverage a cleaned version of the Turkish Learner Corpus and demonstrate the effectiveness of syntactic features, comparing a structural Part-of-Speech n-gram...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents the first application of Native Language Identification (NLI) for the Turkish language</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2307.14850v6" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2307.14850v6" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Y. Uluslu, and G. Schneider, &quot;Turkish Native Language Identification V2,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2307.14850v6')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398404784')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="a few-shot approach for relation extraction domain adaptation using large language models" data-keywords="deep learning transformer ros language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.02377v1" target="_blank">A Few-Shot Approach for Relation Extraction Domain Adaptation using Large Language Models</a>
                            </h3>
                            <p class="card-authors">Vanni Zavarella, Juan Carlos Gamero-Salinas, Sergio Consoli</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Deep Learning</span><span class="keyword-tag">Transformer</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4398403296">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Knowledge graphs (KGs) have been successfully applied to the analysis of complex scientific and technological domains, with automatic KG generation methods typically building upon relation extraction models capturing fine-grained relations between domain entities in text. While these relations are fully applicable across scientific areas, existing models are trained on few domain-specific datasets such as SciERC and do not perform well on new target domains. In this paper, we experiment with leveraging in-context learning capabilities of Large Language Models to perform schema-constrained data...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.02377v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.02377v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'V. Zavarella, J. C. Gamero-Salinas, and S. Consoli, &quot;A Few-Shot Approach for Relation Extraction Domain Adaptation using Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.02377v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398403296')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="fast vocabulary transfer for language model compression" data-keywords="language model cs.cl cs.ai cs.lg" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.09977v1" target="_blank">Fast Vocabulary Transfer for Language Model Compression</a>
                            </h3>
                            <p class="card-authors">Leonidas Gee, Andrea Zugarini, Leonardo Rigutini et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.LG</span></div>
                            <div class="card-details" id="cat-details-4398407040">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Real-world business applications require a trade-off between language model performance and size. We propose a new method for model compression that relies on vocabulary transfer. We evaluate the method on various vertical domains and downstream tasks. Our results indicate that vocabulary transfer can be effectively used in combination with other compression techniques, yielding a significant reduction in model size and inference time while marginally compromising on performance.</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a new method for model compression that relies on vocabulary transfer</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.09977v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.09977v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Gee, A. Zugarini, L. Rigutini, and P. Torroni, &quot;Fast Vocabulary Transfer for Language Model Compression,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.09977v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398407040')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="language-conditioned world model improves policy generalization by reading environmental descriptions" data-keywords="reinforcement learning attention planning cs.cl cs.lg" data-themes="S I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2511.22904v1" target="_blank">Language-conditioned world model improves policy generalization by reading environmental descriptions</a>
                            </h3>
                            <p class="card-authors">Anh Nguyen, Stefan Lee</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Attention</span><span class="keyword-tag">Planning</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398415776">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">To interact effectively with humans in the real world, it is important for agents to understand language that describes the dynamics of the environment--that is, how the environment behaves--rather than just task instructions specifying &quot;what to do&quot;. Understanding this dynamics-descriptive language is important for human-agent interaction and agent behavior. Recent work address this problem using a model-based approach: language is incorporated into a world model, which is then used to learn a behavior policy. However, these existing methods either do not demonstrate policy generalization to u...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a model-based reinforcement learning approach, where a language-conditioned world model is trained through interaction with the environment, and a policy is learned from this model--without planning or expert demonstrations</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2511.22904v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2511.22904v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Nguyen, and S. Lee, &quot;Language-conditioned world model improves policy generalization by reading environmental descriptions,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2511.22904v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398415776')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="exploring gender bias in large language models: an in-depth dive into the german language" data-keywords="perception ros language model cs.cl cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.16557v1" target="_blank">Exploring Gender Bias in Large Language Models: An In-depth Dive into the German Language</a>
                            </h3>
                            <p class="card-authors">Kristin Gnadt, David Thulke, Simone Kopeinik et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Perception</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398402336">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In recent years, various methods have been proposed to evaluate gender bias in large language models (LLMs). A key challenge lies in the transferability of bias measurement methods initially developed for the English language when applied to other languages. This work aims to contribute to this research strand by presenting five German datasets for gender bias evaluation in LLMs. The datasets are grounded in well-established concepts of gender bias and are accessible through multiple methodologies. Our findings, reported for eight multilingual LLM models, reveal unique challenges associated wi...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.16557v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.16557v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Gnadt, D. Thulke, S. Kopeinik, and R. Schl√ºter, &quot;Exploring Gender Bias in Large Language Models: An In-depth Dive into the German Language,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.16557v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398402336')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="native vs non-native language prompting: a comparative analysis" data-keywords="nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.07054v2" target="_blank">Native vs Non-Native Language Prompting: A Comparative Analysis</a>
                            </h3>
                            <p class="card-authors">Mohamed Bayan Kmainasi, Rakif Khan, Ali Ezzat Shahroor et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4398405984">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) have shown remarkable abilities in different fields, including standard Natural Language Processing (NLP) tasks. To elicit knowledge from LLMs, prompts play a key role, consisting of natural language instructions. Most open and closed source LLMs are trained on available labeled and unlabeled resources--digital content such as text, images, audio, and videos. Hence, these models have better knowledge for high-resourced languages but struggle with low-resourced languages. Since prompts play a crucial role in understanding their capabilities, the language used for pr...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.07054v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.07054v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. B. Kmainasi, R. Khan, A. E. Shahroor, B. Bendou, M. Hasanain, and F. Alam, &quot;Native vs Non-Native Language Prompting: A Comparative Analysis,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.07054v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398405984')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="pragmatic competence evaluation of large language models for the korean language" data-keywords="gpt language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.12675v2" target="_blank">Pragmatic Competence Evaluation of Large Language Models for the Korean Language</a>
                            </h3>
                            <p class="card-authors">Dojun Park, Jiwoo Lee, Hyeyun Jeong et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398405216">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Benchmarks play a significant role in the current evaluation of Large Language Models (LLMs), yet they often overlook the models&#x27; abilities to capture the nuances of human language, primarily focusing on evaluating embedded knowledge and technical skills. To address this gap, our study evaluates how well LLMs understand context-dependent expressions from a pragmatic standpoint, specifically in Korean. We use both Multiple-Choice Questions (MCQs) for automatic evaluation and Open-Ended Questions (OEQs) assessed by human experts. Our results show that GPT-4 leads with scores of 81.11 in MCQs and...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.12675v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.12675v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Park, J. Lee, H. Jeong, S. Park, and S. Lee, &quot;Pragmatic Competence Evaluation of Large Language Models for the Korean Language,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.12675v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398405216')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="metasc: test-time safety specification optimization for language models" data-keywords="optimization ros language model cs.cl cs.ai" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.07985v2" target="_blank">MetaSC: Test-Time Safety Specification Optimization for Language Models</a>
                            </h3>
                            <p class="card-authors">V√≠ctor Gallego</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Optimization</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398405648">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We propose a novel dynamic safety framework that optimizes language model (LM) safety reasoning at inference time without modifying model weights. Building on recent advances in self-critique methods, our approach leverages a meta-critique mechanism that iteratively updates safety prompts-termed specifications-to drive the critique and revision process adaptively. This test-time optimization not only improves performance against adversarial jailbreak requests but also in diverse general safety-related tasks, such as avoiding moral harm or pursuing honest responses. Our empirical evaluations ac...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a novel dynamic safety framework that optimizes language model (LM) safety reasoning at inference time without modifying model weights</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.07985v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.07985v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'V. Gallego, &quot;MetaSC: Test-Time Safety Specification Optimization for Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.07985v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398405648')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2022" data-category="cs.cl" data-title="prompting is programming: a query language for large language models" data-keywords="control language model cs.cl cs.ai" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2022</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2212.06094v3" target="_blank">Prompting Is Programming: A Query Language for Large Language Models</a>
                            </h3>
                            <p class="card-authors">Luca Beurer-Kellner, Marc Fischer, Martin Vechev</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Control</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4398405120">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models have demonstrated outstanding performance on a wide range of tasks such as question answering and code generation. On a high level, given an input, a language model can be used to automatically complete the sequence in a statistically-likely way. Based on this, users prompt these models with language instructions or examples, to implement a variety of downstream tasks. Advanced prompting methods can even imply interaction between the language model, a user, and external tools such as calculators. However, to obtain state-of-the-art performance or adapt language models for...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">However, to obtain state-of-the-art performance or adapt language models for specific tasks, complex task- and model-specific programs have to be implemented, which may still require ad-hoc interaction.
  Based on this, we present the novel idea of Language Model Programming (LMP)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2212.06094v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2212.06094v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Beurer-Kellner, M. Fischer, and M. Vechev, &quot;Prompting Is Programming: A Query Language for Large Language Models,&quot; arXiv, 2022. [Online]. Available: http://arxiv.org/abs/2212.06094v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398405120')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="untangling the influence of typology, data and model architecture on ranking transfer languages for cross-lingual pos tagging" data-keywords="lstm ros cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.19979v1" target="_blank">Untangling the Influence of Typology, Data and Model Architecture on Ranking Transfer Languages for Cross-Lingual POS Tagging</a>
                            </h3>
                            <p class="card-authors">Enora Rice, Ali Marashian, Hannah Haynie et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Lstm</span><span class="keyword-tag">Ros</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398404256">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Cross-lingual transfer learning is an invaluable tool for overcoming data scarcity, yet selecting a suitable transfer language remains a challenge. The precise roles of linguistic typology, training data, and model architecture in transfer language choice are not fully understood. We take a holistic approach, examining how both dataset-specific and fine-grained typological features influence transfer language selection for part-of-speech tagging, considering two different sources for morphosyntactic features. While previous work examines these dynamics in the context of bilingual biLSTMS, we e...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.19979v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.19979v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'E. Rice, A. Marashian, H. Haynie, K. v. d. Wense, and A. Palmer, &quot;Untangling the Influence of Typology, Data and Model Architecture on Ranking Transfer Languages for Cross-Lingual POS Tagging,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.19979v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398404256')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="a comprehensive review of state-of-the-art methods for java code generation from natural language text" data-keywords="deep learning transformer rnn nlp cs.cl" data-themes="I M E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2306.06371v1" target="_blank">A Comprehensive Review of State-of-The-Art Methods for Java Code Generation from Natural Language Text</a>
                            </h3>
                            <p class="card-authors">Jessica L√≥pez Espejel, Mahaman Sanoussi Yahaya Alassan, El Mehdi Chouham et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Deep Learning</span><span class="keyword-tag">Transformer</span><span class="keyword-tag">Rnn</span><span class="keyword-tag">Nlp</span></div>
                            <div class="card-details" id="cat-details-4398405744">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Java Code Generation consists in generating automatically Java code from a Natural Language Text. This NLP task helps in increasing programmers&#x27; productivity by providing them with immediate solutions to the simplest and most repetitive tasks. Code generation is a challenging task because of the hard syntactic rules and the necessity of a deep understanding of the semantic aspect of the programming language. Many works tried to tackle this task using either RNN-based, or Transformer-based models. The latter achieved remarkable advancement in the domain and they can be divided into three groups...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2306.06371v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2306.06371v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. L. Espejel, M. S. Y. Alassan, E. M. Chouham, W. Dahhane, and E. H. Ettifouri, &quot;A Comprehensive Review of State-of-The-Art Methods for Java Code Generation from Natural Language Text,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2306.06371v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398405744')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="the generation gap: exploring age bias in the value systems of large language models" data-keywords="ros nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2404.08760v4" target="_blank">The Generation Gap: Exploring Age Bias in the Value Systems of Large Language Models</a>
                            </h3>
                            <p class="card-authors">Siyang Liu, Trish Maturi, Bowen Yi et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398402192">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We explore the alignment of values in Large Language Models (LLMs) with specific age groups, leveraging data from the World Value Survey across thirteen categories. Through a diverse set of prompts tailored to ensure response robustness, we find a general inclination of LLM values towards younger demographics, especially when compared to the US population. Although a general inclination can be observed, we also found that this inclination toward younger groups can be different across different value categories. Additionally, we explore the impact of incorporating age identity information in pr...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2404.08760v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2404.08760v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Liu, T. Maturi, B. Yi, S. Shen, and R. Mihalcea, &quot;The Generation Gap: Exploring Age Bias in the Value Systems of Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2404.08760v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398402192')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="automatic generation of question hints for mathematics problems using large language models in educational technology" data-keywords="gpt imu language model cs.cl cs.ai" data-themes="M E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2411.03495v1" target="_blank">Automatic Generation of Question Hints for Mathematics Problems using Large Language Models in Educational Technology</a>
                            </h3>
                            <p class="card-authors">Junior Cedric Tonga, Benjamin Clement, Pierre-Yves Oudeyer</p>
                            <div class="theme-tags"><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Imu</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398407808">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The automatic generation of hints by Large Language Models (LLMs) within Intelligent Tutoring Systems (ITSs) has shown potential to enhance student learning. However, generating pedagogically sound hints that address student misconceptions and adhere to specific educational objectives remains challenging. This work explores using LLMs (GPT-4o and Llama-3-8B-instruct) as teachers to generate effective hints for students simulated through LLMs (GPT-3.5-turbo, Llama-3-8B-Instruct, or Mistral-7B-instruct-v0.3) tackling math exercises designed for human high-school students, and designed using cogn...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present here the study of several dimensions: 1) identifying error patterns made by simulated students on secondary-level math exercises; 2) developing various prompts for GPT-4o as a teacher and evaluating their effectiveness in generating hints that enable simulated students to self-correct; and 3) testing the best-performing prompts, based on their ability to produce relevant hints and facilitate error correction, with Llama-3-8B-Instruct as the teacher, allowing for a performance comparison with GPT-4o</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2411.03495v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2411.03495v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. C. Tonga, B. Clement, and P. Oudeyer, &quot;Automatic Generation of Question Hints for Mathematics Problems using Large Language Models in Educational Technology,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2411.03495v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398407808')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="understanding survey paper taxonomy about large language models via graph representation learning" data-keywords="language model cs.cl cs.ai cs.ir" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.10409v1" target="_blank">Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning</a>
                            </h3>
                            <p class="card-authors">Jun Zhuang, Casey Kennington</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.IR</span></div>
                            <div class="card-details" id="cat-details-4398406464">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">As new research on Large Language Models (LLMs) continues, it is difficult to keep up with new research and models. To help researchers synthesize the new research many have written survey papers, but even those have become numerous. In this paper, we develop a method to automatically assign survey papers to a taxonomy. We collect the metadata of 144 LLM survey papers and explore three paradigms to classify papers within the taxonomy. Our work indicates that leveraging graph structure information on co-category graphs can significantly outperform the language models in two paradigms; pre-train...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we develop a method to automatically assign survey papers to a taxonomy</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.10409v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.10409v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Zhuang, and C. Kennington, &quot;Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.10409v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398406464')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="using large language models for knowledge engineering (llmke): a case study on wikidata" data-keywords="ros language model cs.cl cs.ai" data-themes="M E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.08491v1" target="_blank">Using Large Language Models for Knowledge Engineering (LLMKE): A Case Study on Wikidata</a>
                            </h3>
                            <p class="card-authors">Bohui Zhang, Ioannis Reklos, Nitisha Jain et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4398404976">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In this work, we explore the use of Large Language Models (LLMs) for knowledge engineering tasks in the context of the ISWC 2023 LM-KBC Challenge. For this task, given subject and relation pairs sourced from Wikidata, we utilize pre-trained LLMs to produce the relevant objects in string format and link them to their respective Wikidata QIDs. We developed a pipeline using LLMs for Knowledge Engineering (LLMKE), combining knowledge probing and Wikidata entity mapping. The method achieved a macro-averaged F1-score of 0.701 across the properties, with the scores varying from 1.00 to 0.328. These r...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We developed a pipeline using LLMs for Knowledge Engineering (LLMKE), combining knowledge probing and Wikidata entity mapping</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.08491v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.08491v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. Zhang, I. Reklos, N. Jain, A. M. Pe√±uela, and E. Simperl, &quot;Using Large Language Models for Knowledge Engineering (LLMKE): A Case Study on Wikidata,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.08491v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398404976')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="conditional and modal reasoning in large language models" data-keywords="ros language model cs.cl cs.ai cs.lo" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.17169v4" target="_blank">Conditional and Modal Reasoning in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Wesley H. Holliday, Matthew Mandelkern, Cedegao E. Zhang</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4398407232">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The reasoning abilities of large language models (LLMs) are the topic of a growing body of research in AI and cognitive science. In this paper, we probe the extent to which twenty-nine LLMs are able to distinguish logically correct inferences from logically fallacious ones. We focus on inference patterns involving conditionals (e.g., &#x27;If Ann has a queen, then Bob has a jack&#x27;) and epistemic modals (e.g., &#x27;Ann might have an ace&#x27;, &#x27;Bob must have a king&#x27;). These inferences have been of special interest to logicians, philosophers, and linguists, since they play a central role in the fundamental hum...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.17169v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.17169v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. H. Holliday, M. Mandelkern, and C. E. Zhang, &quot;Conditional and Modal Reasoning in Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.17169v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398407232')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2022" data-category="cs.cl" data-title="end-to-end spoken language understanding: performance analyses of a voice command task in a low resource setting" data-keywords="neural network optimization cs.cl cs.sd eess.as" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2022</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2207.08179v1" target="_blank">End-to-End Spoken Language Understanding: Performance analyses of a voice command task in a low resource setting</a>
                            </h3>
                            <p class="card-authors">Thierry Desot, Fran√ßois Portet, Michel Vacher</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.SD</span></div>
                            <div class="card-details" id="cat-details-4398406224">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Spoken Language Understanding (SLU) is a core task in most human-machine interaction systems. With the emergence of smart homes, smart phones and smart speakers, SLU has become a key technology for the industry. In a classical SLU approach, an Automatic Speech Recognition (ASR) module transcribes the speech signal into a textual representation from which a Natural Language Understanding (NLU) module extracts semantic information. Recently End-to-End SLU (E2E SLU) based on Deep Neural Networks has gained momentum since it benefits from the joint optimization of the ASR and the NLU parts, hence ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we present a study identifying the signal features and other linguistic properties used by an E2E model to perform the SLU task</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2207.08179v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2207.08179v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. Desot, F. Portet, and M. Vacher, &quot;End-to-End Spoken Language Understanding: Performance analyses of a voice command task in a low resource setting,&quot; arXiv, 2022. [Online]. Available: http://arxiv.org/abs/2207.08179v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398406224')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="enhancing small language models for cross-lingual generalized zero-shot classification with soft prompt tuning" data-keywords="ros nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.19469v2" target="_blank">Enhancing Small Language Models for Cross-Lingual Generalized Zero-Shot Classification with Soft Prompt Tuning</a>
                            </h3>
                            <p class="card-authors">Fred Philippy, Siwen Guo, Cedric Lothritz et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398407184">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In NLP, Zero-Shot Classification (ZSC) has become essential for enabling models to classify text into categories unseen during training, particularly in low-resource languages and domains where labeled data is scarce. While pretrained language models (PLMs) have shown promise in ZSC, they often rely on large training datasets or external knowledge, limiting their applicability in multilingual and low-resource scenarios. Recent approaches leveraging natural language prompts reduce the dependence on large training datasets but struggle to effectively incorporate available labeled data from relat...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address these challenges, we introduce RoSPrompt, a lightweight and data-efficient approach for training soft prompts that enhance cross-lingual ZSC while ensuring robust generalization across data distribution shifts</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.19469v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.19469v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. Philippy, S. Guo, C. Lothritz, J. Klein, and T. F. Bissyand√©, &quot;Enhancing Small Language Models for Cross-Lingual Generalized Zero-Shot Classification with Soft Prompt Tuning,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.19469v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398407184')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="studies with impossible languages falsify lms as models of human language" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2511.11389v1" target="_blank">Studies with impossible languages falsify LMs as models of human language</a>
                            </h3>
                            <p class="card-authors">Jeffrey S. Bowers, Jeff Mitchell</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398406944">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">According to Futrell and Mahowald [arXiv:2501.17047], both infants and language models (LMs) find attested languages easier to learn than impossible languages that have unnatural structures. We review the literature and show that LMs often learn attested and many impossible languages equally well. Difficult to learn impossible languages are simply more complex (or random). LMs are missing human inductive biases that support language acquisition.</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2511.11389v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2511.11389v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. S. Bowers, and J. Mitchell, &quot;Studies with impossible languages falsify LMs as models of human language,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2511.11389v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398406944')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="benchmarking adversarial robustness to bias elicitation in large language models: scalable automated assessment with llm-as-a-judge" data-keywords="manipulation ros language model cs.cl cs.ai" data-themes="E S M L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.07887v2" target="_blank">Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge</a>
                            </h3>
                            <p class="card-authors">Riccardo Cantini, Alessio Orsino, Massimo Ruggiero et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Manipulation</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398401520">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The growing integration of Large Language Models (LLMs) into critical societal domains has raised concerns about embedded biases that can perpetuate stereotypes and undermine fairness. Such biases may stem from historical inequalities in training data, linguistic imbalances, or adversarial manipulation. Despite mitigation efforts, recent studies show that LLMs remain vulnerable to adversarial attacks that elicit biased outputs. This work proposes a scalable benchmarking framework to assess LLM robustness to adversarial bias elicitation. Our methodology involves: (i) systematically probing mode...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Our methodology involves: (i) systematically probing models across multiple tasks targeting diverse sociocultural biases, (ii) quantifying robustness through safety scores using an LLM-as-a-Judge approach, and (iii) employing jailbreak techniques to reveal safety vulnerabilities</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.07887v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.07887v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Cantini, A. Orsino, M. Ruggiero, and D. Talia, &quot;Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.07887v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398401520')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="facilitating large language model russian adaptation with learned embedding propagation" data-keywords="gpt language model cs.cl cs.ai" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.21140v1" target="_blank">Facilitating large language model Russian adaptation with Learned Embedding Propagation</a>
                            </h3>
                            <p class="card-authors">Mikhail Tikhomirov, Daniil Chernyshev</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4398406512">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Rapid advancements of large language model (LLM) technologies led to the introduction of powerful open-source instruction-tuned LLMs that have the same text generation quality as the state-of-the-art counterparts such as GPT-4. While the emergence of such models accelerates the adoption of LLM technologies in sensitive-information environments the authors of such models don not disclose the training data necessary for replication of the results thus making the achievements model-exclusive. Since those open-source models are also multilingual this in turn reduces the benefits of training a lang...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address the limitations and cut the costs of the language adaptation pipeline we propose Learned Embedding Propagation (LEP)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.21140v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.21140v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Tikhomirov, and D. Chernyshev, &quot;Facilitating large language model Russian adaptation with Learned Embedding Propagation,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.21140v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398406512')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="distinct social-linguistic processing between humans and large audio-language models: evidence from model-brain alignment" data-keywords="ros language model cs.cl q-bio.nc" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.19586v2" target="_blank">Distinct social-linguistic processing between humans and large audio-language models: Evidence from model-brain alignment</a>
                            </h3>
                            <p class="card-authors">Hanlin Wu, Xufeng Duan, Zhenguang Cai</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">Q-BIO.NC</span></div>
                            <div class="card-details" id="cat-details-4398415824">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Voice-based AI development faces unique challenges in processing both linguistic and paralinguistic information. This study compares how large audio-language models (LALMs) and humans integrate speaker characteristics during speech comprehension, asking whether LALMs process speaker-contextualized language in ways that parallel human cognitive mechanisms. We compared two LALMs&#x27; (Qwen2-Audio and Ultravox 0.5) processing patterns with human EEG responses. Using surprisal and entropy metrics from the models, we analyzed their sensitivity to speaker-content incongruency across social stereotype vi...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.19586v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.19586v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Wu, X. Duan, and Z. Cai, &quot;Distinct social-linguistic processing between humans and large audio-language models: Evidence from model-brain alignment,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.19586v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398415824')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2022" data-category="cs.cl" data-title="an overview of indian spoken language recognition from machine learning perspective" data-keywords="neural network cs.cl cs.sd eess.as" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2022</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2212.03812v1" target="_blank">An Overview of Indian Spoken Language Recognition from Machine Learning Perspective</a>
                            </h3>
                            <p class="card-authors">Spandan Dey, Md Sahidullah, Goutam Saha</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.SD</span><span class="keyword-tag">EESS.AS</span></div>
                            <div class="card-details" id="cat-details-4398406128">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Automatic spoken language identification (LID) is a very important research field in the era of multilingual voice-command-based human-computer interaction (HCI). A front-end LID module helps to improve the performance of many speech-based applications in the multilingual scenario. India is a populous country with diverse cultures and languages. The majority of the Indian population needs to use their respective native languages for verbal interaction with machines. Therefore, the development of efficient Indian spoken language recognition systems is useful for adapting smart technologies in e...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2212.03812v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2212.03812v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Dey, M. Sahidullah, and G. Saha, &quot;An Overview of Indian Spoken Language Recognition from Machine Learning Perspective,&quot; arXiv, 2022. [Online]. Available: http://arxiv.org/abs/2212.03812v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398406128')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="tiny language models" data-keywords="transformer bert ros nlp language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.14871v2" target="_blank">Tiny language models</a>
                            </h3>
                            <p class="card-authors">Ronit D. Gross, Yarden Tzach, Tal Halevi et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Transformer</span><span class="keyword-tag">Bert</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Nlp</span></div>
                            <div class="card-details" id="cat-details-4398403728">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">A prominent achievement of natural language processing (NLP) is its ability to understand and generate meaningful human language. This capability relies on complex feedforward transformer block architectures pre-trained on large language models (LLMs). However, LLM pre-training is currently feasible only for a few dominant companies due to the immense computational resources required, limiting broader research participation. This creates a critical need for more accessible alternatives. In this study, we explore whether tiny language models (TLMs) exhibit the same key qualitative features of L...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We demonstrate that TLMs exhibit a clear performance gap between pre-trained and non-pre-trained models across classification tasks, indicating the effectiveness of pre-training, even at a tiny scale</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.14871v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.14871v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. D. Gross, Y. Tzach, T. Halevi, E. Koresh, and I. Kanter, &quot;Tiny language models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.14871v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398403728')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="jais and jais-chat: arabic-centric foundation and instruction-tuned open generative large language models" data-keywords="gpt language model cs.cl cs.ai cs.lg" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2308.16149v2" target="_blank">Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models</a>
                            </h3>
                            <p class="card-authors">Neha Sengupta, Sunil Kumar Sahu, Bokang Jia et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4398405600">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-ce...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2308.16149v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2308.16149v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. Sengupta et al., &quot;Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2308.16149v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398405600')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="cs.lg">
                <h2 class="section-header">üè∑Ô∏è Machine Learning <span class="section-count">(12 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="a critical review of causal reasoning benchmarks for large language models" data-keywords="language model cs.lg cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.08029v1" target="_blank">A Critical Review of Causal Reasoning Benchmarks for Large Language Models</a>
                            </h3>
                            <p class="card-authors">Linying Yang, Vik Shirvaikar, Oscar Clivio et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4397314496">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Numerous benchmarks aim to evaluate the capabilities of Large Language Models (LLMs) for causal inference and reasoning. However, many of them can likely be solved through the retrieval of domain knowledge, questioning whether they achieve their purpose. In this review, we present a comprehensive overview of LLM benchmarks for causality. We highlight how recent benchmarks move towards a more thorough definition of causal reasoning by incorporating interventional or counterfactual reasoning. We derive a set of criteria that a useful benchmark or set of benchmarks should aim to satisfy. We hope ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this review, we present a comprehensive overview of LLM benchmarks for causality</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.08029v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.08029v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Yang, V. Shirvaikar, O. Clivio, and F. Falck, &quot;A Critical Review of Causal Reasoning Benchmarks for Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.08029v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397314496')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="wkvquant: quantizing weight and key/value cache for large language models gains more" data-keywords="attention optimization ros language model cs.lg cs.ai cs.cl" data-themes="S I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.12065v2" target="_blank">WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More</a>
                            </h3>
                            <p class="card-authors">Yuxuan Yue, Zhihang Yuan, Haojie Duanmu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4397304896">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process. This paper addresses these challenges by focusing on the quantization of LLMs, a technique that reduces memory consumption by converting model parameters and activations into low-bit integers. We critically analyze the existing quantization approaches, identifying their limitations in balancing the accuracy and efficiency of the quantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ fr...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To advance beyond these limitations, we propose WKVQuant, a PTQ framework especially designed for quantizing weights and the key/value (KV) cache of LLMs</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.12065v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.12065v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Yue, Z. Yuan, H. Duanmu, S. Zhou, J. Wu, and L. Nie, &quot;WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.12065v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397304896')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="unraveling arithmetic in large language models: the role of algebraic structures" data-keywords="transformer attention language model cs.lg cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2411.16260v3" target="_blank">Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures</a>
                            </h3>
                            <p class="card-authors">Fu-Chieh Chang, You-Chen Lin, Pei-Yuan Wu</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Transformer</span><span class="keyword-tag">Attention</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span></div>
                            <div class="card-details" id="cat-details-4397306960">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) have demonstrated remarkable mathematical capabilities, largely driven by chain-of-thought (CoT) prompting, which decomposes complex reasoning into step-by-step solutions. This approach has enabled significant advancements, as evidenced by performance on benchmarks like GSM8K and MATH. However, the mechanisms underlying LLMs&#x27; ability to perform arithmetic in a single step of CoT remain poorly understood. Existing studies debate whether LLMs encode numerical values or rely on symbolic reasoning, while others explore attention and multi-layered processing in arithmet...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose that LLMs learn arithmetic by capturing algebraic structures, such as commutativity and identity properties</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2411.16260v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2411.16260v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. Chang, Y. Lin, and P. Wu, &quot;Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2411.16260v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397306960')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="bridging large language models and graph structure learning models for robust representation learning" data-keywords="ros language model cs.lg cs.ai" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.12096v1" target="_blank">Bridging Large Language Models and Graph Structure Learning Models for Robust Representation Learning</a>
                            </h3>
                            <p class="card-authors">Guangxin Su, Yifan Zhu, Wenjie Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4397311280">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Graph representation learning, involving both node features and graph structures, is crucial for real-world applications but often encounters pervasive noise. State-of-the-art methods typically address noise by focusing separately on node features with large language models (LLMs) and on graph structures with graph structure learning models (GSLMs). In this paper, we introduce LangGSL, a robust framework that integrates the complementary strengths of pre-trained language models and GSLMs to jointly enhance both node feature and graph structure learning. In LangGSL, we first leverage LLMs to fi...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we introduce LangGSL, a robust framework that integrates the complementary strengths of pre-trained language models and GSLMs to jointly enhance both node feature and graph structure learning</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.12096v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.12096v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Su, Y. Zhu, W. Zhang, H. Wang, and Y. Zhang, &quot;Bridging Large Language Models and Graph Structure Learning Models for Robust Representation Learning,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.12096v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397311280')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="unforgettable generalization in language models" data-keywords="transformer ros language model cs.lg cs.cl" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.02228v1" target="_blank">Unforgettable Generalization in Language Models</a>
                            </h3>
                            <p class="card-authors">Eric Zhang, Leshem Chosen, Jacob Andreas</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Transformer</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span></div>
                            <div class="card-details" id="cat-details-4397305040">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">When language models (LMs) are trained to forget (or &quot;unlearn&#x27;&#x27;) a skill, how precisely does their behavior change? We study the behavior of transformer LMs in which tasks have been forgotten via fine-tuning on randomized labels. Such LMs learn to generate near-random predictions for individual examples in the &quot;training&#x27;&#x27; set used for forgetting. Across tasks, however, LMs exhibit extreme variability in whether LM predictions change on examples outside the training set. In some tasks (like entailment classification), forgetting generalizes robustly, and causes models to produce uninformative p...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.02228v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.02228v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'E. Zhang, L. Chosen, and J. Andreas, &quot;Unforgettable Generalization in Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.02228v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397305040')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.lg" data-title="two-stage representation learning for analyzing movement behavior dynamics in people living with dementia" data-keywords="language model cs.lg cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.09173v1" target="_blank">Two-Stage Representation Learning for Analyzing Movement Behavior Dynamics in People Living with Dementia</a>
                            </h3>
                            <p class="card-authors">Jin Cui, Alexander Capstick, Payam Barnaghi et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4398402000">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In remote healthcare monitoring, time series representation learning reveals critical patient behavior patterns from high-frequency data. This study analyzes home activity data from individuals living with dementia by proposing a two-stage, self-supervised learning approach tailored to uncover low-rank structures. The first stage converts time-series activities into text sequences encoded by a pre-trained language model, providing a rich, high-dimensional latent state space using a PageRank-based method. This PageRank vector captures latent state transitions, effectively compressing complex be...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.09173v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.09173v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Cui, A. Capstick, P. Barnaghi, and G. Scott, &quot;Two-Stage Representation Learning for Analyzing Movement Behavior Dynamics in People Living with Dementia,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.09173v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398402000')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.lg" data-title="pb-llm: partially binarized large language models" data-keywords="gpt language model cs.lg cs.ai cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2310.00034v2" target="_blank">PB-LLM: Partially Binarized Large Language Models</a>
                            </h3>
                            <p class="card-authors">Yuzhang Shang, Zhihang Yuan, Qiang Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4398403200">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper explores network binarization, a radical form of quantization, compressing model weights to a single bit, specifically for Large Language Models (LLMs) compression. Due to previous binarization methods collapsing LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of quantized LLMs. Specifically, our exploration first uncovers the ineffectiveness of naive applications of existing binarization algorithms and highlights the imperative role of salient weights in achieving...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Due to previous binarization methods collapsing LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of quantized LLMs</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2310.00034v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2310.00034v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Shang, Z. Yuan, Q. Wu, and Z. Dong, &quot;PB-LLM: Partially Binarized Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2310.00034v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398403200')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="are compressed language models less subgroup robust?" data-keywords="bert language model cs.lg cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.17811v1" target="_blank">Are Compressed Language Models Less Subgroup Robust?</a>
                            </h3>
                            <p class="card-authors">Leonidas Gee, Andrea Zugarini, Novi Quadrianto</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Bert</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398402576">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">To reduce the inference cost of large language models, model compression is increasingly used to create smaller scalable models. However, little is known about their robustness to minority subgroups defined by the labels and attributes of a dataset. In this paper, we investigate the effects of 18 different compression methods and settings on the subgroup robustness of BERT language models. We show that worst-group performance does not depend on model size alone, but also on the compression method used. Additionally, we find that model compression does not always worsen the performance on minor...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.17811v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.17811v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Gee, A. Zugarini, and N. Quadrianto, &quot;Are Compressed Language Models Less Subgroup Robust?,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.17811v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398402576')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="activation sparsity opportunities for compressing general large language models" data-keywords="optimization language model cs.lg cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.12178v2" target="_blank">Activation Sparsity Opportunities for Compressing General Large Language Models</a>
                            </h3>
                            <p class="card-authors">Nobel Dhar, Bobin Deng, Md Romyull Islam et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Optimization</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4398402864">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Deploying local AI models, such as Large Language Models (LLMs), to edge devices can substantially enhance devices&#x27; independent capabilities, alleviate the server&#x27;s burden, and lower the response time. Owing to these tremendous potentials, many big tech companies have released several lightweight Small Language Models (SLMs) to bridge this gap. However, we still have huge motivations to deploy more powerful (LLMs) AI models on edge devices and enhance their smartness level. Unlike the conventional approaches for AI model compression, we investigate activation sparsity. The activation sparsity ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.12178v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.12178v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. Dhar, B. Deng, M. R. Islam, K. F. A. Nasif, L. Zhao, and K. Suo, &quot;Activation Sparsity Opportunities for Compressing General Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.12178v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398402864')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.lg" data-title="understanding reasoning in thinking language models via steering vectors" data-keywords="control ros language model cs.lg cs.ai" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.18167v4" target="_blank">Understanding Reasoning in Thinking Language Models via Steering Vectors</a>
                            </h3>
                            <p class="card-authors">Constantin Venhoff, Iv√°n Arcuschin, Philip Torr et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Control</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span></div>
                            <div class="card-details" id="cat-details-4398405168">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recent advances in large language models (LLMs) have led to the development of thinking language models that generate extensive internal reasoning chains before producing responses. While these models achieve improved performance, controlling their reasoning processes remains challenging. This work presents a steering approach for thinking LLMs by analyzing and manipulating specific reasoning behaviors in DeepSeek-R1-Distill models. Through a systematic experiment on 500 tasks across 10 diverse categories, we identify several reasoning behaviors exhibited by thinking models, including expressi...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We demonstrate that these behaviors are mediated by linear directions in the model&#x27;s activation space and can be controlled using steering vectors</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.18167v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.18167v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Venhoff, I. Arcuschin, P. Torr, A. Conmy, and N. Nanda, &quot;Understanding Reasoning in Thinking Language Models via Steering Vectors,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.18167v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398405168')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2021" data-category="cs.lg" data-title="differentially private fine-tuning of language models" data-keywords="bert gpt nlp language model cs.lg cs.cl cs.cr" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2021</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2110.06500v2" target="_blank">Differentially Private Fine-tuning of Language Models</a>
                            </h3>
                            <p class="card-authors">Da Yu, Saurabh Naik, Arturs Backurs et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Bert</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4398405840">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We give simpler, sparser, and faster algorithms for differentially private fine-tuning of large-scale pre-trained language models, which achieve the state-of-the-art privacy versus utility tradeoffs on many standard NLP tasks. We propose a meta-framework for this problem, inspired by the recent success of highly parameter-efficient methods for fine-tuning. Our experiments show that differentially private adaptations of these approaches outperform previous private algorithms in three important dimensions: utility, privacy, and the computational and memory cost of private training. On many commo...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a meta-framework for this problem, inspired by the recent success of highly parameter-efficient methods for fine-tuning</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2110.06500v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2110.06500v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Yu et al., &quot;Differentially Private Fine-tuning of Language Models,&quot; arXiv, 2021. [Online]. Available: http://arxiv.org/abs/2110.06500v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398405840')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="emergent world models and latent variable estimation in chess-playing language models" data-keywords="gpt language model cs.lg cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.15498v2" target="_blank">Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models</a>
                            </h3>
                            <p class="card-authors">Adam Karvonen</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398405264">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Language models have shown unprecedented capabilities, sparking debate over the source of their performance. Is it merely the outcome of learning syntactic patterns and surface level statistics, or do they extract semantics and a world model from the text? Prior work by Li et al. investigated this by training a GPT model on synthetic, randomly generated Othello games and found that the model learned an internal representation of the board state. We extend this work into the more complex domain of chess, training on real games and investigating our model&#x27;s internal representations using linear ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.15498v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.15498v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Karvonen, &quot;Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.15498v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398405264')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="cs.cv">
                <h2 class="section-header">üè∑Ô∏è Computer Vision <span class="section-count">(11 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cv" data-title="a survey on multimodal large language models" data-keywords="gpt language model cs.cv cs.ai cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2306.13549v4" target="_blank">A Survey on Multimodal Large Language Models</a>
                            </h3>
                            <p class="card-authors">Shukang Yin, Chaoyou Fu, Sirui Zhao et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4397308304">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. I...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2306.13549v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2306.13549v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Yin et al., &quot;A Survey on Multimodal Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2306.13549v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397308304')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cv" data-title="prunevid: visual token pruning for efficient video large language models" data-keywords="ros language model cs.cv" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.16117v1" target="_blank">PruneVid: Visual Token Pruning for Efficient Video Large Language Models</a>
                            </h3>
                            <p class="card-authors">Xiaohu Huang, Hao Zhou, Kai Han</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span></div>
                            <div class="card-details" id="cat-details-4397313680">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In this paper, we introduce PruneVid, a visual token pruning method designed to enhance the efficiency of multi-modal video understanding. Large Language Models (LLMs) have shown promising performance in video tasks due to their extended capabilities in comprehending visual modalities. However, the substantial redundancy in video data presents significant computational challenges for LLMs. To address this issue, we introduce a training-free method that 1) minimizes video redundancy by merging spatial-temporal tokens, and 2) leverages LLMs&#x27; reasoning capabilities to selectively prune visual fea...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we introduce PruneVid, a visual token pruning method designed to enhance the efficiency of multi-modal video understanding</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.16117v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.16117v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. Huang, H. Zhou, and K. Han, &quot;PruneVid: Visual Token Pruning for Efficient Video Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.16117v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397313680')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cv" data-title="exploring the frontier of vision-language models: a survey of current methodologies and future directions" data-keywords="language model cs.cv cs.ai cs.cl" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2404.07214v4" target="_blank">Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions</a>
                            </h3>
                            <p class="card-authors">Akash Ghosh, Arkadeep Acharya, Sriparna Saha et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398403104">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The advent of Large Language Models (LLMs) has significantly reshaped the trajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable limitation, as they are primarily adept at processing textual information. To address this constraint, researchers have endeavored to integrate visual capabilities with LLMs, resulting in the emergence of Vision-Language Models (VLMs). These advanced models are instrumental in tackling more intricate tasks such as image captioning and visual question answering. In our comprehensive survey paper, we delve into the key advancements within the realm...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2404.07214v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2404.07214v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Ghosh, A. Acharya, S. Saha, V. Jain, and A. Chadha, &quot;Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2404.07214v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398403104')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cv" data-title="object detection with multimodal large vision-language models: an in-depth review" data-keywords="deep learning robot computer vision nlp language model cs.cv cs.ai cs.cl" data-themes="I M E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.19294v2" target="_blank">Object Detection with Multimodal Large Vision-Language Models: An In-depth Review</a>
                            </h3>
                            <p class="card-authors">Ranjan Sapkota, Manoj Karkee</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Deep Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Computer Vision</span><span class="keyword-tag">Nlp</span></div>
                            <div class="card-details" id="cat-details-4398401760">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The fusion of language and vision in large vision-language models (LVLMs) has revolutionized deep learning-based object detection by enhancing adaptability, contextual reasoning, and generalization beyond traditional architectures. This in-depth review presents a structured exploration of the state-of-the-art in LVLMs, systematically organized through a three-step research review process. First, we discuss the functioning of vision language models (VLMs) for object detection, describing how these models harness natural language processing (NLP) and computer vision (CV) techniques to revolution...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.19294v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.19294v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Sapkota, and M. Karkee, &quot;Object Detection with Multimodal Large Vision-Language Models: An In-depth Review,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.19294v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398401760')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cv" data-title="evaluation and enhancement of semantic grounding in large vision-language models" data-keywords="ros language model cs.cv cs.cl" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.04041v2" target="_blank">Evaluation and Enhancement of Semantic Grounding in Large Vision-Language Models</a>
                            </h3>
                            <p class="card-authors">Jiaying Lu, Jinmeng Rao, Kezhen Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398401424">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Vision-Language Models (LVLMs) offer remarkable benefits for a variety of vision-language tasks. However, a challenge hindering their application in real-world scenarios, particularly regarding safety, robustness, and reliability, is their constrained semantic grounding ability, which pertains to connecting language to the physical-world entities or concepts referenced in images. Therefore, a crucial need arises for a comprehensive study to assess the semantic grounding ability of widely used LVLMs. Despite the significance, sufficient investigation in this direction is currently lacking...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address this issue, we propose a data-centric enhancement method that aims to improve LVLMs&#x27; semantic grounding ability through multimodal instruction tuning on fine-grained conversations</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.04041v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.04041v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Lu et al., &quot;Evaluation and Enhancement of Semantic Grounding in Large Vision-Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.04041v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398401424')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cv" data-title="pushing boundaries: exploring zero shot object classification with large multimodal models" data-keywords="ros language model cs.cv cs.si" data-themes="S M E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.00127v1" target="_blank">Pushing Boundaries: Exploring Zero Shot Object Classification with Large Multimodal Models</a>
                            </h3>
                            <p class="card-authors">Ashhadul Islam, Md. Rafiul Biswas, Wajdi Zaghouani et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span><span class="keyword-tag">CS.SI</span></div>
                            <div class="card-details" id="cat-details-4398404544">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">$ $The synergy of language and vision models has given rise to Large Language and Vision Assistant models (LLVAs), designed to engage users in rich conversational experiences intertwined with image-based queries. These comprehensive multimodal models seamlessly integrate vision encoders with Large Language Models (LLMs), expanding their applications in general-purpose language and visual comprehension. The advent of Large Multimodal Models (LMMs) heralds a new era in Artificial Intelligence (AI) assistance, extending the horizons of AI utilization. This paper takes a unique perspective on LMMs...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.00127v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.00127v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Islam, M. R. Biswas, W. Zaghouani, S. B. Belhaouari, and Z. Shah, &quot;Pushing Boundaries: Exploring Zero Shot Object Classification with Large Multimodal Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2401.00127v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398404544')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cv" data-title="mquant: unleashing the inference potential of multimodal large language models via full static quantization" data-keywords="attention language model cs.cv cs.ai" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.00425v2" target="_blank">MQuant: Unleashing the Inference Potential of Multimodal Large Language Models via Full Static Quantization</a>
                            </h3>
                            <p class="card-authors">JiangYong Yu, Sifan Zhou, Dawei Yang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4398402672">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Multimodal large language models (MLLMs) have garnered widespread attention due to their ability to understand multimodal input. However, their large parameter sizes and substantial computational demands severely hinder their practical deployment and application.While quantization is an effective way to reduce model size and inference latency, its application to MLLMs remains underexplored. In this paper, we propose MQuant, a post-training quantization (PTQ) framework designed to tackle the unique challenges of multimodal large language models (MLLMs). Conventional quantization often struggles...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose MQuant, a post-training quantization (PTQ) framework designed to tackle the unique challenges of multimodal large language models (MLLMs)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.00425v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.00425v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Yu et al., &quot;MQuant: Unleashing the Inference Potential of Multimodal Large Language Models via Full Static Quantization,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.00425v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398402672')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cv" data-title="lvlm-ehub: a comprehensive evaluation benchmark for large vision-language models" data-keywords="gpt language model cs.cv cs.ai" data-themes="M E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2306.09265v1" target="_blank">LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models</a>
                            </h3>
                            <p class="card-authors">Peng Xu, Wenqi Shao, Kaipeng Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4397314352">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Vision-Language Models (LVLMs) have recently played a dominant role in multimodal vision-language learning. Despite the great success, it lacks a holistic evaluation of their efficacy. This paper presents a comprehensive evaluation of publicly available large multimodal models by building a LVLM evaluation Hub (LVLM-eHub). Our LVLM-eHub consists of $8$ representative LVLMs such as InstructBLIP and MiniGPT-4, which are thoroughly evaluated by a quantitative capability evaluation and an online arena platform. The former evaluates $6$ categories of multimodal capabilities of LVLMs such as v...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents a comprehensive evaluation of publicly available large multimodal models by building a LVLM evaluation Hub (LVLM-eHub)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2306.09265v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2306.09265v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Xu et al., &quot;LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2306.09265v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397314352')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.cv" data-title="mme: a comprehensive evaluation benchmark for multimodal large language models" data-keywords="perception optimization language model cs.cv" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2306.13394v5" target="_blank">MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models</a>
                            </h3>
                            <p class="card-authors">Chaoyou Fu, Peixian Chen, Yunhang Shen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Perception</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span></div>
                            <div class="card-details" id="cat-details-4398403920">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image. However, it is difficult for these case studies to fully reflect the performance of MLLM, lacking a comprehensive evaluation. In this paper, we fill in this blank, presenting the first comprehensive MLLM Evaluation benchmark MME. It measures both perception and cognition abilities on a total of 14 subtasks. In order to avoid data leakage that may arise from direct use of public datasets for evaluation, the ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2306.13394v5" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2306.13394v5" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Fu et al., &quot;MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2306.13394v5')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398403920')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cv" data-title="integrating large language models into a tri-modal architecture for automated depression classification on the daic-woz" data-keywords="lstm gpt ros language model cs.cv cs.ai cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.19340v5" target="_blank">Integrating Large Language Models into a Tri-Modal Architecture for Automated Depression Classification on the DAIC-WOZ</a>
                            </h3>
                            <p class="card-authors">Santosh V. Patapati</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Lstm</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4398403968">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Major Depressive Disorder (MDD) is a pervasive mental health condition that affects 300 million people worldwide. This work presents a novel, BiLSTM-based tri-modal model-level fusion architecture for the binary classification of depression from clinical interview recordings. The proposed architecture incorporates Mel Frequency Cepstral Coefficients, Facial Action Units, and uses a two-shot learning based GPT-4 model to process text data. This is the first work to incorporate large language models into a multi-modal architecture for this task. It achieves impressive results on the DAIC-WOZ AVE...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.19340v5" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.19340v5" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. V. Patapati, &quot;Integrating Large Language Models into a Tri-Modal Architecture for Automated Depression Classification on the DAIC-WOZ,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.19340v5')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398403968')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cv" data-title="thinking in 360¬∞: humanoid visual search in the wild" data-keywords="humanoid control cs.cv" data-themes="S M E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2511.20351v2" target="_blank">Thinking in 360¬∞: Humanoid Visual Search in the Wild</a>
                            </h3>
                            <p class="card-authors">Heyang Yu, Yinan Han, Xiangyu Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Control</span><span class="keyword-tag">CS.CV</span></div>
                            <div class="card-details" id="cat-details-4398444032">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humans rely on the synergistic control of head (cephalomotor) and eye (oculomotor) to efficiently search for visual information in 360¬∞. However, prior approaches to visual search are limited to a static image, neglecting the physical embodiment and its interaction with the 3D world. How can we develop embodied visual search agents as efficient as humans while bypassing the constraints imposed by real-world hardware? To this end, we propose humanoid visual search where a humanoid agent actively rotates its head to search for objects or paths in an immersive world represented by a 360¬∞ panorami...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">How can we develop embodied visual search agents as efficient as humans while bypassing the constraints imposed by real-world hardware? To this end, we propose humanoid visual search where a humanoid agent actively rotates its head to search for objects or paths in an immersive world represented by a 360¬∞ panoramic image</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2511.20351v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2511.20351v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Yu et al., &quot;Thinking in 360¬∞: Humanoid Visual Search in the Wild,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2511.20351v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398444032')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="cs.ai">
                <h2 class="section-header">üè∑Ô∏è Artificial Intelligence <span class="section-count">(10 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.ai" data-title="large language models reasoning abilities under non-ideal conditions after rl-fine-tuning" data-keywords="reinforcement learning ros language model cs.ai" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.04848v1" target="_blank">Large Language Models Reasoning Abilities Under Non-Ideal Conditions After RL-Fine-Tuning</a>
                            </h3>
                            <p class="card-authors">Chang Tian, Matthew B. Blaschko, Mingzhe Xing et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4397308544">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Reinforcement learning (RL) has become a key technique for enhancing the reasoning abilities of large language models (LLMs), with policy-gradient algorithms dominating the post-training stage because of their efficiency and effectiveness. However, most existing benchmarks evaluate large-language-model reasoning under idealized settings, overlooking performance in realistic, non-ideal scenarios. We identify three representative non-ideal scenarios with practical relevance: summary inference, fine-grained noise suppression, and contextual filtering. We introduce a new research direction guided ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce a new research direction guided by brain-science findings that human reasoning remains reliable under imperfect inputs</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.04848v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.04848v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Tian, M. B. Blaschko, M. Xing, X. Li, Y. Yue, and M. Moens, &quot;Large Language Models Reasoning Abilities Under Non-Ideal Conditions After RL-Fine-Tuning,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.04848v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397308544')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.ai" data-title="kglens: towards efficient and effective knowledge probing of large language models with knowledge graphs" data-keywords="simulation ros imu language model cs.ai cs.cl cs.lg" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.11539v3" target="_blank">KGLens: Towards Efficient and Effective Knowledge Probing of Large Language Models with Knowledge Graphs</a>
                            </h3>
                            <p class="card-authors">Shangshang Zheng, He Bai, Yizhe Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Simulation</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Imu</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4398403680">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Language Models (LLMs) might hallucinate facts, while curated Knowledge Graph (KGs) are typically factually reliable especially with domain-specific knowledge. Measuring the alignment between KGs and LLMs can effectively probe the factualness and identify the knowledge blind spots of LLMs. However, verifying the LLMs over extensive KGs can be expensive. In this paper, we present KGLens, a Thompson-sampling-inspired framework aimed at effectively and efficiently measuring the alignment between KGs and LLMs. KGLens features a graph-guided question generator for converting KGs into natural ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we present KGLens, a Thompson-sampling-inspired framework aimed at effectively and efficiently measuring the alignment between KGs and LLMs</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.11539v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.11539v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Zheng, H. Bai, Y. Zhang, Y. Su, X. Niu, and N. Jaitly, &quot;KGLens: Towards Efficient and Effective Knowledge Probing of Large Language Models with Knowledge Graphs,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.11539v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398403680')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.ai" data-title="large language models for interpretable mental health diagnosis" data-keywords="language model cs.ai cs.lo" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2501.07653v2" target="_blank">Large Language Models for Interpretable Mental Health Diagnosis</a>
                            </h3>
                            <p class="card-authors">Brian Hyeongseok Kim, Chao Wang</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.LO</span></div>
                            <div class="card-details" id="cat-details-4398403248">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We propose a clinical decision support system (CDSS) for mental health diagnosis that combines the strengths of large language models (LLMs) and constraint logic programming (CLP). Having a CDSS is important because of the high complexity of diagnostic manuals used by mental health professionals and the danger of diagnostic errors. Our CDSS is a software tool that uses an LLM to translate diagnostic manuals to a logic program and solves the program using an off-the-shelf CLP engine to query a patient&#x27;s diagnosis based on the encoded rules and provided data. By giving domain experts the opportu...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a clinical decision support system (CDSS) for mental health diagnosis that combines the strengths of large language models (LLMs) and constraint logic programming (CLP)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2501.07653v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2501.07653v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. H. Kim, and C. Wang, &quot;Large Language Models for Interpretable Mental Health Diagnosis,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2501.07653v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398403248')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.ai" data-title="ars: adaptive reasoning suppression for efficient large reasoning language models" data-keywords="ros language model cs.ai cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2510.00071v2" target="_blank">ARS: Adaptive Reasoning Suppression for Efficient Large Reasoning Language Models</a>
                            </h3>
                            <p class="card-authors">Dongqi Zheng</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398404640">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Reasoning Language Models (LRLMs or LRMs) demonstrate remarkable capabilities in complex reasoning tasks, but suffer from significant computational inefficiencies due to overthinking phenomena. Existing efficient reasoning methods face the challenge of balancing reasoning quality with inference cost reduction. We propose \textbf{Adaptive Reasoning Suppression (ARS)}, a novel training-free approach that dynamically suppresses redundant reasoning steps while preserving accuracy through adaptive certainty monitoring. ARS introduces a multi-checkpoint certainty estimation mechanism with prog...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose \textbf{Adaptive Reasoning Suppression (ARS)}, a novel training-free approach that dynamically suppresses redundant reasoning steps while preserving accuracy through adaptive certainty monitoring</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2510.00071v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2510.00071v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Zheng, &quot;ARS: Adaptive Reasoning Suppression for Efficient Large Reasoning Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2510.00071v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398404640')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.ai" data-title="from code to play: benchmarking program search for games using large language models" data-keywords="control ros language model cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.04057v2" target="_blank">From Code to Play: Benchmarking Program Search for Games Using Large Language Models</a>
                            </h3>
                            <p class="card-authors">Manuel Eberhardinger, James Goodman, Alexander Dockhorn et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Control</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4398402816">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) have shown impressive capabilities in generating program code, opening exciting opportunities for applying program synthesis to games. In this work, we explore the potential of LLMs to directly synthesize usable code for a wide range of gaming applications, focusing on two programming languages, Python and Java. We use an evolutionary hill-climbing algorithm, where the mutations and seeds of the initial programs are controlled by LLMs. For Python, the framework covers various game-related tasks, including five miniature versions of Atari games, ten levels of Baba i...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.04057v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.04057v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Eberhardinger et al., &quot;From Code to Play: Benchmarking Program Search for Games Using Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.04057v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398402816')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.ai" data-title="graph-of-thought: utilizing large language models to solve complex and dynamic business problems" data-keywords="ros language model cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.06801v2" target="_blank">Graph-of-Thought: Utilizing Large Language Models to Solve Complex and Dynamic Business Problems</a>
                            </h3>
                            <p class="card-authors">Ye Li</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4398405360">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents Graph-of-Thought (GoT), a new model for workflow automation that enhances the flexibility and efficiency of Large Language Models (LLMs) in complex task execution. GoT advances beyond traditional linear and tree-like cognitive models with a graph structure that enables dynamic path selection. The open-source engine GoTFlow demonstrates the practical application of GoT, facilitating automated, data-driven decision-making across various domains. Despite challenges in complexity and transparency, GoTFlow&#x27;s potential for improving business processes is significant, promising ad...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents Graph-of-Thought (GoT), a new model for workflow automation that enhances the flexibility and efficiency of Large Language Models (LLMs) in complex task execution</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.06801v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.06801v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Li, &quot;Graph-of-Thought: Utilizing Large Language Models to Solve Complex and Dynamic Business Problems,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.06801v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398405360')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.ai" data-title="from system 1 to system 2: a survey of reasoning large language models" data-keywords="language model cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.17419v6" target="_blank">From System 1 to System 2: A Survey of Reasoning Large Language Models</a>
                            </h3>
                            <p class="card-authors">Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4398404688">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more accurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently, reasoning LLMs like OpenAI&#x27;s o1/o3 and DeepSeek&#x27;s R1 have demonstrated expert-lev...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.17419v6" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.17419v6" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Li et al., &quot;From System 1 to System 2: A Survey of Reasoning Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.17419v6')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398404688')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.ai" data-title="game theory meets large language models: a systematic survey with taxonomy and new frontiers" data-keywords="language model cs.ai cs.gt cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.09053v2" target="_blank">Game Theory Meets Large Language Models: A Systematic Survey with Taxonomy and New Frontiers</a>
                            </h3>
                            <p class="card-authors">Haoran Sun, Yusen Wu, Peng Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.GT</span><span class="keyword-tag">CS.LG</span></div>
                            <div class="card-details" id="cat-details-4398402720">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Game theory is a foundational framework for analyzing strategic interactions, and its intersection with large language models (LLMs) is a rapidly growing field. However, existing surveys mainly focus narrowly on using game theory to evaluate LLM behavior. This paper provides the first comprehensive survey of the bidirectional relationship between Game Theory and LLMs. We propose a novel taxonomy that categorizes the research in this intersection into four distinct perspectives: (1) evaluating LLMs in game-based scenarios; (2) improving LLMs using game-theoretic concepts for better interpretabi...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a novel taxonomy that categorizes the research in this intersection into four distinct perspectives: (1) evaluating LLMs in game-based scenarios; (2) improving LLMs using game-theoretic concepts for better interpretability and alignment; (3) modeling the competitive landscape of LLM development and its societal impact; and (4) leveraging LLMs to advance game models and to solve corresponding game theory problems</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.09053v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.09053v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Sun et al., &quot;Game Theory Meets Large Language Models: A Systematic Survey with Taxonomy and New Frontiers,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.09053v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398402720')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.ai" data-title="language games as the pathway to artificial superhuman intelligence" data-keywords="multi-agent ros language model cs.ai cs.cl cs.ma" data-themes="E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2501.18924v1" target="_blank">Language Games as the Pathway to Artificial Superhuman Intelligence</a>
                            </h3>
                            <p class="card-authors">Ying Wen, Ziyu Wan, Shao Zhang</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Multi-Agent</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4398403536">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The evolution of large language models (LLMs) toward artificial superhuman intelligence (ASI) hinges on data reproduction, a cyclical process in which models generate, curate and retrain on novel data to refine capabilities. Current methods, however, risk getting stuck in a data reproduction trap: optimizing outputs within fixed human-generated distributions in a closed loop leads to stagnation, as models merely recombine existing knowledge rather than explore new frontiers. In this paper, we propose language games as a pathway to expanded data reproduction, breaking this cycle through three m...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose language games as a pathway to expanded data reproduction, breaking this cycle through three mechanisms: (1) \textit{role fluidity}, which enhances data diversity and coverage by enabling multi-agent systems to dynamically shift roles across tasks; (2) \textit{reward variety}, embedding multiple feedback criteria that can drive complex intelligent behaviors; and (3) \textit{rule plasticity}, iteratively evolving interaction constraints to foster learnability, thereby injecting continual novelty</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2501.18924v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2501.18924v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Wen, Z. Wan, and S. Zhang, &quot;Language Games as the Pathway to Artificial Superhuman Intelligence,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2501.18924v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398403536')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.ai" data-title="revealing hidden bias in ai: lessons from large language models" data-keywords="gpt ros language model cs.ai cs.cy" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.16927v1" target="_blank">Revealing Hidden Bias in AI: Lessons from Large Language Models</a>
                            </h3>
                            <p class="card-authors">Django Beatty, Kritsada Masanthia, Teepakorn Kaphol et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4398402528">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">As large language models (LLMs) become integral to recruitment processes, concerns about AI-induced bias have intensified. This study examines biases in candidate interview reports generated by Claude 3.5 Sonnet, GPT-4o, Gemini 1.5, and Llama 3.1 405B, focusing on characteristics such as gender, race, and age. We evaluate the effectiveness of LLM-based anonymization in reducing these biases. Findings indicate that while anonymization reduces certain biases, particularly gender bias, the degree of effectiveness varies across models and bias types. Notably, Llama 3.1 405B exhibited the lowest ov...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Moreover, our methodology of comparing anonymized and non-anonymized data reveals a novel approach to assessing inherent biases in LLMs beyond recruitment applications</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.16927v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.16927v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Beatty, K. Masanthia, T. Kaphol, and N. Sethi, &quot;Revealing Hidden Bias in AI: Lessons from Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.16927v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398402528')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="cs.cr">
                <h2 class="section-header">üè∑Ô∏è CS.CR <span class="section-count">(6 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cr" data-title="attacks on third-party apis of large language models" data-keywords="ros language model cs.cr cs.ai cs.cl" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2404.16891v1" target="_blank">Attacks on Third-Party APIs of Large Language Models</a>
                            </h3>
                            <p class="card-authors">Wanru Zhao, Vidit Khazanchi, Haodi Xing et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CR</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4397084880">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language model (LLM) services have recently begun offering a plugin ecosystem to interact with third-party API services. This innovation enhances the capabilities of LLMs, but it also introduces risks, as these plugins developed by various third parties cannot be easily trusted. This paper proposes a new attacking framework to examine security and safety vulnerabilities within LLM platforms that incorporate third-party services. Applying our framework specifically to widely used LLMs, we identify real-world malicious attacks across various domains on third-party APIs that can imperceptib...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2404.16891v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2404.16891v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. Zhao, V. Khazanchi, H. Xing, X. He, Q. Xu, and N. D. Lane, &quot;Attacks on Third-Party APIs of Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2404.16891v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397084880')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cr" data-title="large language models merging for enhancing the link stealing attack on graph neural networks" data-keywords="neural network gnn ros language model cs.cr cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.05830v1" target="_blank">Large Language Models Merging for Enhancing the Link Stealing Attack on Graph Neural Networks</a>
                            </h3>
                            <p class="card-authors">Faqian Guan, Tianqing Zhu, Wenhan Chang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Gnn</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4398401712">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Graph Neural Networks (GNNs), specifically designed to process the graph data, have achieved remarkable success in various applications. Link stealing attacks on graph data pose a significant privacy threat, as attackers aim to extract sensitive relationships between nodes (entities), potentially leading to academic misconduct, fraudulent transactions, or other malicious activities. Previous studies have primarily focused on single datasets and did not explore cross-dataset attacks, let alone attacks that leverage the combined knowledge of multiple attackers. However, we find that an attacker ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose a novel link stealing attack method that takes advantage of cross-dataset and Large Language Models (LLMs)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.05830v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.05830v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. Guan, T. Zhu, W. Chang, W. Ren, and W. Zhou, &quot;Large Language Models Merging for Enhancing the Link Stealing Attack on Graph Neural Networks,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.05830v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398401712')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cr" data-title="adashield: safeguarding multimodal large language models from structure-based attack via adaptive shield prompting" data-keywords="language model cs.cr cs.ai" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.09513v1" target="_blank">AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting</a>
                            </h3>
                            <p class="card-authors">Yu Wang, Xiaogeng Liu, Yu Li et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CR</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4398401616">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">With the advent and widespread deployment of Multimodal Large Language Models (MLLMs), the imperative to ensure their safety has become increasingly pronounced. However, with the integration of additional modalities, MLLMs are exposed to new vulnerabilities, rendering them prone to structured-based jailbreak attacks, where semantic content (e.g., &quot;harmful text&quot;) has been injected into the images to mislead MLLMs. In this work, we aim to defend against such threats. Specifically, we propose \textbf{Ada}ptive \textbf{Shield} Prompting (\textbf{AdaShield}), which prepends inputs with defense prom...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Specifically, we propose \textbf{Ada}ptive \textbf{Shield} Prompting (\textbf{AdaShield}), which prepends inputs with defense prompts to defend MLLMs against structure-based jailbreak attacks without fine-tuning MLLMs or training additional modules (e.g., post-stage content detector)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.09513v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.09513v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Wang, X. Liu, Y. Li, M. Chen, and C. Xiao, &quot;AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.09513v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398401616')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cr" data-title="sbfa: single sneaky bit flip attack to break large language models" data-keywords="neural network ros language model cs.cr cs.cl cs.lg" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.21843v1" target="_blank">SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models</a>
                            </h3>
                            <p class="card-authors">Jingkai Guo, Chaitali Chakrabarti, Deliang Fan</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CR</span></div>
                            <div class="card-details" id="cat-details-4398403872">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Model integrity of Large language models (LLMs) has become a pressing security concern with their massive online deployment. Prior Bit-Flip Attacks (BFAs) -- a class of popular AI weight memory fault-injection techniques -- can severely compromise Deep Neural Networks (DNNs): as few as tens of bit flips can degrade accuracy toward random guessing. Recent studies extend BFAs to LLMs and reveal that, despite the intuition of better robustness from modularity and redundancy, only a handful of adversarial bit flips can also cause LLMs&#x27; catastrophic accuracy degradation. However, existing BFA metho...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, for the first time, we propose SBFA (Sneaky Bit-Flip Attack), which collapses LLM performance with only one single bit flip while keeping perturbed values within benign layer-wise weight distribution</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.21843v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.21843v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Guo, C. Chakrabarti, and D. Fan, &quot;SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.21843v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398403872')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.cr" data-title="on trojan signatures in large language models of code" data-keywords="computer vision language model cs.cr cs.lg cs.se" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.16896v2" target="_blank">On Trojan Signatures in Large Language Models of Code</a>
                            </h3>
                            <p class="card-authors">Aftab Hussain, Md Rafiqul Islam Rabin, Mohammad Amin Alipour</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Computer Vision</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CR</span><span class="keyword-tag">CS.LG</span></div>
                            <div class="card-details" id="cat-details-4398404496">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Trojan signatures, as described by Fields et al. (2021), are noticeable differences in the distribution of the trojaned class parameters (weights) and the non-trojaned class parameters of the trojaned model, that can be used to detect the trojaned model. Fields et al. (2021) found trojan signatures in computer vision classification tasks with image models, such as, Resnet, WideResnet, Densenet, and VGG. In this paper, we investigate such signatures in the classifier layer parameters of large language models of source code.
  Our results suggest that trojan signatures could not generalize to LL...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.16896v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.16896v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Hussain, M. R. I. Rabin, and M. A. Alipour, &quot;On Trojan Signatures in Large Language Models of Code,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.16896v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398404496')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.cr" data-title="augmenting anonymized data with ai: exploring the feasibility and limitations of large language models in data enrichment" data-keywords="language model cs.cr cs.et" data-themes="S I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.03778v1" target="_blank">Augmenting Anonymized Data with AI: Exploring the Feasibility and Limitations of Large Language Models in Data Enrichment</a>
                            </h3>
                            <p class="card-authors">Stefano Cirillo, Domenico Desiato, Giuseppe Polese et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CR</span><span class="keyword-tag">CS.ET</span></div>
                            <div class="card-details" id="cat-details-4398405312">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Language Models (LLMs) have demonstrated advanced capabilities in both text generation and comprehension, and their application to data archives might facilitate the privatization of sensitive information about the data subjects. In fact, the information contained in data often includes sensitive and personally identifiable details. This data, if not safeguarded, may bring privacy risks in terms of both disclosure and identification. Furthermore, the application of anonymisation techniques, such as k-anonymity, can lead to a significant reduction in the amount of data within data sources...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To this end, we designed new ad-hoc prompt template engineering strategies to perform anonymized Data Augmentation and assess the effectiveness of LLM-based approaches in providing anonymized data</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.03778v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.03778v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Cirillo, D. Desiato, G. Polese, M. M. L. Sebillo, and G. Solimando, &quot;Augmenting Anonymized Data with AI: Exploring the Feasibility and Limitations of Large Language Models in Data Enrichment,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.03778v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398405312')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="cs.hc">
                <h2 class="section-header">üè∑Ô∏è Human-Computer Interaction <span class="section-count">(4 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.hc" data-title="exploring large language models to facilitate variable autonomy for human-robot teaming" data-keywords="transformer gpt robot multi-robot language model cs.hc cs.ai cs.ro" data-themes="E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.07214v3" target="_blank">Exploring Large Language Models to Facilitate Variable Autonomy for Human-Robot Teaming</a>
                            </h3>
                            <p class="card-authors">Younes Lakhnati, Max Pascher, Jens Gerken</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Transformer</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Robot</span></div>
                            <div class="card-details" id="cat-details-4397313104">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In a rapidly evolving digital landscape autonomous tools and robots are becoming commonplace. Recognizing the significance of this development, this paper explores the integration of Large Language Models (LLMs) like Generative pre-trained transformer (GPT) into human-robot teaming environments to facilitate variable autonomy through the means of verbal human-robot communication. In this paper, we introduce a novel framework for such a GPT-powered multi-robot testbed environment, based on a Unity Virtual Reality (VR) setting. This system allows users to interact with robot agents through natur...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we introduce a novel framework for such a GPT-powered multi-robot testbed environment, based on a Unity Virtual Reality (VR) setting</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.07214v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.07214v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Lakhnati, M. Pascher, and J. Gerken, &quot;Exploring Large Language Models to Facilitate Variable Autonomy for Human-Robot Teaming,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.07214v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397313104')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.hc" data-title="exploring bengali religious dialect biases in large language models with evaluation perspectives" data-keywords="gpt ros language model cs.hc cs.cl cs.cy" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.18376v1" target="_blank">Exploring Bengali Religious Dialect Biases in Large Language Models with Evaluation Perspectives</a>
                            </h3>
                            <p class="card-authors">Azmine Toushik Wasi, Raima Islam, Mst Rafia Islam et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.HC</span></div>
                            <div class="card-details" id="cat-details-4397307920">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">While Large Language Models (LLM) have created a massive technological impact in the past decade, allowing for human-enabled applications, they can produce output that contains stereotypes and biases, especially when using low-resource languages. This can be of great ethical concern when dealing with sensitive topics such as religion. As a means toward making LLMS more fair, we explore bias from a religious perspective in Bengali, focusing specifically on two main religious dialects: Hindu and Muslim-majority dialects. Here, we perform different experiments and audit showing the comparative an...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.18376v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.18376v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. T. Wasi, R. Islam, M. R. Islam, T. H. Rafi, and D. Chae, &quot;Exploring Bengali Religious Dialect Biases in Large Language Models with Evaluation Perspectives,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.18376v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397307920')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.hc" data-title="large language models will change the way children think about technology and impact every interaction paradigm" data-keywords="language model cs.hc cs.ai cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.13667v1" target="_blank">Large Language Models Will Change The Way Children Think About Technology And Impact Every Interaction Paradigm</a>
                            </h3>
                            <p class="card-authors">Russell Beale</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.HC</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398403584">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents a hopeful perspective on the potentially dramatic impacts of Large Language Models on how we children learn and how they will expect to interact with technology. We review the effects of LLMs on education so far, and make the case that these effects are minor compared to the upcoming changes that are occurring. We present a small scenario and self-ethnographic study demonstrating the effects of these changes, and define five significant considerations that interactive systems designers will have to accommodate in the future.</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents a hopeful perspective on the potentially dramatic impacts of Large Language Models on how we children learn and how they will expect to interact with technology</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.13667v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.13667v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Beale, &quot;Large Language Models Will Change The Way Children Think About Technology And Impact Every Interaction Paradigm,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.13667v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398403584')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2024" data-category="cs.hc" data-title="users&#x27; perception on appropriateness of robotic coaching assistant&#x27;s disclosure behaviors" data-keywords="robot perception cs.hc" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.10550v1" target="_blank">Users&#x27; Perception on Appropriateness of Robotic Coaching Assistant&#x27;s Disclosure Behaviors</a>
                            </h3>
                            <p class="card-authors">Atikkhan Faridkhan Nilgar, Manuel Dietrich, Kristof Van Laerhoven</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Perception</span><span class="keyword-tag">CS.HC</span></div>
                            <div class="card-details" id="cat-details-4399348416">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Social robots have emerged as valuable contributors to individuals&#x27; well-being coaching. Notably, their integration into long-term human coaching trials shows particular promise, emphasizing a complementary role alongside human coaches rather than outright replacement. In this context, robots serve as supportive entities during coaching sessions, offering insights based on their knowledge about users&#x27; well-being and activity. Traditionally, such insights have been gathered through methods like written self-reports or wearable data visualizations. However, the disclosure of people&#x27;s information...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.10550v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.10550v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. F. Nilgar, M. Dietrich, and K. V. Laerhoven, &quot;Users&#x27; Perception on Appropriateness of Robotic Coaching Assistant&#x27;s Disclosure Behaviors,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.10550v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399348416')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="cs.se">
                <h2 class="section-header">üè∑Ô∏è CS.SE <span class="section-count">(3 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cs.se" data-title="a survey of aiops in the era of large language models" data-keywords="attention ros language model cs.se cs.cl" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.12472v1" target="_blank">A Survey of AIOps in the Era of Large Language Models</a>
                            </h3>
                            <p class="card-authors">Lingzhe Zhang, Tong Jia, Mengxi Jia et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.SE</span></div>
                            <div class="card-details" id="cat-details-4397304560">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">As large language models (LLMs) grow increasingly sophisticated and pervasive, their application to various Artificial Intelligence for IT Operations (AIOps) tasks has garnered significant attention. However, a comprehensive understanding of the impact, potential, and limitations of LLMs in AIOps remains in its infancy. To address this gap, we conducted a detailed survey of LLM4AIOps, focusing on how LLMs can optimize processes and improve outcomes in this domain. We analyzed 183 research papers published between January 2020 and December 2024 to answer four key research questions (RQs). In RQ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.12472v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.12472v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Zhang et al., &quot;A Survey of AIOps in the Era of Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.12472v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397304560')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.se" data-title="metal: metamorphic testing framework for analyzing large-language model qualities" data-keywords="language model cs.se cs.ai cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.06056v1" target="_blank">METAL: Metamorphic Testing Framework for Analyzing Large-Language Model Qualities</a>
                            </h3>
                            <p class="card-authors">Sangwon Hyun, Mingyu Guo, M. Ali Babar</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.SE</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398401376">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large-Language Models (LLMs) have shifted the paradigm of natural language data processing. However, their black-boxed and probabilistic characteristics can lead to potential risks in the quality of outputs in diverse LLM applications. Recent studies have tested Quality Attributes (QAs), such as robustness or fairness, of LLMs by generating adversarial input texts. However, existing studies have limited their coverage of QAs and tasks in LLMs and are difficult to extend. Additionally, these studies have only used one evaluation metric, Attack Success Rate (ASR), to assess the effectiveness of ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a MEtamorphic Testing for Analyzing LLMs (METAL) framework to address these issues by applying Metamorphic Testing (MT) techniques</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.06056v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.06056v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Hyun, M. Guo, and M. A. Babar, &quot;METAL: Metamorphic Testing Framework for Analyzing Large-Language Model Qualities,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.06056v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398401376')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="cs.se" data-title="assurance for autonomy -- jpl&#x27;s past research, lessons learned, and future directions" data-keywords="robot control cs.se cs.ro" data-themes="M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2305.11902v1" target="_blank">Assurance for Autonomy -- JPL&#x27;s past research, lessons learned, and future directions</a>
                            </h3>
                            <p class="card-authors">Martin S. Feather, Alessandro Pinto</p>
                            <div class="theme-tags"><span class="theme-badge theme-M">Manipulation</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Control</span><span class="keyword-tag">CS.SE</span><span class="keyword-tag">CS.RO</span></div>
                            <div class="card-details" id="cat-details-4398404880">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Robotic space missions have long depended on automation, defined in the 2015 NASA Technology Roadmaps as &quot;the automatically-controlled operation of an apparatus, process, or system using a pre-planned set of instructions (e.g., a command sequence),&quot; to react to events when a rapid response is required. Autonomy, defined there as &quot;the capacity of a system to achieve goals while operating independently from external control,&quot; is required when a wide variation in circumstances precludes responses being pre-planned, instead autonomy follows an on-board deliberative process to determine the situati...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2305.11902v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2305.11902v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. S. Feather, and A. Pinto, &quot;Assurance for Autonomy -- JPL&#x27;s past research, lessons learned, and future directions,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2305.11902v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398404880')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="eess.as">
                <h2 class="section-header">üè∑Ô∏è EESS.AS <span class="section-count">(2 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2023" data-category="eess.as" data-title="acoustic prompt tuning: empowering large language models with audition capabilities" data-keywords="ros language model eess.as" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.00249v2" target="_blank">Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities</a>
                            </h3>
                            <p class="card-authors">Jinhua Liang, Xubo Liu, Wenwu Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">EESS.AS</span></div>
                            <div class="card-details" id="cat-details-4398401568">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The auditory system plays a substantial role in shaping the overall human perceptual experience. While prevailing large language models (LLMs) and visual language models (VLMs) have shown their promise in solving a wide variety of language and vision understanding tasks, only a few of them can be generalised to the audio domain without compromising their domain-specific capability. In this work, we introduce Acoustic Prompt Tuning (APT), a new adapter extending LLMs and VLMs to the audio domain by injecting audio embeddings to the input of LLMs, namely soft prompting. Specifically, APT applies...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we introduce Acoustic Prompt Tuning (APT), a new adapter extending LLMs and VLMs to the audio domain by injecting audio embeddings to the input of LLMs, namely soft prompting</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.00249v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.00249v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Liang, X. Liu, W. Wang, M. D. Plumbley, H. Phan, and E. Benetos, &quot;Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.00249v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398401568')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="eess.as" data-title="speechprompt: prompting speech language models for speech processing tasks" data-keywords="language model eess.as cs.ai cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.13040v1" target="_blank">SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks</a>
                            </h3>
                            <p class="card-authors">Kai-Wei Chang, Haibin Wu, Yu-Kai Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">EESS.AS</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4398404304">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Prompting has become a practical method for utilizing pre-trained language models (LMs). This approach offers several advantages. It allows an LM to adapt to new tasks with minimal training and parameter updates, thus achieving efficiency in both storage and computation. Additionally, prompting modifies only the LM&#x27;s inputs and harnesses the generative capabilities of language models to address various downstream tasks in a unified manner. This significantly reduces the need for human labor in designing task-specific models. These advantages become even more evident as the number of tasks serv...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.13040v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.13040v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Chang et al., &quot;SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.13040v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398404304')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="cond-mat.mtrl-sci">
                <h2 class="section-header">üè∑Ô∏è COND-MAT.MTRL-SCI <span class="section-count">(1 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="cond-mat.mtrl-sci" data-title="hierarchical multi-agent large language model reasoning for autonomous functional materials discovery" data-keywords="multi-agent simulation ros imu language model cond-mat.mtrl-sci cs.ai cs.cl" data-themes="I E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Multi-Agent Systems</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2512.13930v1" target="_blank">Hierarchical Multi-agent Large Language Model Reasoning for Autonomous Functional Materials Discovery</a>
                            </h3>
                            <p class="card-authors">Samuel Rothfarb, Megan C. Davis, Ivana Matanovic et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Multi-Agent</span><span class="keyword-tag">Simulation</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Imu</span></div>
                            <div class="card-details" id="cat-details-4397308448">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Artificial intelligence is reshaping scientific exploration, but most methods automate procedural tasks without engaging in scientific reasoning, limiting autonomy in discovery. We introduce Materials Agents for Simulation and Theory in Electronic-structure Reasoning (MASTER), an active learning framework where large language models autonomously design, execute, and interpret atomistic simulations. In MASTER, a multimodal system translates natural language into density functional theory workflows, while higher-level reasoning agents guide discovery through a hierarchy of strategies, including ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce Materials Agents for Simulation and Theory in Electronic-structure Reasoning (MASTER), an active learning framework where large language models autonomously design, execute, and interpret atomistic simulations</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2512.13930v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2512.13930v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Rothfarb, M. C. Davis, I. Matanovic, B. Li, E. F. Holby, and W. J. M. Kort-Kamp, &quot;Hierarchical Multi-agent Large Language Model Reasoning for Autonomous Functional Materials Discovery,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2512.13930v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4397308448')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="cs.ce">
                <h2 class="section-header">üè∑Ô∏è CS.CE <span class="section-count">(1 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.ce" data-title="leveraging large language models for institutional portfolio management: persona-based ensembles" data-keywords="ros language model cs.ce cs.ma" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2411.19515v1" target="_blank">Leveraging Large Language Models for Institutional Portfolio Management: Persona-Based Ensembles</a>
                            </h3>
                            <p class="card-authors">Yoshia Abe, Shuhei Matsuo, Ryoma Kondo et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CE</span><span class="keyword-tag">CS.MA</span></div>
                            <div class="card-details" id="cat-details-4398404064">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) have demonstrated promising performance in various financial applications, though their potential in complex investment strategies remains underexplored. To address this gap, we investigate how LLMs can predict price movements in stock and bond portfolios using economic indicators, enabling portfolio adjustments akin to those employed by institutional investors. Additionally, we explore the impact of incorporating different personas within LLMs, using an ensemble approach to leverage their diverse predictions. Our findings show that LLM-based strategies, especially...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2411.19515v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2411.19515v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Abe, S. Matsuo, R. Kondo, and R. Hisano, &quot;Leveraging Large Language Models for Institutional Portfolio Management: Persona-Based Ensembles,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2411.19515v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398404064')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="q-bio.qm">
                <h2 class="section-header">üè∑Ô∏è Q-BIO.QM <span class="section-count">(1 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="q-bio.qm" data-title="gene-associated disease discovery powered by large language models" data-keywords="language model q-bio.qm cs.ir" data-themes="I E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.09490v1" target="_blank">Gene-associated Disease Discovery Powered by Large Language Models</a>
                            </h3>
                            <p class="card-authors">Jiayu Chang, Shiyu Wang, Chen Ling et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">Q-BIO.QM</span><span class="keyword-tag">CS.IR</span></div>
                            <div class="card-details" id="cat-details-4398404448">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The intricate relationship between genetic variation and human diseases has been a focal point of medical research, evidenced by the identification of risk genes regarding specific diseases. The advent of advanced genome sequencing techniques has significantly improved the efficiency and cost-effectiveness of detecting these genetic markers, playing a crucial role in disease diagnosis and forming the basis for clinical decision-making and early risk assessment. To overcome the limitations of existing databases that record disease-gene associations from existing literature, which often lack rea...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To overcome the limitations of existing databases that record disease-gene associations from existing literature, which often lack real-time updates, we propose a novel framework employing Large Language Models (LLMs) for the discovery of diseases associated with specific genes</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.09490v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.09490v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Chang, S. Wang, C. Ling, Z. Qin, and L. Zhao, &quot;Gene-associated Disease Discovery Powered by Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.09490v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398404448')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="cs.ne">
                <h2 class="section-header">üè∑Ô∏è Neural & Evolutionary <span class="section-count">(1 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2024" data-category="cs.ne" data-title="evolutionary computation in the era of large language model: survey and roadmap" data-keywords="optimization ros language model cs.ne cs.ai cs.cl" data-themes="M E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.10034v3" target="_blank">Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap</a>
                            </h3>
                            <p class="card-authors">Xingyu Wu, Sheng-hao Wu, Jibin Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Optimization</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.NE</span></div>
                            <div class="card-details" id="cat-details-4398404592">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride towards artificial general intelligence. The interplay between LLMs and evolutionary algorithms (EAs), despite differing in objectives and methodologies, share a common pursuit of applicability in complex problems. Meanwhile, EA can provide an optimization framework for LLM&#x27;s further enhancement under black-box settings, empowering LLM with flexible global search capacities. On the other hand, the abundant domain knowledge inhere...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.10034v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.10034v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. Wu, S. Wu, J. Wu, L. Feng, and K. C. Tan, &quot;Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.10034v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398404592')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="q-bio.gn">
                <h2 class="section-header">üè∑Ô∏è Q-BIO.GN <span class="section-count">(1 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-rating="skip" data-source="arXiv" data-year="2025" data-category="q-bio.gn" data-title="deepseq: high-throughput single-cell rna sequencing data labeling via web search-augmented agentic generative ai foundation models" data-keywords="q-bio.gn cs.ai cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.13817v1" target="_blank">DeepSeq: High-Throughput Single-Cell RNA Sequencing Data Labeling via Web Search-Augmented Agentic Generative AI Foundation Models</a>
                            </h3>
                            <p class="card-authors">Saleem A. Al Dajani, Abel Sanchez, John R. Williams</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div><span class="badge rating-skip">‚è≠Ô∏è Skip</span>
                            <div class="card-keywords"><span class="keyword-tag">Q-BIO.GN</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.LG</span></div>
                            <div class="card-details" id="cat-details-4398402960">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Generative AI foundation models offer transformative potential for processing structured biological data, particularly in single-cell RNA sequencing, where datasets are rapidly scaling toward billions of cells. We propose the use of agentic foundation models with real-time web search to automate the labeling of experimental data, achieving up to 82.5% accuracy. This addresses a key bottleneck in supervised learning for structured omics data by increasing annotation throughput without manual curation and human error. Our approach enables the development of virtual cell foundation models capable...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose the use of agentic foundation models with real-time web search to automate the labeling of experimental data, achieving up to 82.5% accuracy</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.13817v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.13817v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. A. A. Dajani, A. Sanchez, and J. R. Williams, &quot;DeepSeq: High-Throughput Single-Cell RNA Sequencing Data Labeling via Web Search-Augmented Agentic Generative AI Foundation Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.13817v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398402960')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="nlin.ao">
                <h2 class="section-header">üè∑Ô∏è NLIN.AO <span class="section-count">(1 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-rating="optional" data-source="arXiv" data-year="2024" data-category="nlin.ao" data-title="self-organized attractoring in locomoting animals and robots: an emerging field" data-keywords="robot locomotion coordination control imu nlin.ao cond-mat.dis-nn" data-themes="S I L R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.13581v1" target="_blank">Self-organized attractoring in locomoting animals and robots: an emerging field</a>
                            </h3>
                            <p class="card-authors">Bulcs√∫ S√°ndor, Claudius Gros</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-optional">üìñ Optional</span>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Coordination</span><span class="keyword-tag">Control</span></div>
                            <div class="card-details" id="cat-details-4399348992">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Locomotion may be induced on three levels. On a classical level, actuators and limbs follow the sequence of open-loop top-down control signals they receive. Limbs may move alternatively on their own, which implies that interlimb coordination must be mediated either by the body or via decentralized inter-limb signaling. In this case, when embodiment is present, two types of controllers are conceivable for the actuators of the limbs, local pacemaker circuits and control principles based on self-organized embodiment. The latter, self-organized control, is based on limit cycles and chaotic attract...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.13581v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.13581v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. S√°ndor, and C. Gros, &quot;Self-organized attractoring in locomoting animals and robots: an emerging field,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.13581v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4399348992')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="eess.sy">
                <h2 class="section-header">üè∑Ô∏è Signal Processing <span class="section-count">(1 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-rating="must_read" data-source="arXiv" data-year="2024" data-category="eess.sy" data-title="structural optimization of lightweight bipedal robot via serl" data-keywords="reinforcement learning robot bipedal locomotion optimization eess.sy cs.ai" data-themes="I L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.15632v1" target="_blank">Structural Optimization of Lightweight Bipedal Robot via SERL</a>
                            </h3>
                            <p class="card-authors">Yi Cheng, Chenxi Han, Yuheng Min et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-L">Locomotion</span></div><span class="badge rating-must_read">‚≠ê Must Read</span>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4398433232">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Designing a bipedal robot is a complex and challenging task, especially when dealing with a multitude of structural parameters. Traditional design methods often rely on human intuition and experience. However, such approaches are time-consuming, labor-intensive, lack theoretical guidance and hard to obtain optimal design results within vast design spaces, thus failing to full exploit the inherent performance potential of robots. In this context, this paper introduces the SERL (Structure Evolution Reinforcement Learning) algorithm, which combines reinforcement learning for locomotion tasks with...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.15632v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.15632v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Cheng, C. Han, Y. Min, L. Ye, H. Liu, and H. Liu, &quot;Structural Optimization of Lightweight Bipedal Robot via SERL,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.15632v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4398433232')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

        </div>
    </div>

    <!-- Citation Tooltip -->
    <div class="cite-tooltip" id="cite-tooltip">‚úì IEEE Citation Copied!</div>

    <script>
        let currentSource = 'all';
        let currentYear = 'all';
        let currentCategory = 'all';
        let currentRating = 'all';
        let currentSearch = '';
        let currentView = 'year';

        function setView(view) {
            currentView = view;
            document.getElementById('btn-year').classList.toggle('active', view === 'year');
            document.getElementById('btn-category').classList.toggle('active', view === 'category');
            document.getElementById('year-view').classList.toggle('hidden', view !== 'year');
            document.getElementById('category-view').classList.toggle('hidden', view !== 'category');

            // Reset filters when switching views
            currentYear = 'all';
            currentCategory = 'all';
            currentRating = 'all';
            document.querySelectorAll('.year-btn, .cat-btn, .rating-btn').forEach(btn => btn.classList.remove('active'));
            document.querySelector('.year-btn').classList.add('active');
            document.querySelector('.cat-btn').classList.add('active');

            applyFilters();
        }

        function applyFilters() {
            const activeView = currentView === 'year' ? '#year-view' : '#category-view';

            document.querySelectorAll(activeView + ' .paper-card').forEach(card => {
                const matchSource = currentSource === 'all' || card.dataset.source === currentSource;
                const matchYear = currentYear === 'all' || card.dataset.year === String(currentYear);
                const matchCategory = currentCategory === 'all' || card.dataset.category === currentCategory;
                const matchRating = currentRating === 'all' || card.dataset.rating === currentRating;
                const matchSearch = currentSearch === '' ||
                    card.dataset.title.includes(currentSearch) ||
                    card.dataset.keywords.includes(currentSearch);

                if (matchSource && matchYear && matchCategory && matchRating && matchSearch) {
                    card.classList.remove('hidden');
                } else {
                    card.classList.add('hidden');
                }
            });

            // Hide empty sections
            document.querySelectorAll(activeView + ' .group-section').forEach(section => {
                const visibleCards = section.querySelectorAll('.paper-card:not(.hidden)');
                section.style.display = visibleCards.length === 0 ? 'none' : 'block';
            });
        }

        function filterBySource(source) {
            currentSource = source;
            document.querySelectorAll('.source-btn').forEach(btn => btn.classList.remove('active'));
            event.target.classList.add('active');
            applyFilters();
        }

        function filterByYear(year) {
            currentYear = year;
            document.querySelectorAll('.year-btn').forEach(btn => btn.classList.remove('active'));
            event.target.classList.add('active');
            applyFilters();
        }

        
        function filterByRating(rating) {
            currentRating = rating;
            document.querySelectorAll('.rating-btn').forEach(btn => btn.classList.remove('active'));
            event.target.classList.add('active');
            applyFilters();
        }

function filterByCategory(category) {
            currentCategory = category;
            document.querySelectorAll('.cat-btn').forEach(btn => btn.classList.remove('active'));
            event.target.classList.add('active');
            applyFilters();
        }

        function searchPapers(query) {
            currentSearch = query.toLowerCase();
            applyFilters();
        }

        function toggleCard(btn, detailsId) {
            const details = document.getElementById(detailsId);
            details.classList.toggle('show');
            btn.textContent = details.classList.contains('show') ? 'Show Less' : 'Show More';
        }

        function copyCitation(btn, citation) {
            // Decode HTML entities
            const textarea = document.createElement('textarea');
            textarea.innerHTML = citation;
            const decodedCitation = textarea.value;

            // Copy to clipboard
            navigator.clipboard.writeText(decodedCitation).then(() => {
                // Visual feedback on button
                const originalText = btn.innerHTML;
                btn.innerHTML = '‚úì Copied!';
                btn.classList.add('copied');

                // Show tooltip
                const tooltip = document.getElementById('cite-tooltip');
                tooltip.classList.add('show');

                // Reset after 2 seconds
                setTimeout(() => {
                    btn.innerHTML = originalText;
                    btn.classList.remove('copied');
                    tooltip.classList.remove('show');
                }, 2000);
            }).catch(err => {
                // Fallback for older browsers
                const textArea = document.createElement('textarea');
                textArea.value = decodedCitation;
                textArea.style.position = 'fixed';
                textArea.style.left = '-999999px';
                document.body.appendChild(textArea);
                textArea.select();
                try {
                    document.execCommand('copy');
                    btn.innerHTML = '‚úì Copied!';
                    btn.classList.add('copied');
                    setTimeout(() => {
                        btn.innerHTML = 'üìã Cite';
                        btn.classList.remove('copied');
                    }, 2000);
                } catch (e) {
                    alert('Failed to copy citation. Please copy manually:\n\n' + decodedCitation);
                }
                document.body.removeChild(textArea);
            });
        }
    </script>
</body>
</html>
