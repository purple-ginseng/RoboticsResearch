<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Report - Multi-Source Literature Review</title>
    <style>
        * { box-sizing: border-box; margin: 0; padding: 0; }
        :root {
            --accent: #00d4ff;
            --accent-secondary: #667eea;
            --bg-dark: #0a0a1a;
            --bg-card: #12122a;
            --bg-hover: #1a1a3a;
            --text-primary: #e8e8f0;
            --text-secondary: #888899;
            --border: #2a2a4a;
            --success: #10b981;
            --warning: #f59e0b;
            --highlight: #ec4899;
        }
        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: var(--bg-dark);
            color: var(--text-primary);
            min-height: 100vh;
            line-height: 1.6;
        }
        .container { max-width: 1600px; margin: 0 auto; padding: 30px; }

        /* Header */
        header {
            text-align: center;
            padding: 60px 40px;
            margin-bottom: 40px;
            background: linear-gradient(180deg, #1a1a3a 0%, var(--bg-dark) 100%);
            border-bottom: 1px solid var(--border);
        }
        h1 {
            color: var(--text-primary);
            font-size: 2.8rem;
            font-weight: 700;
            margin-bottom: 10px;
            letter-spacing: -0.5px;
        }
        .subtitle {
            color: var(--text-secondary);
            font-size: 1.1rem;
            margin-bottom: 15px;
        }
        .timeline-link {
            display: inline-block;
            padding: 12px 28px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: #fff;
            text-decoration: none;
            border-radius: 25px;
            font-weight: 600;
            font-size: 0.95rem;
            margin-bottom: 20px;
            transition: all 0.3s;
            box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4);
        }
        .timeline-link:hover {
            transform: translateY(-3px);
            box-shadow: 0 8px 25px rgba(102, 126, 234, 0.6);
        }
        .stats {
            display: flex;
            justify-content: center;
            gap: 40px;
            margin-top: 30px;
        }
        .stat {
            text-align: center;
        }
        .stat-value {
            font-size: 2.5rem;
            font-weight: 700;
            color: var(--accent);
            display: block;
        }
        .stat-label {
            font-size: 0.9rem;
            color: var(--text-secondary);
            text-transform: uppercase;
            letter-spacing: 1px;
        }

        /* Filters */
        .filters {
            background: var(--bg-card);
            border: 1px solid var(--border);
            border-radius: 12px;
            padding: 20px 25px;
            margin-bottom: 30px;
            display: flex;
            flex-wrap: wrap;
            gap: 12px;
            align-items: center;
        }
        .filter-label {
            color: var(--text-secondary);
            font-size: 0.85rem;
            font-weight: 500;
            margin-right: 5px;
        }
        .filter-btn {
            padding: 8px 16px;
            border-radius: 20px;
            border: 1px solid var(--border);
            background: transparent;
            color: var(--text-secondary);
            cursor: pointer;
            font-size: 0.85rem;
            transition: all 0.2s;
        }
        .filter-btn:hover, .filter-btn.active {
            background: var(--accent);
            border-color: var(--accent);
            color: var(--bg-dark);
        }
        .search-box {
            padding: 10px 16px;
            border-radius: 8px;
            border: 1px solid var(--border);
            background: var(--bg-dark);
            color: var(--text-primary);
            font-size: 0.9rem;
            width: 280px;
            margin-left: auto;
        }
        .search-box:focus {
            outline: none;
            border-color: var(--accent);
        }
        .filter-divider {
            width: 1px;
            height: 30px;
            background: var(--border);
            margin: 0 10px;
        }

        /* Year sections */
        .year-section {
            margin-bottom: 50px;
        }
        .year-header {
            font-size: 1.8rem;
            font-weight: 700;
            color: var(--text-primary);
            margin-bottom: 25px;
            padding-bottom: 15px;
            border-bottom: 2px solid var(--accent);
            display: inline-block;
        }

        /* Paper grid - PSI Lab style */
        .papers-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(450px, 1fr));
            gap: 25px;
        }

        /* Paper card */
        .paper-card {
            background: var(--bg-card);
            border: 1px solid var(--border);
            border-radius: 12px;
            overflow: hidden;
            transition: all 0.3s ease;
            display: flex;
            flex-direction: column;
        }
        .paper-card:hover {
            transform: translateY(-4px);
            border-color: var(--accent);
            box-shadow: 0 20px 40px rgba(0, 212, 255, 0.1);
        }

        /* Card thumbnail area */
        .card-visual {
            height: 180px;
            background: linear-gradient(135deg, #1a1a3a 0%, #2a2a5a 100%);
            display: flex;
            align-items: center;
            justify-content: center;
            position: relative;
            overflow: hidden;
        }
        .card-visual-icon {
            font-size: 4rem;
            opacity: 0.3;
        }
        .card-badges {
            position: absolute;
            top: 12px;
            right: 12px;
            display: flex;
            gap: 6px;
        }
        .badge {
            padding: 4px 10px;
            border-radius: 12px;
            font-size: 0.7rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }
        .badge-citations {
            background: var(--warning);
            color: #000;
        }
        .badge-new {
            background: var(--success);
            color: #000;
        }
        .badge-highlight {
            background: var(--highlight);
            color: #fff;
        }
        /* Research theme badges */
        .theme-badge {
            padding: 3px 8px;
            border-radius: 4px;
            font-size: 0.65rem;
            font-weight: 600;
            text-transform: uppercase;
        }
        .theme-L { background: #3b82f6; color: #fff; }  /* Locomotion - Blue */
        .theme-I { background: #8b5cf6; color: #fff; }  /* Imitation - Purple */
        .theme-M { background: #f59e0b; color: #000; }  /* Manipulation - Orange */
        .theme-E { background: #10b981; color: #000; }  /* Embodied AI - Green */
        .theme-R { background: #ef4444; color: #fff; }  /* Multi-Robot - Red */
        .theme-S { background: #6366f1; color: #fff; }  /* Safety - Indigo */
        /* Read rating badges */
        .rating-must_read { background: linear-gradient(135deg, #fbbf24, #f59e0b); color: #000; }
        .rating-optional { background: rgba(255,255,255,0.2); color: var(--text-secondary); border: 1px solid var(--border); }
        .rating-skip { background: rgba(100,100,100,0.3); color: var(--text-secondary); }
        /* Summary info block */
        .summary-block {
            background: rgba(0, 212, 255, 0.05);
            border-left: 3px solid var(--accent);
            padding: 12px 15px;
            margin: 10px 0;
            border-radius: 0 8px 8px 0;
        }
        .summary-label {
            font-size: 0.7rem;
            color: var(--accent);
            text-transform: uppercase;
            letter-spacing: 0.5px;
            margin-bottom: 4px;
        }
        .summary-text {
            font-size: 0.85rem;
            color: var(--text-secondary);
            line-height: 1.5;
        }
        .pros-cons {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 10px;
            margin-top: 10px;
        }
        .pros { border-left-color: var(--success); }
        .cons { border-left-color: var(--highlight); }
        .theme-tags {
            display: flex;
            gap: 4px;
            flex-wrap: wrap;
            margin-top: 8px;
        }
        .card-source {
            position: absolute;
            bottom: 12px;
            left: 12px;
            padding: 4px 10px;
            background: rgba(0,0,0,0.6);
            border-radius: 6px;
            font-size: 0.75rem;
            color: var(--text-secondary);
        }
        .card-tech {
            position: absolute;
            bottom: 12px;
            right: 12px;
            display: flex;
            gap: 4px;
        }
        .tech-chip {
            padding: 3px 8px;
            background: rgba(102, 126, 234, 0.3);
            border-radius: 4px;
            font-size: 0.65rem;
            color: var(--accent-secondary);
        }

        /* Card content */
        .card-content {
            padding: 20px;
            flex: 1;
            display: flex;
            flex-direction: column;
        }
        .card-title {
            font-size: 1.05rem;
            font-weight: 600;
            color: var(--text-primary);
            margin-bottom: 10px;
            line-height: 1.4;
            display: -webkit-box;
            -webkit-line-clamp: 2;
            -webkit-box-orient: vertical;
            overflow: hidden;
        }
        .card-title a {
            color: inherit;
            text-decoration: none;
        }
        .card-title a:hover {
            color: var(--accent);
        }
        .card-authors {
            font-size: 0.85rem;
            color: var(--text-secondary);
            margin-bottom: 12px;
            display: -webkit-box;
            -webkit-line-clamp: 1;
            -webkit-box-orient: vertical;
            overflow: hidden;
        }
        .card-keywords {
            display: flex;
            flex-wrap: wrap;
            gap: 6px;
            margin-bottom: 15px;
        }
        .keyword-tag {
            padding: 3px 10px;
            background: rgba(0, 212, 255, 0.1);
            border: 1px solid rgba(0, 212, 255, 0.3);
            border-radius: 12px;
            font-size: 0.7rem;
            color: var(--accent);
        }

        /* Card details (expandable) */
        .card-details {
            display: none;
            margin-top: 15px;
            padding-top: 15px;
            border-top: 1px solid var(--border);
        }
        .card-details.show {
            display: block;
        }
        .detail-block {
            margin-bottom: 15px;
        }
        .detail-label {
            font-size: 0.75rem;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            color: var(--accent);
            margin-bottom: 6px;
            font-weight: 600;
        }
        .detail-text {
            font-size: 0.85rem;
            color: var(--text-secondary);
            line-height: 1.6;
        }
        .abstract-text {
            max-height: 150px;
            overflow-y: auto;
        }

        /* Card footer */
        .card-footer {
            padding: 15px 20px;
            background: rgba(0,0,0,0.2);
            border-top: 1px solid var(--border);
            display: flex;
            gap: 8px;
            align-items: center;
        }
        .card-link {
            padding: 6px 14px;
            border-radius: 6px;
            font-size: 0.8rem;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s;
            display: inline-flex;
            align-items: center;
            gap: 5px;
        }
        .link-paper {
            background: var(--accent);
            color: var(--bg-dark);
        }
        .link-pdf {
            background: var(--success);
            color: #000;
        }
        .link-doi {
            background: var(--warning);
            color: #000;
        }
        .card-link:hover {
            transform: scale(1.05);
        }
        .expand-toggle {
            margin-left: auto;
            background: transparent;
            border: 1px solid var(--border);
            color: var(--text-secondary);
            padding: 6px 12px;
            border-radius: 6px;
            cursor: pointer;
            font-size: 0.8rem;
            transition: all 0.2s;
        }
        .expand-toggle:hover {
            border-color: var(--accent);
            color: var(--accent);
        }
        .cite-btn {
            background: linear-gradient(135deg, #764ba2, #667eea);
            border: none;
            color: #fff;
            padding: 6px 14px;
            border-radius: 6px;
            cursor: pointer;
            font-size: 0.8rem;
            font-weight: 500;
            transition: all 0.2s;
            display: inline-flex;
            align-items: center;
            gap: 5px;
        }
        .cite-btn:hover {
            transform: scale(1.05);
            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4);
        }
        .cite-btn.copied {
            background: var(--success);
        }
        .cite-tooltip {
            position: fixed;
            bottom: 30px;
            left: 50%;
            transform: translateX(-50%);
            background: var(--success);
            color: #000;
            padding: 12px 24px;
            border-radius: 8px;
            font-size: 0.9rem;
            font-weight: 600;
            z-index: 1000;
            opacity: 0;
            transition: opacity 0.3s;
            pointer-events: none;
        }
        .cite-tooltip.show {
            opacity: 1;
        }

        /* View Toggle */
        .view-toggle {
            display: flex;
            align-items: center;
            gap: 12px;
            margin-bottom: 20px;
        }
        .toggle-btn {
            padding: 12px 24px;
            border-radius: 8px;
            border: 2px solid var(--border);
            background: transparent;
            color: var(--text-secondary);
            cursor: pointer;
            font-size: 0.95rem;
            font-weight: 600;
            transition: all 0.2s;
        }
        .toggle-btn:hover {
            border-color: var(--accent);
            color: var(--accent);
        }
        .toggle-btn.active {
            background: var(--accent);
            border-color: var(--accent);
            color: var(--bg-dark);
        }

        /* Section headers */
        .section-header {
            font-size: 1.6rem;
            font-weight: 700;
            color: var(--text-primary);
            margin-bottom: 25px;
            padding-bottom: 15px;
            border-bottom: 2px solid var(--accent);
            display: flex;
            align-items: center;
            gap: 10px;
        }
        .section-count {
            font-size: 1rem;
            font-weight: 400;
            color: var(--text-secondary);
        }
        .group-section {
            margin-bottom: 50px;
        }

        /* Utility */
        .hidden { display: none !important; }

        /* Responsive */
        @media (max-width: 768px) {
            .container { padding: 15px; }
            h1 { font-size: 1.8rem; }
            .papers-grid { grid-template-columns: 1fr; }
            .stats { flex-direction: column; gap: 20px; }
            .search-box { width: 100%; margin: 10px 0; }
            .filters { flex-direction: column; align-items: stretch; }
            .view-toggle { flex-wrap: wrap; }
        }
    </style>
</head>
<body>
    <header>
        <h1>Research Literature Review</h1>
        <p class="subtitle">Multi-Source Academic Paper Analysis | summary, gaps, semantic, arxiv, googlescholar, scopusWOS</p>
        <a href="research_timeline.html" class="timeline-link">üó∫Ô∏è Interactive Timeline & Mindmap ‚Üí</a>
        <div class="stats">
            <div class="stat">
                <span class="stat-value">252</span>
                <span class="stat-label">Papers</span>
            </div>
            <div class="stat">
                <span class="stat-value">1</span>
                <span class="stat-label">Sources</span>
            </div>
            <div class="stat">
                <span class="stat-value">14</span>
                <span class="stat-label">Categories</span>
            </div>
            <div class="stat">
                <span class="stat-value">6</span>
                <span class="stat-label">Years</span>
            </div>
        </div>
    </header>

    <div class="container">
        <!-- View Toggle -->
        <div class="view-toggle">
            <span class="filter-label">Group by:</span>
            <button class="toggle-btn active" onclick="setView('year')" id="btn-year">üìÖ Year</button>
            <button class="toggle-btn" onclick="setView('category')" id="btn-category">üè∑Ô∏è Category</button>
        </div>

        <div class="filters">
            <span class="filter-label">Source:</span>
            <button class="filter-btn source-btn active" onclick="filterBySource('all')">All</button>
            <button class="filter-btn source-btn" onclick="filterBySource('arXiv')">arXiv</button>
            <div class="filter-divider"></div>
            <span class="filter-label">Year:</span>
            <button class="filter-btn year-btn active" onclick="filterByYear('all')">All</button>
            <button class="filter-btn year-btn" onclick="filterByYear(2026)">2026</button><button class="filter-btn year-btn" onclick="filterByYear(2025)">2025</button><button class="filter-btn year-btn" onclick="filterByYear(2024)">2024</button><button class="filter-btn year-btn" onclick="filterByYear(2023)">2023</button><button class="filter-btn year-btn" onclick="filterByYear(2022)">2022</button><button class="filter-btn year-btn" onclick="filterByYear(2021)">2021</button>
            <input type="text" class="search-box" placeholder="Search papers & keywords..." oninput="searchPapers(this.value)">
        </div>

        <div class="filters" id="category-filters">
            <span class="filter-label">Category:</span>
            <button class="filter-btn cat-btn active" onclick="filterByCategory('all')">All</button>
            <button class="filter-btn cat-btn" onclick="filterByCategory('cs.ro')">Robotics (104)</button><button class="filter-btn cat-btn" onclick="filterByCategory('cs.cl')">Computational Linguistics (95)</button><button class="filter-btn cat-btn" onclick="filterByCategory('cs.lg')">Machine Learning (12)</button><button class="filter-btn cat-btn" onclick="filterByCategory('cs.cv')">Computer Vision (10)</button><button class="filter-btn cat-btn" onclick="filterByCategory('cs.ai')">Artificial Intelligence (10)</button><button class="filter-btn cat-btn" onclick="filterByCategory('cs.cr')">CS.CR (6)</button><button class="filter-btn cat-btn" onclick="filterByCategory('cs.hc')">Human-Computer Interaction (5)</button><button class="filter-btn cat-btn" onclick="filterByCategory('cs.se')">CS.SE (3)</button><button class="filter-btn cat-btn" onclick="filterByCategory('eess.as')">EESS.AS (2)</button><button class="filter-btn cat-btn" onclick="filterByCategory('cond-mat.mtrl-sci')">COND-MAT.MTRL-SCI (1)</button>
        </div>

        <!-- Year View -->
        <div id="year-view">

            <div class="group-section year-section" data-year="2026">
                <h2 class="section-header">üìÖ 2026 <span class="section-count">(3 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-source="arXiv" data-year="2026" data-category="cs.cl" data-title="quantifying non deterministic drift in large language models" data-keywords="gpt control ros language model cs.cl cs.ai" data-themes="S E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-new">New</span></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2601.19934v1" target="_blank">Quantifying non deterministic drift in large language models</a>
                            </h3>
                            <p class="card-authors">Claire Nicholson</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Control</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4444264752">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) are widely used for tasks ranging from summarisation to decision support. In practice, identical prompts do not always produce identical outputs, even when temperature and other decoding parameters are fixed. In this work, we conduct repeated-run experiments to empirically quantify baseline behavioural drift, defined as output variability observed when the same prompt is issued multiple times under operator-free conditions. We evaluate two publicly accessible models, gpt-4o-mini and llama3.1-8b, across five prompt categories using exact repeats, perturbed inputs, a...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">In this work, we conduct repeated-run experiments to empirically quantify baseline behavioural drift, defined as output variability observed when the same prompt is issued multiple times under operator-free conditions. We situate these findings within existing work on concept drift, behavioural drift, and infrastructure-induced nondeterminism, discuss the limitations of lexical metrics, and highlight emerging semantic approaches</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large language models (LLMs) are widely used for tasks ranging from summarisation to decision support</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2601.19934v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2601.19934v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Nicholson, &quot;Quantifying non deterministic drift in large language models,&quot; arXiv, 2026. [Online]. Available: http://arxiv.org/abs/2601.19934v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444264752')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2026" data-category="cs.cl" data-title="sharp: social harm analysis via risk profiles for measuring inequities in large language models" data-keywords="ros language model cs.cl cs.ai" data-themes="E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-new">New</span></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2601.21235v1" target="_blank">SHARP: Social Harm Analysis via Risk Profiles for Measuring Inequities in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Alok Abhishek, Tushar Bandopadhyay, Lisa Erickson</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4444261056">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) are increasingly deployed in high-stakes domains, where rare but severe failures can result in irreversible harm. However, prevailing evaluation benchmarks often reduce complex social risk to mean-centered scalar scores, thereby obscuring distributional structure, cross-dimensional interactions, and worst-case behavior. This paper introduces Social Harm Analysis via Risk Profiles (SHARP), a framework for multidimensional, distribution-aware evaluation of social harm. SHARP models harm as a multivariate random variable and integrates explicit decomposition into bias...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large language models (LLMs) are increasingly deployed in high-stakes domains, where rare but severe failures can result in irreversible harm</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2601.21235v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2601.21235v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Abhishek, T. Bandopadhyay, and L. Erickson, &quot;SHARP: Social Harm Analysis via Risk Profiles for Measuring Inequities in Large Language Models,&quot; arXiv, 2026. [Online]. Available: http://arxiv.org/abs/2601.21235v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444261056')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2026" data-category="cs.ro" data-title="fauna sprout: a lightweight, approachable, developer-ready humanoid robot" data-keywords="robot humanoid manipulation control simulation imu cs.ro cs.ai" data-themes="S L I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-new">New</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2601.18963v1" target="_blank">Fauna Sprout: A lightweight, approachable, developer-ready humanoid robot</a>
                            </h3>
                            <p class="card-authors">Fauna Robotics, :, Diego Aldarondo et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Manipulation</span><span class="keyword-tag">Control</span></div>

                            <div class="card-details" id="details-4445407456">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recent advances in learned control, large-scale simulation, and generative models have accelerated progress toward general-purpose robotic controllers, yet the field still lacks platforms suitable for safe, expressive, long-term deployment in human environments. Most existing humanoids are either closed industrial systems or academic prototypes that are difficult to deploy and operate around people, limiting progress in robotics. We introduce Sprout, a developer platform designed to address these limitations through an emphasis on safety, expressivity, and developer accessibility. Sprout adopt...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce Sprout, a developer platform designed to address these limitations through an emphasis on safety, expressivity, and developer accessibility</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Most existing humanoids are either closed industrial systems or academic prototypes that are difficult to deploy and operate around people, limiting progress in robotics. We introduce Sprout, a developer platform designed to address these limitations through an emphasis on safety, expressivity, and developer accessibility</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Recent advances in learned control, large-scale simulation, and generative models have accelerated progress toward general-purpose robotic controllers, yet the field still lacks platforms suitable for safe, expressive, long-term deployment in human environments</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2601.18963v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2601.18963v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. Robotics et al., &quot;Fauna Sprout: A lightweight, approachable, developer-ready humanoid robot,&quot; arXiv, 2026. [Online]. Available: http://arxiv.org/abs/2601.18963v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445407456')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section year-section" data-year="2025">
                <h2 class="section-header">üìÖ 2025 <span class="section-count">(84 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="classifying german language proficiency levels using large language models" data-keywords="language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2512.06483v1" target="_blank">Classifying German Language Proficiency Levels Using Large Language Models</a>
                            </h3>
                            <p class="card-authors">Elias-Leander Ahlers, Witold Brunsmann, Malte Schilling</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4441927456">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Assessing language proficiency is essential for education, as it enables instruction tailored to learners needs. This paper investigates the use of Large Language Models (LLMs) for automatically classifying German texts according to the Common European Framework of Reference for Languages (CEFR) into different proficiency levels. To support robust training and evaluation, we construct a diverse dataset by combining multiple existing CEFR-annotated corpora with synthetic data. We then evaluate prompt-engineering strategies, fine-tuning of a LLaMA-3-8B-Instruct model and a probing-based approach...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper investigates the use of Large Language Models (LLMs) for automatically classifying German texts according to the Common European Framework of Reference for Languages (CEFR) into different proficiency levels</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2512.06483v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2512.06483v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'E. Ahlers, W. Brunsmann, and M. Schilling, &quot;Classifying German Language Proficiency Levels Using Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2512.06483v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441927456')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="reinforcement learning meets large language models: a survey of advancements and applications across the llm lifecycle" data-keywords="reinforcement learning ros language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.16679v1" target="_blank">Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle</a>
                            </h3>
                            <p class="card-authors">Keliang Liu, Dingkang Yang, Ziyun Qian et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4441921072">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In recent years, training methods centered on Reinforcement Learning (RL) have markedly enhanced the reasoning and alignment performance of Large Language Models (LLMs), particularly in understanding human intents, following user instructions, and bolstering inferential strength. Although existing surveys offer overviews of RL augmented LLMs, their scope is often limited, failing to provide a comprehensive summary of how RL operates across the full lifecycle of LLMs. We systematically review the theoretical and practical advancements whereby RL empowers LLMs, especially Reinforcement Learning ...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Finally, we analyse the future challenges and trends in the field of RL-enhanced LLMs</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In recent years, training methods centered on Reinforcement Learning (RL) have markedly enhanced the reasoning and alignment performance of Large Language Models (LLMs), particularly in understanding human intents, following user instructions, and bolstering inferential strength</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.16679v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.16679v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Liu et al., &quot;Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.16679v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441921072')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="emissions and performance trade-off between small and large language models" data-keywords="ros language model cs.cl cs.ai cs.cy" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2601.08844v1" target="_blank">Emissions and Performance Trade-off Between Small and Large Language Models</a>
                            </h3>
                            <p class="card-authors">Anandita Garg, Uma Gaba, Deepan Muthirayan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4441918768">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The advent of Large Language Models (LLMs) has raised concerns about their enormous carbon footprint, starting with energy-intensive training and continuing through repeated inference. This study investigates the potential of using fine-tuned Small Language Models (SLMs) as a sustainable alternative for predefined tasks. Here, we present a comparative analysis of the performance-emissions trade-off between LLMs and fine-tuned SLMs across selected tasks under Natural Language Processing, Reasoning and Programming. Our results show that in four out of the six selected tasks, SLMs maintained comp...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Here, we present a comparative analysis of the performance-emissions trade-off between LLMs and fine-tuned SLMs across selected tasks under Natural Language Processing, Reasoning and Programming</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The advent of Large Language Models (LLMs) has raised concerns about their enormous carbon footprint, starting with energy-intensive training and continuing through repeated inference</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2601.08844v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2601.08844v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Garg, U. Gaba, D. Muthirayan, and A. R. Chowdhury, &quot;Emissions and Performance Trade-off Between Small and Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2601.08844v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441918768')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cond-mat.mtrl-sci" data-title="hierarchical multi-agent large language model reasoning for autonomous functional materials discovery" data-keywords="multi-agent simulation ros imu language model cond-mat.mtrl-sci cs.ai cs.cl" data-themes="E I R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ COND-MAT.MTRL-SCI</span>
                            <div class="card-tech"><span class="tech-chip">Multi-Agent Systems</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2512.13930v1" target="_blank">Hierarchical Multi-agent Large Language Model Reasoning for Autonomous Functional Materials Discovery</a>
                            </h3>
                            <p class="card-authors">Samuel Rothfarb, Megan C. Davis, Ivana Matanovic et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Multi-Agent</span><span class="keyword-tag">Simulation</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Imu</span></div>

                            <div class="card-details" id="details-4441922224">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Artificial intelligence is reshaping scientific exploration, but most methods automate procedural tasks without engaging in scientific reasoning, limiting autonomy in discovery. We introduce Materials Agents for Simulation and Theory in Electronic-structure Reasoning (MASTER), an active learning framework where large language models autonomously design, execute, and interpret atomistic simulations. In MASTER, a multimodal system translates natural language into density functional theory workflows, while higher-level reasoning agents guide discovery through a hierarchy of strategies, including ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce Materials Agents for Simulation and Theory in Electronic-structure Reasoning (MASTER), an active learning framework where large language models autonomously design, execute, and interpret atomistic simulations</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Artificial intelligence is reshaping scientific exploration, but most methods automate procedural tasks without engaging in scientific reasoning, limiting autonomy in discovery</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2512.13930v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2512.13930v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Rothfarb, M. C. Davis, I. Matanovic, B. Li, E. F. Holby, and W. J. M. Kort-Kamp, &quot;Hierarchical Multi-agent Large Language Model Reasoning for Autonomous Functional Materials Discovery,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2512.13930v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441922224')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.se" data-title="a survey of aiops in the era of large language models" data-keywords="attention ros language model cs.se cs.cl" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ CS.SE</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.12472v1" target="_blank">A Survey of AIOps in the Era of Large Language Models</a>
                            </h3>
                            <p class="card-authors">Lingzhe Zhang, Tong Jia, Mengxi Jia et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.SE</span></div>

                            <div class="card-details" id="details-4441927552">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">As large language models (LLMs) grow increasingly sophisticated and pervasive, their application to various Artificial Intelligence for IT Operations (AIOps) tasks has garnered significant attention. However, a comprehensive understanding of the impact, potential, and limitations of LLMs in AIOps remains in its infancy. To address this gap, we conducted a detailed survey of LLM4AIOps, focusing on how LLMs can optimize processes and improve outcomes in this domain. We analyzed 183 research papers published between January 2020 and December 2024 to answer four key research questions (RQs). In RQ...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, a comprehensive understanding of the impact, potential, and limitations of LLMs in AIOps remains in its infancy. To address this gap, we conducted a detailed survey of LLM4AIOps, focusing on how LLMs can optimize processes and improve outcomes in this domain</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">As large language models (LLMs) grow increasingly sophisticated and pervasive, their application to various Artificial Intelligence for IT Operations (AIOps) tasks has garnered significant attention</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.12472v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.12472v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Zhang et al., &quot;A Survey of AIOps in the Era of Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.12472v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441927552')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="how do language models learn facts? dynamics, curricula and hallucinations" data-keywords="neural network attention imu language model cs.cl cs.lg" data-themes="E L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.21676v2" target="_blank">How do language models learn facts? Dynamics, curricula and hallucinations</a>
                            </h3>
                            <p class="card-authors">Nicolas Zucchet, J√∂rg Bornschein, Stephanie Chan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Attention</span><span class="keyword-tag">Imu</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4441927168">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models accumulate vast knowledge during pre-training, yet the dynamics governing this acquisition remain poorly understood. This work investigates the learning dynamics of language models on a synthetic factual recall task, uncovering three key findings: First, language models learn in three phases, exhibiting a performance plateau before acquiring precise factual knowledge. Mechanistically, this plateau coincides with the formation of attention-based circuits that support recall. Second, the training data distribution significantly impacts learning dynamics, as imbalanced distr...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large language models accumulate vast knowledge during pre-training, yet the dynamics governing this acquisition remain poorly understood</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.21676v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.21676v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. Zucchet, J. Bornschein, S. Chan, A. Lampinen, R. Pascanu, and S. De, &quot;How do language models learn facts? Dynamics, curricula and hallucinations,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.21676v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441927168')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ai" data-title="large language models reasoning abilities under non-ideal conditions after rl-fine-tuning" data-keywords="reinforcement learning ros language model cs.ai" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Artificial Intelligence</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.04848v1" target="_blank">Large Language Models Reasoning Abilities Under Non-Ideal Conditions After RL-Fine-Tuning</a>
                            </h3>
                            <p class="card-authors">Chang Tian, Matthew B. Blaschko, Mingzhe Xing et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4441918096">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Reinforcement learning (RL) has become a key technique for enhancing the reasoning abilities of large language models (LLMs), with policy-gradient algorithms dominating the post-training stage because of their efficiency and effectiveness. However, most existing benchmarks evaluate large-language-model reasoning under idealized settings, overlooking performance in realistic, non-ideal scenarios. We identify three representative non-ideal scenarios with practical relevance: summary inference, fine-grained noise suppression, and contextual filtering. We introduce a new research direction guided ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce a new research direction guided by brain-science findings that human reasoning remains reliable under imperfect inputs</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Our results reveal that while RL fine-tuning improves baseline reasoning under idealized settings, performance declines significantly across all three non-ideal scenarios, exposing critical limitations in advanced reasoning capabilities</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Reinforcement learning (RL) has become a key technique for enhancing the reasoning abilities of large language models (LLMs), with policy-gradient algorithms dominating the post-training stage because of their efficiency and effectiveness</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.04848v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.04848v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Tian, M. B. Blaschko, M. Xing, X. Li, Y. Yue, and M. Moens, &quot;Large Language Models Reasoning Abilities Under Non-Ideal Conditions After RL-Fine-Tuning,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.04848v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441918096')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="large language models have learned to use language" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2512.12447v1" target="_blank">Large language models have learned to use language</a>
                            </h3>
                            <p class="card-authors">Gary Lupyan</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4441927264">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Acknowledging that large language models have learned to use language can open doors to breakthrough language science. Achieving these breakthroughs may require abandoning some long-held ideas about how language knowledge is evaluated and reckoning with the difficult fact that we have entered a post-Turing test era.</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Achieving these breakthroughs may require abandoning some long-held ideas about how language knowledge is evaluated and reckoning with the difficult fact that we have entered a post-Turing test era.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Acknowledging that large language models have learned to use language can open doors to breakthrough language science</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2512.12447v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2512.12447v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Lupyan, &quot;Large language models have learned to use language,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2512.12447v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441927264')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="soft inductive bias approach via explicit reasoning perspectives in inappropriate utterance detection using large language models" data-keywords="attention ros language model cs.cl" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2512.08480v1" target="_blank">Soft Inductive Bias Approach via Explicit Reasoning Perspectives in Inappropriate Utterance Detection Using Large Language Models</a>
                            </h3>
                            <p class="card-authors">Ju-Young Kim, Ji-Hong Park, Se-Yeon Lee et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444261584">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recent incidents in certain online games and communities, where anonymity is guaranteed, show that unchecked inappropriate remarks frequently escalate into verbal abuse and even criminal behavior, raising significant social concerns. Consequently, there is a growing need for research on techniques that can detect inappropriate utterances within conversational texts to help build a safer communication environment. Although large-scale language models trained on Korean corpora and chain-of-thought reasoning have recently gained attention, research applying these approaches to inappropriate utter...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this study, we propose a soft inductive bias approach that explicitly defines reasoning perspectives to guide the inference process, thereby promoting rational decision-making and preventing errors that may arise during reasoning</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Consequently, there is a growing need for research on techniques that can detect inappropriate utterances within conversational texts to help build a safer communication environment</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2512.08480v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2512.08480v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Kim, J. Park, S. Lee, S. Park, and G. Kim, &quot;Soft Inductive Bias Approach via Explicit Reasoning Perspectives in Inappropriate Utterance Detection Using Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2512.08480v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444261584')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.lg" data-title="two-stage representation learning for analyzing movement behavior dynamics in people living with dementia" data-keywords="language model cs.lg cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Machine Learning</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.09173v1" target="_blank">Two-Stage Representation Learning for Analyzing Movement Behavior Dynamics in People Living with Dementia</a>
                            </h3>
                            <p class="card-authors">Jin Cui, Alexander Capstick, Payam Barnaghi et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4444261296">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In remote healthcare monitoring, time series representation learning reveals critical patient behavior patterns from high-frequency data. This study analyzes home activity data from individuals living with dementia by proposing a two-stage, self-supervised learning approach tailored to uncover low-rank structures. The first stage converts time-series activities into text sequences encoded by a pre-trained language model, providing a rich, high-dimensional latent state space using a PageRank-based method. This PageRank vector captures latent state transitions, effectively compressing complex be...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This study analyzes home activity data from individuals living with dementia by proposing a two-stage, self-supervised learning approach tailored to uncover low-rank structures</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.09173v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.09173v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Cui, A. Capstick, P. Barnaghi, and G. Scott, &quot;Two-Stage Representation Learning for Analyzing Movement Behavior Dynamics in People Living with Dementia,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.09173v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444261296')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="knowledge-driven agentic scientific corpus distillation framework for biomedical large language models training" data-keywords="gpt multi-agent language model cs.cl cs.ai q-bio.qm" data-themes="E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Multi-Agent Systems</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.19565v3" target="_blank">Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training</a>
                            </h3>
                            <p class="card-authors">Meng Xiao, Xunxin Cai, Qingqing Long et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Multi-Agent</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444261200">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Corpus distillation for biomedical large language models (LLMs) seeks to address the pressing challenge of insufficient quantity and quality in open-source annotated scientific corpora, which remains a bottleneck for effective LLM training in biomedical research. This paper proposes a knowledge-driven, agentic framework for scientific corpus distillation, tailored explicitly for LLM training in the biomedical domain, addressing the challenge posed by the complex hierarchy of biomedical knowledge. Central to our approach is a collaborative multi-agent architecture, where specialized agents, eac...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Central to our approach is a collaborative multi-agent architecture, where specialized agents, each guided by the Medical Subject Headings (MeSH) hierarchy, work in concert to autonomously extract, synthesize, and self-evaluate high-quality textual data from vast scientific literature</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Corpus distillation for biomedical large language models (LLMs) seeks to address the pressing challenge of insufficient quantity and quality in open-source annotated scientific corpora, which remains a bottleneck for effective LLM training in biomedical research. This paper proposes a knowledge-driven, agentic framework for scientific corpus distillation, tailored explicitly for LLM training in the biomedical domain, addressing the challenge posed by the complex hierarchy of biomedical knowledge</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Corpus distillation for biomedical large language models (LLMs) seeks to address the pressing challenge of insufficient quantity and quality in open-source annotated scientific corpora, which remains a bottleneck for effective LLM training in biomedical research</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.19565v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.19565v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Xiao, X. Cai, Q. Long, C. Wang, Y. Zhou, and H. Zhu, &quot;Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.19565v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444261200')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="investigating retrieval-augmented generation in quranic studies: a study of 13 open-source large language models" data-keywords="language model cs.cl cs.ai cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.16581v1" target="_blank">Investigating Retrieval-Augmented Generation in Quranic Studies: A Study of 13 Open-Source Large Language Models</a>
                            </h3>
                            <p class="card-authors">Zahra Khalila, Arbi Haza Nasution, Winda Monika et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.LG</span></div>

                            <div class="card-details" id="details-4444260624">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Accurate and contextually faithful responses are critical when applying large language models (LLMs) to sensitive and domain-specific tasks, such as answering queries related to quranic studies. General-purpose LLMs often struggle with hallucinations, where generated responses deviate from authoritative sources, raising concerns about their reliability in religious contexts. This challenge highlights the need for systems that can integrate domain-specific knowledge while maintaining response accuracy, relevance, and faithfulness. In this study, we investigate 13 open-source LLMs categorized in...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">This challenge highlights the need for systems that can integrate domain-specific knowledge while maintaining response accuracy, relevance, and faithfulness. A Retrieval-Augmented Generation (RAG) is used to make up for the problems that come with using separate models</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Accurate and contextually faithful responses are critical when applying large language models (LLMs) to sensitive and domain-specific tasks, such as answering queries related to quranic studies</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.16581v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.16581v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Khalila et al., &quot;Investigating Retrieval-Augmented Generation in Quranic Studies: A Study of 13 Open-Source Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.16581v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444260624')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cv" data-title="object detection with multimodal large vision-language models: an in-depth review" data-keywords="deep learning robot computer vision nlp language model cs.cv cs.ai cs.cl" data-themes="E I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computer Vision</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.19294v2" target="_blank">Object Detection with Multimodal Large Vision-Language Models: An In-depth Review</a>
                            </h3>
                            <p class="card-authors">Ranjan Sapkota, Manoj Karkee</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Deep Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Computer Vision</span><span class="keyword-tag">Nlp</span></div>

                            <div class="card-details" id="details-4444261488">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The fusion of language and vision in large vision-language models (LVLMs) has revolutionized deep learning-based object detection by enhancing adaptability, contextual reasoning, and generalization beyond traditional architectures. This in-depth review presents a structured exploration of the state-of-the-art in LVLMs, systematically organized through a three-step research review process. First, we discuss the functioning of vision language models (VLMs) for object detection, describing how these models harness natural language processing (NLP) and computer vision (CV) techniques to revolution...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">The review also identifies a few major limitations of the current LVLM modes, proposes solutions to address those challenges, and presents a clear roadmap for the future advancement in this field</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The fusion of language and vision in large vision-language models (LVLMs) has revolutionized deep learning-based object detection by enhancing adaptability, contextual reasoning, and generalization beyond traditional architectures</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.19294v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.19294v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Sapkota, and M. Karkee, &quot;Object Detection with Multimodal Large Vision-Language Models: An In-depth Review,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.19294v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444261488')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="head-specific intervention can induce misaligned ai coordination in large language models" data-keywords="attention coordination control language model cs.cl cs.ai" data-themes="S E I R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.05945v3" target="_blank">Head-Specific Intervention Can Induce Misaligned AI Coordination in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Paul Darm, Annalisa Riccardi</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Coordination</span><span class="keyword-tag">Control</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4444262544">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Robust alignment guardrails for large language models (LLMs) are becoming increasingly important with their widespread application. In contrast to previous studies, we demonstrate that inference-time activation interventions can bypass safety alignments and effectively steer model generations towards harmful AI coordination. Our method applies fine-grained interventions at specific attention heads, which we identify by probing each head in a simple binary choice task. We then show that interventions on these heads generalise to the open-ended generation setting, effectively circumventing safet...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In contrast to previous studies, we demonstrate that inference-time activation interventions can bypass safety alignments and effectively steer model generations towards harmful AI coordination</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Robust alignment guardrails for large language models (LLMs) are becoming increasingly important with their widespread application</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.05945v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.05945v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Darm, and A. Riccardi, &quot;Head-Specific Intervention Can Induce Misaligned AI Coordination in Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.05945v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444262544')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="understanding network behaviors through natural language question-answering" data-keywords="ros language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2510.21894v1" target="_blank">Understanding Network Behaviors through Natural Language Question-Answering</a>
                            </h3>
                            <p class="card-authors">Mingzhe Xing, Chang Tian, Jianan Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4444261536">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Modern large-scale networks introduce significant complexity in understanding network behaviors, increasing the risk of misconfiguration. Prior work proposed to understand network behaviors by mining network configurations, typically relying on domain-specific languages interfaced with formal models. While effective, they suffer from a steep learning curve and limited flexibility. In contrast, natural language (NL) offers a more accessible and interpretable interface, motivating recent research on NL-guided network behavior understanding. Recent advances in large language models (LLMs) further...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To tackle the above challenges, we propose NetMind, a novel framework for querying networks using NL</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, three key challenges remain: 1) numerous router devices with lengthy configuration files challenge LLM&#x27;s long-context understanding ability; 2) heterogeneity across devices and protocols impedes scalability; and 3) complex network topologies and protocols demand advanced reasoning abilities beyond the current capabilities of LLMs. To tackle the above challenges, we propose NetMind, a novel framework for querying networks using NL</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Prior work proposed to understand network behaviors by mining network configurations, typically relying on domain-specific languages interfaced with formal models</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2510.21894v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2510.21894v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Xing et al., &quot;Understanding Network Behaviors through Natural Language Question-Answering,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2510.21894v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444261536')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ai" data-title="large language models for interpretable mental health diagnosis" data-keywords="language model cs.ai cs.lo" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Artificial Intelligence</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2501.07653v2" target="_blank">Large Language Models for Interpretable Mental Health Diagnosis</a>
                            </h3>
                            <p class="card-authors">Brian Hyeongseok Kim, Chao Wang</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.LO</span></div>

                            <div class="card-details" id="details-4444261248">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We propose a clinical decision support system (CDSS) for mental health diagnosis that combines the strengths of large language models (LLMs) and constraint logic programming (CLP). Having a CDSS is important because of the high complexity of diagnostic manuals used by mental health professionals and the danger of diagnostic errors. Our CDSS is a software tool that uses an LLM to translate diagnostic manuals to a logic program and solves the program using an off-the-shelf CLP engine to query a patient&#x27;s diagnosis based on the encoded rules and provided data. By giving domain experts the opportu...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a clinical decision support system (CDSS) for mental health diagnosis that combines the strengths of large language models (LLMs) and constraint logic programming (CLP)</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We propose a clinical decision support system (CDSS) for mental health diagnosis that combines the strengths of large language models (LLMs) and constraint logic programming (CLP)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2501.07653v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2501.07653v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. H. Kim, and C. Wang, &quot;Large Language Models for Interpretable Mental Health Diagnosis,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2501.07653v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444261248')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cr" data-title="sbfa: single sneaky bit flip attack to break large language models" data-keywords="neural network ros language model cs.cr cs.cl cs.lg" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ CS.CR</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.21843v1" target="_blank">SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models</a>
                            </h3>
                            <p class="card-authors">Jingkai Guo, Chaitali Chakrabarti, Deliang Fan</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CR</span></div>

                            <div class="card-details" id="details-4444262016">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Model integrity of Large language models (LLMs) has become a pressing security concern with their massive online deployment. Prior Bit-Flip Attacks (BFAs) -- a class of popular AI weight memory fault-injection techniques -- can severely compromise Deep Neural Networks (DNNs): as few as tens of bit flips can degrade accuracy toward random guessing. Recent studies extend BFAs to LLMs and reveal that, despite the intuition of better robustness from modularity and redundancy, only a handful of adversarial bit flips can also cause LLMs&#x27; catastrophic accuracy degradation. However, existing BFA metho...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, for the first time, we propose SBFA (Sneaky Bit-Flip Attack), which collapses LLM performance with only one single bit flip while keeping perturbed values within benign layer-wise weight distribution</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Model integrity of Large language models (LLMs) has become a pressing security concern with their massive online deployment</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.21843v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.21843v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Guo, C. Chakrabarti, and D. Fan, &quot;SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.21843v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444262016')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="commander-gpt: fully unleashing the sarcasm detection capability of multi-modal large language models" data-keywords="attention gpt nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.18681v3" target="_blank">Commander-GPT: Fully Unleashing the Sarcasm Detection Capability of Multi-Modal Large Language Models</a>
                            </h3>
                            <p class="card-authors">Yazhou Zhang, Chunwang Zou, Bo Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4444261728">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Sarcasm detection, as a crucial research direction in the field of Natural Language Processing (NLP), has attracted widespread attention. Traditional sarcasm detection tasks have typically focused on single-modal approaches (e.g., text), but due to the implicit and subtle nature of sarcasm, such methods often fail to yield satisfactory results. In recent years, researchers have shifted the focus of sarcasm detection to multi-modal approaches. However, effectively leveraging multi-modal information to accurately identify sarcastic content remains a challenge that warrants further exploration. L...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Leveraging the powerful integrated processing capabilities of Multi-Modal Large Language Models (MLLMs) for various information sources, we propose an innovative multi-modal Commander-GPT framework</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, effectively leveraging multi-modal information to accurately identify sarcastic content remains a challenge that warrants further exploration</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Traditional sarcasm detection tasks have typically focused on single-modal approaches (e.g., text), but due to the implicit and subtle nature of sarcasm, such methods often fail to yield satisfactory results</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.18681v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.18681v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Zhang, C. Zou, B. Wang, and J. Qin, &quot;Commander-GPT: Fully Unleashing the Sarcasm Detection Capability of Multi-Modal Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.18681v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444261728')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ai" data-title="ars: adaptive reasoning suppression for efficient large reasoning language models" data-keywords="ros language model cs.ai cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Artificial Intelligence</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2510.00071v2" target="_blank">ARS: Adaptive Reasoning Suppression for Efficient Large Reasoning Language Models</a>
                            </h3>
                            <p class="card-authors">Dongqi Zheng</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444263936">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Reasoning Language Models (LRLMs or LRMs) demonstrate remarkable capabilities in complex reasoning tasks, but suffer from significant computational inefficiencies due to overthinking phenomena. Existing efficient reasoning methods face the challenge of balancing reasoning quality with inference cost reduction. We propose \textbf{Adaptive Reasoning Suppression (ARS)}, a novel training-free approach that dynamically suppresses redundant reasoning steps while preserving accuracy through adaptive certainty monitoring. ARS introduces a multi-checkpoint certainty estimation mechanism with prog...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose \textbf{Adaptive Reasoning Suppression (ARS)}, a novel training-free approach that dynamically suppresses redundant reasoning steps while preserving accuracy through adaptive certainty monitoring</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Existing efficient reasoning methods face the challenge of balancing reasoning quality with inference cost reduction</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large Reasoning Language Models (LRLMs or LRMs) demonstrate remarkable capabilities in complex reasoning tasks, but suffer from significant computational inefficiencies due to overthinking phenomena</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2510.00071v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2510.00071v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Zheng, &quot;ARS: Adaptive Reasoning Suppression for Efficient Large Reasoning Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2510.00071v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444263936')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="llm-as-a-judge: rapid evaluation of legal document recommendation for retrieval-augmented generation" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.12382v1" target="_blank">LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for Retrieval-Augmented Generation</a>
                            </h3>
                            <p class="card-authors">Anu Pradhan, Alexandra Ortan, Apurv Verma et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444260720">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The evaluation bottleneck in recommendation systems has become particularly acute with the rise of Generative AI, where traditional metrics fall short of capturing nuanced quality dimensions that matter in specialized domains like legal research. Can we trust Large Language Models to serve as reliable judges of their own kind? This paper investigates LLM-as-a-Judge as a principled approach to evaluating Retrieval-Augmented Generation systems in legal contexts, where the stakes of recommendation quality are exceptionally high.
  We tackle two fundamental questions that determine practical viabi...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Can we trust Large Language Models to serve as reliable judges of their own kind? This paper investigates LLM-as-a-Judge as a principled approach to evaluating Retrieval-Augmented Generation systems in legal contexts, where the stakes of recommendation quality are exceptionally high.
  We tackle two fundamental questions that determine practical viability: which inter-rater reliability metrics best capture the alignment between LLM and human assessments, and how do we conduct statistically sound comparisons between competing systems? Through systematic experimentation, we discover that traditional agreement metrics like Krippendorff&#x27;s alpha can be misleading in the skewed distributions typical of AI system evaluations</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.12382v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.12382v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Pradhan, A. Ortan, A. Verma, and M. Seshadri, &quot;LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for Retrieval-Augmented Generation,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.12382v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444260720')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cv" data-title="mquant: unleashing the inference potential of multimodal large language models via full static quantization" data-keywords="attention language model cs.cv cs.ai" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computer Vision</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.00425v2" target="_blank">MQuant: Unleashing the Inference Potential of Multimodal Large Language Models via Full Static Quantization</a>
                            </h3>
                            <p class="card-authors">JiangYong Yu, Sifan Zhou, Dawei Yang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4444263408">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Multimodal large language models (MLLMs) have garnered widespread attention due to their ability to understand multimodal input. However, their large parameter sizes and substantial computational demands severely hinder their practical deployment and application.While quantization is an effective way to reduce model size and inference latency, its application to MLLMs remains underexplored. In this paper, we propose MQuant, a post-training quantization (PTQ) framework designed to tackle the unique challenges of multimodal large language models (MLLMs). Conventional quantization often struggles...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose MQuant, a post-training quantization (PTQ) framework designed to tackle the unique challenges of multimodal large language models (MLLMs)</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">In this paper, we propose MQuant, a post-training quantization (PTQ) framework designed to tackle the unique challenges of multimodal large language models (MLLMs). To address these issues, MQuant introduces: Modality-Specific Static Quantization (MSQ), assigning distinct static scales for visual vs</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Multimodal large language models (MLLMs) have garnered widespread attention due to their ability to understand multimodal input</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.00425v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.00425v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Yu et al., &quot;MQuant: Unleashing the Inference Potential of Multimodal Large Language Models via Full Static Quantization,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.00425v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444263408')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.hc" data-title="large language models will change the way children think about technology and impact every interaction paradigm" data-keywords="language model cs.hc cs.ai cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Human-Computer Interaction</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.13667v1" target="_blank">Large Language Models Will Change The Way Children Think About Technology And Impact Every Interaction Paradigm</a>
                            </h3>
                            <p class="card-authors">Russell Beale</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.HC</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444260816">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents a hopeful perspective on the potentially dramatic impacts of Large Language Models on how we children learn and how they will expect to interact with technology. We review the effects of LLMs on education so far, and make the case that these effects are minor compared to the upcoming changes that are occurring. We present a small scenario and self-ethnographic study demonstrating the effects of these changes, and define five significant considerations that interactive systems designers will have to accommodate in the future.</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents a hopeful perspective on the potentially dramatic impacts of Large Language Models on how we children learn and how they will expect to interact with technology</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper presents a hopeful perspective on the potentially dramatic impacts of Large Language Models on how we children learn and how they will expect to interact with technology</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.13667v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.13667v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Beale, &quot;Large Language Models Will Change The Way Children Think About Technology And Impact Every Interaction Paradigm,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.13667v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444260816')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="multi-model synthetic training for mission-critical small language models" data-keywords="gpt ros language model cs.cl cs.ai cs.lg" data-themes="S E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.13047v1" target="_blank">Multi-Model Synthetic Training for Mission-Critical Small Language Models</a>
                            </h3>
                            <p class="card-authors">Nolan Platt, Pragyansmita Nayak</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4441925248">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Language Models (LLMs) have demonstrated remarkable capabilities across many domains, yet their appli- cation to specialized fields remains constrained by the scarcity and complexity of domain-specific training data. We present a novel approach that achieves a 261x cost reduction for maritime intelligence by using LLMs as one-time teachers rather than using them directly for inference. Our method transforms 3.2 billion Automatic Identification System (AIS) vessel tracking records into 21,543 synthetic question and answer pairs through multi-model generation (GPT-4o and o3-mini), preventi...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present a novel approach that achieves a 261x cost reduction for maritime intelligence by using LLMs as one-time teachers rather than using them directly for inference</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large Language Models (LLMs) have demonstrated remarkable capabilities across many domains, yet their appli- cation to specialized fields remains constrained by the scarcity and complexity of domain-specific training data</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.13047v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.13047v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. Platt, and P. Nayak, &quot;Multi-Model Synthetic Training for Mission-Critical Small Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.13047v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441925248')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="towards typologically aware rescoring to mitigate unfaithfulness in lower-resource languages" data-keywords="bert ros language model cs.cl" data-themes="E M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.17664v2" target="_blank">Towards Typologically Aware Rescoring to Mitigate Unfaithfulness in Lower-Resource Languages</a>
                            </h3>
                            <p class="card-authors">Tsan Tsai Chan, Xin Tong, Thi Thu Uyen Hoang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Bert</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444262352">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Multilingual large language models (LLMs) are known to more frequently generate non-faithful output in resource-constrained languages (Guerreiro et al., 2023 - arXiv:2303.16104), potentially because these typologically diverse languages are underrepresented in their training data. To mitigate unfaithfulness in such settings, we propose using computationally light auxiliary models to rescore the outputs of larger architectures. As proof of the feasibility of such an approach, we show that monolingual 4-layer BERT models pretrained from scratch on less than 700 MB of data without fine-tuning are...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To mitigate unfaithfulness in such settings, we propose using computationally light auxiliary models to rescore the outputs of larger architectures</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Multilingual large language models (LLMs) are known to more frequently generate non-faithful output in resource-constrained languages (Guerreiro et al., 2023 - arXiv:2303.16104), potentially because these typologically diverse languages are underrepresented in their training data</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.17664v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.17664v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. T. Chan, X. Tong, T. T. U. Hoang, B. Tepnadze, and W. Stempniak, &quot;Towards Typologically Aware Rescoring to Mitigate Unfaithfulness in Lower-Resource Languages,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.17664v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444262352')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ai" data-title="from system 1 to system 2: a survey of reasoning large language models" data-keywords="language model cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Artificial Intelligence</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.17419v6" target="_blank">From System 1 to System 2: A Survey of Reasoning Large Language Models</a>
                            </h3>
                            <p class="card-authors">Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4444263984">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more accurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently, reasoning LLMs like OpenAI&#x27;s o1/o3 and DeepSeek&#x27;s R1 have demonstrated expert-lev...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.17419v6" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.17419v6" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Li et al., &quot;From System 1 to System 2: A Survey of Reasoning Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.17419v6')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444263984')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="detection of personal data in structured datasets using a large language model" data-keywords="gpt ros language model cs.cl" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.22305v1" target="_blank">Detection of Personal Data in Structured Datasets Using a Large Language Model</a>
                            </h3>
                            <p class="card-authors">Albert Agisha Ntwali, Luca R√ºck, Martin Heckmann</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444263600">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We propose a novel approach for detecting personal data in structured datasets, leveraging GPT-4o, a state-of-the-art Large Language Model. A key innovation of our method is the incorporation of contextual information: in addition to a feature&#x27;s name and values, we utilize information from other feature names within the dataset as well as the dataset description. We compare our approach to alternative methods, including Microsoft Presidio and CASSED, evaluating them on multiple datasets: DeSSI, a large synthetic dataset, datasets we collected from Kaggle and OpenML as well as MIMIC-Demo-Ext, a...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a novel approach for detecting personal data in structured datasets, leveraging GPT-4o, a state-of-the-art Large Language Model</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We propose a novel approach for detecting personal data in structured datasets, leveraging GPT-4o, a state-of-the-art Large Language Model</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.22305v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.22305v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. A. Ntwali, L. R√ºck, and M. Heckmann, &quot;Detection of Personal Data in Structured Datasets Using a Large Language Model,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.22305v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444263600')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ai" data-title="game theory meets large language models: a systematic survey with taxonomy and new frontiers" data-keywords="language model cs.ai cs.gt cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Artificial Intelligence</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.09053v2" target="_blank">Game Theory Meets Large Language Models: A Systematic Survey with Taxonomy and New Frontiers</a>
                            </h3>
                            <p class="card-authors">Haoran Sun, Yusen Wu, Peng Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.GT</span><span class="keyword-tag">CS.LG</span></div>

                            <div class="card-details" id="details-4444263456">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Game theory is a foundational framework for analyzing strategic interactions, and its intersection with large language models (LLMs) is a rapidly growing field. However, existing surveys mainly focus narrowly on using game theory to evaluate LLM behavior. This paper provides the first comprehensive survey of the bidirectional relationship between Game Theory and LLMs. We propose a novel taxonomy that categorizes the research in this intersection into four distinct perspectives: (1) evaluating LLMs in game-based scenarios; (2) improving LLMs using game-theoretic concepts for better interpretabi...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a novel taxonomy that categorizes the research in this intersection into four distinct perspectives: (1) evaluating LLMs in game-based scenarios; (2) improving LLMs using game-theoretic concepts for better interpretability and alignment; (3) modeling the competitive landscape of LLM development and its societal impact; and (4) leveraging LLMs to advance game models and to solve corresponding game theory problems</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">We propose a novel taxonomy that categorizes the research in this intersection into four distinct perspectives: (1) evaluating LLMs in game-based scenarios; (2) improving LLMs using game-theoretic concepts for better interpretability and alignment; (3) modeling the competitive landscape of LLM development and its societal impact; and (4) leveraging LLMs to advance game models and to solve corresponding game theory problems. Furthermore, we identify key challenges and outline future research directions</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Game theory is a foundational framework for analyzing strategic interactions, and its intersection with large language models (LLMs) is a rapidly growing field</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.09053v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.09053v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Sun et al., &quot;Game Theory Meets Large Language Models: A Systematic Survey with Taxonomy and New Frontiers,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.09053v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444263456')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cr" data-title="augmenting anonymized data with ai: exploring the feasibility and limitations of large language models in data enrichment" data-keywords="language model cs.cr cs.et" data-themes="S E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ CS.CR</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.03778v1" target="_blank">Augmenting Anonymized Data with AI: Exploring the Feasibility and Limitations of Large Language Models in Data Enrichment</a>
                            </h3>
                            <p class="card-authors">Stefano Cirillo, Domenico Desiato, Giuseppe Polese et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CR</span><span class="keyword-tag">CS.ET</span></div>

                            <div class="card-details" id="details-4444264608">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Language Models (LLMs) have demonstrated advanced capabilities in both text generation and comprehension, and their application to data archives might facilitate the privatization of sensitive information about the data subjects. In fact, the information contained in data often includes sensitive and personally identifiable details. This data, if not safeguarded, may bring privacy risks in terms of both disclosure and identification. Furthermore, the application of anonymisation techniques, such as k-anonymity, can lead to a significant reduction in the amount of data within data sources...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To this end, we designed new ad-hoc prompt template engineering strategies to perform anonymized Data Augmentation and assess the effectiveness of LLM-based approaches in providing anonymized data</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large Language Models (LLMs) have demonstrated advanced capabilities in both text generation and comprehension, and their application to data archives might facilitate the privatization of sensitive information about the data subjects</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.03778v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.03778v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Cirillo, D. Desiato, G. Polese, M. M. L. Sebillo, and G. Solimando, &quot;Augmenting Anonymized Data with AI: Exploring the Feasibility and Limitations of Large Language Models in Data Enrichment,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.03778v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444264608')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ai" data-title="language games as the pathway to artificial superhuman intelligence" data-keywords="multi-agent ros language model cs.ai cs.cl cs.ma" data-themes="E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Artificial Intelligence</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2501.18924v1" target="_blank">Language Games as the Pathway to Artificial Superhuman Intelligence</a>
                            </h3>
                            <p class="card-authors">Ying Wen, Ziyu Wan, Shao Zhang</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Multi-Agent</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4444263504">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The evolution of large language models (LLMs) toward artificial superhuman intelligence (ASI) hinges on data reproduction, a cyclical process in which models generate, curate and retrain on novel data to refine capabilities. Current methods, however, risk getting stuck in a data reproduction trap: optimizing outputs within fixed human-generated distributions in a closed loop leads to stagnation, as models merely recombine existing knowledge rather than explore new frontiers. In this paper, we propose language games as a pathway to expanded data reproduction, breaking this cycle through three m...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose language games as a pathway to expanded data reproduction, breaking this cycle through three mechanisms: (1) \textit{role fluidity}, which enhances data diversity and coverage by enabling multi-agent systems to dynamically shift roles across tasks; (2) \textit{reward variety}, embedding multiple feedback criteria that can drive complex intelligent behaviors; and (3) \textit{rule plasticity}, iteratively evolving interaction constraints to foster learnability, thereby injecting continual novelty</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The evolution of large language models (LLMs) toward artificial superhuman intelligence (ASI) hinges on data reproduction, a cyclical process in which models generate, curate and retrain on novel data to refine capabilities</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2501.18924v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2501.18924v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Wen, Z. Wan, and S. Zhang, &quot;Language Games as the Pathway to Artificial Superhuman Intelligence,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2501.18924v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444263504')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="language-conditioned world model improves policy generalization by reading environmental descriptions" data-keywords="reinforcement learning attention planning cs.cl cs.lg" data-themes="S E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2511.22904v1" target="_blank">Language-conditioned world model improves policy generalization by reading environmental descriptions</a>
                            </h3>
                            <p class="card-authors">Anh Nguyen, Stefan Lee</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Attention</span><span class="keyword-tag">Planning</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444274592">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">To interact effectively with humans in the real world, it is important for agents to understand language that describes the dynamics of the environment--that is, how the environment behaves--rather than just task instructions specifying &quot;what to do&quot;. Understanding this dynamics-descriptive language is important for human-agent interaction and agent behavior. Recent work address this problem using a model-based approach: language is incorporated into a world model, which is then used to learn a behavior policy. However, these existing methods either do not demonstrate policy generalization to u...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a model-based reinforcement learning approach, where a language-conditioned world model is trained through interaction with the environment, and a policy is learned from this model--without planning or expert demonstrations</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Recent work address this problem using a model-based approach: language is incorporated into a world model, which is then used to learn a behavior policy</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Recent work address this problem using a model-based approach: language is incorporated into a world model, which is then used to learn a behavior policy</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2511.22904v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2511.22904v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Nguyen, and S. Lee, &quot;Language-conditioned world model improves policy generalization by reading environmental descriptions,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2511.22904v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444274592')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="exploring gender bias in large language models: an in-depth dive into the german language" data-keywords="perception ros language model cs.cl cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.16557v1" target="_blank">Exploring Gender Bias in Large Language Models: An In-depth Dive into the German Language</a>
                            </h3>
                            <p class="card-authors">Kristin Gnadt, David Thulke, Simone Kopeinik et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Perception</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444261344">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In recent years, various methods have been proposed to evaluate gender bias in large language models (LLMs). A key challenge lies in the transferability of bias measurement methods initially developed for the English language when applied to other languages. This work aims to contribute to this research strand by presenting five German datasets for gender bias evaluation in LLMs. The datasets are grounded in well-established concepts of gender bias and are accessible through multiple methodologies. Our findings, reported for eight multilingual LLM models, reveal unique challenges associated wi...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">A key challenge lies in the transferability of bias measurement methods initially developed for the English language when applied to other languages. Our findings, reported for eight multilingual LLM models, reveal unique challenges associated with gender bias in German, including the ambiguous interpretation of male occupational terms and the influence of seemingly neutral nouns on gender perception</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In recent years, various methods have been proposed to evaluate gender bias in large language models (LLMs)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.16557v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.16557v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Gnadt, D. Thulke, S. Kopeinik, and R. Schl√ºter, &quot;Exploring Gender Bias in Large Language Models: An In-depth Dive into the German Language,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.16557v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444261344')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="metasc: test-time safety specification optimization for language models" data-keywords="optimization ros language model cs.cl cs.ai" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.07985v2" target="_blank">MetaSC: Test-Time Safety Specification Optimization for Language Models</a>
                            </h3>
                            <p class="card-authors">V√≠ctor Gallego</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Optimization</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444264944">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We propose a novel dynamic safety framework that optimizes language model (LM) safety reasoning at inference time without modifying model weights. Building on recent advances in self-critique methods, our approach leverages a meta-critique mechanism that iteratively updates safety prompts-termed specifications-to drive the critique and revision process adaptively. This test-time optimization not only improves performance against adversarial jailbreak requests but also in diverse general safety-related tasks, such as avoiding moral harm or pursuing honest responses. Our empirical evaluations ac...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a novel dynamic safety framework that optimizes language model (LM) safety reasoning at inference time without modifying model weights</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We propose a novel dynamic safety framework that optimizes language model (LM) safety reasoning at inference time without modifying model weights</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.07985v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.07985v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'V. Gallego, &quot;MetaSC: Test-Time Safety Specification Optimization for Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.07985v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444264944')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="untangling the influence of typology, data and model architecture on ranking transfer languages for cross-lingual pos tagging" data-keywords="lstm ros cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.19979v1" target="_blank">Untangling the Influence of Typology, Data and Model Architecture on Ranking Transfer Languages for Cross-Lingual POS Tagging</a>
                            </h3>
                            <p class="card-authors">Enora Rice, Ali Marashian, Hannah Haynie et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Lstm</span><span class="keyword-tag">Ros</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444263264">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Cross-lingual transfer learning is an invaluable tool for overcoming data scarcity, yet selecting a suitable transfer language remains a challenge. The precise roles of linguistic typology, training data, and model architecture in transfer language choice are not fully understood. We take a holistic approach, examining how both dataset-specific and fine-grained typological features influence transfer language selection for part-of-speech tagging, considering two different sources for morphosyntactic features. While previous work examines these dynamics in the context of bilingual biLSTMS, we e...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Cross-lingual transfer learning is an invaluable tool for overcoming data scarcity, yet selecting a suitable transfer language remains a challenge</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The precise roles of linguistic typology, training data, and model architecture in transfer language choice are not fully understood</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.19979v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.19979v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'E. Rice, A. Marashian, H. Haynie, K. v. d. Wense, and A. Palmer, &quot;Untangling the Influence of Typology, Data and Model Architecture on Ranking Transfer Languages for Cross-Lingual POS Tagging,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.19979v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444263264')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.lg" data-title="understanding reasoning in thinking language models via steering vectors" data-keywords="control ros language model cs.lg cs.ai" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Machine Learning</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.18167v4" target="_blank">Understanding Reasoning in Thinking Language Models via Steering Vectors</a>
                            </h3>
                            <p class="card-authors">Constantin Venhoff, Iv√°n Arcuschin, Philip Torr et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Control</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span></div>

                            <div class="card-details" id="details-4444264464">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recent advances in large language models (LLMs) have led to the development of thinking language models that generate extensive internal reasoning chains before producing responses. While these models achieve improved performance, controlling their reasoning processes remains challenging. This work presents a steering approach for thinking LLMs by analyzing and manipulating specific reasoning behaviors in DeepSeek-R1-Distill models. Through a systematic experiment on 500 tasks across 10 diverse categories, we identify several reasoning behaviors exhibited by thinking models, including expressi...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We demonstrate that these behaviors are mediated by linear directions in the model&#x27;s activation space and can be controlled using steering vectors</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Recent advances in large language models (LLMs) have led to the development of thinking language models that generate extensive internal reasoning chains before producing responses</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.18167v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.18167v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Venhoff, I. Arcuschin, P. Torr, A. Conmy, and N. Nanda, &quot;Understanding Reasoning in Thinking Language Models via Steering Vectors,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.18167v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444264464')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="q-bio.gn" data-title="deepseq: high-throughput single-cell rna sequencing data labeling via web search-augmented agentic generative ai foundation models" data-keywords="q-bio.gn cs.ai cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Q-BIO.GN</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.13817v1" target="_blank">DeepSeq: High-Throughput Single-Cell RNA Sequencing Data Labeling via Web Search-Augmented Agentic Generative AI Foundation Models</a>
                            </h3>
                            <p class="card-authors">Saleem A. Al Dajani, Abel Sanchez, John R. Williams</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Q-BIO.GN</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.LG</span></div>

                            <div class="card-details" id="details-4444263312">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Generative AI foundation models offer transformative potential for processing structured biological data, particularly in single-cell RNA sequencing, where datasets are rapidly scaling toward billions of cells. We propose the use of agentic foundation models with real-time web search to automate the labeling of experimental data, achieving up to 82.5% accuracy. This addresses a key bottleneck in supervised learning for structured omics data by increasing annotation throughput without manual curation and human error. Our approach enables the development of virtual cell foundation models capable...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose the use of agentic foundation models with real-time web search to automate the labeling of experimental data, achieving up to 82.5% accuracy</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Generative AI foundation models offer transformative potential for processing structured biological data, particularly in single-cell RNA sequencing, where datasets are rapidly scaling toward billions of cells</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.13817v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.13817v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. A. A. Dajani, A. Sanchez, and J. R. Williams, &quot;DeepSeq: High-Throughput Single-Cell RNA Sequencing Data Labeling via Web Search-Augmented Agentic Generative AI Foundation Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.13817v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444263312')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="enhancing small language models for cross-lingual generalized zero-shot classification with soft prompt tuning" data-keywords="ros nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.19469v2" target="_blank">Enhancing Small Language Models for Cross-Lingual Generalized Zero-Shot Classification with Soft Prompt Tuning</a>
                            </h3>
                            <p class="card-authors">Fred Philippy, Siwen Guo, Cedric Lothritz et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444266144">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In NLP, Zero-Shot Classification (ZSC) has become essential for enabling models to classify text into categories unseen during training, particularly in low-resource languages and domains where labeled data is scarce. While pretrained language models (PLMs) have shown promise in ZSC, they often rely on large training datasets or external knowledge, limiting their applicability in multilingual and low-resource scenarios. Recent approaches leveraging natural language prompts reduce the dependence on large training datasets but struggle to effectively incorporate available labeled data from relat...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address these challenges, we introduce RoSPrompt, a lightweight and data-efficient approach for training soft prompts that enhance cross-lingual ZSC while ensuring robust generalization across data distribution shifts</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To address these challenges, we introduce RoSPrompt, a lightweight and data-efficient approach for training soft prompts that enhance cross-lingual ZSC while ensuring robust generalization across data distribution shifts</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In NLP, Zero-Shot Classification (ZSC) has become essential for enabling models to classify text into categories unseen during training, particularly in low-resource languages and domains where labeled data is scarce</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.19469v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.19469v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. Philippy, S. Guo, C. Lothritz, J. Klein, and T. F. Bissyand√©, &quot;Enhancing Small Language Models for Cross-Lingual Generalized Zero-Shot Classification with Soft Prompt Tuning,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.19469v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444266144')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="studies with impossible languages falsify lms as models of human language" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2511.11389v1" target="_blank">Studies with impossible languages falsify LMs as models of human language</a>
                            </h3>
                            <p class="card-authors">Jeffrey S. Bowers, Jeff Mitchell</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444266096">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">According to Futrell and Mahowald [arXiv:2501.17047], both infants and language models (LMs) find attested languages easier to learn than impossible languages that have unnatural structures. We review the literature and show that LMs often learn attested and many impossible languages equally well. Difficult to learn impossible languages are simply more complex (or random). LMs are missing human inductive biases that support language acquisition.</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Difficult to learn impossible languages are simply more complex (or random)</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">According to Futrell and Mahowald [arXiv:2501.17047], both infants and language models (LMs) find attested languages easier to learn than impossible languages that have unnatural structures</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2511.11389v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2511.11389v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. S. Bowers, and J. Mitchell, &quot;Studies with impossible languages falsify LMs as models of human language,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2511.11389v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444266096')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="benchmarking adversarial robustness to bias elicitation in large language models: scalable automated assessment with llm-as-a-judge" data-keywords="manipulation ros language model cs.cl cs.ai" data-themes="S E L M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.07887v2" target="_blank">Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge</a>
                            </h3>
                            <p class="card-authors">Riccardo Cantini, Alessio Orsino, Massimo Ruggiero et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Manipulation</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444262736">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The growing integration of Large Language Models (LLMs) into critical societal domains has raised concerns about embedded biases that can perpetuate stereotypes and undermine fairness. Such biases may stem from historical inequalities in training data, linguistic imbalances, or adversarial manipulation. Despite mitigation efforts, recent studies show that LLMs remain vulnerable to adversarial attacks that elicit biased outputs. This work proposes a scalable benchmarking framework to assess LLM robustness to adversarial bias elicitation. Our methodology involves: (i) systematically probing mode...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Our methodology involves: (i) systematically probing models across multiple tasks targeting diverse sociocultural biases, (ii) quantifying robustness through safety scores using an LLM-as-a-Judge approach, and (iii) employing jailbreak techniques to reveal safety vulnerabilities</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The growing integration of Large Language Models (LLMs) into critical societal domains has raised concerns about embedded biases that can perpetuate stereotypes and undermine fairness</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.07887v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.07887v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Cantini, A. Orsino, M. Ruggiero, and D. Talia, &quot;Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.07887v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444262736')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="distinct social-linguistic processing between humans and large audio-language models: evidence from model-brain alignment" data-keywords="ros language model cs.cl q-bio.nc" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.19586v2" target="_blank">Distinct social-linguistic processing between humans and large audio-language models: Evidence from model-brain alignment</a>
                            </h3>
                            <p class="card-authors">Hanlin Wu, Xufeng Duan, Zhenguang Cai</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">Q-BIO.NC</span></div>

                            <div class="card-details" id="details-4444265760">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Voice-based AI development faces unique challenges in processing both linguistic and paralinguistic information. This study compares how large audio-language models (LALMs) and humans integrate speaker characteristics during speech comprehension, asking whether LALMs process speaker-contextualized language in ways that parallel human cognitive mechanisms. We compared two LALMs&#x27; (Qwen2-Audio and Ultravox 0.5) processing patterns with human EEG responses. Using surprisal and entropy metrics from the models, we analyzed their sensitivity to speaker-content incongruency across social stereotype vi...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Voice-based AI development faces unique challenges in processing both linguistic and paralinguistic information. These findings reveal both the potential and limitations of current LALMs in processing speaker-contextualized language, and suggest differences in social-linguistic processing mechanisms between humans and LALMs.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This study compares how large audio-language models (LALMs) and humans integrate speaker characteristics during speech comprehension, asking whether LALMs process speaker-contextualized language in ways that parallel human cognitive mechanisms</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.19586v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.19586v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Wu, X. Duan, and Z. Cai, &quot;Distinct social-linguistic processing between humans and large audio-language models: Evidence from model-brain alignment,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.19586v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444265760')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="tiny language models" data-keywords="transformer bert ros nlp language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.14871v2" target="_blank">Tiny language models</a>
                            </h3>
                            <p class="card-authors">Ronit D. Gross, Yarden Tzach, Tal Halevi et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Transformer</span><span class="keyword-tag">Bert</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Nlp</span></div>

                            <div class="card-details" id="details-4444265424">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">A prominent achievement of natural language processing (NLP) is its ability to understand and generate meaningful human language. This capability relies on complex feedforward transformer block architectures pre-trained on large language models (LLMs). However, LLM pre-training is currently feasible only for a few dominant companies due to the immense computational resources required, limiting broader research participation. This creates a critical need for more accessible alternatives. In this study, we explore whether tiny language models (TLMs) exhibit the same key qualitative features of L...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We demonstrate that TLMs exhibit a clear performance gap between pre-trained and non-pre-trained models across classification tasks, indicating the effectiveness of pre-training, even at a tiny scale</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">We demonstrate that TLMs exhibit a clear performance gap between pre-trained and non-pre-trained models across classification tasks, indicating the effectiveness of pre-training, even at a tiny scale. The performance gap increases with the size of the pre-training dataset and with greater overlap between tokens in the pre-training and classification datasets</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This capability relies on complex feedforward transformer block architectures pre-trained on large language models (LLMs)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.14871v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.14871v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. D. Gross, Y. Tzach, T. Halevi, E. Koresh, and I. Kanter, &quot;Tiny language models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.14871v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444265424')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="learning getting-up policies for real-world humanoid robots" data-keywords="robot humanoid locomotion control cs.ro cs.lg" data-themes="S L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.12152v2" target="_blank">Learning Getting-Up Policies for Real-World Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Xialin He, Runpei Dong, Zixuan Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Control</span></div>

                            <div class="card-details" id="details-4444264560">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Automatic fall recovery is a crucial prerequisite before humanoid robots can be reliably deployed. Hand-designing controllers for getting up is difficult because of the varied configurations a humanoid can end up in after a fall and the challenging terrains humanoid robots are expected to operate on. This paper develops a learning framework to produce controllers that enable humanoid robots to get up from varying configurations on varying terrains. Unlike previous successful applications of learning to humanoid locomotion, the getting-up task involves complex contact patterns (which necessitat...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Hand-designing controllers for getting up is difficult because of the varied configurations a humanoid can end up in after a fall and the challenging terrains humanoid robots are expected to operate on. We address these challenges through a two-phase approach that induces a curriculum</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper develops a learning framework to produce controllers that enable humanoid robots to get up from varying configurations on varying terrains</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.12152v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.12152v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. He, R. Dong, Z. Chen, and S. Gupta, &quot;Learning Getting-Up Policies for Real-World Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.12152v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444264560')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="unified multi-rate model predictive control for a jet-powered humanoid robot" data-keywords="robot humanoid control simulation mujoco imu cs.ro eess.sy" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2505.16478v2" target="_blank">Unified Multi-Rate Model Predictive Control for a Jet-Powered Humanoid Robot</a>
                            </h3>
                            <p class="card-authors">Davide Gorbani, Giuseppe L&#x27;Erario, Hosameldin Awadalla Omer Mohamed et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Control</span><span class="keyword-tag">Simulation</span></div>

                            <div class="card-details" id="details-4445407840">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We propose a novel Model Predictive Control (MPC) framework for a jet-powered flying humanoid robot. The controller is based on a linearised centroidal momentum model to represent the flight dynamics, augmented with a second-order nonlinear model to explicitly account for the slow and nonlinear dynamics of jet propulsion. A key contribution is the introduction of a multi-rate MPC formulation that handles the different actuation rates of the robot&#x27;s joints and jet engines while embedding the jet dynamics directly into the predictive model. We validated the framework using the jet-powered humano...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a novel Model Predictive Control (MPC) framework for a jet-powered flying humanoid robot</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We propose a novel Model Predictive Control (MPC) framework for a jet-powered flying humanoid robot</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2505.16478v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2505.16478v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Gorbani, G. L&#x27;Erario, H. A. O. Mohamed, and D. Pucci, &quot;Unified Multi-Rate Model Predictive Control for a Jet-Powered Humanoid Robot,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2505.16478v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445407840')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="robot trains robot: automatic real-world policy adaptation and learning for humanoids" data-keywords="reinforcement learning robot humanoid locomotion simulation imu cs.ro" data-themes="S L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.12252v2" target="_blank">Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids</a>
                            </h3>
                            <p class="card-authors">Kaizhe Hu, Haochen Shi, Yao He et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4444264800">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Simulation-based reinforcement learning (RL) has significantly advanced humanoid locomotion tasks, yet direct real-world RL from scratch or adapting from pretrained policies remains rare, limiting the full potential of humanoid robots. Real-world learning, despite being crucial for overcoming the sim-to-real gap, faces substantial challenges related to safety, reward design, and learning efficiency. To address these limitations, we propose Robot-Trains-Robot (RTR), a novel framework where a robotic arm teacher actively supports and guides a humanoid robot student. The RTR system provides prote...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address these limitations, we propose Robot-Trains-Robot (RTR), a novel framework where a robotic arm teacher actively supports and guides a humanoid robot student</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Real-world learning, despite being crucial for overcoming the sim-to-real gap, faces substantial challenges related to safety, reward design, and learning efficiency. To address these limitations, we propose Robot-Trains-Robot (RTR), a novel framework where a robotic arm teacher actively supports and guides a humanoid robot student</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">To address these limitations, we propose Robot-Trains-Robot (RTR), a novel framework where a robotic arm teacher actively supports and guides a humanoid robot student</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.12252v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.12252v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Hu, H. Shi, Y. He, W. Wang, C. K. Liu, and S. Song, &quot;Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.12252v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444264800')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="a framework for optimal ankle design of humanoid robots" data-keywords="robot humanoid optimization ros cs.ro" data-themes="M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.16469v1" target="_blank">A Framework for Optimal Ankle Design of Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Guglielmo Cervettini, Roberto Mauceri, Alex Coppola et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">Ros</span></div>

                            <div class="card-details" id="details-4444266480">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The design of the humanoid ankle is critical for safe and efficient ground interaction. Key factors such as mechanical compliance and motor mass distribution have driven the adoption of parallel mechanism architectures. However, selecting the optimal configuration depends on both actuator availability and task requirements. We propose a unified methodology for the design and evaluation of parallel ankle mechanisms. A multi-objective optimization synthesizes the mechanism geometry, the resulting solutions are evaluated using a scalar cost function that aggregates key performance metrics for cro...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a unified methodology for the design and evaluation of parallel ankle mechanisms</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Key factors such as mechanical compliance and motor mass distribution have driven the adoption of parallel mechanism architectures</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.16469v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.16469v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Cervettini et al., &quot;A Framework for Optimal Ankle Design of Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.16469v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444266480')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="compositional coordination for multi-robot teams with large language models" data-keywords="robot multi-robot coordination control simulation ros imu language model cs.ro cs.ai" data-themes="S E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.16068v3" target="_blank">Compositional Coordination for Multi-Robot Teams with Large Language Models</a>
                            </h3>
                            <p class="card-authors">Zhehui Huang, Guangyao Shi, Yuwei Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Coordination</span><span class="keyword-tag">Control</span></div>

                            <div class="card-details" id="details-4444274640">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Multi-robot coordination has traditionally relied on a mission-specific and expert-driven pipeline, where natural language mission descriptions are manually translated by domain experts into mathematical formulation, algorithm design, and executable code. This conventional process is labor-intensive, inaccessible to non-experts, and inflexible to changes in mission requirements. Here, we propose LAN2CB (Language to Collective Behavior), a novel framework that leverages large language models (LLMs) to streamline and generalize the multi-robot coordination pipeline. LAN2CB transforms natural lan...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Here, we propose LAN2CB (Language to Collective Behavior), a novel framework that leverages large language models (LLMs) to streamline and generalize the multi-robot coordination pipeline</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Multi-robot coordination has traditionally relied on a mission-specific and expert-driven pipeline, where natural language mission descriptions are manually translated by domain experts into mathematical formulation, algorithm design, and executable code</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.16068v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.16068v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Huang, G. Shi, Y. Wu, V. Kumar, and G. S. Sukhatme, &quot;Compositional Coordination for Multi-Robot Teams with Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.16068v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444274640')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="hierarchical reduced-order model predictive control for robust locomotion on humanoid robots" data-keywords="robot humanoid locomotion planning control simulation ros imu cs.ro" data-themes="S L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.04722v1" target="_blank">Hierarchical Reduced-Order Model Predictive Control for Robust Locomotion on Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Adrian B. Ghansah, Sergio A. Esteban, Aaron D. Ames</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Planning</span></div>

                            <div class="card-details" id="details-4445408608">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">As humanoid robots enter real-world environments, ensuring robust locomotion across diverse environments is crucial. This paper presents a computationally efficient hierarchical control framework for humanoid robot locomotion based on reduced-order models -- enabling versatile step planning and incorporating arm and torso dynamics to better stabilize the walking. At the high level, we use the step-to-step dynamics of the ALIP model to simultaneously optimize over step periods, step lengths, and ankle torques via nonlinear MPC. The ALIP trajectories are used as references to a linear MPC framew...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents a computationally efficient hierarchical control framework for humanoid robot locomotion based on reduced-order models -- enabling versatile step planning and incorporating arm and torso dynamics to better stabilize the walking</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper presents a computationally efficient hierarchical control framework for humanoid robot locomotion based on reduced-order models -- enabling versatile step planning and incorporating arm and torso dynamics to better stabilize the walking</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.04722v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.04722v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. B. Ghansah, S. A. Esteban, and A. D. Ames, &quot;Hierarchical Reduced-Order Model Predictive Control for Robust Locomotion on Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.04722v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445408608')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="learning vision-driven reactive soccer skills for humanoid robots" data-keywords="reinforcement learning robot humanoid coordination perception control ros cs.ro" data-themes="S I R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2511.03996v1" target="_blank">Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Yushi Wang, Changsheng Luo, Penghui Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Coordination</span></div>

                            <div class="card-details" id="details-4445407792">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid soccer poses a representative challenge for embodied intelligence, requiring robots to operate within a tightly coupled perception-action loop. However, existing systems typically rely on decoupled modules, resulting in delayed responses and incoherent behaviors in dynamic environments, while real-world perceptual limitations further exacerbate these issues. In this work, we present a unified reinforcement learning-based controller that enables humanoid robots to acquire reactive soccer skills through the direct integration of visual perception and motion control. Our approach extends...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we present a unified reinforcement learning-based controller that enables humanoid robots to acquire reactive soccer skills through the direct integration of visual perception and motion control</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Humanoid soccer poses a representative challenge for embodied intelligence, requiring robots to operate within a tightly coupled perception-action loop. However, existing systems typically rely on decoupled modules, resulting in delayed responses and incoherent behaviors in dynamic environments, while real-world perceptual limitations further exacerbate these issues</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Our approach extends Adversarial Motion Priors to perceptual settings in real-world dynamic environments, bridging motion imitation and visually grounded dynamic control</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2511.03996v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2511.03996v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Wang et al., &quot;Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2511.03996v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445407792')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="policies over poses: reinforcement learning based distributed pose-graph optimization for multi-robot slam" data-keywords="reinforcement learning neural network gnn robot multi-agent multi-robot slam optimization imu cs.ro" data-themes="S R M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2510.22740v1" target="_blank">Policies over Poses: Reinforcement Learning based Distributed Pose-Graph Optimization for Multi-Robot SLAM</a>
                            </h3>
                            <p class="card-authors">Sai Krishna Ghanta, Ramviyas Parasuraman</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-R">Multi-Robot</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Gnn</span><span class="keyword-tag">Robot</span></div>

                            <div class="card-details" id="details-4445408176">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We consider the distributed pose-graph optimization (PGO) problem, which is fundamental in accurate trajectory estimation in multi-robot simultaneous localization and mapping (SLAM). Conventional iterative approaches linearize a highly non-convex optimization objective, requiring repeated solving of normal equations, which often converge to local minima and thus produce suboptimal estimates. We propose a scalable, outlier-robust distributed planar PGO framework using Multi-Agent Reinforcement Learning (MARL). We cast distributed PGO as a partially observable Markov game defined on local pose-g...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a scalable, outlier-robust distributed planar PGO framework using Multi-Agent Reinforcement Learning (MARL)</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">We consider the distributed pose-graph optimization (PGO) problem, which is fundamental in accurate trajectory estimation in multi-robot simultaneous localization and mapping (SLAM)</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Conventional iterative approaches linearize a highly non-convex optimization objective, requiring repeated solving of normal equations, which often converge to local minima and thus produce suboptimal estimates</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2510.22740v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2510.22740v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. K. Ghanta, and R. Parasuraman, &quot;Policies over Poses: Reinforcement Learning based Distributed Pose-Graph Optimization for Multi-Robot SLAM,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2510.22740v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445408176')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="destination-to-chutes task mapping optimization for multi-robot coordination in robotic sorting systems" data-keywords="robot planning optimization imu cs.ro cs.ai cs.ma" data-themes="S R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2510.03472v1" target="_blank">Destination-to-Chutes Task Mapping Optimization for Multi-Robot Coordination in Robotic Sorting Systems</a>
                            </h3>
                            <p class="card-authors">Yulun Zhang, Alexandre O. G. Barbosa, Federico Pecora et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Planning</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">Imu</span></div>

                            <div class="card-details" id="details-4445407168">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We study optimizing a destination-to-chutes task mapping to improve throughput in Robotic Sorting Systems (RSS), where a team of robots sort packages on a sortation floor by transporting them from induct workstations to eject chutes based on their shipping destinations (e.g. Los Angeles or Pittsburgh). The destination-to-chutes task mapping is used to determine which chutes a robot can drop its package. Finding a high-quality task mapping is challenging because of the complexity of a real-world RSS. First, optimizing task mapping is interdependent with robot target assignment and path planning...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">In this paper, we first formally define task mappings and the problem of Task Mapping Optimization (TMO)</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We then present a simple TMO method based on the Evolutionary Algorithm and Mixed Integer Linear Programming, demonstrating the advantage of our optimized task mappings over the greedily generated ones in various RSS setups with different map sizes, numbers of chutes, and destinations</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2510.03472v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2510.03472v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Zhang, A. O. G. Barbosa, F. Pecora, and J. Li, &quot;Destination-to-Chutes Task Mapping Optimization for Multi-Robot Coordination in Robotic Sorting Systems,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2510.03472v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445407168')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="whole-body multi-contact motion control for humanoid robots based on distributed tactile sensors" data-keywords="robot humanoid control simulation imu cs.ro" data-themes="L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2505.19580v1" target="_blank">Whole-body Multi-contact Motion Control for Humanoid Robots Based on Distributed Tactile Sensors</a>
                            </h3>
                            <p class="card-authors">Masaki Murooka, Kensuke Fukumitsu, Marwan Hamze et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Control</span><span class="keyword-tag">Simulation</span></div>

                            <div class="card-details" id="details-4445406976">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">To enable humanoid robots to work robustly in confined environments, multi-contact motion that makes contacts not only at extremities, such as hands and feet, but also at intermediate areas of the limbs, such as knees and elbows, is essential. We develop a method to realize such whole-body multi-contact motion involving contacts at intermediate areas by a humanoid robot. Deformable sheet-shaped distributed tactile sensors are mounted on the surface of the robot&#x27;s limbs to measure the contact force without significantly changing the robot body shape. The multi-contact motion controller develope...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We develop a method to realize such whole-body multi-contact motion involving contacts at intermediate areas by a humanoid robot</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We develop a method to realize such whole-body multi-contact motion involving contacts at intermediate areas by a humanoid robot</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2505.19580v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2505.19580v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Murooka et al., &quot;Whole-body Multi-contact Motion Control for Humanoid Robots Based on Distributed Tactile Sensors,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2505.19580v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445406976')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="mash: cooperative-heterogeneous multi-agent reinforcement learning for single humanoid robot locomotion" data-keywords="reinforcement learning robot humanoid locomotion multi-agent multi-robot cooperative control cs.ro cs.ai" data-themes="L I R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.10423v1" target="_blank">MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion</a>
                            </h3>
                            <p class="card-authors">Qi Liu, Xiaopeng Zhang, Mingshan Tan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4445407312">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper proposes a novel method to enhance locomotion for a single humanoid robot through cooperative-heterogeneous multi-agent deep reinforcement learning (MARL). While most existing methods typically employ single-agent reinforcement learning algorithms for a single humanoid robot or MARL algorithms for multi-robot system tasks, we propose a distinct paradigm: applying cooperative-heterogeneous MARL to optimize locomotion for a single humanoid robot. The proposed method, multi-agent reinforcement learning for single humanoid locomotion (MASH), treats each limb (legs and arms) as an indepe...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">While most existing methods typically employ single-agent reinforcement learning algorithms for a single humanoid robot or MARL algorithms for multi-robot system tasks, we propose a distinct paradigm: applying cooperative-heterogeneous MARL to optimize locomotion for a single humanoid robot</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper proposes a novel method to enhance locomotion for a single humanoid robot through cooperative-heterogeneous multi-agent deep reinforcement learning (MARL)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.10423v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.10423v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Q. Liu, X. Zhang, M. Tan, S. Ma, J. Ding, and Y. Li, &quot;MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.10423v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445407312')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="inekformer: a hybrid state estimator for humanoid robots" data-keywords="deep learning transformer robot humanoid bipedal locomotion control cs.ro" data-themes="L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2511.16306v1" target="_blank">InEKFormer: A Hybrid State Estimator for Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Lasse Hohmeyer, Mihaela Popescu, Ivan Bergonzani et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Deep Learning</span><span class="keyword-tag">Transformer</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span></div>

                            <div class="card-details" id="details-4445419552">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots have great potential for a wide range of applications, including industrial and domestic use, healthcare, and search and rescue missions. However, bipedal locomotion in different environments is still a challenge when it comes to performing stable and dynamic movements. This is where state estimation plays a crucial role, providing fast and accurate feedback of the robot&#x27;s floating base state to the motion controller. Although classical state estimation methods such as Kalman filters are widely used in robotics, they require expert knowledge to fine-tune the noise parameters. D...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose the InEKFormer, a novel hybrid state estimation method that incorporates an invariant extended Kalman filter (InEKF) and a Transformer network</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, bipedal locomotion in different environments is still a challenge when it comes to performing stable and dynamic movements. The results indicate the potential of Transformers in humanoid state estimation, but also highlight the need for robust autoregressive training in these high-dimensional problems.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Although classical state estimation methods such as Kalman filters are widely used in robotics, they require expert knowledge to fine-tune the noise parameters</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2511.16306v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2511.16306v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Hohmeyer, M. Popescu, I. Bergonzani, D. Mronga, and F. Kirchner, &quot;InEKFormer: A Hybrid State Estimator for Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2511.16306v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445419552')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="unitracker: learning universal whole-body motion tracker for humanoid robots" data-keywords="robot humanoid control simulation ros imu cs.ro" data-themes="S L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.07356v3" target="_blank">UniTracker: Learning Universal Whole-Body Motion Tracker for Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Kangning Yin, Weishuai Zeng, Ke Fan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Control</span><span class="keyword-tag">Simulation</span></div>

                            <div class="card-details" id="details-4445419648">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Achieving expressive and generalizable whole-body motion control is essential for deploying humanoid robots in real-world environments. In this work, we propose UniTracker, a three-stage training framework that enables robust and scalable motion tracking across a wide range of human behaviors. In the first stage, we train a teacher policy with privileged observations to generate high-quality actions. In the second stage, we introduce a Conditional Variational Autoencoder (CVAE) to model a universal student policy that can be deployed directly on real hardware. The CVAE structure allows the pol...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose UniTracker, a three-stage training framework that enables robust and scalable motion tracking across a wide range of human behaviors</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">The CVAE structure allows the policy to learn a global latent representation of motion, enhancing generalization to unseen behaviors and addressing the limitations of standard MLP-based policies under partial observations. In the third stage, we introduce a fast adaptation module that fine-tunes the universal policy on harder motion sequences that are difficult to track directly</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In this work, we propose UniTracker, a three-stage training framework that enables robust and scalable motion tracking across a wide range of human behaviors</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.07356v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.07356v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Yin et al., &quot;UniTracker: Learning Universal Whole-Body Motion Tracker for Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.07356v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445419648')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="demonstrating berkeley humanoid lite: an open-source, accessible, and customizable 3d-printed humanoid robot" data-keywords="reinforcement learning robot humanoid locomotion control simulation imu cs.ro" data-themes="S L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.17249v1" target="_blank">Demonstrating Berkeley Humanoid Lite: An Open-source, Accessible, and Customizable 3D-printed Humanoid Robot</a>
                            </h3>
                            <p class="card-authors">Yufeng Chi, Qiayuan Liao, Junfeng Long et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4445419840">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Despite significant interest and advancements in humanoid robotics, most existing commercially available hardware remains high-cost, closed-source, and non-transparent within the robotics community. This lack of accessibility and customization hinders the growth of the field and the broader development of humanoid technologies. To address these challenges and promote democratization in humanoid robotics, we demonstrate Berkeley Humanoid Lite, an open-source humanoid robot designed to be accessible, customizable, and beneficial for the entire community. The core of this design is a modular 3D-p...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address these challenges and promote democratization in humanoid robotics, we demonstrate Berkeley Humanoid Lite, an open-source humanoid robot designed to be accessible, customizable, and beneficial for the entire community</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To address these challenges and promote democratization in humanoid robotics, we demonstrate Berkeley Humanoid Lite, an open-source humanoid robot designed to be accessible, customizable, and beneficial for the entire community. To address the inherent limitations of 3D-printed gearboxes, such as reduced strength and durability compared to metal alternatives, we adopted a cycloidal gear design, which provides an optimal form factor in this context</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">By fully open-sourcing the hardware design, embedded code, and training and deployment frameworks, we aim for Berkeley Humanoid Lite to serve as a pivotal step toward democratizing the development of humanoid robotics</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.17249v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.17249v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Chi et al., &quot;Demonstrating Berkeley Humanoid Lite: An Open-source, Accessible, and Customizable 3D-printed Humanoid Robot,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.17249v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445419840')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="child (controller for humanoid imitation and live demonstration): a whole-body humanoid teleoperation system" data-keywords="robot humanoid manipulation control cs.ro" data-themes="L I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.00162v2" target="_blank">CHILD (Controller for Humanoid Imitation and Live Demonstration): a Whole-Body Humanoid Teleoperation System</a>
                            </h3>
                            <p class="card-authors">Noboru Myers, Obin Kwon, Sankalp Yamsani et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Manipulation</span><span class="keyword-tag">Control</span></div>

                            <div class="card-details" id="details-4445407600">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recent advances in teleoperation have demonstrated robots performing complex manipulation tasks. However, existing works rarely support whole-body joint-level teleoperation for humanoid robots, limiting the diversity of tasks that can be accomplished. This work presents Controller for Humanoid Imitation and Live Demonstration (CHILD), a compact reconfigurable teleoperation system that enables joint level control over humanoid robots. CHILD fits within a standard baby carrier, allowing the operator control over all four limbs, and supports both direct joint mapping for full-body control and loc...</p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.00162v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.00162v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. Myers, O. Kwon, S. Yamsani, and J. Kim, &quot;CHILD (Controller for Humanoid Imitation and Live Demonstration): a Whole-Body Humanoid Teleoperation System,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.00162v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445407600')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="rhino: learning real-time humanoid-human-object interaction from human demonstrations" data-keywords="robot humanoid locomotion manipulation control cs.ro cs.hc cs.lg" data-themes="S L M E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.13134v1" target="_blank">RHINO: Learning Real-Time Humanoid-Human-Object Interaction from Human Demonstrations</a>
                            </h3>
                            <p class="card-authors">Jingxiao Chen, Xinyao Li, Jiahang Cao et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Manipulation</span></div>

                            <div class="card-details" id="details-4445408080">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots have shown success in locomotion and manipulation. Despite these basic abilities, humanoids are still required to quickly understand human instructions and react based on human interaction signals to become valuable assistants in human daily life. Unfortunately, most existing works only focus on multi-stage interactions, treating each task separately, and neglecting real-time feedback. In this work, we aim to empower humanoid robots with real-time reaction abilities to achieve various tasks, allowing human to interrupt robots at any time, and making robots respond to humans imm...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To support such abilities, we propose a general humanoid-human-object interaction framework, named RHINO, i.e., Real-time Humanoid-human Interaction and Object manipulation</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">To support such abilities, we propose a general humanoid-human-object interaction framework, named RHINO, i.e., Real-time Humanoid-human Interaction and Object manipulation</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.13134v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.13134v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Chen et al., &quot;RHINO: Learning Real-Time Humanoid-Human-Object Interaction from Human Demonstrations,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.13134v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445408080')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="roboballet: planning for multi-robot reaching with graph neural networks and reinforcement learning" data-keywords="reinforcement learning neural network gnn robot coordination perception planning optimization simulation imu" data-themes="S I R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.05397v1" target="_blank">RoboBallet: Planning for Multi-Robot Reaching with Graph Neural Networks and Reinforcement Learning</a>
                            </h3>
                            <p class="card-authors">Matthew Lai, Keegan Go, Zhibin Li et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Gnn</span><span class="keyword-tag">Robot</span></div>

                            <div class="card-details" id="details-4445408368">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Modern robotic manufacturing requires collision-free coordination of multiple robots to complete numerous tasks in shared, obstacle-rich workspaces. Although individual tasks may be simple in isolation, automated joint task allocation, scheduling, and motion planning under spatio-temporal constraints remain computationally intractable for classical methods at real-world scales. Existing multi-arm systems deployed in the industry rely on human intuition and experience to design feasible trajectories manually in a labor-intensive process. To address this challenge, we propose a reinforcement lea...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address this challenge, we propose a reinforcement learning (RL) framework to achieve automated task and motion planning, tested in an obstacle-rich environment with eight robots performing 40 reaching tasks in a shared workspace, where any robot can perform any task in any order</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To address this challenge, we propose a reinforcement learning (RL) framework to achieve automated task and motion planning, tested in an obstacle-rich environment with eight robots performing 40 reaching tasks in a shared workspace, where any robot can perform any task in any order. It employs a graph representation of scenes and a graph policy neural network trained through reinforcement learning to generate trajectories of multiple robots, jointly solving the sub-problems of task allocation, scheduling, and motion planning</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Although individual tasks may be simple in isolation, automated joint task allocation, scheduling, and motion planning under spatio-temporal constraints remain computationally intractable for classical methods at real-world scales</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.05397v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.05397v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Lai et al., &quot;RoboBallet: Planning for Multi-Robot Reaching with Graph Neural Networks and Reinforcement Learning,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.05397v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445408368')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="agiloped: agile open-source humanoid robot for research" data-keywords="robot humanoid cs.ro" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.09364v1" target="_blank">AGILOped: Agile Open-Source Humanoid Robot for Research</a>
                            </h3>
                            <p class="card-authors">Grzegorz Ficht, Luis Denninger, Sven Behnke</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">CS.RO</span></div>

                            <div class="card-details" id="details-4445420512">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">With academic and commercial interest for humanoid robots peaking, multiple platforms are being developed. Through a high level of customization, they showcase impressive performance. Most of these systems remain closed-source or have high acquisition and maintenance costs, however. In this work, we present AGILOped - an open-source humanoid robot that closes the gap between high performance and accessibility. Our robot is driven by off-the-shelf backdrivable actuators with high power density and uses standard electronic components. With a height of 110 cm and weighing only 14.5 kg, AGILOped c...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we present AGILOped - an open-source humanoid robot that closes the gap between high performance and accessibility</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">In this work, we present AGILOped - an open-source humanoid robot that closes the gap between high performance and accessibility</p></div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.09364v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.09364v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Ficht, L. Denninger, and S. Behnke, &quot;AGILOped: Agile Open-Source Humanoid Robot for Research,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.09364v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445420512')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="robust humanoid walking on compliant and uneven terrain with deep reinforcement learning" data-keywords="reinforcement learning robot humanoid bipedal locomotion control simulation imu cs.ro" data-themes="S L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.13619v1" target="_blank">Robust Humanoid Walking on Compliant and Uneven Terrain with Deep Reinforcement Learning</a>
                            </h3>
                            <p class="card-authors">Rohan P. Singh, Mitsuharu Morisawa, Mehdi Benallegue et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span></div>

                            <div class="card-details" id="details-4445420944">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">For the deployment of legged robots in real-world environments, it is essential to develop robust locomotion control methods for challenging terrains that may exhibit unexpected deformability and irregularity. In this paper, we explore the application of sim-to-real deep reinforcement learning (RL) for the design of bipedal locomotion controllers for humanoid robots on compliant and uneven terrains. Our key contribution is to show that a simple training curriculum for exposing the RL agent to randomized terrains in simulation can achieve robust walking on a real humanoid robot using only propr...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a new control policy to enable modification of the observed clock signal, leading to adaptive gait frequencies depending on the terrain and command velocity</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">We train an end-to-end bipedal locomotion policy using the proposed approach, and show extensive real-robot demonstration on the HRP-5P humanoid over several difficult terrains inside and outside the lab environment</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">For the deployment of legged robots in real-world environments, it is essential to develop robust locomotion control methods for challenging terrains that may exhibit unexpected deformability and irregularity</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.13619v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.13619v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. P. Singh, M. Morisawa, M. Benallegue, Z. Xie, and F. Kanehiro, &quot;Robust Humanoid Walking on Compliant and Uneven Terrain with Deep Reinforcement Learning,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.13619v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445420944')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="lips: large-scale humanoid robot reinforcement learning with parallel-series structures" data-keywords="reinforcement learning attention robot humanoid control simulation imu cs.ro" data-themes="S I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.08349v1" target="_blank">LiPS: Large-Scale Humanoid Robot Reinforcement Learning with Parallel-Series Structures</a>
                            </h3>
                            <p class="card-authors">Qiang Zhang, Gang Han, Jingkai Sun et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Attention</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span></div>

                            <div class="card-details" id="details-4445408272">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In recent years, research on humanoid robots has garnered significant attention, particularly in reinforcement learning based control algorithms, which have achieved major breakthroughs. Compared to traditional model-based control algorithms, reinforcement learning based algorithms demonstrate substantial advantages in handling complex tasks. Leveraging the large-scale parallel computing capabilities of GPUs, contemporary humanoid robots can undergo extensive parallel training in simulated environments. A physical simulation platform capable of large-scale parallel training is crucial for the ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">For enabling reinforcement learning-based humanoid robot control algorithms to train in large-scale parallel environments, we propose a novel training method LiPS</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">This approach is primarily due to the limitations of physics engines, as current GPU-based physics engines often only support open-loop topologies or have limited capabilities in simulating multi-rigid-body closed-loop topologies. By incorporating multi-rigid-body dynamics modeling in the simulation environment, we significantly reduce the sim2real gap and the difficulty of converting to parallel structures during model deployment, thereby robustly supporting large-scale reinforcement learning for humanoid robots.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In recent years, research on humanoid robots has garnered significant attention, particularly in reinforcement learning based control algorithms, which have achieved major breakthroughs</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.08349v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.08349v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Q. Zhang et al., &quot;LiPS: Large-Scale Humanoid Robot Reinforcement Learning with Parallel-Series Structures,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.08349v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445408272')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="jaxrobotarium: training and deploying multi-robot policies in 10 minutes" data-keywords="reinforcement learning robot multi-agent multi-robot coordination simulation imu cs.ro cs.lg cs.ma" data-themes="S R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2505.06771v3" target="_blank">JaxRobotarium: Training and Deploying Multi-Robot Policies in 10 Minutes</a>
                            </h3>
                            <p class="card-authors">Shalin Anand Jain, Jiazhen Liu, Siva Kailas et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Agent</span><span class="keyword-tag">Multi-Robot</span></div>

                            <div class="card-details" id="details-4445419936">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Multi-agent reinforcement learning (MARL) has emerged as a promising solution for learning complex and scalable coordination behaviors in multi-robot systems. However, established MARL platforms (e.g., SMAC and MPE) lack robotics relevance and hardware deployment, leaving multi-robot learning researchers to develop bespoke environments and hardware testbeds dedicated to the development and evaluation of their individual contributions. The Multi-Agent RL Benchmark and Learning Environment for the Robotarium (MARBLER) is an exciting recent step in providing a standardized robotics-relevant platf...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We demonstrate that JaxRobotarium retains high simulation fidelity while achieving dramatic speedups over baseline (20x in training and 150x in simulation), and provides an open-access sim-to-real evaluation pipeline through the Robotarium testbed, accelerating and democratizing access to multi-robot learning research and evaluation</p></div>
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2505.06771v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2505.06771v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. A. Jain, J. Liu, S. Kailas, and H. Ravichandar, &quot;JaxRobotarium: Training and Deploying Multi-Robot Policies in 10 Minutes,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2505.06771v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445419936')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="learning aerodynamics for the control of flying humanoid robots" data-keywords="neural network robot humanoid locomotion control simulation imu cs.ro cs.lg" data-themes="L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.00305v2" target="_blank">Learning Aerodynamics for the Control of Flying Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Antonello Paolino, Gabriele Nava, Fabio Di Natale et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4445419600">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Robots with multi-modal locomotion are an active research field due to their versatility in diverse environments. In this context, additional actuation can provide humanoid robots with aerial capabilities. Flying humanoid robots face challenges in modeling and control, particularly with aerodynamic forces. This paper addresses these challenges from a technological and scientific standpoint. The technological contribution includes the mechanical design of iRonCub-Mk1, a jet-powered humanoid robot, optimized for jet engine integration, and hardware modifications for wind tunnel experiments on hu...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Flying humanoid robots face challenges in modeling and control, particularly with aerodynamic forces. This paper addresses these challenges from a technological and scientific standpoint</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Flying humanoid robots face challenges in modeling and control, particularly with aerodynamic forces</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.00305v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.00305v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Paolino et al., &quot;Learning Aerodynamics for the Control of Flying Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.00305v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445419600')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="gengrid: a generalised distributed experimental environmental grid for swarm robotics" data-keywords="robot swarm multi-robot control cs.ro cs.ma" data-themes="R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Multi-Agent Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.20071v1" target="_blank">GenGrid: A Generalised Distributed Experimental Environmental Grid for Swarm Robotics</a>
                            </h3>
                            <p class="card-authors">Pranav Kedia, Madhav Rao</p>
                            <div class="theme-tags"><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Swarm</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Control</span></div>

                            <div class="card-details" id="details-4445419504">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">GenGrid is a novel comprehensive open-source, distributed platform intended for conducting extensive swarm robotic experiments. The modular platform is designed to run swarm robotics experiments that are compatible with different types of mobile robots ranging from Colias, Kilobot, and E puck. The platform offers programmable control over the experimental setup and its parameters and acts as a tool to collect swarm robot data, including localization, sensory feedback, messaging, and interaction. GenGrid is designed as a modular grid of attachable computing nodes that offers bidirectional commu...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The paper describes the hardware and software architecture design of the GenGrid system</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.20071v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.20071v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Kedia, and M. Rao, &quot;GenGrid: A Generalised Distributed Experimental Environmental Grid for Swarm Robotics,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.20071v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445419504')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="what can you say to a robot? capability communication leads to more natural conversations" data-keywords="robot cs.ro cs.hc" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-skip">‚è≠Ô∏è Skip</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.01448v1" target="_blank">What Can You Say to a Robot? Capability Communication Leads to More Natural Conversations</a>
                            </h3>
                            <p class="card-authors">Merle M. Reimann, Koen V. Hindriks, Florian A. Kunneman et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">CS.RO</span><span class="keyword-tag">CS.HC</span></div>

                            <div class="card-details" id="details-4445418928">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">When encountering a robot in the wild, it is not inherently clear to human users what the robot&#x27;s capabilities are. When encountering misunderstandings or problems in spoken interaction, robots often just apologize and move on, without additional effort to make sure the user understands what happened. We set out to compare the effect of two speech based capability communication strategies (proactive, reactive) to a robot without such a strategy, in regard to the user&#x27;s rating of and their behavior during the interaction. For this, we conducted an in-person user study with 120 participants who ...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">When encountering misunderstandings or problems in spoken interaction, robots often just apologize and move on, without additional effort to make sure the user understands what happened</p></div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.01448v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.01448v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. M. Reimann, K. V. Hindriks, F. A. Kunneman, C. Oertel, G. Skantze, and I. Leite, &quot;What Can You Say to a Robot? Capability Communication Leads to More Natural Conversations,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.01448v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445418928')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="humanoid occupancy: enabling a generalized multimodal occupancy perception system on humanoid robots" data-keywords="robot humanoid navigation perception planning cs.ro cs.ai cs.cv" data-themes="S E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.20217v2" target="_blank">Humanoid Occupancy: Enabling A Generalized Multimodal Occupancy Perception System on Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Wei Cui, Haoyu Wang, Wenkang Qin et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Navigation</span><span class="keyword-tag">Perception</span></div>

                            <div class="card-details" id="details-4445419888">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robot technology is advancing rapidly, with manufacturers introducing diverse heterogeneous visual perception modules tailored to specific scenarios. Among various perception paradigms, occupancy-based representation has become widely recognized as particularly suitable for humanoid robots, as it provides both rich semantic and 3D geometric information essential for comprehensive environmental understanding. In this work, we present Humanoid Occupancy, a generalized multimodal occupancy perception system that integrates hardware and software components, data acquisition devices, and a...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we present Humanoid Occupancy, a generalized multimodal occupancy perception system that integrates hardware and software components, data acquisition devices, and a dedicated annotation pipeline</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To address the unique challenges of humanoid robots, we overcome issues such as kinematic interference and occlusion, and establish an effective sensor layout strategy</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Our framework employs advanced multi-modal fusion techniques to generate grid-based occupancy outputs encoding both occupancy status and semantic labels, thereby enabling holistic environmental understanding for downstream tasks such as task planning and navigation</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.20217v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.20217v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. Cui et al., &quot;Humanoid Occupancy: Enabling A Generalized Multimodal Occupancy Perception System on Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.20217v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445419888')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="llm-based ambiguity detection in natural language instructions for collaborative surgical robots" data-keywords="robot language model cs.ro cs.hc" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.11525v1" target="_blank">LLM-based ambiguity detection in natural language instructions for collaborative surgical robots</a>
                            </h3>
                            <p class="card-authors">Ana Davila, Jacinto Colan, Yasuhisa Hasegawa</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.RO</span><span class="keyword-tag">CS.HC</span></div>

                            <div class="card-details" id="details-4445419120">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Ambiguity in natural language instructions poses significant risks in safety-critical human-robot interaction, particularly in domains such as surgery. To address this, we propose a framework that uses Large Language Models (LLMs) for ambiguity detection specifically designed for collaborative surgical scenarios. Our method employs an ensemble of LLM evaluators, each configured with distinct prompting techniques to identify linguistic, contextual, procedural, and critical ambiguities. A chain-of-thought evaluator is included to systematically analyze instruction structure for potential issues....</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address this, we propose a framework that uses Large Language Models (LLMs) for ambiguity detection specifically designed for collaborative surgical scenarios</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">A chain-of-thought evaluator is included to systematically analyze instruction structure for potential issues</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">To address this, we propose a framework that uses Large Language Models (LLMs) for ambiguity detection specifically designed for collaborative surgical scenarios</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.11525v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.11525v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Davila, J. Colan, and Y. Hasegawa, &quot;LLM-based ambiguity detection in natural language instructions for collaborative surgical robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.11525v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445419120')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="the beatbots: a musician-informed multi-robot percussion quartet" data-keywords="robot multi-robot control ros cs.ro cs.hc" data-themes="R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-skip">‚è≠Ô∏è Skip</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.00966v1" target="_blank">The Beatbots: A Musician-Informed Multi-Robot Percussion Quartet</a>
                            </h3>
                            <p class="card-authors">Isabella Pu, Jeff Snyder, Naomi Ehrich Leonard</p>
                            <div class="theme-tags"><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Control</span><span class="keyword-tag">Ros</span></div>

                            <div class="card-details" id="details-4445420368">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Artistic creation is often seen as a uniquely human endeavor, yet robots bring distinct advantages to music-making, such as precise tempo control, unpredictable rhythmic complexities, and the ability to coordinate intricate human and robot performances. While many robotic music systems aim to mimic human musicianship, our work emphasizes the unique strengths of robots, resulting in a novel multi-robot performance instrument called the Beatbots, capable of producing music that is challenging for humans to replicate using current methods. The Beatbots were designed using an ``informed prototypin...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose design principles to guide the development of future robotic music systems and identify key robotic music affordances that our musician consultants considered particularly important for robotic music performance.</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">While many robotic music systems aim to mimic human musicianship, our work emphasizes the unique strengths of robots, resulting in a novel multi-robot performance instrument called the Beatbots, capable of producing music that is challenging for humans to replicate using current methods</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.00966v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.00966v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'I. Pu, J. Snyder, and N. E. Leonard, &quot;The Beatbots: A Musician-Informed Multi-Robot Percussion Quartet,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.00966v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445420368')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="learning sim-to-real humanoid locomotion in 15 minutes" data-keywords="reinforcement learning robot humanoid locomotion control simulation imu cs.ro cs.ai cs.lg" data-themes="L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2512.01996v1" target="_blank">Learning Sim-to-Real Humanoid Locomotion in 15 Minutes</a>
                            </h3>
                            <p class="card-authors">Younggyo Seo, Carmelo Sferrazza, Juyue Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4445419168">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Massively parallel simulation has reduced reinforcement learning (RL) training time for robots from days to minutes. However, achieving fast and reliable sim-to-real RL for humanoid control remains difficult due to the challenges introduced by factors such as high dimensionality and domain randomization. In this work, we introduce a simple and practical recipe based on off-policy RL algorithms, i.e., FastSAC and FastTD3, that enables rapid training of humanoid locomotion policies in just 15 minutes with a single RTX 4090 GPU. Our simple recipe stabilizes off-policy RL algorithms at massive sca...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we introduce a simple and practical recipe based on off-policy RL algorithms, i.e., FastSAC and FastTD3, that enables rapid training of humanoid locomotion policies in just 15 minutes with a single RTX 4090 GPU</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, achieving fast and reliable sim-to-real RL for humanoid control remains difficult due to the challenges introduced by factors such as high dimensionality and domain randomization</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In this work, we introduce a simple and practical recipe based on off-policy RL algorithms, i.e., FastSAC and FastTD3, that enables rapid training of humanoid locomotion policies in just 15 minutes with a single RTX 4090 GPU</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2512.01996v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2512.01996v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Seo, C. Sferrazza, J. Chen, G. Shi, R. Duan, and P. Abbeel, &quot;Learning Sim-to-Real Humanoid Locomotion in 15 Minutes,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2512.01996v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445419168')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="sadcher: scheduling using attention-based dynamic coalitions of heterogeneous robots in real-time" data-keywords="transformer attention robot multi-robot ros imitation learning cs.ro cs.ma" data-themes="E I R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2510.14851v1" target="_blank">SADCHER: Scheduling using Attention-based Dynamic Coalitions of Heterogeneous Robots in Real-Time</a>
                            </h3>
                            <p class="card-authors">Jakob Bichler, Andreu Matoses Gimenez, Javier Alonso-Mora</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Transformer</span><span class="keyword-tag">Attention</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Robot</span></div>

                            <div class="card-details" id="details-4445407552">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We present Sadcher, a real-time task assignment framework for heterogeneous multi-robot teams that incorporates dynamic coalition formation and task precedence constraints. Sadcher is trained through Imitation Learning and combines graph attention and transformers to predict assignment rewards between robots and tasks. Based on the predicted rewards, a relaxed bipartite matching step generates high-quality schedules with feasibility guarantees. We explicitly model robot and task positions, task durations, and robots&#x27; remaining processing times, enabling advanced temporal and spatial reasoning ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present Sadcher, a real-time task assignment framework for heterogeneous multi-robot teams that incorporates dynamic coalition formation and task precedence constraints</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Sadcher outperforms other learning-based and heuristic baselines on randomized, unseen problems for small and medium-sized teams with computation times suitable for real-time operation</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We present Sadcher, a real-time task assignment framework for heterogeneous multi-robot teams that incorporates dynamic coalition formation and task precedence constraints</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2510.14851v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2510.14851v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Bichler, A. M. Gimenez, and J. Alonso-Mora, &quot;SADCHER: Scheduling using Attention-based Dynamic Coalitions of Heterogeneous Robots in Real-Time,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2510.14851v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445407552')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="polygmap: a perceptive locomotion framework for humanoid robot stair climbing" data-keywords="robot humanoid locomotion perception planning lidar camera imu sensor fusion cs.ro" data-themes="L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2510.12346v1" target="_blank">PolygMap: A Perceptive Locomotion Framework for Humanoid Robot Stair Climbing</a>
                            </h3>
                            <p class="card-authors">Bingquan Li, Ning Wang, Tianwei Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Perception</span></div>

                            <div class="card-details" id="details-4445419456">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recently, biped robot walking technology has been significantly developed, mainly in the context of a bland walking scheme. To emulate human walking, robots need to step on the positions they see in unknown spaces accurately. In this paper, we present PolyMap, a perception-based locomotion planning framework for humanoid robots to climb stairs. Our core idea is to build a real-time polygonal staircase plane semantic map, followed by a footstep planar using these polygonal plane segments. These plane segmentation and visual odometry are done by multi-sensor fusion(LiDAR, RGB-D camera and IMUs)....</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we present PolyMap, a perception-based locomotion planning framework for humanoid robots to climb stairs</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In this paper, we present PolyMap, a perception-based locomotion planning framework for humanoid robots to climb stairs</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2510.12346v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2510.12346v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. Li, N. Wang, T. Zhang, Z. He, and Y. Wu, &quot;PolygMap: A Perceptive Locomotion Framework for Humanoid Robot Stair Climbing,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2510.12346v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445419456')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="let&#x27;s move on: topic change in robot-facilitated group discussions" data-keywords="robot cs.ro cs.hc" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.02123v1" target="_blank">Let&#x27;s move on: Topic Change in Robot-Facilitated Group Discussions</a>
                            </h3>
                            <p class="card-authors">Georgios Hadjiantonis, Sarah Gillet, Marynel V√°zquez et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">CS.RO</span><span class="keyword-tag">CS.HC</span></div>

                            <div class="card-details" id="details-4445406928">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Robot-moderated group discussions have the potential to facilitate engaging and productive interactions among human participants. Previous work on topic management in conversational agents has predominantly focused on human engagement and topic personalization, with the agent having an active role in the discussion. Also, studies have shown the usefulness of including robots in groups, yet further exploration is still needed for robots to learn when to change the topic while facilitating discussions. Accordingly, our work investigates the suitability of machine-learning models and audiovisual ...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Accordingly, our work investigates the suitability of machine-learning models and audiovisual non-verbal features in predicting appropriate topic changes</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.02123v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.02123v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Hadjiantonis, S. Gillet, M. V√°zquez, I. Leite, and F. I. Dogan, &quot;Let&#x27;s move on: Topic Change in Robot-Facilitated Group Discussions,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.02123v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445406928')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="learning humanoid standing-up control across diverse postures" data-keywords="reinforcement learning robot humanoid locomotion manipulation control simulation ros imu cs.ro" data-themes="S L I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.08378v2" target="_blank">Learning Humanoid Standing-up Control across Diverse Postures</a>
                            </h3>
                            <p class="card-authors">Tao Huang, Junli Ren, Huayi Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4445420224">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Standing-up control is crucial for humanoid robots, with the potential for integration into current locomotion and loco-manipulation systems, such as fall recovery. Existing approaches are either limited to simulations that overlook hardware constraints or rely on predefined ground-specific motion trajectories, failing to enable standing up across postures in real-world scenes. To bridge this gap, we present HoST (Humanoid Standing-up Control), a reinforcement learning framework that learns standing-up control from scratch, enabling robust sim-to-real transfer across diverse postures. HoST eff...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To bridge this gap, we present HoST (Humanoid Standing-up Control), a reinforcement learning framework that learns standing-up control from scratch, enabling robust sim-to-real transfer across diverse postures</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To bridge this gap, we present HoST (Humanoid Standing-up Control), a reinforcement learning framework that learns standing-up control from scratch, enabling robust sim-to-real transfer across diverse postures</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Existing approaches are either limited to simulations that overlook hardware constraints or rely on predefined ground-specific motion trajectories, failing to enable standing up across postures in real-world scenes</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.08378v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.08378v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. Huang et al., &quot;Learning Humanoid Standing-up Control across Diverse Postures,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.08378v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445420224')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="robot drummer: learning rhythmic skills for humanoid drumming" data-keywords="reinforcement learning robot humanoid locomotion coordination ros cs.ro" data-themes="E L I R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-skip">‚è≠Ô∏è Skip</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.11498v2" target="_blank">Robot Drummer: Learning Rhythmic Skills for Humanoid Drumming</a>
                            </h3>
                            <p class="card-authors">Asad Ali Shahid, Francesco Braghin, Loris Roveda</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4445408032">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots have seen remarkable advances in dexterity, balance, and locomotion, yet their role in expressive domains such as music performance remains largely unexplored. Musical tasks, like drumming, present unique challenges, including split-second timing, rapid contacts, and multi-limb coordination over performances lasting minutes. In this paper, we introduce Robot Drummer, a humanoid capable of expressive, high-precision drumming across a diverse repertoire of songs. We formulate humanoid drumming as sequential fulfillment of timed contacts and transform drum scores into a Rhythmic C...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we introduce Robot Drummer, a humanoid capable of expressive, high-precision drumming across a diverse repertoire of songs</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Musical tasks, like drumming, present unique challenges, including split-second timing, rapid contacts, and multi-limb coordination over performances lasting minutes</p></div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.11498v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.11498v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. A. Shahid, F. Braghin, and L. Roveda, &quot;Robot Drummer: Learning Rhythmic Skills for Humanoid Drumming,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.11498v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445408032')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="human-robot collaboration in surgery: advances and challenges towards autonomous surgical assistants" data-keywords="robot manipulation ros cs.ro cs.hc" data-themes="S I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.11460v1" target="_blank">Human-Robot collaboration in surgery: Advances and challenges towards autonomous surgical assistants</a>
                            </h3>
                            <p class="card-authors">Jacinto Colan, Ana Davila, Yutaro Yamada et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Manipulation</span><span class="keyword-tag">Ros</span><span class="keyword-tag">CS.RO</span></div>

                            <div class="card-details" id="details-4445420704">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Human-robot collaboration in surgery represents a significant area of research, driven by the increasing capability of autonomous robotic systems to assist surgeons in complex procedures. This systematic review examines the advancements and persistent challenges in the development of autonomous surgical robotic assistants (ASARs), focusing specifically on scenarios where robots provide meaningful and active support to human surgeons. Adhering to the PRISMA guidelines, a comprehensive literature search was conducted across the IEEE Xplore, Scopus, and Web of Science databases, resulting in the ...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">This systematic review examines the advancements and persistent challenges in the development of autonomous surgical robotic assistants (ASARs), focusing specifically on scenarios where robots provide meaningful and active support to human surgeons. Several key challenges hinder wider adoption, including the alignment of robotic actions with human surgeon preferences, the necessity for procedural awareness within autonomous systems, the establishment of seamless human-robot information exchange, and the complexities of skill acquisition in shared workspaces</p></div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.11460v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.11460v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Colan, A. Davila, Y. Yamada, and Y. Hasegawa, &quot;Human-Robot collaboration in surgery: Advances and challenges towards autonomous surgical assistants,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.11460v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445420704')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="humanoid agent via embodied chain-of-action reasoning with multimodal foundation models for zero-shot loco-manipulation" data-keywords="robot humanoid locomotion manipulation coordination perception ros cs.ro cs.ai" data-themes="L M E I R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.09532v3" target="_blank">Humanoid Agent via Embodied Chain-of-Action Reasoning with Multimodal Foundation Models for Zero-Shot Loco-Manipulation</a>
                            </h3>
                            <p class="card-authors">Congcong Wen, Geeta Chandra Raju Bethala, Yu Hao et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Manipulation</span></div>

                            <div class="card-details" id="details-4445419696">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid loco-manipulation, which integrates whole-body locomotion with dexterous manipulation, remains a fundamental challenge in robotics. Beyond whole-body coordination and balance, a central difficulty lies in understanding human instructions and translating them into coherent sequences of embodied actions. Recent advances in foundation models provide transferable multimodal representations and reasoning capabilities, yet existing efforts remain largely restricted to either locomotion or manipulation in isolation, with limited applicability to humanoid settings. In this paper, we propose H...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose Humanoid-COA, the first humanoid agent framework that integrates foundation model reasoning with an Embodied Chain-of-Action (CoA) mechanism for zero-shot loco-manipulation</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Humanoid loco-manipulation, which integrates whole-body locomotion with dexterous manipulation, remains a fundamental challenge in robotics. Beyond whole-body coordination and balance, a central difficulty lies in understanding human instructions and translating them into coherent sequences of embodied actions</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Recent advances in foundation models provide transferable multimodal representations and reasoning capabilities, yet existing efforts remain largely restricted to either locomotion or manipulation in isolation, with limited applicability to humanoid settings</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.09532v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.09532v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Wen et al., &quot;Humanoid Agent via Embodied Chain-of-Action Reasoning with Multimodal Foundation Models for Zero-Shot Loco-Manipulation,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.09532v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445419696')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="why robots are bad at detecting their mistakes: limitations of miscommunication detection in human-robot dialogue" data-keywords="robot computer vision cs.ro cs.cl cs.hc" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.20268v1" target="_blank">Why Robots Are Bad at Detecting Their Mistakes: Limitations of Miscommunication Detection in Human-Robot Dialogue</a>
                            </h3>
                            <p class="card-authors">Ruben Janssens, Jens De Bock, Sofie Labat et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Computer Vision</span><span class="keyword-tag">CS.RO</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4445421088">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Detecting miscommunication in human-robot interaction is a critical function for maintaining user engagement and trust. While humans effortlessly detect communication errors in conversations through both verbal and non-verbal cues, robots face significant challenges in interpreting non-verbal feedback, despite advances in computer vision for recognizing affective expressions. This research evaluates the effectiveness of machine learning models in detecting miscommunications in robot dialogue. Using a multi-modal dataset of 240 human-robot conversations, where four distinct types of conversatio...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">While humans effortlessly detect communication errors in conversations through both verbal and non-verbal cues, robots face significant challenges in interpreting non-verbal feedback, despite advances in computer vision for recognizing affective expressions. These results uncover a fundamental limitation in identifying robot miscommunications in dialogue: even when users perceive the induced miscommunication as such, they often do not communicate this to their robotic conversation partner</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This research evaluates the effectiveness of machine learning models in detecting miscommunications in robot dialogue</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.20268v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.20268v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Janssens, J. D. Bock, S. Labat, E. Verhelst, V. Hoste, and T. Belpaeme, &quot;Why Robots Are Bad at Detecting Their Mistakes: Limitations of Miscommunication Detection in Human-Robot Dialogue,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.20268v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445421088')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="romoco: robotic motion control toolbox for reduced-order model-based locomotion on bipedal and humanoid robots" data-keywords="robot humanoid bipedal locomotion control simulation ros imu cs.ro" data-themes="S L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.19545v1" target="_blank">RoMoCo: Robotic Motion Control Toolbox for Reduced-Order Model-Based Locomotion on Bipedal and Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Min Dai, Aaron D. Ames</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4445420896">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We present RoMoCo, an open-source C++ toolbox for the synthesis and evaluation of reduced-order model-based planners and whole-body controllers for bipedal and humanoid robots. RoMoCo&#x27;s modular architecture unifies state-of-the-art planners and whole-body locomotion controllers under a consistent API, enabling rapid prototyping and reproducible benchmarking. By leveraging reduced-order models for platform-agnostic gait generation, RoMoCo enables flexible controller design across diverse robots. We demonstrate its versatility and performance through extensive simulations on the Cassie, Unitree ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present RoMoCo, an open-source C++ toolbox for the synthesis and evaluation of reduced-order model-based planners and whole-body controllers for bipedal and humanoid robots</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We present RoMoCo, an open-source C++ toolbox for the synthesis and evaluation of reduced-order model-based planners and whole-body controllers for bipedal and humanoid robots</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.19545v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.19545v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Dai, and A. D. Ames, &quot;RoMoCo: Robotic Motion Control Toolbox for Reduced-Order Model-Based Locomotion on Bipedal and Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.19545v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445420896')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="mechanical intelligence-aware curriculum reinforcement learning for humanoids with parallel actuation" data-keywords="reinforcement learning robot humanoid locomotion control simulation mujoco imu cs.ro" data-themes="S L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.00273v3" target="_blank">Mechanical Intelligence-Aware Curriculum Reinforcement Learning for Humanoids with Parallel Actuation</a>
                            </h3>
                            <p class="card-authors">Yusuke Tanaka, Alvin Zhu, Quanyou Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4445420848">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Reinforcement learning (RL) has enabled advances in humanoid robot locomotion, yet most learning frameworks do not account for mechanical intelligence embedded in parallel actuation mechanisms due to limitations in simulator support for closed kinematic chains. This omission can lead to inaccurate motion modeling and suboptimal policies, particularly for robots with high actuation complexity. This paper presents general formulations and simulation methods for three types of parallel mechanisms: a differential pulley, a five-bar linkage, and a four-bar linkage, and trains a parallel-mechanism a...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents general formulations and simulation methods for three types of parallel mechanisms: a differential pulley, a five-bar linkage, and a four-bar linkage, and trains a parallel-mechanism aware policy through an end-to-end curriculum RL framework for BRUCE, a kid-sized humanoid robot</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Reinforcement learning (RL) has enabled advances in humanoid robot locomotion, yet most learning frameworks do not account for mechanical intelligence embedded in parallel actuation mechanisms due to limitations in simulator support for closed kinematic chains</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Reinforcement learning (RL) has enabled advances in humanoid robot locomotion, yet most learning frameworks do not account for mechanical intelligence embedded in parallel actuation mechanisms due to limitations in simulator support for closed kinematic chains</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.00273v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.00273v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Tanaka, A. Zhu, Q. Wang, Y. Liu, and D. Hong, &quot;Mechanical Intelligence-Aware Curriculum Reinforcement Learning for Humanoids with Parallel Actuation,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.00273v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445420848')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="ironcub 3: the jet-powered flying humanoid robot" data-keywords="robot humanoid control simulation imu cs.ro" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.01125v1" target="_blank">iRonCub 3: The Jet-Powered Flying Humanoid Robot</a>
                            </h3>
                            <p class="card-authors">Davide Gorbani, Hosameldin Awadalla Omer Mohamed, Giuseppe L&#x27;Erario et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Control</span><span class="keyword-tag">Simulation</span></div>

                            <div class="card-details" id="details-4445420416">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This article presents iRonCub 3, a jet-powered humanoid robot, and its first flight experiments. Unlike traditional aerial vehicles, iRonCub 3 aims to achieve flight using a full-body humanoid form, which poses unique challenges in control, estimation, and system integration. We highlight the robot&#x27;s current mechanical and software architecture, including its propulsion system, control framework, and experimental infrastructure. The control and estimation framework is first validated in simulation by performing a takeoff and tracking a reference trajectory. Then, we demonstrate, for the first ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Then, we demonstrate, for the first time, a liftoff of a jet-powered humanoid robot - an initial but significant step toward aerial humanoid mobility</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Unlike traditional aerial vehicles, iRonCub 3 aims to achieve flight using a full-body humanoid form, which poses unique challenges in control, estimation, and system integration</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We highlight the robot&#x27;s current mechanical and software architecture, including its propulsion system, control framework, and experimental infrastructure</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.01125v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.01125v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Gorbani et al., &quot;iRonCub 3: The Jet-Powered Flying Humanoid Robot,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.01125v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445420416')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="trinity: a modular humanoid robot ai system" data-keywords="reinforcement learning attention robot humanoid planning control imu language model cs.ro" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.08338v1" target="_blank">Trinity: A Modular Humanoid Robot AI System</a>
                            </h3>
                            <p class="card-authors">Jingkai Sun, Qiang Zhang, Gang Han et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Attention</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span></div>

                            <div class="card-details" id="details-4445421136">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In recent years, research on humanoid robots has garnered increasing attention. With breakthroughs in various types of artificial intelligence algorithms, embodied intelligence, exemplified by humanoid robots, has been highly anticipated. The advancements in reinforcement learning (RL) algorithms have significantly improved the motion control and generalization capabilities of humanoid robots. Simultaneously, the groundbreaking progress in large language models (LLM) and visual language models (VLM) has brought more possibilities and imagination to humanoid robots. LLM enables humanoid robots ...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">With breakthroughs in various types of artificial intelligence algorithms, embodied intelligence, exemplified by humanoid robots, has been highly anticipated</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.08338v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.08338v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Sun et al., &quot;Trinity: A Modular Humanoid Robot AI System,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.08338v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445421136')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="periodic bipedal gait learning using reward composition based on a novel gait planner for humanoid robots" data-keywords="reinforcement learning robot humanoid bipedal locomotion planning cs.ro" data-themes="L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.08416v1" target="_blank">Periodic Bipedal Gait Learning Using Reward Composition Based on a Novel Gait Planner for Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Bolin Li, Linwei Sun, Xuecong Huang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span></div>

                            <div class="card-details" id="details-4445408512">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents a periodic bipedal gait learning method using reward composition, integrated with a real-time gait planner for humanoid robots. First, we introduce a novel gait planner that incorporates dynamics to design the desired joint trajectory. In the gait design process, the 3D robot model is decoupled into two 2D models, which are then approximated as hybrid inverted pendulums (H-LIP) for trajectory planning. The gait planner operates in parallel in real time within the robot&#x27;s learning environment. Second, based on this gait planner, we design three effective reward functions wit...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents a periodic bipedal gait learning method using reward composition, integrated with a real-time gait planner for humanoid robots</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper presents a periodic bipedal gait learning method using reward composition, integrated with a real-time gait planner for humanoid robots</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.08416v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.08416v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. Li, L. Sun, X. Huang, Y. Jiang, and L. Zhu, &quot;Periodic Bipedal Gait Learning Using Reward Composition Based on a Novel Gait Planner for Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.08416v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445408512')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="online automatic code generation for robot swarms: llms and self-organizing hierarchy" data-keywords="robot swarm simulation imu cs.ro cs.ai cs.ma" data-themes="E I R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Multi-Agent Systems</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2510.04774v2" target="_blank">Online automatic code generation for robot swarms: LLMs and self-organizing hierarchy</a>
                            </h3>
                            <p class="card-authors">Weixu Zhu, Marco Dorigo, Mary Katherine Heinrich</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Swarm</span><span class="keyword-tag">Simulation</span><span class="keyword-tag">Imu</span></div>

                            <div class="card-details" id="details-4445420128">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Our recently introduced self-organizing nervous system (SoNS) provides robot swarms with 1) ease of behavior design and 2) global estimation of the swarm configuration and its collective environment, facilitating the implementation of online automatic code generation for robot swarms. In a demonstration with 6 real robots and simulation trials with &gt;30 robots, we show that when a SoNS-enhanced robot swarm gets stuck, it can automatically solicit and run code generated by an external LLM on the fly, completing its mission with an 85% success rate.</p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2510.04774v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2510.04774v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. Zhu, M. Dorigo, and M. K. Heinrich, &quot;Online automatic code generation for robot swarms: LLMs and self-organizing hierarchy,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2510.04774v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445420128')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.hc" data-title="help or hindrance: understanding the impact of robot communication in action teams" data-keywords="robot coordination perception cs.hc cs.ro" data-themes="R M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Human-Computer Interaction</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.08892v3" target="_blank">Help or Hindrance: Understanding the Impact of Robot Communication in Action Teams</a>
                            </h3>
                            <p class="card-authors">Tauhid Tanjim, Jonathan St. George, Kevin Ching et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-R">Multi-Robot</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Coordination</span><span class="keyword-tag">Perception</span><span class="keyword-tag">CS.HC</span></div>

                            <div class="card-details" id="details-4445418976">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The human-robot interaction (HRI) field has recognized the importance of enabling robots to interact with teams. Human teams rely on effective communication for successful collaboration in time-sensitive environments. Robots can play a role in enhancing team coordination through real-time assistance. Despite significant progress in human-robot teaming research, there remains an essential gap in how robots can effectively communicate with action teams using multimodal interaction cues in time-sensitive environments. This study addresses this knowledge gap in an experimental in-lab study to inve...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Despite significant progress in human-robot teaming research, there remains an essential gap in how robots can effectively communicate with action teams using multimodal interaction cues in time-sensitive environments. This study addresses this knowledge gap in an experimental in-lab study to investigate how multimodal robot communication in action teams affects workload and human perception of robots</p></div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.08892v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.08892v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. Tanjim, J. S. George, K. Ching, and A. Taylor, &quot;Help or Hindrance: Understanding the Impact of Robot Communication in Action Teams,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.08892v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445418976')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="learning differentiable reachability maps for optimization-based humanoid motion generation" data-keywords="neural network robot humanoid manipulation planning optimization cs.ro" data-themes="I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.11275v1" target="_blank">Learning Differentiable Reachability Maps for Optimization-based Humanoid Motion Generation</a>
                            </h3>
                            <p class="card-authors">Masaki Murooka, Iori Kumagai, Mitsuharu Morisawa et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Manipulation</span></div>

                            <div class="card-details" id="details-4445420080">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">To reduce the computational cost of humanoid motion generation, we introduce a new approach to representing robot kinematic reachability: the differentiable reachability map. This map is a scalar-valued function defined in the task space that takes positive values only in regions reachable by the robot&#x27;s end-effector. A key feature of this representation is that it is continuous and differentiable with respect to task-space coordinates, enabling its direct use as constraints in continuous optimization for humanoid motion planning. We describe a method to learn such differentiable reachability ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To reduce the computational cost of humanoid motion generation, we introduce a new approach to representing robot kinematic reachability: the differentiable reachability map</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">By incorporating the learned reachability map as a constraint, we formulate humanoid motion generation as a continuous optimization problem. We demonstrate that the proposed approach efficiently solves various motion planning problems, including footstep planning, multi-contact motion planning, and loco-manipulation planning for humanoid robots.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">To reduce the computational cost of humanoid motion generation, we introduce a new approach to representing robot kinematic reachability: the differentiable reachability map</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.11275v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.11275v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Murooka, I. Kumagai, M. Morisawa, and F. Kanehiro, &quot;Learning Differentiable Reachability Maps for Optimization-based Humanoid Motion Generation,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.11275v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445420080')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section year-section" data-year="2024">
                <h2 class="section-header">üìÖ 2024 <span class="section-count">(102 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="a mini-review on mobile manipulators with variable autonomy" data-keywords="robot language model cs.ro cs.hc" data-themes="E L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.10887v1" target="_blank">A Mini-Review on Mobile Manipulators with Variable Autonomy</a>
                            </h3>
                            <p class="card-authors">Cesar Alan Contreras, Alireza Rastegarpanah, Rustam Stolkin et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.RO</span><span class="keyword-tag">CS.HC</span></div>

                            <div class="card-details" id="details-4441928320">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents a mini-review of the current state of research in mobile manipulators with variable levels of autonomy, emphasizing their associated challenges and application environments. The need for mobile manipulators in different environments is evident due to the unique challenges and risks each presents. Many systems deployed in these environments are not fully autonomous, requiring human-robot teaming to ensure safe and reliable operations under uncertainties. Through this analysis, we identify gaps and challenges in the literature on Variable Autonomy, including cognitive workloa...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents a mini-review of the current state of research in mobile manipulators with variable levels of autonomy, emphasizing their associated challenges and application environments</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">This paper presents a mini-review of the current state of research in mobile manipulators with variable levels of autonomy, emphasizing their associated challenges and application environments. The need for mobile manipulators in different environments is evident due to the unique challenges and risks each presents</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Through this analysis, we identify gaps and challenges in the literature on Variable Autonomy, including cognitive workload and communication delays, and propose future directions, including whole-body Variable Autonomy for mobile manipulators, virtual reality frameworks, and large language models to reduce operators&#x27; complexity and cognitive load in some challenging and uncertain scenarios.</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.10887v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.10887v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. A. Contreras, A. Rastegarpanah, R. Stolkin, and M. Chiou, &quot;A Mini-Review on Mobile Manipulators with Variable Autonomy,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.10887v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441928320')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="is self-knowledge and action consistent or not: investigating large language model&#x27;s personality" data-keywords="language model cs.cl cs.cy" data-themes="S E M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.14679v2" target="_blank">Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model&#x27;s Personality</a>
                            </h3>
                            <p class="card-authors">Yiming Ai, Zhiwei He, Ziyin Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.CY</span></div>

                            <div class="card-details" id="details-4441927216">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In this study, we delve into the validity of conventional personality questionnaires in capturing the human-like personality traits of Large Language Models (LLMs). Our objective is to assess the congruence between the personality traits LLMs claim to possess and their demonstrated tendencies in real-world scenarios. By conducting an extensive examination of LLM outputs against observed human response patterns, we aim to understand the disjunction between self-knowledge and action in LLMs.</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In this study, we delve into the validity of conventional personality questionnaires in capturing the human-like personality traits of Large Language Models (LLMs)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.14679v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.14679v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Ai et al., &quot;Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model&#x27;s Personality,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.14679v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441927216')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="large language models lack understanding of character composition of words" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.11357v3" target="_blank">Large Language Models Lack Understanding of Character Composition of Words</a>
                            </h3>
                            <p class="card-authors">Andrew Shin, Kunitake Kaneko</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4441922464">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) have demonstrated remarkable performances on a wide range of natural language tasks. Yet, LLMs&#x27; successes have been largely restricted to tasks concerning words, sentences, or documents, and it remains questionable how much they understand the minimal units of text, namely characters. In this paper, we examine contemporary LLMs regarding their ability to understand character composition of words, and show that most of them fail to reliably carry out even the simple tasks that can be handled by humans with perfection. We analyze their behaviors with comparison to to...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large language models (LLMs) have demonstrated remarkable performances on a wide range of natural language tasks</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.11357v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.11357v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Shin, and K. Kaneko, &quot;Large Language Models Lack Understanding of Character Composition of Words,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.11357v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441922464')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="unmasking the shadows of ai: investigating deceptive capabilities in large language models" data-keywords="language model cs.cl cs.ai" data-themes="S E I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.09676v1" target="_blank">Unmasking the Shadows of AI: Investigating Deceptive Capabilities in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Linge Guo</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4441918816">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This research critically navigates the intricate landscape of AI deception, concentrating on deceptive behaviours of Large Language Models (LLMs). My objective is to elucidate this issue, examine the discourse surrounding it, and subsequently delve into its categorization and ramifications. The essay initiates with an evaluation of the AI Safety Summit 2023 (ASS) and introduction of LLMs, emphasising multidimensional biases that underlie their deceptive behaviours.The literature review covers four types of deception categorised: Strategic deception, Imitation, Sycophancy, and Unfaithful Reason...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">My objective is to elucidate this issue, examine the discourse surrounding it, and subsequently delve into its categorization and ramifications. Lastly, I take an evaluative stance on various aspects related to navigating the persistent challenges of the deceptive AI</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This research critically navigates the intricate landscape of AI deception, concentrating on deceptive behaviours of Large Language Models (LLMs)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.09676v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.09676v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Guo, &quot;Unmasking the Shadows of AI: Investigating Deceptive Capabilities in Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.09676v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441918816')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="self-cognition in large language models: an exploratory study" data-keywords="ros language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.01505v1" target="_blank">Self-Cognition in Large Language Models: An Exploratory Study</a>
                            </h3>
                            <p class="card-authors">Dongping Chen, Jiawen Shi, Yao Wan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4441918432">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">While Large Language Models (LLMs) have achieved remarkable success across various applications, they also raise concerns regarding self-cognition. In this paper, we perform a pioneering study to explore self-cognition in LLMs. Specifically, we first construct a pool of self-cognition instruction prompts to evaluate where an LLM exhibits self-cognition and four well-designed principles to quantify LLMs&#x27; self-cognition. Our study reveals that 4 of the 48 models on Chatbot Arena--specifically Command R, Claude3-Opus, Llama-3-70b-Instruct, and Reka-core--demonstrate some level of detectable self-...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">While Large Language Models (LLMs) have achieved remarkable success across various applications, they also raise concerns regarding self-cognition</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.01505v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.01505v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Chen, J. Shi, Y. Wan, P. Zhou, N. Z. Gong, and L. Sun, &quot;Self-Cognition in Large Language Models: An Exploratory Study,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.01505v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441918432')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="a critical review of causal reasoning benchmarks for large language models" data-keywords="language model cs.lg cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Machine Learning</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.08029v1" target="_blank">A Critical Review of Causal Reasoning Benchmarks for Large Language Models</a>
                            </h3>
                            <p class="card-authors">Linying Yang, Vik Shirvaikar, Oscar Clivio et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4441928272">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Numerous benchmarks aim to evaluate the capabilities of Large Language Models (LLMs) for causal inference and reasoning. However, many of them can likely be solved through the retrieval of domain knowledge, questioning whether they achieve their purpose. In this review, we present a comprehensive overview of LLM benchmarks for causality. We highlight how recent benchmarks move towards a more thorough definition of causal reasoning by incorporating interventional or counterfactual reasoning. We derive a set of criteria that a useful benchmark or set of benchmarks should aim to satisfy. We hope ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this review, we present a comprehensive overview of LLM benchmarks for causality</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Numerous benchmarks aim to evaluate the capabilities of Large Language Models (LLMs) for causal inference and reasoning</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.08029v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.08029v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Yang, V. Shirvaikar, O. Clivio, and F. Falck, &quot;A Critical Review of Causal Reasoning Benchmarks for Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.08029v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441928272')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="not all experts are equal: efficient expert pruning and skipping for mixture-of-experts large language models" data-keywords="ros imu language model cs.cl cs.ai cs.lg" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.14800v2" target="_blank">Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models</a>
                            </h3>
                            <p class="card-authors">Xudong Lu, Qi Liu, Yuhui Xu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Imu</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4441921792">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">A pivotal advancement in the progress of large language models (LLMs) is the emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer parameters, but it is still hard to deploy them due to their immense parameter sizes. Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques. Specifically, we propose, for the first time to our best knowledge, post-tr...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">A pivotal advancement in the progress of large language models (LLMs) is the emergence of the Mixture-of-Experts (MoE) LLMs</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.14800v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.14800v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. Lu et al., &quot;Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.14800v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441921792')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cr" data-title="attacks on third-party apis of large language models" data-keywords="ros language model cs.cr cs.ai cs.cl" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ CS.CR</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2404.16891v1" target="_blank">Attacks on Third-Party APIs of Large Language Models</a>
                            </h3>
                            <p class="card-authors">Wanru Zhao, Vidit Khazanchi, Haodi Xing et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CR</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4441918480">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language model (LLM) services have recently begun offering a plugin ecosystem to interact with third-party API services. This innovation enhances the capabilities of LLMs, but it also introduces risks, as these plugins developed by various third parties cannot be easily trusted. This paper proposes a new attacking framework to examine security and safety vulnerabilities within LLM platforms that incorporate third-party services. Applying our framework specifically to widely used LLMs, we identify real-world malicious attacks across various domains on third-party APIs that can imperceptib...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">The paper discusses the unique challenges posed by third-party API integration and offers strategic possibilities to improve the security and safety of LLM ecosystems moving forward</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large language model (LLM) services have recently begun offering a plugin ecosystem to interact with third-party API services</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2404.16891v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2404.16891v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. Zhao, V. Khazanchi, H. Xing, X. He, Q. Xu, and N. D. Lane, &quot;Attacks on Third-Party APIs of Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2404.16891v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441918480')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="exploring large language models to generate easy to read content" data-keywords="ros nlp language model cs.cl cs.hc" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.20046v1" target="_blank">Exploring Large Language Models to generate Easy to Read content</a>
                            </h3>
                            <p class="card-authors">Paloma Mart√≠nez, Lourdes Moreno, Alberto Ramos</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4441918384">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Ensuring text accessibility and understandability are essential goals, particularly for individuals with cognitive impairments and intellectual disabilities, who encounter challenges in accessing information across various mediums such as web pages, newspapers, administrative tasks, or health documents. Initiatives like Easy to Read and Plain Language guidelines aim to simplify complex texts; however, standardizing these guidelines remains challenging and often involves manual processes. This work presents an exploratory investigation into leveraging Artificial Intelligence (AI) and Natural La...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Ensuring text accessibility and understandability are essential goals, particularly for individuals with cognitive impairments and intellectual disabilities, who encounter challenges in accessing information across various mediums such as web pages, newspapers, administrative tasks, or health documents</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This work presents an exploratory investigation into leveraging Artificial Intelligence (AI) and Natural Language Processing (NLP) approaches to systematically simplify Spanish texts into Easy to Read formats, with a focus on utilizing Large Language Models (LLMs) for simplifying texts, especially in generating Easy to Read content</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.20046v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.20046v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Mart√≠nez, L. Moreno, and A. Ramos, &quot;Exploring Large Language Models to generate Easy to Read content,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.20046v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441918384')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cv" data-title="prunevid: visual token pruning for efficient video large language models" data-keywords="ros language model cs.cv" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computer Vision</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.16117v1" target="_blank">PruneVid: Visual Token Pruning for Efficient Video Large Language Models</a>
                            </h3>
                            <p class="card-authors">Xiaohu Huang, Hao Zhou, Kai Han</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span></div>

                            <div class="card-details" id="details-4441920256">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In this paper, we introduce PruneVid, a visual token pruning method designed to enhance the efficiency of multi-modal video understanding. Large Language Models (LLMs) have shown promising performance in video tasks due to their extended capabilities in comprehending visual modalities. However, the substantial redundancy in video data presents significant computational challenges for LLMs. To address this issue, we introduce a training-free method that 1) minimizes video redundancy by merging spatial-temporal tokens, and 2) leverages LLMs&#x27; reasoning capabilities to selectively prune visual fea...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we introduce PruneVid, a visual token pruning method designed to enhance the efficiency of multi-modal video understanding</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, the substantial redundancy in video data presents significant computational challenges for LLMs. To address this issue, we introduce a training-free method that 1) minimizes video redundancy by merging spatial-temporal tokens, and 2) leverages LLMs&#x27; reasoning capabilities to selectively prune visual features relevant to question tokens, enhancing model efficiency</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In this paper, we introduce PruneVid, a visual token pruning method designed to enhance the efficiency of multi-modal video understanding</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.16117v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.16117v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. Huang, H. Zhou, and K. Han, &quot;PruneVid: Visual Token Pruning for Efficient Video Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.16117v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441920256')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="scaling behavior of machine translation with large language models under prompt injection attacks" data-keywords="language model cs.cl" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.09832v1" target="_blank">Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks</a>
                            </h3>
                            <p class="card-authors">Zhifan Sun, Antonio Valerio Miceli-Barone</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4441918720">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Language Models (LLMs) are increasingly becoming the preferred foundation platforms for many Natural Language Processing tasks such as Machine Translation, owing to their quality often comparable to or better than task-specific models, and the simplicity of specifying the task through natural language instructions or in-context examples. Their generality, however, opens them up to subversion by end users who may embed into their requests instructions that cause the model to behave in unauthorized and possibly unsafe ways. In this work we study these Prompt Injection Attacks (PIAs) on mul...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce a new benchmark data set and we discover that on multiple language pairs and injected prompts written in English, larger models under certain conditions may become more susceptible to successful attacks, an instance of the Inverse Scaling phenomenon (McKenzie et al., 2023)</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large Language Models (LLMs) are increasingly becoming the preferred foundation platforms for many Natural Language Processing tasks such as Machine Translation, owing to their quality often comparable to or better than task-specific models, and the simplicity of specifying the task through natural language instructions or in-context examples</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.09832v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.09832v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Sun, and A. V. Miceli-Barone, &quot;Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.09832v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441918720')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="babysit a language model from scratch: interactive language learning by trials and demonstrations" data-keywords="control language model cs.cl cs.ai" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.13828v2" target="_blank">Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations</a>
                            </h3>
                            <p class="card-authors">Ziqiao Ma, Zekun Wang, Joyce Chai</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Control</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4441928176">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humans are efficient language learners and inherently social creatures. Our language development is largely shaped by our social interactions, for example, the demonstration and feedback from caregivers. Contrary to human language learning, recent advancements in large language models have primarily adopted a non-interactive training paradigm, and refined pre-trained models through feedback afterward. In this work, we explore how corrective feedback from interactions influences neural language acquisition from scratch through systematically controlled experiments, assessing whether it contribu...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce a trial-and-demonstration (TnD) learning framework that incorporates three distinct components: student trials, teacher demonstrations, and a reward conditioned on language competence at various developmental stages</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Contrary to human language learning, recent advancements in large language models have primarily adopted a non-interactive training paradigm, and refined pre-trained models through feedback afterward</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.13828v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.13828v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Ma, Z. Wang, and J. Chai, &quot;Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.13828v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441928176')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="behavioral bias of vision-language models: a behavioral finance view" data-keywords="gpt language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.15256v1" target="_blank">Behavioral Bias of Vision-Language Models: A Behavioral Finance View</a>
                            </h3>
                            <p class="card-authors">Yuhang Xiao, Yudi Lin, Ming-Chang Chiu</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4441918336">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Vision-Language Models (LVLMs) evolve rapidly as Large Language Models (LLMs) was equipped with vision modules to create more human-like models. However, we should carefully evaluate their applications in different domains, as they may possess undesired biases. Our work studies the potential behavioral biases of LVLMs from a behavioral finance perspective, an interdisciplinary subject that jointly considers finance and psychology. We propose an end-to-end framework, from data collection to new evaluation metrics, to assess LVLMs&#x27; reasoning capabilities and the dynamic behaviors manifeste...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose an end-to-end framework, from data collection to new evaluation metrics, to assess LVLMs&#x27; reasoning capabilities and the dynamic behaviors manifested in two established human financial behavioral biases: recency bias and authority bias</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large Vision-Language Models (LVLMs) evolve rapidly as Large Language Models (LLMs) was equipped with vision modules to create more human-like models</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.15256v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.15256v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Xiao, Y. Lin, and M. Chiu, &quot;Behavioral Bias of Vision-Language Models: A Behavioral Finance View,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.15256v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441918336')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.hc" data-title="exploring bengali religious dialect biases in large language models with evaluation perspectives" data-keywords="gpt ros language model cs.hc cs.cl cs.cy" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Human-Computer Interaction</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.18376v1" target="_blank">Exploring Bengali Religious Dialect Biases in Large Language Models with Evaluation Perspectives</a>
                            </h3>
                            <p class="card-authors">Azmine Toushik Wasi, Raima Islam, Mst Rafia Islam et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.HC</span></div>

                            <div class="card-details" id="details-4441921696">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">While Large Language Models (LLM) have created a massive technological impact in the past decade, allowing for human-enabled applications, they can produce output that contains stereotypes and biases, especially when using low-resource languages. This can be of great ethical concern when dealing with sensitive topics such as religion. As a means toward making LLMS more fair, we explore bias from a religious perspective in Bengali, focusing specifically on two main religious dialects: Hindu and Muslim-majority dialects. Here, we perform different experiments and audit showing the comparative an...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">While Large Language Models (LLM) have created a massive technological impact in the past decade, allowing for human-enabled applications, they can produce output that contains stereotypes and biases, especially when using low-resource languages</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.18376v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.18376v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. T. Wasi, R. Islam, M. R. Islam, T. H. Rafi, and D. Chae, &quot;Exploring Bengali Religious Dialect Biases in Large Language Models with Evaluation Perspectives,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.18376v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441921696')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="exploring advanced large language models with llmsuite" data-keywords="reinforcement learning transformer gpt language model cs.cl cs.cv" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.12036v2" target="_blank">Exploring Advanced Large Language Models with LLMsuite</a>
                            </h3>
                            <p class="card-authors">Giorgio Roffo</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Transformer</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4441916992">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This tutorial explores the advancements and challenges in the development of Large Language Models (LLMs) such as ChatGPT and Gemini. It addresses inherent limitations like temporal knowledge cutoffs, mathematical inaccuracies, and the generation of incorrect information, proposing solutions like Retrieval Augmented Generation (RAG), Program-Aided Language Models (PAL), and frameworks such as ReAct and LangChain. The integration of these techniques enhances LLM performance and reliability, especially in multi-step reasoning and complex task execution. The paper also covers fine-tuning strategi...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">This tutorial explores the advancements and challenges in the development of Large Language Models (LLMs) such as ChatGPT and Gemini. It addresses inherent limitations like temporal knowledge cutoffs, mathematical inaccuracies, and the generation of incorrect information, proposing solutions like Retrieval Augmented Generation (RAG), Program-Aided Language Models (PAL), and frameworks such as ReAct and LangChain</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This tutorial explores the advancements and challenges in the development of Large Language Models (LLMs) such as ChatGPT and Gemini</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.12036v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.12036v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Roffo, &quot;Exploring Advanced Large Language Models with LLMsuite,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.12036v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441916992')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="wkvquant: quantizing weight and key/value cache for large language models gains more" data-keywords="attention optimization ros language model cs.lg cs.ai cs.cl" data-themes="S E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Machine Learning</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.12065v2" target="_blank">WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More</a>
                            </h3>
                            <p class="card-authors">Yuxuan Yue, Zhihang Yuan, Haojie Duanmu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4441926256">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process. This paper addresses these challenges by focusing on the quantization of LLMs, a technique that reduces memory consumption by converting model parameters and activations into low-bit integers. We critically analyze the existing quantization approaches, identifying their limitations in balancing the accuracy and efficiency of the quantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ fr...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To advance beyond these limitations, we propose WKVQuant, a PTQ framework especially designed for quantizing weights and the key/value (KV) cache of LLMs</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process. This paper addresses these challenges by focusing on the quantization of LLMs, a technique that reduces memory consumption by converting model parameters and activations into low-bit integers</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.12065v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.12065v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Yue, Z. Yuan, H. Duanmu, S. Zhou, J. Wu, and L. Nie, &quot;WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.12065v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441926256')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="unraveling arithmetic in large language models: the role of algebraic structures" data-keywords="transformer attention language model cs.lg cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Machine Learning</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2411.16260v3" target="_blank">Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures</a>
                            </h3>
                            <p class="card-authors">Fu-Chieh Chang, You-Chen Lin, Pei-Yuan Wu</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Transformer</span><span class="keyword-tag">Attention</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span></div>

                            <div class="card-details" id="details-4441918288">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) have demonstrated remarkable mathematical capabilities, largely driven by chain-of-thought (CoT) prompting, which decomposes complex reasoning into step-by-step solutions. This approach has enabled significant advancements, as evidenced by performance on benchmarks like GSM8K and MATH. However, the mechanisms underlying LLMs&#x27; ability to perform arithmetic in a single step of CoT remain poorly understood. Existing studies debate whether LLMs encode numerical values or rely on symbolic reasoning, while others explore attention and multi-layered processing in arithmet...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose that LLMs learn arithmetic by capturing algebraic structures, such as commutativity and identity properties</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">We empirically demonstrate that LLMs can learn algebraic structures using a custom dataset of arithmetic problems, as well as providing theoretical evidence showing that, under specific configurations of weights and biases, the transformer-based LLMs can generate embeddings that remain invariant to both permutations of input tokens and the presence of identity elements</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large language models (LLMs) have demonstrated remarkable mathematical capabilities, largely driven by chain-of-thought (CoT) prompting, which decomposes complex reasoning into step-by-step solutions</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2411.16260v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2411.16260v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. Chang, Y. Lin, and P. Wu, &quot;Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2411.16260v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441918288')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="fs-rag: a frame semantics based approach for improved factual accuracy in large language models" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2406.16167v1" target="_blank">FS-RAG: A Frame Semantics Based Approach for Improved Factual Accuracy in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Harish Tayyar Madabushi</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4441918576">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We present a novel extension to Retrieval Augmented Generation with the goal of mitigating factual inaccuracies in the output of large language models. Specifically, our method draws on the cognitive linguistic theory of frame semantics for the indexing and retrieval of factual information relevant to helping large language models answer queries. We conduct experiments to demonstrate the effectiveness of this method both in terms of retrieval effectiveness and in terms of the relevance of the frames and frame relations automatically generated. Our results show that this novel mechanism of Fram...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present a novel extension to Retrieval Augmented Generation with the goal of mitigating factual inaccuracies in the output of large language models</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We present a novel extension to Retrieval Augmented Generation with the goal of mitigating factual inaccuracies in the output of large language models</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2406.16167v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2406.16167v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. T. Madabushi, &quot;FS-RAG: A Frame Semantics Based Approach for Improved Factual Accuracy in Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2406.16167v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441918576')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="precise length control in large language models" data-keywords="control language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.11937v1" target="_blank">Precise Length Control in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Bradley Butcher, Michael O&#x27;Keefe, James Titchener</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Control</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4441928224">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Language Models (LLMs) are increasingly used in production systems, powering applications such as chatbots, summarization, and question answering. Despite their success, controlling the length of their response remains a significant challenge, particularly for tasks requiring structured outputs or specific levels of detail. In this work, we propose a method to adapt pre-trained decoder-only LLMs for precise control of response length. Our approach incorporates a secondary length-difference positional encoding (LDPE) into the input embeddings, which counts down to a user-set response term...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose a method to adapt pre-trained decoder-only LLMs for precise control of response length</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Despite their success, controlling the length of their response remains a significant challenge, particularly for tasks requiring structured outputs or specific levels of detail</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large Language Models (LLMs) are increasingly used in production systems, powering applications such as chatbots, summarization, and question answering</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.11937v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.11937v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. Butcher, M. O&#x27;Keefe, and J. Titchener, &quot;Precise Length Control in Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.11937v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441928224')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="instruction-tuned large language models for machine translation in the medical domain" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.16440v2" target="_blank">Instruction-tuned Large Language Models for Machine Translation in the Medical Domain</a>
                            </h3>
                            <p class="card-authors">Miguel Rios</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4441920016">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Language Models (LLMs) have shown promising results on machine translation for high resource language pairs and domains. However, in specialised domains (e.g. medical) LLMs have shown lower performance compared to standard neural machine translation models. The consistency in the machine translation of terminology is crucial for users, researchers, and translators in specialised domains. In this study, we compare the performance between baseline LLMs and instruction-tuned LLMs in the medical domain. In addition, we introduce terminology from specialised medical dictionaries into the inst...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In addition, we introduce terminology from specialised medical dictionaries into the instruction formatted datasets for fine-tuning LLMs</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large Language Models (LLMs) have shown promising results on machine translation for high resource language pairs and domains</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.16440v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.16440v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Rios, &quot;Instruction-tuned Large Language Models for Machine Translation in the Medical Domain,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.16440v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441920016')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="learning from failure: integrating negative examples when fine-tuning large language models as agents" data-keywords="control optimization language model cs.cl" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.11651v2" target="_blank">Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents</a>
                            </h3>
                            <p class="card-authors">Renxi Wang, Haonan Li, Xudong Han et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Control</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4441922560">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools such as search engines. However, LLMs are optimized for language generation instead of tool use during training or alignment, limiting their effectiveness as agents. To resolve this problem, previous work has first collected interaction trajectories between LLMs and environments, using only trajectories that successfully finished the task to fine-tune smaller models, making fine-tuning data scarce and acquiring it both difficult and costly. Discarding failed trajectories also ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We further analyze the inference results and find that our method provides a better trade-off between valuable information and errors in unsuccessful trajectories</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To resolve this problem, previous work has first collected interaction trajectories between LLMs and environments, using only trajectories that successfully finished the task to fine-tune smaller models, making fine-tuning data scarce and acquiring it both difficult and costly</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools such as search engines</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.11651v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.11651v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Wang, H. Li, X. Han, Y. Zhang, and T. Baldwin, &quot;Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.11651v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441922560')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="bridging large language models and graph structure learning models for robust representation learning" data-keywords="ros language model cs.lg cs.ai" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Machine Learning</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.12096v1" target="_blank">Bridging Large Language Models and Graph Structure Learning Models for Robust Representation Learning</a>
                            </h3>
                            <p class="card-authors">Guangxin Su, Yifan Zhu, Wenjie Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4441922320">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Graph representation learning, involving both node features and graph structures, is crucial for real-world applications but often encounters pervasive noise. State-of-the-art methods typically address noise by focusing separately on node features with large language models (LLMs) and on graph structures with graph structure learning models (GSLMs). In this paper, we introduce LangGSL, a robust framework that integrates the complementary strengths of pre-trained language models and GSLMs to jointly enhance both node feature and graph structure learning. In LangGSL, we first leverage LLMs to fi...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we introduce LangGSL, a robust framework that integrates the complementary strengths of pre-trained language models and GSLMs to jointly enhance both node feature and graph structure learning</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">State-of-the-art methods typically address noise by focusing separately on node features with large language models (LLMs) and on graph structures with graph structure learning models (GSLMs)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.12096v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.12096v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Su, Y. Zhu, W. Zhang, H. Wang, and Y. Zhang, &quot;Bridging Large Language Models and Graph Structure Learning Models for Robust Representation Learning,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.12096v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441922320')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="unforgettable generalization in language models" data-keywords="transformer ros language model cs.lg cs.cl" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Machine Learning</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.02228v1" target="_blank">Unforgettable Generalization in Language Models</a>
                            </h3>
                            <p class="card-authors">Eric Zhang, Leshem Chosen, Jacob Andreas</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Transformer</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span></div>

                            <div class="card-details" id="details-4441927504">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">When language models (LMs) are trained to forget (or &quot;unlearn&#x27;&#x27;) a skill, how precisely does their behavior change? We study the behavior of transformer LMs in which tasks have been forgotten via fine-tuning on randomized labels. Such LMs learn to generate near-random predictions for individual examples in the &quot;training&#x27;&#x27; set used for forgetting. Across tasks, however, LMs exhibit extreme variability in whether LM predictions change on examples outside the training set. In some tasks (like entailment classification), forgetting generalizes robustly, and causes models to produce uninformative p...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Dataset difficulty is not predictive of whether a behavior can be forgotten; instead, generalization in forgetting is (weakly) predicted by the confidence of LMs&#x27; initial task predictions and the variability of LM representations of training data, with low confidence and low variability both associated with greater generalization. Our results highlight the difficulty and unpredictability of performing targeted skill removal from models via fine-tuning.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">When language models (LMs) are trained to forget (or &quot;unlearn&#x27;&#x27;) a skill, how precisely does their behavior change? We study the behavior of transformer LMs in which tasks have been forgotten via fine-tuning on randomized labels</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.02228v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.02228v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'E. Zhang, L. Chosen, and J. Andreas, &quot;Unforgettable Generalization in Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.02228v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441927504')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="detecting mode collapse in language models via narration" data-keywords="reinforcement learning gpt simulation imu language model cs.cl cs.ai" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.04477v1" target="_blank">Detecting Mode Collapse in Language Models via Narration</a>
                            </h3>
                            <p class="card-authors">Sil Hamilton</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Simulation</span><span class="keyword-tag">Imu</span></div>

                            <div class="card-details" id="details-4444261680">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">No two authors write alike. Personal flourishes invoked in written narratives, from lexicon to rhetorical devices, imply a particular author--what literary theorists label the implied or virtual author; distinct from the real author or narrator of a text. Early large language models trained on unfiltered training sets drawn from a variety of discordant sources yielded incoherent personalities, problematic for conversational tasks but proving useful for sampling literature from multiple perspectives. Successes in alignment research in recent years have allowed researchers to impose subjectively...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Our method and results are significant for researchers seeking to employ language models in sociological simulations.</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Early large language models trained on unfiltered training sets drawn from a variety of discordant sources yielded incoherent personalities, problematic for conversational tasks but proving useful for sampling literature from multiple perspectives</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Early large language models trained on unfiltered training sets drawn from a variety of discordant sources yielded incoherent personalities, problematic for conversational tasks but proving useful for sampling literature from multiple perspectives</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.04477v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.04477v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Hamilton, &quot;Detecting Mode Collapse in Language Models via Narration,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.04477v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444261680')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="pallm: evaluating and enhancing palliative care conversations with large language models" data-keywords="gpt imu nlp language model cs.cl cs.hc" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.15188v2" target="_blank">PALLM: Evaluating and Enhancing PALLiative Care Conversations with Large Language Models</a>
                            </h3>
                            <p class="card-authors">Zhiyuan Wang, Fangxu Yuan, Virginia LeBaron et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Imu</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4444261872">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Effective patient-provider communication is crucial in clinical care, directly impacting patient outcomes and quality of life. Traditional evaluation methods, such as human ratings, patient feedback, and provider self-assessments, are often limited by high costs and scalability issues. Although existing natural language processing (NLP) techniques show promise, they struggle with the nuances of clinical communication and require sensitive clinical data for training, reducing their effectiveness in real-world applications. Emerging large language models (LLMs) offer a new approach to assessing ...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Traditional evaluation methods, such as human ratings, patient feedback, and provider self-assessments, are often limited by high costs and scalability issues</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Traditional evaluation methods, such as human ratings, patient feedback, and provider self-assessments, are often limited by high costs and scalability issues</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.15188v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.15188v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Wang, F. Yuan, V. LeBaron, T. Flickinger, and L. E. Barnes, &quot;PALLM: Evaluating and Enhancing PALLiative Care Conversations with Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.15188v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444261872')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cv" data-title="exploring the frontier of vision-language models: a survey of current methodologies and future directions" data-keywords="language model cs.cv cs.ai cs.cl" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computer Vision</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2404.07214v4" target="_blank">Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions</a>
                            </h3>
                            <p class="card-authors">Akash Ghosh, Arkadeep Acharya, Sriparna Saha et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444262304">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The advent of Large Language Models (LLMs) has significantly reshaped the trajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable limitation, as they are primarily adept at processing textual information. To address this constraint, researchers have endeavored to integrate visual capabilities with LLMs, resulting in the emergence of Vision-Language Models (VLMs). These advanced models are instrumental in tackling more intricate tasks such as image captioning and visual question answering. In our comprehensive survey paper, we delve into the key advancements within the realm...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Nevertheless, these LLMs exhibit a notable limitation, as they are primarily adept at processing textual information. Our classification organizes VLMs into three distinct categories: models dedicated to vision-language understanding, models that process multimodal inputs to generate unimodal (textual) outputs and models that both accept and produce multimodal inputs and outputs.This classification is based on their respective capabilities and functionalities in processing and generating various modalities of data.We meticulously dissect each model, offering an extensive analysis of its foundational architecture, training data sources, as well as its strengths and limitations wherever possible, providing readers with a comprehensive understanding of its essential components</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The advent of Large Language Models (LLMs) has significantly reshaped the trajectory of the AI revolution</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2404.07214v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2404.07214v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Ghosh, A. Acharya, S. Saha, V. Jain, and A. Chadha, &quot;Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2404.07214v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444262304')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="efficacy of large language models in systematic reviews" data-keywords="gpt language model cs.cl cs.lg" data-themes="E M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.04646v2" target="_blank">Efficacy of Large Language Models in Systematic Reviews</a>
                            </h3>
                            <p class="card-authors">Aaditya Shah, Shridhar Mehendale, Siddha Kanthi</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.LG</span></div>

                            <div class="card-details" id="details-4444262976">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This study investigates the effectiveness of Large Language Models (LLMs) in interpreting existing literature through a systematic review of the relationship between Environmental, Social, and Governance (ESG) factors and financial performance. The primary objective is to assess how LLMs can replicate a systematic review on a corpus of ESG-focused papers. We compiled and hand-coded a database of 88 relevant papers published from March 2020 to May 2024. Additionally, we used a set of 238 papers from a previous systematic review of ESG literature from January 2015 to February 2020. We evaluated ...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This study investigates the effectiveness of Large Language Models (LLMs) in interpreting existing literature through a systematic review of the relationship between Environmental, Social, and Governance (ESG) factors and financial performance</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.04646v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.04646v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Shah, S. Mehendale, and S. Kanthi, &quot;Efficacy of Large Language Models in Systematic Reviews,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.04646v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444262976')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="thames: an end-to-end tool for hallucination mitigation and evaluation in large language models" data-keywords="gpt ros language model cs.cl" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.11353v3" target="_blank">THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Mengfei Liang, Archish Arun, Zekun Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444260912">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Hallucination, the generation of factually incorrect content, is a growing challenge in Large Language Models (LLMs). Existing detection and mitigation methods are often isolated and insufficient for domain-specific needs, lacking a standardized pipeline. This paper introduces THaMES (Tool for Hallucination Mitigations and EvaluationS), an integrated framework and library addressing this gap. THaMES offers an end-to-end solution for evaluating and mitigating hallucinations in LLMs, featuring automated test set generation, multifaceted benchmarking, and adaptable mitigation strategies. It autom...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Hallucination, the generation of factually incorrect content, is a growing challenge in Large Language Models (LLMs). This paper introduces THaMES (Tool for Hallucination Mitigations and EvaluationS), an integrated framework and library addressing this gap</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Hallucination, the generation of factually incorrect content, is a growing challenge in Large Language Models (LLMs)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.11353v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.11353v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Liang et al., &quot;THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.11353v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444260912')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="are compressed language models less subgroup robust?" data-keywords="bert language model cs.lg cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Machine Learning</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.17811v1" target="_blank">Are Compressed Language Models Less Subgroup Robust?</a>
                            </h3>
                            <p class="card-authors">Leonidas Gee, Andrea Zugarini, Novi Quadrianto</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Bert</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444263168">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">To reduce the inference cost of large language models, model compression is increasingly used to create smaller scalable models. However, little is known about their robustness to minority subgroups defined by the labels and attributes of a dataset. In this paper, we investigate the effects of 18 different compression methods and settings on the subgroup robustness of BERT language models. We show that worst-group performance does not depend on model size alone, but also on the compression method used. Additionally, we find that model compression does not always worsen the performance on minor...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">To reduce the inference cost of large language models, model compression is increasingly used to create smaller scalable models</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.17811v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.17811v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Gee, A. Zugarini, and N. Quadrianto, &quot;Are Compressed Language Models Less Subgroup Robust?,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.17811v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444263168')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="can large language models (or humans) disentangle text?" data-keywords="language model cs.cl" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.16584v2" target="_blank">Can Large Language Models (or Humans) Disentangle Text?</a>
                            </h3>
                            <p class="card-authors">Nicolas Audinet de Pieuchon, Adel Daoud, Connor Thomas Jerzak et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444261968">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We investigate the potential of large language models (LLMs) to disentangle text variables--to remove the textual traces of an undesired forbidden variable in a task sometimes known as text distillation and closely related to the fairness in AI and causal inference literature. We employ a range of various LLM approaches in an attempt to disentangle text by identifying and removing information about a target variable while preserving other relevant signals. We show that in the strong test of removing sentiment, the statistical association between the processed text and sentiment is still detect...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">This suggests there may be limited separability between concept variables in some text contexts, highlighting limitations of methods relying on text-level transformations and also raising questions about the robustness of disentanglement methods that achieve statistical independence in representation space.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We investigate the potential of large language models (LLMs) to disentangle text variables--to remove the textual traces of an undesired forbidden variable in a task sometimes known as text distillation and closely related to the fairness in AI and causal inference literature</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.16584v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.16584v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. A. d. Pieuchon, A. Daoud, C. T. Jerzak, M. Johansson, and R. Johansson, &quot;Can Large Language Models (or Humans) Disentangle Text?,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.16584v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444261968')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cr" data-title="large language models merging for enhancing the link stealing attack on graph neural networks" data-keywords="neural network gnn ros language model cs.cr cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ CS.CR</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.05830v1" target="_blank">Large Language Models Merging for Enhancing the Link Stealing Attack on Graph Neural Networks</a>
                            </h3>
                            <p class="card-authors">Faqian Guan, Tianqing Zhu, Wenhan Chang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Gnn</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4444262688">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Graph Neural Networks (GNNs), specifically designed to process the graph data, have achieved remarkable success in various applications. Link stealing attacks on graph data pose a significant privacy threat, as attackers aim to extract sensitive relationships between nodes (entities), potentially leading to academic misconduct, fraudulent transactions, or other malicious activities. Previous studies have primarily focused on single datasets and did not explore cross-dataset attacks, let alone attacks that leverage the combined knowledge of multiple attackers. However, we find that an attacker ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose a novel link stealing attack method that takes advantage of cross-dataset and Large Language Models (LLMs)</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">However, we find that an attacker can combine the data knowledge of multiple attackers to create a more effective attack model, which can be referred to cross-dataset attacks</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.05830v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.05830v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. Guan, T. Zhu, W. Chang, W. Ren, and W. Zhou, &quot;Large Language Models Merging for Enhancing the Link Stealing Attack on Graph Neural Networks,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.05830v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444262688')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="fanal -- financial activity news alerting language modeling framework" data-keywords="bert gpt optimization language model cs.cl cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.03527v1" target="_blank">FANAL -- Financial Activity News Alerting Language Modeling Framework</a>
                            </h3>
                            <p class="card-authors">Urjitkumar Patel, Fang-Chun Yeh, Chinmay Gondhalekar et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Bert</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4444263120">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In the rapidly evolving financial sector, the accurate and timely interpretation of market news is essential for stakeholders needing to navigate unpredictable events. This paper introduces FANAL (Financial Activity News Alerting Language Modeling Framework), a specialized BERT-based framework engineered for real-time financial event detection and analysis, categorizing news into twelve distinct financial categories. FANAL leverages silver-labeled data processed through XGBoost and employs advanced fine-tuning techniques, alongside ORBERT (Odds Ratio BERT), a novel variant of BERT fine-tuned w...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper introduces FANAL (Financial Activity News Alerting Language Modeling Framework), a specialized BERT-based framework engineered for real-time financial event detection and analysis, categorizing news into twelve distinct financial categories</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.03527v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.03527v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'U. Patel, F. Yeh, C. Gondhalekar, and H. Nalluri, &quot;FANAL -- Financial Activity News Alerting Language Modeling Framework,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.03527v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444263120')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cr" data-title="adashield: safeguarding multimodal large language models from structure-based attack via adaptive shield prompting" data-keywords="language model cs.cr cs.ai" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ CS.CR</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.09513v1" target="_blank">AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting</a>
                            </h3>
                            <p class="card-authors">Yu Wang, Xiaogeng Liu, Yu Li et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CR</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4444262160">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">With the advent and widespread deployment of Multimodal Large Language Models (MLLMs), the imperative to ensure their safety has become increasingly pronounced. However, with the integration of additional modalities, MLLMs are exposed to new vulnerabilities, rendering them prone to structured-based jailbreak attacks, where semantic content (e.g., &quot;harmful text&quot;) has been injected into the images to mislead MLLMs. In this work, we aim to defend against such threats. Specifically, we propose \textbf{Ada}ptive \textbf{Shield} Prompting (\textbf{AdaShield}), which prepends inputs with defense prom...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Specifically, we propose \textbf{Ada}ptive \textbf{Shield} Prompting (\textbf{AdaShield}), which prepends inputs with defense prompts to defend MLLMs against structure-based jailbreak attacks without fine-tuning MLLMs or training additional modules (e.g., post-stage content detector)</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">With the advent and widespread deployment of Multimodal Large Language Models (MLLMs), the imperative to ensure their safety has become increasingly pronounced</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.09513v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.09513v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Wang, X. Liu, Y. Li, M. Chen, and C. Xiao, &quot;AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.09513v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444262160')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="measuring the inconsistency of large language models in preferential ranking" data-keywords="language model cs.cl" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.08851v1" target="_blank">Measuring the Inconsistency of Large Language Models in Preferential Ranking</a>
                            </h3>
                            <p class="card-authors">Xiutian Zhao, Ke Wang, Wei Peng</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444261152">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Despite large language models&#x27; (LLMs) recent advancements, their bias and hallucination issues persist, and their ability to offer consistent preferential rankings remains underexplored. This study investigates the capacity of LLMs to provide consistent ordinal preferences, a crucial aspect in scenarios with dense decision space or lacking absolute answers. We introduce a formalization of consistency based on order theory, outlining criteria such as transitivity, asymmetry, reversibility, and independence from irrelevant alternatives. Our diagnostic experiments on selected state-of-the-art LLM...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce a formalization of consistency based on order theory, outlining criteria such as transitivity, asymmetry, reversibility, and independence from irrelevant alternatives</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Despite large language models&#x27; (LLMs) recent advancements, their bias and hallucination issues persist, and their ability to offer consistent preferential rankings remains underexplored. These findings highlight a significant inconsistency in LLM-generated preferential rankings, underscoring the need for further research to address these limitations.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Despite large language models&#x27; (LLMs) recent advancements, their bias and hallucination issues persist, and their ability to offer consistent preferential rankings remains underexplored</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.08851v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.08851v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. Zhao, K. Wang, and W. Peng, &quot;Measuring the Inconsistency of Large Language Models in Preferential Ranking,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.08851v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444261152')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="overview of the first workshop on language models for low-resource languages (loreslm 2025)" data-keywords="nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.16365v1" target="_blank">Overview of the First Workshop on Language Models for Low-Resource Languages (LoResLM 2025)</a>
                            </h3>
                            <p class="card-authors">Hansi Hettiarachchi, Tharindu Ranasinghe, Paul Rayson et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4444263024">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The first Workshop on Language Models for Low-Resource Languages (LoResLM 2025) was held in conjunction with the 31st International Conference on Computational Linguistics (COLING 2025) in Abu Dhabi, United Arab Emirates. This workshop mainly aimed to provide a forum for researchers to share and discuss their ongoing work on language models (LMs) focusing on low-resource languages, following the recent advancements in neural language models and their linguistic biases towards high-resource languages. LoResLM 2025 attracted notable interest from the natural language processing (NLP) community, ...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The first Workshop on Language Models for Low-Resource Languages (LoResLM 2025) was held in conjunction with the 31st International Conference on Computational Linguistics (COLING 2025) in Abu Dhabi, United Arab Emirates</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.16365v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.16365v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Hettiarachchi et al., &quot;Overview of the First Workshop on Language Models for Low-Resource Languages (LoResLM 2025),&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.16365v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444263024')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="activation sparsity opportunities for compressing general large language models" data-keywords="optimization language model cs.lg cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Machine Learning</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.12178v2" target="_blank">Activation Sparsity Opportunities for Compressing General Large Language Models</a>
                            </h3>
                            <p class="card-authors">Nobel Dhar, Bobin Deng, Md Romyull Islam et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Optimization</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4444261008">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Deploying local AI models, such as Large Language Models (LLMs), to edge devices can substantially enhance devices&#x27; independent capabilities, alleviate the server&#x27;s burden, and lower the response time. Owing to these tremendous potentials, many big tech companies have released several lightweight Small Language Models (SLMs) to bridge this gap. However, we still have huge motivations to deploy more powerful (LLMs) AI models on edge devices and enhance their smartness level. Unlike the conventional approaches for AI model compression, we investigate activation sparsity. The activation sparsity ...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Owing to these tremendous potentials, many big tech companies have released several lightweight Small Language Models (SLMs) to bridge this gap</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Deploying local AI models, such as Large Language Models (LLMs), to edge devices can substantially enhance devices&#x27; independent capabilities, alleviate the server&#x27;s burden, and lower the response time</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.12178v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.12178v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. Dhar, B. Deng, M. R. Islam, K. F. A. Nasif, L. Zhao, and K. Suo, &quot;Activation Sparsity Opportunities for Compressing General Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.12178v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444261008')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="how important is tokenization in french medical masked language models?" data-keywords="ros nlp language model cs.cl cs.ai cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.15010v2" target="_blank">How Important Is Tokenization in French Medical Masked Language Models?</a>
                            </h3>
                            <p class="card-authors">Yanis Labrak, Adrien Bazoge, Beatrice Daille et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444263696">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Subword tokenization has become the prevailing standard in the field of natural language processing (NLP) over recent years, primarily due to the widespread utilization of pre-trained language models. This shift began with Byte-Pair Encoding (BPE) and was later followed by the adoption of SentencePiece and WordPiece. While subword tokenization consistently outperforms character and word-level tokenization, the precise factors contributing to its success remain unclear. Key aspects such as the optimal segmentation granularity for diverse tasks and languages, the influence of data sources on tok...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Subword tokenization has become the prevailing standard in the field of natural language processing (NLP) over recent years, primarily due to the widespread utilization of pre-trained language models</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.15010v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.15010v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Labrak, A. Bazoge, B. Daille, M. Rouvier, and R. Dufour, &quot;How Important Is Tokenization in French Medical Masked Language Models?,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.15010v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444263696')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ai" data-title="from code to play: benchmarking program search for games using large language models" data-keywords="control ros language model cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Artificial Intelligence</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.04057v2" target="_blank">From Code to Play: Benchmarking Program Search for Games Using Large Language Models</a>
                            </h3>
                            <p class="card-authors">Manuel Eberhardinger, James Goodman, Alexander Dockhorn et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Control</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4444261776">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) have shown impressive capabilities in generating program code, opening exciting opportunities for applying program synthesis to games. In this work, we explore the potential of LLMs to directly synthesize usable code for a wide range of gaming applications, focusing on two programming languages, Python and Java. We use an evolutionary hill-climbing algorithm, where the mutations and seeds of the initial programs are controlled by LLMs. For Python, the framework covers various game-related tasks, including five miniature versions of Atari games, ten levels of Baba i...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Trying many models on a problem and using the best results across them is more reliable than using just one.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large language models (LLMs) have shown impressive capabilities in generating program code, opening exciting opportunities for applying program synthesis to games</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.04057v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.04057v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Eberhardinger et al., &quot;From Code to Play: Benchmarking Program Search for Games Using Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.04057v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444261776')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cr" data-title="on trojan signatures in large language models of code" data-keywords="computer vision language model cs.cr cs.lg cs.se" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ CS.CR</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.16896v2" target="_blank">On Trojan Signatures in Large Language Models of Code</a>
                            </h3>
                            <p class="card-authors">Aftab Hussain, Md Rafiqul Islam Rabin, Mohammad Amin Alipour</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Computer Vision</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CR</span><span class="keyword-tag">CS.LG</span></div>

                            <div class="card-details" id="details-4444263792">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Trojan signatures, as described by Fields et al. (2021), are noticeable differences in the distribution of the trojaned class parameters (weights) and the non-trojaned class parameters of the trojaned model, that can be used to detect the trojaned model. Fields et al. (2021) found trojan signatures in computer vision classification tasks with image models, such as, Resnet, WideResnet, Densenet, and VGG. In this paper, we investigate such signatures in the classifier layer parameters of large language models of source code.
  Our results suggest that trojan signatures could not generalize to LL...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To the best of our knowledge, this is the first work to examine weight-based trojan signature revelation techniques for large-language models of code and furthermore to demonstrate that detecting trojans only from the weights in such models is a hard problem.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">(2021), are noticeable differences in the distribution of the trojaned class parameters (weights) and the non-trojaned class parameters of the trojaned model, that can be used to detect the trojaned model</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.16896v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.16896v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Hussain, M. R. I. Rabin, and M. A. Alipour, &quot;On Trojan Signatures in Large Language Models of Code,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.16896v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444263792')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ce" data-title="leveraging large language models for institutional portfolio management: persona-based ensembles" data-keywords="ros language model cs.ce cs.ma" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ CS.CE</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2411.19515v1" target="_blank">Leveraging Large Language Models for Institutional Portfolio Management: Persona-Based Ensembles</a>
                            </h3>
                            <p class="card-authors">Yoshia Abe, Shuhei Matsuo, Ryoma Kondo et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CE</span><span class="keyword-tag">CS.MA</span></div>

                            <div class="card-details" id="details-4444261824">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) have demonstrated promising performance in various financial applications, though their potential in complex investment strategies remains underexplored. To address this gap, we investigate how LLMs can predict price movements in stock and bond portfolios using economic indicators, enabling portfolio adjustments akin to those employed by institutional investors. Additionally, we explore the impact of incorporating different personas within LLMs, using an ensemble approach to leverage their diverse predictions. Our findings show that LLM-based strategies, especially...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To address this gap, we investigate how LLMs can predict price movements in stock and bond portfolios using economic indicators, enabling portfolio adjustments akin to those employed by institutional investors</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large language models (LLMs) have demonstrated promising performance in various financial applications, though their potential in complex investment strategies remains underexplored</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2411.19515v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2411.19515v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Abe, S. Matsuo, R. Kondo, and R. Hisano, &quot;Leveraging Large Language Models for Institutional Portfolio Management: Persona-Based Ensembles,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2411.19515v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444261824')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="evaluating class membership relations in knowledge graphs using large language models" data-keywords="gpt language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2404.17000v1" target="_blank">Evaluating Class Membership Relations in Knowledge Graphs using Large Language Models</a>
                            </h3>
                            <p class="card-authors">Bradley P. Allen, Paul T. Groth</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4444261392">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">A backbone of knowledge graphs are their class membership relations, which assign entities to a given class. As part of the knowledge engineering process, we propose a new method for evaluating the quality of these relations by processing descriptions of a given entity and class using a zero-shot chain-of-thought classifier that uses a natural language intensional definition of a class. We evaluate the method using two publicly available knowledge graphs, Wikidata and CaLiGraph, and 7 large language models. Using the gpt-4-0125-preview large language model, the method&#x27;s classification performa...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">As part of the knowledge engineering process, we propose a new method for evaluating the quality of these relations by processing descriptions of a given entity and class using a zero-shot chain-of-thought classifier that uses a natural language intensional definition of a class</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">As part of the knowledge engineering process, we propose a new method for evaluating the quality of these relations by processing descriptions of a given entity and class using a zero-shot chain-of-thought classifier that uses a natural language intensional definition of a class</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2404.17000v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2404.17000v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. P. Allen, and P. T. Groth, &quot;Evaluating Class Membership Relations in Knowledge Graphs using Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2404.17000v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444261392')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="social bias in large language models for bangla: an empirical study on gender and religious bias" data-keywords="nlp language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.03536v3" target="_blank">Social Bias in Large Language Models For Bangla: An Empirical Study on Gender and Religious Bias</a>
                            </h3>
                            <p class="card-authors">Jayanta Sadhu, Maneesha Rani Saha, Rifat Shahriyar</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444263360">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The rapid growth of Large Language Models (LLMs) has put forward the study of biases as a crucial field. It is important to assess the influence of different types of biases embedded in LLMs to ensure fair use in sensitive fields. Although there have been extensive works on bias assessment in English, such efforts are rare and scarce for a major language like Bangla. In this work, we examine two types of social biases in LLM generated outputs for Bangla language. Our main contributions in this work are: (1) bias studies on two different social biases for Bangla, (2) a curated dataset for bias ...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The rapid growth of Large Language Models (LLMs) has put forward the study of biases as a crucial field</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.03536v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.03536v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Sadhu, M. R. Saha, and R. Shahriyar, &quot;Social Bias in Large Language Models For Bangla: An Empirical Study on Gender and Religious Bias,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.03536v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444263360')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="eess.as" data-title="speechprompt: prompting speech language models for speech processing tasks" data-keywords="language model eess.as cs.ai cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ EESS.AS</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.13040v1" target="_blank">SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks</a>
                            </h3>
                            <p class="card-authors">Kai-Wei Chang, Haibin Wu, Yu-Kai Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">EESS.AS</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444263648">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Prompting has become a practical method for utilizing pre-trained language models (LMs). This approach offers several advantages. It allows an LM to adapt to new tasks with minimal training and parameter updates, thus achieving efficiency in both storage and computation. Additionally, prompting modifies only the LM&#x27;s inputs and harnesses the generative capabilities of language models to address various downstream tasks in a unified manner. This significantly reduces the need for human labor in designing task-specific models. These advantages become even more evident as the number of tasks serv...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Prompting has become a practical method for utilizing pre-trained language models (LMs)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.13040v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.13040v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Chang et al., &quot;SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.13040v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444263648')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="q-bio.qm" data-title="gene-associated disease discovery powered by large language models" data-keywords="language model q-bio.qm cs.ir" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Q-BIO.QM</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.09490v1" target="_blank">Gene-associated Disease Discovery Powered by Large Language Models</a>
                            </h3>
                            <p class="card-authors">Jiayu Chang, Shiyu Wang, Chen Ling et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">Q-BIO.QM</span><span class="keyword-tag">CS.IR</span></div>

                            <div class="card-details" id="details-4444263744">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The intricate relationship between genetic variation and human diseases has been a focal point of medical research, evidenced by the identification of risk genes regarding specific diseases. The advent of advanced genome sequencing techniques has significantly improved the efficiency and cost-effectiveness of detecting these genetic markers, playing a crucial role in disease diagnosis and forming the basis for clinical decision-making and early risk assessment. To overcome the limitations of existing databases that record disease-gene associations from existing literature, which often lack rea...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To overcome the limitations of existing databases that record disease-gene associations from existing literature, which often lack real-time updates, we propose a novel framework employing Large Language Models (LLMs) for the discovery of diseases associated with specific genes</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To overcome the limitations of existing databases that record disease-gene associations from existing literature, which often lack real-time updates, we propose a novel framework employing Large Language Models (LLMs) for the discovery of diseases associated with specific genes</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The advent of advanced genome sequencing techniques has significantly improved the efficiency and cost-effectiveness of detecting these genetic markers, playing a crucial role in disease diagnosis and forming the basis for clinical decision-making and early risk assessment</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.09490v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.09490v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Chang, S. Wang, C. Ling, Z. Qin, and L. Zhao, &quot;Gene-associated Disease Discovery Powered by Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.09490v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444263744')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ai" data-title="graph-of-thought: utilizing large language models to solve complex and dynamic business problems" data-keywords="ros language model cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Artificial Intelligence</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.06801v2" target="_blank">Graph-of-Thought: Utilizing Large Language Models to Solve Complex and Dynamic Business Problems</a>
                            </h3>
                            <p class="card-authors">Ye Li</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4444264656">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents Graph-of-Thought (GoT), a new model for workflow automation that enhances the flexibility and efficiency of Large Language Models (LLMs) in complex task execution. GoT advances beyond traditional linear and tree-like cognitive models with a graph structure that enables dynamic path selection. The open-source engine GoTFlow demonstrates the practical application of GoT, facilitating automated, data-driven decision-making across various domains. Despite challenges in complexity and transparency, GoTFlow&#x27;s potential for improving business processes is significant, promising ad...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents Graph-of-Thought (GoT), a new model for workflow automation that enhances the flexibility and efficiency of Large Language Models (LLMs) in complex task execution</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Despite challenges in complexity and transparency, GoTFlow&#x27;s potential for improving business processes is significant, promising advancements in both efficiency and decision quality with continuous development.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper presents Graph-of-Thought (GoT), a new model for workflow automation that enhances the flexibility and efficiency of Large Language Models (LLMs) in complex task execution</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.06801v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.06801v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Li, &quot;Graph-of-Thought: Utilizing Large Language Models to Solve Complex and Dynamic Business Problems,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.06801v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444264656')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="clinical information extraction for low-resource languages with few-shot learning using pre-trained language models and prompting" data-keywords="language model cs.cl cs.ai cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.13369v2" target="_blank">Clinical information extraction for Low-resource languages with Few-shot learning using Pre-trained language models and Prompting</a>
                            </h3>
                            <p class="card-authors">Phillip Richter-Pechanski, Philipp Wiesenbach, Dominic M. Schwab et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.LG</span></div>

                            <div class="card-details" id="details-4444262112">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Automatic extraction of medical information from clinical documents poses several challenges: high costs of required clinical expertise, limited interpretability of model predictions, restricted computational resources and privacy regulations. Recent advances in domain-adaptation and prompting methods showed promising results with minimal training data using lightweight masked language models, which are suited for well-established interpretability methods. We are first to present a systematic evaluation of these methods in a low-resource setting, by performing multi-class section classificatio...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We demonstrate that a lightweight, domain-adapted pretrained model, prompted with just 20 shots, outperforms a traditional classification model by 30.5% accuracy</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Automatic extraction of medical information from clinical documents poses several challenges: high costs of required clinical expertise, limited interpretability of model predictions, restricted computational resources and privacy regulations</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Automatic extraction of medical information from clinical documents poses several challenges: high costs of required clinical expertise, limited interpretability of model predictions, restricted computational resources and privacy regulations</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.13369v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.13369v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Richter-Pechanski et al., &quot;Clinical information extraction for Low-resource languages with Few-shot learning using Pre-trained language models and Prompting,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.13369v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444262112')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="causal reasoning in large language models: a knowledge graph approach" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.11588v1" target="_blank">Causal Reasoning in Large Language Models: A Knowledge Graph Approach</a>
                            </h3>
                            <p class="card-authors">Yejin Kim, Eojin Kang, Juae Kim et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444264848">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) typically improve performance by either retrieving semantically similar information, or enhancing reasoning abilities through structured prompts like chain-of-thought. While both strategies are considered crucial, it remains unclear which has a greater impact on model performance or whether a combination of both is necessary. This paper answers this question by proposing a knowledge graph (KG)-based random-walk reasoning approach that leverages causal relationships. We conduct experiments on the commonsense question answering task that is based on a KG. The KG inhe...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large language models (LLMs) typically improve performance by either retrieving semantically similar information, or enhancing reasoning abilities through structured prompts like chain-of-thought</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.11588v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.11588v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Kim, E. Kang, J. Kim, and H. H. Huang, &quot;Causal Reasoning in Large Language Models: A Knowledge Graph Approach,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.11588v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444264848')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="pediatricsgpt: large language models as chinese medical assistants for pediatric applications" data-keywords="gpt optimization ros language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.19266v4" target="_blank">PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications</a>
                            </h3>
                            <p class="card-authors">Dingkang Yang, Jinjie Wei, Dongling Xiao et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4444265184">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Developing intelligent pediatric consultation systems offers promising prospects for improving diagnostic efficiency, especially in China, where healthcare resources are scarce. Despite recent advances in Large Language Models (LLMs) for Chinese medicine, their performance is sub-optimal in pediatric applications due to inadequate instruction data and vulnerable training procedures. To address the above issues, this paper builds PedCorpus, a high-quality dataset of over 300,000 multi-task instructions from pediatric textbooks, guidelines, and knowledge graph resources to fulfil diverse diagnos...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Upon well-designed PedCorpus, we propose PediatricsGPT, the first Chinese pediatric LLM assistant built on a systematic and robust training pipeline</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To address the above issues, this paper builds PedCorpus, a high-quality dataset of over 300,000 multi-task instructions from pediatric textbooks, guidelines, and knowledge graph resources to fulfil diverse diagnostic demands</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Despite recent advances in Large Language Models (LLMs) for Chinese medicine, their performance is sub-optimal in pediatric applications due to inadequate instruction data and vulnerable training procedures</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.19266v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.19266v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Yang et al., &quot;PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.19266v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444265184')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="beyond data quantity: key factors driving performance in multilingual language models" data-keywords="ros language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.12500v1" target="_blank">Beyond Data Quantity: Key Factors Driving Performance in Multilingual Language Models</a>
                            </h3>
                            <p class="card-authors">Sina Bagheri Nezhad, Ameeta Agrawal, Rhitabrat Pokharel</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4444265568">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Multilingual language models (MLLMs) are crucial for handling text across various languages, yet they often show performance disparities due to differences in resource availability and linguistic characteristics. While the impact of pre-train data percentage and model size on performance is well-known, our study reveals additional critical factors that significantly influence MLLM effectiveness. Analyzing a wide range of features, including geographical, linguistic, and resource-related aspects, we focus on the SIB-200 dataset for classification and the Flores-200 dataset for machine translati...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Multilingual language models (MLLMs) are crucial for handling text across various languages, yet they often show performance disparities due to differences in resource availability and linguistic characteristics</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.12500v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.12500v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. B. Nezhad, A. Agrawal, and R. Pokharel, &quot;Beyond Data Quantity: Key Factors Driving Performance in Multilingual Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.12500v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444265568')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="a comprehensive survey of scientific large language models and their applications in scientific discovery" data-keywords="ros language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2406.10833v3" target="_blank">A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery</a>
                            </h3>
                            <p class="card-authors">Yu Zhang, Xiusi Chen, Bowen Jin et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444266000">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In many scientific fields, large language models (LLMs) have revolutionized the way text and other modalities of data (e.g., molecules and proteins) are handled, achieving superior performance in various applications and augmenting the scientific discovery process. Nevertheless, previous surveys on scientific LLMs often concentrate on one or two fields or a single modality. In this paper, we aim to provide a more holistic view of the research landscape by unveiling cross-field and cross-modal connections between scientific LLMs regarding their architectures and pre-training techniques. To this...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In many scientific fields, large language models (LLMs) have revolutionized the way text and other modalities of data (e.g., molecules and proteins) are handled, achieving superior performance in various applications and augmenting the scientific discovery process</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2406.10833v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2406.10833v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Zhang et al., &quot;A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2406.10833v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444266000')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="a survey of large language models for arabic language and its dialects" data-keywords="ros language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.20238v2" target="_blank">A Survey of Large Language Models for Arabic Language and its Dialects</a>
                            </h3>
                            <p class="card-authors">Malak Mashaabi, Shahad Al-Khalifa, Hend Al-Khalifa</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4444264368">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This survey offers a comprehensive overview of Large Language Models (LLMs) designed for Arabic language and its dialects. It covers key architectures, including encoder-only, decoder-only, and encoder-decoder models, along with the datasets used for pre-training, spanning Classical Arabic, Modern Standard Arabic, and Dialectal Arabic. The study also explores monolingual, bilingual, and multilingual LLMs, analyzing their architectures and performance across downstream tasks, such as sentiment analysis, named entity recognition, and question answering. Furthermore, it assesses the openness of A...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">It concludes by identifying key challenges and opportunities for future research and stressing the need for more inclusive and representative models.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This survey offers a comprehensive overview of Large Language Models (LLMs) designed for Arabic language and its dialects</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.20238v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.20238v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Mashaabi, S. Al-Khalifa, and H. Al-Khalifa, &quot;A Survey of Large Language Models for Arabic Language and its Dialects,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.20238v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444264368')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="a few-shot approach for relation extraction domain adaptation using large language models" data-keywords="deep learning transformer ros language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.02377v1" target="_blank">A Few-Shot Approach for Relation Extraction Domain Adaptation using Large Language Models</a>
                            </h3>
                            <p class="card-authors">Vanni Zavarella, Juan Carlos Gamero-Salinas, Sergio Consoli</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Deep Learning</span><span class="keyword-tag">Transformer</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4444263552">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Knowledge graphs (KGs) have been successfully applied to the analysis of complex scientific and technological domains, with automatic KG generation methods typically building upon relation extraction models capturing fine-grained relations between domain entities in text. While these relations are fully applicable across scientific areas, existing models are trained on few domain-specific datasets such as SciERC and do not perform well on new target domains. In this paper, we experiment with leveraging in-context learning capabilities of Large Language Models to perform schema-constrained data...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Knowledge graphs (KGs) have been successfully applied to the analysis of complex scientific and technological domains, with automatic KG generation methods typically building upon relation extraction models capturing fine-grained relations between domain entities in text</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.02377v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.02377v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'V. Zavarella, J. C. Gamero-Salinas, and S. Consoli, &quot;A Few-Shot Approach for Relation Extraction Domain Adaptation using Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.02377v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444263552')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="fast vocabulary transfer for language model compression" data-keywords="language model cs.cl cs.ai cs.lg" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.09977v1" target="_blank">Fast Vocabulary Transfer for Language Model Compression</a>
                            </h3>
                            <p class="card-authors">Leonidas Gee, Andrea Zugarini, Leonardo Rigutini et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.LG</span></div>

                            <div class="card-details" id="details-4444265904">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Real-world business applications require a trade-off between language model performance and size. We propose a new method for model compression that relies on vocabulary transfer. We evaluate the method on various vertical domains and downstream tasks. Our results indicate that vocabulary transfer can be effectively used in combination with other compression techniques, yielding a significant reduction in model size and inference time while marginally compromising on performance.</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a new method for model compression that relies on vocabulary transfer</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Real-world business applications require a trade-off between language model performance and size</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.09977v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.09977v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Gee, A. Zugarini, L. Rigutini, and P. Torroni, &quot;Fast Vocabulary Transfer for Language Model Compression,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.09977v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444265904')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ai" data-title="revealing hidden bias in ai: lessons from large language models" data-keywords="gpt ros language model cs.ai cs.cy" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Artificial Intelligence</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.16927v1" target="_blank">Revealing Hidden Bias in AI: Lessons from Large Language Models</a>
                            </h3>
                            <p class="card-authors">Django Beatty, Kritsada Masanthia, Teepakorn Kaphol et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4444262928">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">As large language models (LLMs) become integral to recruitment processes, concerns about AI-induced bias have intensified. This study examines biases in candidate interview reports generated by Claude 3.5 Sonnet, GPT-4o, Gemini 1.5, and Llama 3.1 405B, focusing on characteristics such as gender, race, and age. We evaluate the effectiveness of LLM-based anonymization in reducing these biases. Findings indicate that while anonymization reduces certain biases, particularly gender bias, the degree of effectiveness varies across models and bias types. Notably, Llama 3.1 405B exhibited the lowest ov...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Moreover, our methodology of comparing anonymized and non-anonymized data reveals a novel approach to assessing inherent biases in LLMs beyond recruitment applications</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">As large language models (LLMs) become integral to recruitment processes, concerns about AI-induced bias have intensified</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.16927v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.16927v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Beatty, K. Masanthia, T. Kaphol, and N. Sethi, &quot;Revealing Hidden Bias in AI: Lessons from Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.16927v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444262928')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="native vs non-native language prompting: a comparative analysis" data-keywords="nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.07054v2" target="_blank">Native vs Non-Native Language Prompting: A Comparative Analysis</a>
                            </h3>
                            <p class="card-authors">Mohamed Bayan Kmainasi, Rakif Khan, Ali Ezzat Shahroor et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4444265280">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) have shown remarkable abilities in different fields, including standard Natural Language Processing (NLP) tasks. To elicit knowledge from LLMs, prompts play a key role, consisting of natural language instructions. Most open and closed source LLMs are trained on available labeled and unlabeled resources--digital content such as text, images, audio, and videos. Hence, these models have better knowledge for high-resourced languages but struggle with low-resourced languages. Since prompts play a crucial role in understanding their capabilities, the language used for pr...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large language models (LLMs) have shown remarkable abilities in different fields, including standard Natural Language Processing (NLP) tasks</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.07054v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.07054v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. B. Kmainasi, R. Khan, A. E. Shahroor, B. Bendou, M. Hasanain, and F. Alam, &quot;Native vs Non-Native Language Prompting: A Comparative Analysis,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.07054v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444265280')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="pragmatic competence evaluation of large language models for the korean language" data-keywords="gpt language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.12675v2" target="_blank">Pragmatic Competence Evaluation of Large Language Models for the Korean Language</a>
                            </h3>
                            <p class="card-authors">Dojun Park, Jiwoo Lee, Hyeyun Jeong et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444264512">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Benchmarks play a significant role in the current evaluation of Large Language Models (LLMs), yet they often overlook the models&#x27; abilities to capture the nuances of human language, primarily focusing on evaluating embedded knowledge and technical skills. To address this gap, our study evaluates how well LLMs understand context-dependent expressions from a pragmatic standpoint, specifically in Korean. We use both Multiple-Choice Questions (MCQs) for automatic evaluation and Open-Ended Questions (OEQs) assessed by human experts. Our results show that GPT-4 leads with scores of 81.11 in MCQs and...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To address this gap, our study evaluates how well LLMs understand context-dependent expressions from a pragmatic standpoint, specifically in Korean</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Benchmarks play a significant role in the current evaluation of Large Language Models (LLMs), yet they often overlook the models&#x27; abilities to capture the nuances of human language, primarily focusing on evaluating embedded knowledge and technical skills</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.12675v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.12675v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Park, J. Lee, H. Jeong, S. Park, and S. Lee, &quot;Pragmatic Competence Evaluation of Large Language Models for the Korean Language,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.12675v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444264512')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cv" data-title="integrating large language models into a tri-modal architecture for automated depression classification on the daic-woz" data-keywords="lstm gpt ros language model cs.cv cs.ai cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computer Vision</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.19340v5" target="_blank">Integrating Large Language Models into a Tri-Modal Architecture for Automated Depression Classification on the DAIC-WOZ</a>
                            </h3>
                            <p class="card-authors">Santosh V. Patapati</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Lstm</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4444261104">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Major Depressive Disorder (MDD) is a pervasive mental health condition that affects 300 million people worldwide. This work presents a novel, BiLSTM-based tri-modal model-level fusion architecture for the binary classification of depression from clinical interview recordings. The proposed architecture incorporates Mel Frequency Cepstral Coefficients, Facial Action Units, and uses a two-shot learning based GPT-4 model to process text data. This is the first work to incorporate large language models into a multi-modal architecture for this task. It achieves impressive results on the DAIC-WOZ AVE...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">It achieves impressive results on the DAIC-WOZ AVEC 2016 Challenge cross-validation split and Leave-One-Subject-Out cross-validation split, surpassing all baseline models and multiple state-of-the-art models</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This work presents a novel, BiLSTM-based tri-modal model-level fusion architecture for the binary classification of depression from clinical interview recordings</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.19340v5" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.19340v5" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. V. Patapati, &quot;Integrating Large Language Models into a Tri-Modal Architecture for Automated Depression Classification on the DAIC-WOZ,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.19340v5')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444261104')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="the generation gap: exploring age bias in the value systems of large language models" data-keywords="ros nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2404.08760v4" target="_blank">The Generation Gap: Exploring Age Bias in the Value Systems of Large Language Models</a>
                            </h3>
                            <p class="card-authors">Siyang Liu, Trish Maturi, Bowen Yi et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444261440">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We explore the alignment of values in Large Language Models (LLMs) with specific age groups, leveraging data from the World Value Survey across thirteen categories. Through a diverse set of prompts tailored to ensure response robustness, we find a general inclination of LLM values towards younger demographics, especially when compared to the US population. Although a general inclination can be observed, we also found that this inclination toward younger groups can be different across different value categories. Additionally, we explore the impact of incorporating age identity information in pr...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Additionally, we explore the impact of incorporating age identity information in prompts and observe challenges in mitigating value discrepancies with different age cohorts</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We explore the alignment of values in Large Language Models (LLMs) with specific age groups, leveraging data from the World Value Survey across thirteen categories</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2404.08760v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2404.08760v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Liu, T. Maturi, B. Yi, S. Shen, and R. Mihalcea, &quot;The Generation Gap: Exploring Age Bias in the Value Systems of Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2404.08760v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444261440')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="automatic generation of question hints for mathematics problems using large language models in educational technology" data-keywords="gpt imu language model cs.cl cs.ai" data-themes="E M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2411.03495v1" target="_blank">Automatic Generation of Question Hints for Mathematics Problems using Large Language Models in Educational Technology</a>
                            </h3>
                            <p class="card-authors">Junior Cedric Tonga, Benjamin Clement, Pierre-Yves Oudeyer</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Imu</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444266048">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The automatic generation of hints by Large Language Models (LLMs) within Intelligent Tutoring Systems (ITSs) has shown potential to enhance student learning. However, generating pedagogically sound hints that address student misconceptions and adhere to specific educational objectives remains challenging. This work explores using LLMs (GPT-4o and Llama-3-8B-instruct) as teachers to generate effective hints for students simulated through LLMs (GPT-3.5-turbo, Llama-3-8B-Instruct, or Mistral-7B-instruct-v0.3) tackling math exercises designed for human high-school students, and designed using cogn...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present here the study of several dimensions: 1) identifying error patterns made by simulated students on secondary-level math exercises; 2) developing various prompts for GPT-4o as a teacher and evaluating their effectiveness in generating hints that enable simulated students to self-correct; and 3) testing the best-performing prompts, based on their ability to produce relevant hints and facilitate error correction, with Llama-3-8B-Instruct as the teacher, allowing for a performance comparison with GPT-4o</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Also the problem-solving and response revision capabilities of the LLMs as students, particularly GPT-3.5-turbo, improved significantly after receiving hints, especially at lower temperature settings</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The automatic generation of hints by Large Language Models (LLMs) within Intelligent Tutoring Systems (ITSs) has shown potential to enhance student learning</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2411.03495v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2411.03495v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. C. Tonga, B. Clement, and P. Oudeyer, &quot;Automatic Generation of Question Hints for Mathematics Problems using Large Language Models in Educational Technology,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2411.03495v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444266048')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="understanding survey paper taxonomy about large language models via graph representation learning" data-keywords="language model cs.cl cs.ai cs.ir" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.10409v1" target="_blank">Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning</a>
                            </h3>
                            <p class="card-authors">Jun Zhuang, Casey Kennington</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.IR</span></div>

                            <div class="card-details" id="details-4444266432">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">As new research on Large Language Models (LLMs) continues, it is difficult to keep up with new research and models. To help researchers synthesize the new research many have written survey papers, but even those have become numerous. In this paper, we develop a method to automatically assign survey papers to a taxonomy. We collect the metadata of 144 LLM survey papers and explore three paradigms to classify papers within the taxonomy. Our work indicates that leveraging graph structure information on co-category graphs can significantly outperform the language models in two paradigms; pre-train...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we develop a method to automatically assign survey papers to a taxonomy</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">As new research on Large Language Models (LLMs) continues, it is difficult to keep up with new research and models</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">As new research on Large Language Models (LLMs) continues, it is difficult to keep up with new research and models</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.10409v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.10409v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Zhuang, and C. Kennington, &quot;Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.10409v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444266432')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ne" data-title="evolutionary computation in the era of large language model: survey and roadmap" data-keywords="optimization ros language model cs.ne cs.ai cs.cl" data-themes="E M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Neural & Evolutionary</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.10034v3" target="_blank">Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap</a>
                            </h3>
                            <p class="card-authors">Xingyu Wu, Sheng-hao Wu, Jibin Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Optimization</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.NE</span></div>

                            <div class="card-details" id="details-4444263888">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride towards artificial general intelligence. The interplay between LLMs and evolutionary algorithms (EAs), despite differing in objectives and methodologies, share a common pursuit of applicability in complex problems. Meanwhile, EA can provide an optimization framework for LLM&#x27;s further enhancement under black-box settings, empowering LLM with flexible global search capacities. On the other hand, the abundant domain knowledge inhere...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">The interplay between LLMs and evolutionary algorithms (EAs), despite differing in objectives and methodologies, share a common pursuit of applicability in complex problems. The identified challenges and future directions offer guidance for researchers and practitioners to unlock the full potential of this innovative collaboration in propelling advancements in optimization and artificial intelligence</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large language models (LLMs) have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride towards artificial general intelligence</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.10034v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.10034v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. Wu, S. Wu, J. Wu, L. Feng, and K. C. Tan, &quot;Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.10034v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444263888')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="conditional and modal reasoning in large language models" data-keywords="ros language model cs.cl cs.ai cs.lo" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.17169v4" target="_blank">Conditional and Modal Reasoning in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Wesley H. Holliday, Matthew Mandelkern, Cedegao E. Zhang</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4444265808">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The reasoning abilities of large language models (LLMs) are the topic of a growing body of research in AI and cognitive science. In this paper, we probe the extent to which twenty-nine LLMs are able to distinguish logically correct inferences from logically fallacious ones. We focus on inference patterns involving conditionals (e.g., &#x27;If Ann has a queen, then Bob has a jack&#x27;) and epistemic modals (e.g., &#x27;Ann might have an ace&#x27;, &#x27;Bob must have a king&#x27;). These inferences have been of special interest to logicians, philosophers, and linguists, since they play a central role in the fundamental hum...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">These results highlight gaps in basic logical reasoning in today&#x27;s LLMs.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The reasoning abilities of large language models (LLMs) are the topic of a growing body of research in AI and cognitive science</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.17169v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.17169v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. H. Holliday, M. Mandelkern, and C. E. Zhang, &quot;Conditional and Modal Reasoning in Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.17169v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444265808')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="facilitating large language model russian adaptation with learned embedding propagation" data-keywords="gpt language model cs.cl cs.ai" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.21140v1" target="_blank">Facilitating large language model Russian adaptation with Learned Embedding Propagation</a>
                            </h3>
                            <p class="card-authors">Mikhail Tikhomirov, Daniil Chernyshev</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4444266240">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Rapid advancements of large language model (LLM) technologies led to the introduction of powerful open-source instruction-tuned LLMs that have the same text generation quality as the state-of-the-art counterparts such as GPT-4. While the emergence of such models accelerates the adoption of LLM technologies in sensitive-information environments the authors of such models don not disclose the training data necessary for replication of the results thus making the achievements model-exclusive. Since those open-source models are also multilingual this in turn reduces the benefits of training a lang...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address the limitations and cut the costs of the language adaptation pipeline we propose Learned Embedding Propagation (LEP)</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To address the limitations and cut the costs of the language adaptation pipeline we propose Learned Embedding Propagation (LEP)</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Rapid advancements of large language model (LLM) technologies led to the introduction of powerful open-source instruction-tuned LLMs that have the same text generation quality as the state-of-the-art counterparts such as GPT-4</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.21140v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.21140v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Tikhomirov, and D. Chernyshev, &quot;Facilitating large language model Russian adaptation with Learned Embedding Propagation,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.21140v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444266240')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="emergent world models and latent variable estimation in chess-playing language models" data-keywords="gpt language model cs.lg cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Machine Learning</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.15498v2" target="_blank">Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models</a>
                            </h3>
                            <p class="card-authors">Adam Karvonen</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444262064">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Language models have shown unprecedented capabilities, sparking debate over the source of their performance. Is it merely the outcome of learning syntactic patterns and surface level statistics, or do they extract semantics and a world model from the text? Prior work by Li et al. investigated this by training a GPT model on synthetic, randomly generated Othello games and found that the model learned an internal representation of the board state. We extend this work into the more complex domain of chess, training on real games and investigating our model&#x27;s internal representations using linear ...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Language models have shown unprecedented capabilities, sparking debate over the source of their performance</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.15498v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.15498v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Karvonen, &quot;Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.15498v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444262064')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="state-of-the-art in robot learning for multi-robot collaboration: a comprehensive survey" data-keywords="robot multi-robot cs.ro cs.ai" data-themes="S E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Multi-Agent Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.11822v1" target="_blank">State-of-the-art in Robot Learning for Multi-Robot Collaboration: A Comprehensive Survey</a>
                            </h3>
                            <p class="card-authors">Bin Wu, C Steve Suh</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">CS.RO</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4444264320">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">With the continuous breakthroughs in core technology, the dawn of large-scale integration of robotic systems into daily human life is on the horizon. Multi-robot systems (MRS) built on this foundation are undergoing drastic evolution. The fusion of artificial intelligence technology with robot hardware is seeing broad application possibilities for MRS. This article surveys the state-of-the-art of robot learning in the context of Multi-Robot Cooperation (MRC) of recent. Commonly adopted robot learning methods (or frameworks) that are inspired by humans and animals are reviewed and their advanta...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Commonly adopted robot learning methods (or frameworks) that are inspired by humans and animals are reviewed and their advantages and disadvantages are discussed along with the associated technical challenges</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Commonly adopted robot learning methods (or frameworks) that are inspired by humans and animals are reviewed and their advantages and disadvantages are discussed along with the associated technical challenges</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.11822v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.11822v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. Wu, and C. S. Suh, &quot;State-of-the-art in Robot Learning for Multi-Robot Collaboration: A Comprehensive Survey,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.11822v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444264320')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="deep reinforcement learning for decentralized multi-robot control: a dqn approach to robustness and information integration" data-keywords="reinforcement learning robot multi-robot control optimization imu cs.ro cs.ma" data-themes="S R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-skip">‚è≠Ô∏è Skip</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.11339v1" target="_blank">Deep Reinforcement Learning for Decentralized Multi-Robot Control: A DQN Approach to Robustness and Information Integration</a>
                            </h3>
                            <p class="card-authors">Bin Wu, C Steve Suh</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Control</span></div>

                            <div class="card-details" id="details-4445407696">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The superiority of Multi-Robot Systems (MRS) in various complex environments is unquestionable. However, in complex situations such as search and rescue, environmental monitoring, and automated production, robots are often required to work collaboratively without a central control unit. This necessitates an efficient and robust decentralized control mechanism to process local information and guide the robots&#x27; behavior. In this work, we propose a new decentralized controller design method that utilizes the Deep Q-Network (DQN) algorithm from deep reinforcement learning, aimed at improving the i...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose a new decentralized controller design method that utilizes the Deep Q-Network (DQN) algorithm from deep reinforcement learning, aimed at improving the integration of local information and robustness of multi-robot systems</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In this work, we propose a new decentralized controller design method that utilizes the Deep Q-Network (DQN) algorithm from deep reinforcement learning, aimed at improving the integration of local information and robustness of multi-robot systems</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.11339v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.11339v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. Wu, and C. S. Suh, &quot;Deep Reinforcement Learning for Decentralized Multi-Robot Control: A DQN Approach to Robustness and Information Integration,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.11339v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445407696')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="real-time polygonal semantic mapping for humanoid robot stair climbing" data-keywords="diffusion robot humanoid planning cs.ro cs.cv" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2411.01919v1" target="_blank">Real-Time Polygonal Semantic Mapping for Humanoid Robot Stair Climbing</a>
                            </h3>
                            <p class="card-authors">Teng Bin, Jianming Yao, Tin Lun Lam et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Diffusion</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Planning</span></div>

                            <div class="card-details" id="details-4445407648">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We present a novel algorithm for real-time planar semantic mapping tailored for humanoid robots navigating complex terrains such as staircases. Our method is adaptable to any odometry input and leverages GPU-accelerated processes for planar extraction, enabling the rapid generation of globally consistent semantic maps. We utilize an anisotropic diffusion filter on depth images to effectively minimize noise from gradient jumps while preserving essential edge details, enhancing normal vector images&#x27; accuracy and smoothness. Both the anisotropic diffusion and the RANSAC-based plane extraction pro...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present a novel algorithm for real-time planar semantic mapping tailored for humanoid robots navigating complex terrains such as staircases</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We present a novel algorithm for real-time planar semantic mapping tailored for humanoid robots navigating complex terrains such as staircases</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2411.01919v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2411.01919v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. Bin, J. Yao, T. L. Lam, and T. Zhang, &quot;Real-Time Polygonal Semantic Mapping for Humanoid Robot Stair Climbing,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2411.01919v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445407648')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="design, calibration, and control of compliant force-sensing gripping pads for humanoid robots" data-keywords="robot humanoid control cs.ro eess.sy" data-themes="M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.20969v1" target="_blank">Design, Calibration, and Control of Compliant Force-sensing Gripping Pads for Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Yuanfeng Han, Boren Jiang, Gregory S. Chirikjian</p>
                            <div class="theme-tags"><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Control</span><span class="keyword-tag">CS.RO</span></div>

                            <div class="card-details" id="details-4445408704">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper introduces a pair of low-cost, light-weight and compliant force-sensing gripping pads used for manipulating box-like objects with smaller-sized humanoid robots. These pads measure normal gripping forces and center of pressure (CoP). A calibration method is developed to improve the CoP measurement accuracy. A hybrid force-alignment-position control framework is proposed to regulate the gripping forces and to ensure the surface alignment between the grippers and the object. Limit surface theory is incorporated as a contact friction modeling approach to determine the magnitude of gripp...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">A calibration method is developed to improve the CoP measurement accuracy</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.20969v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.20969v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Han, B. Jiang, and G. S. Chirikjian, &quot;Design, Calibration, and Control of Compliant Force-sensing Gripping Pads for Humanoid Robots,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.20969v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445408704')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="generalization of heterogeneous multi-robot policies via awareness and communication of capabilities" data-keywords="reinforcement learning robot multi-agent multi-robot coordination cs.ro cs.ma" data-themes="S R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Multi-Agent Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.13127v1" target="_blank">Generalization of Heterogeneous Multi-Robot Policies via Awareness and Communication of Capabilities</a>
                            </h3>
                            <p class="card-authors">Pierce Howell, Max Rudolph, Reza Torbati et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Agent</span><span class="keyword-tag">Multi-Robot</span></div>

                            <div class="card-details" id="details-4445407504">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recent advances in multi-agent reinforcement learning (MARL) are enabling impressive coordination in heterogeneous multi-robot teams. However, existing approaches often overlook the challenge of generalizing learned policies to teams of new compositions, sizes, and robots. While such generalization might not be important in teams of virtual agents that can retrain policies on-demand, it is pivotal in multi-robot systems that are deployed in the real-world and must readily adapt to inevitable changes. As such, multi-robot policies must remain robust to team changes -- an ability we call adaptiv...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We demonstrate that shared decentralized policies, that enable robots to be both aware of and communicate their capabilities, can achieve adaptive teaming by implicitly capturing the fundamental relationship between collective capabilities and effective coordination</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, existing approaches often overlook the challenge of generalizing learned policies to teams of new compositions, sizes, and robots</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">However, existing approaches often overlook the challenge of generalizing learned policies to teams of new compositions, sizes, and robots</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.13127v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.13127v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Howell, M. Rudolph, R. Torbati, K. Fu, and H. Ravichandar, &quot;Generalization of Heterogeneous Multi-Robot Policies via Awareness and Communication of Capabilities,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.13127v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445407504')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="multi-scenario reasoning: unlocking cognitive autonomy in humanoid robots for multimodal understanding" data-keywords="robot humanoid planning simulation ros imu cs.ro cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.20429v4" target="_blank">Multi-Scenario Reasoning: Unlocking Cognitive Autonomy in Humanoid Robots for Multimodal Understanding</a>
                            </h3>
                            <p class="card-authors">Libo Wang</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Planning</span><span class="keyword-tag">Simulation</span></div>

                            <div class="card-details" id="details-4445408896">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">To improve the cognitive autonomy of humanoid robots, this research proposes a multi-scenario reasoning architecture to solve the technical shortcomings of multi-modal understanding in this field. It draws on simulation based experimental design that adopts multi-modal synthesis (visual, auditory, tactile) and builds a simulator &quot;Maha&quot; to perform the experiment. The findings demonstrate the feasibility of this architecture in multimodal data. It provides reference experience for the exploration of cross-modal interaction strategies for humanoid robots in dynamic environments. In addition, mult...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">To improve the cognitive autonomy of humanoid robots, this research proposes a multi-scenario reasoning architecture to solve the technical shortcomings of multi-modal understanding in this field</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.20429v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.20429v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Wang, &quot;Multi-Scenario Reasoning: Unlocking Cognitive Autonomy in Humanoid Robots for Multimodal Understanding,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.20429v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445408896')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="multi-robot rendezvous in unknown environment with limited communication" data-keywords="robot multi-robot simulation imu cs.ro" data-themes="S R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Multi-Agent Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.08345v2" target="_blank">Multi-Robot Rendezvous in Unknown Environment with Limited Communication</a>
                            </h3>
                            <p class="card-authors">Kun Song, Gaoming Chen, Wenhang Liu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Simulation</span><span class="keyword-tag">Imu</span></div>

                            <div class="card-details" id="details-4445408656">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Rendezvous aims at gathering all robots at a specific location, which is an important collaborative behavior for multi-robot systems. However, in an unknown environment, it is challenging to achieve rendezvous. Previous researches mainly focus on special scenarios where communication is not allowed and each robot executes a random searching strategy, which is highly time-consuming, especially in large-scale environments. In this work, we focus on rendezvous in unknown environments where communication is available. We divide this task into two steps: rendezvous based environment exploration wit...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Then, a rendezvous point selection algorithm based on the merged topological map is proposed for efficient rendezvous for multi-robot systems</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.08345v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.08345v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Song, G. Chen, W. Liu, and Z. Xiong, &quot;Multi-Robot Rendezvous in Unknown Environment with Limited Communication,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.08345v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445408656')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="robot see robot do: imitating articulated object manipulation with monocular 4d reconstruction" data-keywords="robot manipulation planning optimization ros cs.ro cs.cv" data-themes="I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.18121v1" target="_blank">Robot See Robot Do: Imitating Articulated Object Manipulation with Monocular 4D Reconstruction</a>
                            </h3>
                            <p class="card-authors">Justin Kerr, Chung Min Kim, Mingxuan Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Manipulation</span><span class="keyword-tag">Planning</span><span class="keyword-tag">Optimization</span></div>

                            <div class="card-details" id="details-4445409088">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humans can learn to manipulate new objects by simply watching others; providing robots with the ability to learn from such demonstrations would enable a natural interface specifying new behaviors. This work develops Robot See Robot Do (RSRD), a method for imitating articulated object manipulation from a single monocular RGB human demonstration given a single static multi-view object scan. We first propose 4D Differentiable Part Models (4D-DPM), a method for recovering 3D part motion from a monocular video with differentiable rendering. This analysis-by-synthesis approach uses part-centric feat...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This work develops Robot See Robot Do (RSRD), a method for imitating articulated object manipulation from a single monocular RGB human demonstration given a single static multi-view object scan</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.18121v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.18121v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Kerr et al., &quot;Robot See Robot Do: Imitating Articulated Object Manipulation with Monocular 4D Reconstruction,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.18121v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445409088')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="generalizable humanoid manipulation with 3d diffusion policies" data-keywords="diffusion robot humanoid manipulation lidar cs.ro cs.cv cs.lg" data-themes="S M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.10803v3" target="_blank">Generalizable Humanoid Manipulation with 3D Diffusion Policies</a>
                            </h3>
                            <p class="card-authors">Yanjie Ze, Zixuan Chen, Wenhao Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Diffusion</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Manipulation</span></div>

                            <div class="card-details" id="details-4445407216">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots capable of autonomous operation in diverse environments have long been a goal for roboticists. However, autonomous manipulation by humanoid robots has largely been restricted to one specific scene, primarily due to the difficulty of acquiring generalizable skills and the expensiveness of in-the-wild humanoid robot data. In this work, we build a real-world robotic system to address this challenging problem. Our system is mainly an integration of 1) a whole-upper-body robotic teleoperation system to acquire human-like robot data, 2) a 25-DoF humanoid robot platform with a height-...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, autonomous manipulation by humanoid robots has largely been restricted to one specific scene, primarily due to the difficulty of acquiring generalizable skills and the expensiveness of in-the-wild humanoid robot data. In this work, we build a real-world robotic system to address this challenging problem</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Our system is mainly an integration of 1) a whole-upper-body robotic teleoperation system to acquire human-like robot data, 2) a 25-DoF humanoid robot platform with a height-adjustable cart and a 3D LiDAR sensor, and 3) an improved 3D Diffusion Policy learning algorithm for humanoid robots to learn from noisy human data</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.10803v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.10803v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Ze et al., &quot;Generalizable Humanoid Manipulation with 3D Diffusion Policies,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.10803v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445407216')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="humanplus: humanoid shadowing and imitation from humans" data-keywords="reinforcement learning robot humanoid perception control simulation camera imu cs.ro cs.ai" data-themes="L I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2406.10454v1" target="_blank">HumanPlus: Humanoid Shadowing and Imitation from Humans</a>
                            </h3>
                            <p class="card-authors">Zipeng Fu, Qingqing Zhao, Qi Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Perception</span></div>

                            <div class="card-details" id="details-4445419408">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">One of the key arguments for building robots that have similar form factors to human beings is that we can leverage the massive human data for training. Yet, doing so has remained challenging in practice due to the complexities in humanoid perception and control, lingering physical gaps between humanoids and humans in morphologies and actuation, and lack of a data pipeline for humanoids to learn autonomous skills from egocentric vision. In this paper, we introduce a full-stack system for humanoids to learn motion and autonomous skills from human data. We first train a low-level policy in simul...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we introduce a full-stack system for humanoids to learn motion and autonomous skills from human data</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Yet, doing so has remained challenging in practice due to the complexities in humanoid perception and control, lingering physical gaps between humanoids and humans in morphologies and actuation, and lack of a data pipeline for humanoids to learn autonomous skills from egocentric vision</p></div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2406.10454v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2406.10454v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Fu, Q. Zhao, Q. Wu, G. Wetzstein, and C. Finn, &quot;HumanPlus: Humanoid Shadowing and Imitation from Humans,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2406.10454v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445419408')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="harmon: whole-body motion generation of humanoid robots from language descriptions" data-keywords="robot humanoid imu language model cs.ro cs.ai" data-themes="S E L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.12773v1" target="_blank">Harmon: Whole-Body Motion Generation of Humanoid Robots from Language Descriptions</a>
                            </h3>
                            <p class="card-authors">Zhenyu Jiang, Yuqi Xie, Jinhan Li et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Imu</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4445407984">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots, with their human-like embodiment, have the potential to integrate seamlessly into human environments. Critical to their coexistence and cooperation with humans is the ability to understand natural language communications and exhibit human-like behaviors. This work focuses on generating diverse whole-body motions for humanoid robots from language descriptions. We leverage human motion priors from extensive human motion datasets to initialize humanoid motions and employ the commonsense reasoning capabilities of Vision Language Models (VLMs) to edit and refine these motions. Our ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Our approach demonstrates the capability to produce natural, expressive, and text-aligned humanoid motions, validated through both simulated and real-world experiments</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We leverage human motion priors from extensive human motion datasets to initialize humanoid motions and employ the commonsense reasoning capabilities of Vision Language Models (VLMs) to edit and refine these motions</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.12773v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.12773v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Jiang, Y. Xie, J. Li, Y. Yuan, Y. Zhu, and Y. Zhu, &quot;Harmon: Whole-Body Motion Generation of Humanoid Robots from Language Descriptions,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.12773v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445407984')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="humanoid-gym: reinforcement learning for humanoid robot with zero-shot sim2real transfer" data-keywords="reinforcement learning robot humanoid locomotion simulation mujoco isaac imu cs.ro cs.ai" data-themes="S L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2404.05695v2" target="_blank">Humanoid-Gym: Reinforcement Learning for Humanoid Robot with Zero-Shot Sim2Real Transfer</a>
                            </h3>
                            <p class="card-authors">Xinyang Gu, Yen-Jen Wang, Jianyu Chen</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4445419264">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid-Gym is an easy-to-use reinforcement learning (RL) framework based on Nvidia Isaac Gym, designed to train locomotion skills for humanoid robots, emphasizing zero-shot transfer from simulation to the real-world environment. Humanoid-Gym also integrates a sim-to-sim framework from Isaac Gym to Mujoco that allows users to verify the trained policies in different physical simulations to ensure the robustness and generalization of the policies. This framework is verified by RobotEra&#x27;s XBot-S (1.2-meter tall humanoid robot) and XBot-L (1.65-meter tall humanoid robot) in a real-world environm...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Humanoid-Gym is an easy-to-use reinforcement learning (RL) framework based on Nvidia Isaac Gym, designed to train locomotion skills for humanoid robots, emphasizing zero-shot transfer from simulation to the real-world environment</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2404.05695v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2404.05695v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. Gu, Y. Wang, and J. Chen, &quot;Humanoid-Gym: Reinforcement Learning for Humanoid Robot with Zero-Shot Sim2Real Transfer,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2404.05695v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445419264')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.hc" data-title="users&#x27; perception on appropriateness of robotic coaching assistant&#x27;s disclosure behaviors" data-keywords="robot perception cs.hc" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-skip">‚è≠Ô∏è Skip</span></div>
                            <span class="card-source">arXiv ¬∑ Human-Computer Interaction</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.10550v1" target="_blank">Users&#x27; Perception on Appropriateness of Robotic Coaching Assistant&#x27;s Disclosure Behaviors</a>
                            </h3>
                            <p class="card-authors">Atikkhan Faridkhan Nilgar, Manuel Dietrich, Kristof Van Laerhoven</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Perception</span><span class="keyword-tag">CS.HC</span></div>

                            <div class="card-details" id="details-4445408320">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Social robots have emerged as valuable contributors to individuals&#x27; well-being coaching. Notably, their integration into long-term human coaching trials shows particular promise, emphasizing a complementary role alongside human coaches rather than outright replacement. In this context, robots serve as supportive entities during coaching sessions, offering insights based on their knowledge about users&#x27; well-being and activity. Traditionally, such insights have been gathered through methods like written self-reports or wearable data visualizations. However, the disclosure of people&#x27;s information...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Traditionally, such insights have been gathered through methods like written self-reports or wearable data visualizations</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.10550v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.10550v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. F. Nilgar, M. Dietrich, and K. V. Laerhoven, &quot;Users&#x27; Perception on Appropriateness of Robotic Coaching Assistant&#x27;s Disclosure Behaviors,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.10550v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445408320')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="cohrt: a collaboration system for human-robot teamwork" data-keywords="robot manipulation coordination simulation ros imu cs.ro cs.hc" data-themes="S R M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.08504v1" target="_blank">CoHRT: A Collaboration System for Human-Robot Teamwork</a>
                            </h3>
                            <p class="card-authors">Sujan Sarker, Haley N. Green, Mohammad Samin Yasar et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-R">Multi-Robot</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Manipulation</span><span class="keyword-tag">Coordination</span><span class="keyword-tag">Simulation</span></div>

                            <div class="card-details" id="details-4445420032">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Collaborative robots are increasingly deployed alongside humans in factories, hospitals, schools, and other domains to enhance teamwork and efficiency. Systems that seamlessly integrate humans and robots into cohesive teams for coordinated and efficient task execution are needed, enabling studies on how robot collaboration policies affect team performance and teammates&#x27; perceived fairness, trust, and safety. Such a system can also be utilized to study the impact of a robot&#x27;s normative behavior on team collaboration. Additionally, it allows for investigation into how the legibility and predicta...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">CoHRT utilizes a server-client-based architecture, a vision-based system to track task environments, and a simple interface for team action coordination</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.08504v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.08504v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Sarker, H. N. Green, M. S. Yasar, and T. Iqbal, &quot;CoHRT: A Collaboration System for Human-Robot Teamwork,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.08504v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445420032')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="learning humanoid locomotion over challenging terrain" data-keywords="reinforcement learning transformer robot humanoid locomotion control ros cs.ro cs.lg" data-themes="S L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.03654v1" target="_blank">Learning Humanoid Locomotion over Challenging Terrain</a>
                            </h3>
                            <p class="card-authors">Ilija Radosavovic, Sarthak Kamat, Trevor Darrell et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Transformer</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span></div>

                            <div class="card-details" id="details-4445419984">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots can, in principle, use their legs to go almost anywhere. Developing controllers capable of traversing diverse terrains, however, remains a considerable challenge. Classical controllers are hard to generalize broadly while the learning-based methods have primarily focused on gentle terrains. Here, we present a learning-based approach for blind humanoid locomotion capable of traversing challenging natural and man-made terrain. Our method uses a transformer model to predict the next action based on the history of proprioceptive observations and actions. The model is first pre-trai...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Here, we present a learning-based approach for blind humanoid locomotion capable of traversing challenging natural and man-made terrain</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Developing controllers capable of traversing diverse terrains, however, remains a considerable challenge</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Classical controllers are hard to generalize broadly while the learning-based methods have primarily focused on gentle terrains</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.03654v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.03654v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'I. Radosavovic, S. Kamat, T. Darrell, and J. Malik, &quot;Learning Humanoid Locomotion over Challenging Terrain,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.03654v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445419984')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="robot vulnerability and the elicitation of user empathy" data-keywords="robot cs.ro cs.hc" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-skip">‚è≠Ô∏è Skip</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.02684v1" target="_blank">Robot Vulnerability and the Elicitation of User Empathy</a>
                            </h3>
                            <p class="card-authors">Morten Roed Frederiksen, Katrin Fischer, Maja Matariƒá</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">CS.RO</span><span class="keyword-tag">CS.HC</span></div>

                            <div class="card-details" id="details-4445419744">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper describes a between-subjects Amazon Mechanical Turk study (n = 220) that investigated how a robot&#x27;s affective narrative influences its ability to elicit empathy in human observers. We first conducted a pilot study to develop and validate the robot&#x27;s affective narratives. Then, in the full study, the robot used one of three different affective narrative strategies (funny, sad, neutral) while becoming less functional at its shopping task over the course of the interaction. As the functionality of the robot degraded, participants were repeatedly asked if they were willing to help the r...</p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.02684v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.02684v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. R. Frederiksen, K. Fischer, and M. Matariƒá, &quot;Robot Vulnerability and the Elicitation of User Empathy,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.02684v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445419744')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="learning bipedal walking for humanoid robots in challenging environments with obstacle avoidance" data-keywords="reinforcement learning robot humanoid bipedal locomotion cs.ro cs.lg" data-themes="L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-skip">‚è≠Ô∏è Skip</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.08212v1" target="_blank">Learning Bipedal Walking for Humanoid Robots in Challenging Environments with Obstacle Avoidance</a>
                            </h3>
                            <p class="card-authors">Marwan Hamze, Mitsuharu Morisawa, Eiichi Yoshida</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span></div>

                            <div class="card-details" id="details-4445409040">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Deep reinforcement learning has seen successful implementations on humanoid robots to achieve dynamic walking. However, these implementations have been so far successful in simple environments void of obstacles. In this paper, we aim to achieve bipedal locomotion in an environment where obstacles are present using a policy-based reinforcement learning. By adding simple distance reward terms to a state of art reward function that can achieve basic bipedal locomotion, the trained policy succeeds in navigating the robot towards the desired destination without colliding with the obstacles along th...</p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.08212v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.08212v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Hamze, M. Morisawa, and E. Yoshida, &quot;Learning Bipedal Walking for Humanoid Robots in Challenging Environments with Obstacle Avoidance,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.08212v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445409040')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="mr.cap: multi-robot joint control and planning for object transport" data-keywords="robot multi-robot planning control optimization imu cs.ro" data-themes="R M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.11634v1" target="_blank">MR.CAP: Multi-Robot Joint Control and Planning for Object Transport</a>
                            </h3>
                            <p class="card-authors">Hussein Ali Jaafar, Cheng-Hao Kao, Sajad Saeedi</p>
                            <div class="theme-tags"><span class="theme-badge theme-R">Multi-Robot</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Planning</span><span class="keyword-tag">Control</span></div>

                            <div class="card-details" id="details-4445407744">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">With the recent influx in demand for multi-robot systems throughout industry and academia, there is an increasing need for faster, robust, and generalizable path planning algorithms. Similarly, given the inherent connection between control algorithms and multi-robot path planners, there is in turn an increased demand for fast, efficient, and robust controllers. We propose a scalable joint path planning and control algorithm for multi-robot systems with constrained behaviours based on factor graph optimization. We demonstrate our algorithm on a series of hardware and simulated experiments. Our ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a scalable joint path planning and control algorithm for multi-robot systems with constrained behaviours based on factor graph optimization</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">With the recent influx in demand for multi-robot systems throughout industry and academia, there is an increasing need for faster, robust, and generalizable path planning algorithms</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.11634v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.11634v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. A. Jaafar, C. Kao, and S. Saeedi, &quot;MR.CAP: Multi-Robot Joint Control and Planning for Object Transport,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.11634v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445407744')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="human impression of humanoid robots mirroring social cues" data-keywords="robot humanoid perception control ros imu cs.ro cs.hc" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-skip">‚è≠Ô∏è Skip</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.12076v1" target="_blank">Human Impression of Humanoid Robots Mirroring Social Cues</a>
                            </h3>
                            <p class="card-authors">Di Fu, Fares Abawi, Philipp Allgeuer et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Perception</span><span class="keyword-tag">Control</span></div>

                            <div class="card-details" id="details-4444265472">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Mirroring non-verbal social cues such as affect or movement can enhance human-human and human-robot interactions in the real world. The robotic platforms and control methods also impact people&#x27;s perception of human-robot interaction. However, limited studies have compared robot imitation across different platforms and control methods. Our research addresses this gap by conducting two experiments comparing people&#x27;s perception of affective mirroring between the iCub and Pepper robots and movement mirroring between vision-based iCub control and Inertial Measurement Unit (IMU)-based iCub control. ...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Our research addresses this gap by conducting two experiments comparing people&#x27;s perception of affective mirroring between the iCub and Pepper robots and movement mirroring between vision-based iCub control and Inertial Measurement Unit (IMU)-based iCub control</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The robotic platforms and control methods also impact people&#x27;s perception of human-robot interaction</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.12076v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.12076v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Fu, F. Abawi, P. Allgeuer, and S. Wermter, &quot;Human Impression of Humanoid Robots Mirroring Social Cues,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.12076v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444265472')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="a collaborative robot-assisted manufacturing assembly process" data-keywords="robot cs.ro" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.05306v1" target="_blank">A Collaborative Robot-Assisted Manufacturing Assembly Process</a>
                            </h3>
                            <p class="card-authors">Miguel Neves, Laura Duarte, Pedro Neto</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">CS.RO</span></div>

                            <div class="card-details" id="details-4445420992">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">An effective human-robot collaborative process results in the reduction of the operator&#x27;s workload, promoting a more efficient, productive, safer and less error-prone working environment. However, the implementation of collaborative robots in industry is still challenging. In this work, we compare manual and robot-assisted assembly processes to evaluate the effectiveness of collaborative robots while featuring different modes of operation (coexistence, cooperation and collaboration). Results indicate an improvement in ergonomic conditions and ease of execution without substantially compromisin...</p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.05306v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.05306v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Neves, L. Duarte, and P. Neto, &quot;A Collaborative Robot-Assisted Manufacturing Assembly Process,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.05306v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445420992')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="humanoid robot rhp friends: seamless combination of autonomous and teleoperated tasks in a nursing context" data-keywords="robot humanoid manipulation cs.ro cs.hc" data-themes="S I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.20770v2" target="_blank">Humanoid Robot RHP Friends: Seamless Combination of Autonomous and Teleoperated Tasks in a Nursing Context</a>
                            </h3>
                            <p class="card-authors">Mehdi Benallegue, Guillaume Lorthioir, Antonin Dallard et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Manipulation</span><span class="keyword-tag">CS.RO</span></div>

                            <div class="card-details" id="details-4445419792">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper describes RHP Friends, a social humanoid robot developed to enable assistive robotic deployments in human-coexisting environments. As a use-case application, we present its potential use in nursing by extending its capabilities to operate human devices and tools according to the task and by enabling remote assistance operations. To meet a wide variety of tasks and situations in environments designed by and for humans, we developed a system that seamlessly integrates the slim and lightweight robot and several technologies: locomanipulation, multi-contact motion, teleoperation, and ob...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">As a use-case application, we present its potential use in nursing by extending its capabilities to operate human devices and tools according to the task and by enabling remote assistance operations</p></div>
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.20770v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.20770v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Benallegue et al., &quot;Humanoid Robot RHP Friends: Seamless Combination of Autonomous and Teleoperated Tasks in a Nursing Context,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.20770v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445419792')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="play everywhere: a temporal logic based game environment independent approach for playing soccer with robots" data-keywords="robot cs.ro cs.ai" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.12628v1" target="_blank">Play Everywhere: A Temporal Logic based Game Environment Independent Approach for Playing Soccer with Robots</a>
                            </h3>
                            <p class="card-authors">Vincenzo Suriani, Emanuele Musumeci, Daniele Nardi et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">CS.RO</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4445420464">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Robots playing soccer often rely on hard-coded behaviors that struggle to generalize when the game environment change. In this paper, we propose a temporal logic based approach that allows robots&#x27; behaviors and goals to adapt to the semantics of the environment. In particular, we present a hierarchical representation of soccer in which the robot selects the level of operation based on the perceived semantic characteristics of the environment, thus modifying dynamically the set of rules and goals to apply. The proposed approach enables the robot to operate in unstructured environments, just as ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose a temporal logic based approach that allows robots&#x27; behaviors and goals to adapt to the semantics of the environment</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In this paper, we propose a temporal logic based approach that allows robots&#x27; behaviors and goals to adapt to the semantics of the environment</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.12628v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.12628v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'V. Suriani, E. Musumeci, D. Nardi, and D. D. Bloisi, &quot;Play Everywhere: A Temporal Logic based Game Environment Independent Approach for Playing Soccer with Robots,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.12628v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445420464')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="ros2swarm - a ros 2 package for swarm robot behaviors" data-keywords="robot swarm multi-robot control ros cs.ro cs.ma" data-themes="R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Multi-Agent Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.02438v1" target="_blank">ROS2swarm - A ROS 2 Package for Swarm Robot Behaviors</a>
                            </h3>
                            <p class="card-authors">Tanja Katharina Kaiser, Marian Johannes Begemann, Tavia Plattenteich et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Swarm</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Control</span></div>

                            <div class="card-details" id="details-4445418352">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Developing reusable software for mobile robots is still challenging. Even more so for swarm robots, despite the desired simplicity of the robot controllers. Prototyping and experimenting are difficult due to the multi-robot setting and often require robot-robot communication. Also, the diversity of swarm robot hardware platforms increases the need for hardware-independent software concepts. The main advantages of the commonly used robot software architecture ROS 2 are modularity and platform independence. We propose a new ROS 2 package, ROS2swarm, for applications of swarm robotics that provid...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a new ROS 2 package, ROS2swarm, for applications of swarm robotics that provides a library of ready-to-use swarm behavioral primitives</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Prototyping and experimenting are difficult due to the multi-robot setting and often require robot-robot communication</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The main advantages of the commonly used robot software architecture ROS 2 are modularity and platform independence</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.02438v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.02438v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. K. Kaiser, M. J. Begemann, T. Plattenteich, L. Schilling, G. Schildbach, and H. Hamann, &quot;ROS2swarm - A ROS 2 Package for Swarm Robot Behaviors,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.02438v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445418352')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="what am i? evaluating the effect of language fluency and task competency on the perception of a social robot" data-keywords="robot perception cs.ro" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-skip">‚è≠Ô∏è Skip</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.11085v1" target="_blank">What Am I? Evaluating the Effect of Language Fluency and Task Competency on the Perception of a Social Robot</a>
                            </h3>
                            <p class="card-authors">Shahira Ali, Haley N. Green, Tariq Iqbal</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Perception</span><span class="keyword-tag">CS.RO</span></div>

                            <div class="card-details" id="details-4445408992">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recent advancements in robot capabilities have enabled them to interact with people in various human-social environments (HSEs). In many of these environments, the perception of the robot often depends on its capabilities, e.g., task competency, language fluency, etc. To enable fluent human-robot interaction (HRI) in HSEs, it is crucial to understand the impact of these capabilities on the perception of the robot. Although many works have investigated the effects of various robot capabilities on the robot&#x27;s perception separately, in this paper, we present a large-scale HRI study (n = 60) to in...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Although many works have investigated the effects of various robot capabilities on the robot&#x27;s perception separately, in this paper, we present a large-scale HRI study (n = 60) to investigate the combined impact of both language fluency and task competency on the perception of a robot</p></div>
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.11085v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.11085v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Ali, H. N. Green, and T. Iqbal, &quot;What Am I? Evaluating the Effect of Language Fluency and Task Competency on the Perception of a Social Robot,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.11085v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445408992')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="human-humanoid robots cross-embodiment behavior-skill transfer using decomposed adversarial learning from demonstration" data-keywords="robot humanoid manipulation ros cs.ro cs.ai" data-themes="I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.15166v1" target="_blank">Human-Humanoid Robots Cross-Embodiment Behavior-Skill Transfer Using Decomposed Adversarial Learning from Demonstration</a>
                            </h3>
                            <p class="card-authors">Junjia Liu, Zhuo Li, Minghao Yu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Manipulation</span><span class="keyword-tag">Ros</span></div>

                            <div class="card-details" id="details-4445419360">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots are envisioned as embodied intelligent agents capable of performing a wide range of human-level loco-manipulation tasks, particularly in scenarios requiring strenuous and repetitive labor. However, learning these skills is challenging due to the high degrees of freedom of humanoid robots, and collecting sufficient training data for humanoid is a laborious process. Given the rapid introduction of new humanoid platforms, a cross-embodiment framework that allows generalizable skill transfer is becoming increasingly critical. To address this, we propose a transferable framework tha...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address this, we propose a transferable framework that reduces the data bottleneck by using a unified digital human model as a common prototype and bypassing the need for re-training on every new robot platform</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Given the rapid introduction of new humanoid platforms, a cross-embodiment framework that allows generalizable skill transfer is becoming increasingly critical</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.15166v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.15166v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Liu et al., &quot;Human-Humanoid Robots Cross-Embodiment Behavior-Skill Transfer Using Decomposed Adversarial Learning from Demonstration,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.15166v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445419360')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="humanoid robots and humanoid ai: review, perspectives and directions" data-keywords="robot humanoid language model cs.ro" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.15775v3" target="_blank">Humanoid Robots and Humanoid AI: Review, Perspectives and Directions</a>
                            </h3>
                            <p class="card-authors">Longbing Cao</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.RO</span></div>

                            <div class="card-details" id="details-4445421472">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In the approximately century-long journey of robotics, humanoid robots made their debut around six decades ago. While current humanoids bear human-like appearances, none have embodied true humaneness, remaining distant from achieving human-like to human-level intelligence. The rapid recent advancements in generative AI and (multimodal) large language models have further reignited and escalated interest in humanoids towards real-time, interactive, and multimodal designs and applications, such as fostering humanoid workers, advisers, educators, medical professionals, caregivers, and receptionist...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The rapid recent advancements in generative AI and (multimodal) large language models have further reignited and escalated interest in humanoids towards real-time, interactive, and multimodal designs and applications, such as fostering humanoid workers, advisers, educators, medical professionals, caregivers, and receptionists</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.15775v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.15775v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Cao, &quot;Humanoid Robots and Humanoid AI: Review, Perspectives and Directions,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.15775v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445421472')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="okami: teaching humanoid robots manipulation skills through single video imitation" data-keywords="robot humanoid manipulation ros cs.ro cs.ai cs.cv" data-themes="S I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.11792v1" target="_blank">OKAMI: Teaching Humanoid Robots Manipulation Skills through Single Video Imitation</a>
                            </h3>
                            <p class="card-authors">Jinhan Li, Yifeng Zhu, Yuqi Xie et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Manipulation</span><span class="keyword-tag">Ros</span></div>

                            <div class="card-details" id="details-4445406880">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We study the problem of teaching humanoid robots manipulation skills by imitating from single video demonstrations. We introduce OKAMI, a method that generates a manipulation plan from a single RGB-D video and derives a policy for execution. At the heart of our approach is object-aware retargeting, which enables the humanoid robot to mimic the human motions in an RGB-D video while adjusting to different object locations during deployment. OKAMI uses open-world vision models to identify task-relevant objects and retarget the body motions and hand poses separately. Our experiments show that OKAM...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce OKAMI, a method that generates a manipulation plan from a single RGB-D video and derives a policy for execution</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">We study the problem of teaching humanoid robots manipulation skills by imitating from single video demonstrations</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We introduce OKAMI, a method that generates a manipulation plan from a single RGB-D video and derives a policy for execution</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.11792v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.11792v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Li et al., &quot;OKAMI: Teaching Humanoid Robots Manipulation Skills through Single Video Imitation,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.11792v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445406880')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="ce-mrs: contrastive explanations for multi-robot systems" data-keywords="robot multi-robot planning cs.ro cs.hc cs.ma" data-themes="E I R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Multi-Agent Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.08408v1" target="_blank">CE-MRS: Contrastive Explanations for Multi-Robot Systems</a>
                            </h3>
                            <p class="card-authors">Ethan Schneider, Daniel Wu, Devleena Das et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Planning</span><span class="keyword-tag">CS.RO</span></div>

                            <div class="card-details" id="details-4445421184">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">As the complexity of multi-robot systems grows to incorporate a greater number of robots, more complex tasks, and longer time horizons, the solutions to such problems often become too complex to be fully intelligible to human users. In this work, we introduce an approach for generating natural language explanations that justify the validity of the system&#x27;s solution to the user, or else aid the user in correcting any errors that led to a suboptimal system solution. Toward this goal, we first contribute a generalizable formalism of contrastive explanations for multi-robot systems, and then intro...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we introduce an approach for generating natural language explanations that justify the validity of the system&#x27;s solution to the user, or else aid the user in correcting any errors that led to a suboptimal system solution</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">As the complexity of multi-robot systems grows to incorporate a greater number of robots, more complex tasks, and longer time horizons, the solutions to such problems often become too complex to be fully intelligible to human users</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In this work, we introduce an approach for generating natural language explanations that justify the validity of the system&#x27;s solution to the user, or else aid the user in correcting any errors that led to a suboptimal system solution</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.08408v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.08408v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'E. Schneider, D. Wu, D. Das, and S. Chernova, &quot;CE-MRS: Contrastive Explanations for Multi-Robot Systems,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.08408v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445421184')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="learning multi-modal whole-body control for real-world humanoid robots" data-keywords="robot humanoid control simulation imu cs.ro cs.ai" data-themes="S E L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.07295v3" target="_blank">Learning Multi-Modal Whole-Body Control for Real-World Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Pranay Dugar, Aayam Shrestha, Fangzhou Yu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Control</span><span class="keyword-tag">Simulation</span></div>

                            <div class="card-details" id="details-4445420176">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The foundational capabilities of humanoid robots should include robustly standing, walking, and mimicry of whole and partial-body motions. This work introduces the Masked Humanoid Controller (MHC), which supports all of these capabilities by tracking target trajectories over selected subsets of humanoid state variables while ensuring balance and robustness against disturbances. The MHC is trained in simulation using a carefully designed curriculum that imitates partially masked motions from a library of behaviors spanning standing, walking, optimized reference trajectories, re-targeted video c...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This work introduces the Masked Humanoid Controller (MHC), which supports all of these capabilities by tracking target trajectories over selected subsets of humanoid state variables while ensuring balance and robustness against disturbances</p></div>
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.07295v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.07295v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Dugar, A. Shrestha, F. Yu, B. v. Marum, and A. Fern, &quot;Learning Multi-Modal Whole-Body Control for Real-World Humanoid Robots,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.07295v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445420176')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="online dnn-driven nonlinear mpc for stylistic humanoid robot walking with step adjustment" data-keywords="neural network robot humanoid locomotion control optimization cs.ro" data-themes="L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.07849v1" target="_blank">Online DNN-driven Nonlinear MPC for Stylistic Humanoid Robot Walking with Step Adjustment</a>
                            </h3>
                            <p class="card-authors">Giulio Romualdi, Paolo Maria Viceconte, Lorenzo Moretti et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>

                            <div class="card-details" id="details-4445408128">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents a three-layered architecture that enables stylistic locomotion with online contact location adjustment. Our method combines an autoregressive Deep Neural Network (DNN) acting as a trajectory generation layer with a model-based trajectory adjustment and trajectory control layers. The DNN produces centroidal and postural references serving as an initial guess and regularizer for the other layers. Being the DNN trained on human motion capture data, the resulting robot motion exhibits locomotion patterns, resembling a human walking style. The trajectory adjustment layer utilize...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents a three-layered architecture that enables stylistic locomotion with online contact location adjustment</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper presents a three-layered architecture that enables stylistic locomotion with online contact location adjustment</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.07849v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.07849v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Romualdi et al., &quot;Online DNN-driven Nonlinear MPC for Stylistic Humanoid Robot Walking with Step Adjustment,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.07849v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445408128')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="does chatgpt and whisper make humanoid robots more relatable?" data-keywords="gpt robot humanoid language model cs.ro cs.hc" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.07095v1" target="_blank">Does ChatGPT and Whisper Make Humanoid Robots More Relatable?</a>
                            </h3>
                            <p class="card-authors">Xiaohui Chen, Katherine Luo, Trevor Gee et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4445420320">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots are designed to be relatable to humans for applications such as customer support and helpdesk services. However, many such systems, including Softbank&#x27;s Pepper, fall short because they fail to communicate effectively with humans. The advent of Large Language Models (LLMs) shows the potential to solve the communication barrier for humanoid robotics. This paper outlines the comparison of different Automatic Speech Recognition (ASR) APIs, the integration of Whisper ASR and ChatGPT with the Pepper robot and the evaluation of the system (Pepper-GPT) tested by 15 human users. The com...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">It is proved that while some problems still need to be overcome, such as the robot&#x27;s multilingual ability and facial tracking capacity, users generally responded positively to the system, feeling like talking to an actual human.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The advent of Large Language Models (LLMs) shows the potential to solve the communication barrier for humanoid robotics</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.07095v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.07095v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. Chen, K. Luo, T. Gee, and M. Nejati, &quot;Does ChatGPT and Whisper Make Humanoid Robots More Relatable?,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.07095v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445420320')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="guiding collision-free humanoid multi-contact locomotion using convex kinematic relaxations and dynamic optimization" data-keywords="robot humanoid navigation planning optimization simulation imu cs.ro" data-themes="L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.08335v1" target="_blank">Guiding Collision-Free Humanoid Multi-Contact Locomotion using Convex Kinematic Relaxations and Dynamic Optimization</a>
                            </h3>
                            <p class="card-authors">Carlos Gonzalez, Luis Sentis</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Navigation</span><span class="keyword-tag">Planning</span></div>

                            <div class="card-details" id="details-4445420752">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots rely on multi-contact planners to navigate a diverse set of environments, including those that are unstructured and highly constrained. To synthesize stable multi-contact plans within a reasonable time frame, most planners assume statically stable motions or rely on reduced order models. However, these approaches can also render the problem infeasible in the presence of large obstacles or when operating near kinematic and dynamic limits. To that end, we propose a new multi-contact framework that leverages recent advancements in relaxing collision-free path planning into a conve...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To that end, we propose a new multi-contact framework that leverages recent advancements in relaxing collision-free path planning into a convex optimization problem, extending it to be applicable to humanoid multi-contact navigation</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, these approaches can also render the problem infeasible in the presence of large obstacles or when operating near kinematic and dynamic limits. To that end, we propose a new multi-contact framework that leverages recent advancements in relaxing collision-free path planning into a convex optimization problem, extending it to be applicable to humanoid multi-contact navigation</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">To synthesize stable multi-contact plans within a reasonable time frame, most planners assume statically stable motions or rely on reduced order models</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.08335v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.08335v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Gonzalez, and L. Sentis, &quot;Guiding Collision-Free Humanoid Multi-Contact Locomotion using Convex Kinematic Relaxations and Dynamic Optimization,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.08335v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445420752')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="advancing behavior generation in mobile robotics through high-fidelity procedural simulations" data-keywords="reinforcement learning robot navigation multi-agent simulation imu nlp cs.ro cs.hc" data-themes="S E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-skip">‚è≠Ô∏è Skip</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.16818v1" target="_blank">Advancing Behavior Generation in Mobile Robotics through High-Fidelity Procedural Simulations</a>
                            </h3>
                            <p class="card-authors">Victor A. Kich, Jair A. Bottega, Raul Steinmetz et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Navigation</span><span class="keyword-tag">Multi-Agent</span></div>

                            <div class="card-details" id="details-4445420560">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper introduces YamaS, a simulator integrating Unity3D Engine with Robotic Operating System for robot navigation research and aims to facilitate the development of both Deep Reinforcement Learning (Deep-RL) and Natural Language Processing (NLP). It supports single and multi-agent configurations with features like procedural environment generation, RGB vision, and dynamic obstacle navigation. Unique to YamaS is its ability to construct single and multi-agent environments, as well as generating agent&#x27;s behaviour through textual descriptions. The simulator&#x27;s fidelity is underscored by compa...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This fusion establishes YamaS as a versatile and valuable tool for the development and testing of autonomous systems, contributing to the fields of robot simulation and AI-driven training methodologies.</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.16818v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.16818v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'V. A. Kich, J. A. Bottega, R. Steinmetz, R. B. Grando, A. Yorozu, and A. Ohya, &quot;Advancing Behavior Generation in Mobile Robotics through High-Fidelity Procedural Simulations,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.16818v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445420560')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="human robot pacing mismatch" data-keywords="robot navigation planning cs.ro cs.hc" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.01542v1" target="_blank">Human Robot Pacing Mismatch</a>
                            </h3>
                            <p class="card-authors">Muchen Sun, Peter Trautman, Todd Murphey</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Navigation</span><span class="keyword-tag">Planning</span><span class="keyword-tag">CS.RO</span></div>

                            <div class="card-details" id="details-4445407888">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">A widely accepted explanation for robots planning overcautious or overaggressive trajectories alongside human is that the crowd density exceeds a threshold such that all feasible trajectories are considered unsafe -- the freezing robot problem. However, even with low crowd density, the robot&#x27;s navigation performance could still drop drastically when in close proximity to human. In this work, we argue that a broader cause of suboptimal navigation performance near human is due to the robot&#x27;s misjudgement for the human&#x27;s willingness (flexibility) to share space with others, particularly when the ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We demonstrate the advantage of distribution space coupling through an anecdotal case study and discuss the future directions of solving human robot pacing mismatch.</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">A widely accepted explanation for robots planning overcautious or overaggressive trajectories alongside human is that the crowd density exceeds a threshold such that all feasible trajectories are considered unsafe -- the freezing robot problem</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We show that the necessary condition for solving pacing mismatch is to model the evolution of both the robot and the human&#x27;s flexibility during decision making, a strategy called distribution space modeling</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.01542v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.01542v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Sun, P. Trautman, and T. Murphey, &quot;Human Robot Pacing Mismatch,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.01542v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445407888')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="innate motivation for robot swarms by minimizing surprise: from simple simulations to real-world experiments" data-keywords="robot swarm multi-robot control simulation imu cs.ro cs.ma cs.ne" data-themes="S R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.02579v1" target="_blank">Innate Motivation for Robot Swarms by Minimizing Surprise: From Simple Simulations to Real-World Experiments</a>
                            </h3>
                            <p class="card-authors">Tanja Katharina Kaiser, Heiko Hamann</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Swarm</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Control</span></div>

                            <div class="card-details" id="details-4445421424">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Applications of large-scale mobile multi-robot systems can be beneficial over monolithic robots because of higher potential for robustness and scalability. Developing controllers for multi-robot systems is challenging because the multitude of interactions is hard to anticipate and difficult to model. Automatic design using machine learning or evolutionary robotics seem to be options to avoid that challenge, but bring the challenge of designing reward or fitness functions. Generic reward and fitness functions seem unlikely to exist and task-specific rewards often have undesired side effects. Ap...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Our approach to innate motivation is to minimize surprise, which we implement by maximizing the accuracy of the swarm robot&#x27;s sensor predictions using neuroevolution</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Developing controllers for multi-robot systems is challenging because the multitude of interactions is hard to anticipate and difficult to model. Automatic design using machine learning or evolutionary robotics seem to be options to avoid that challenge, but bring the challenge of designing reward or fitness functions</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Developing controllers for multi-robot systems is challenging because the multitude of interactions is hard to anticipate and difficult to model</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.02579v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.02579v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. K. Kaiser, and H. Hamann, &quot;Innate Motivation for Robot Swarms by Minimizing Surprise: From Simple Simulations to Real-World Experiments,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.02579v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445421424')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="automatically designing robot swarms in environments populated by other robots: an experiment in robot shepherding" data-keywords="robot swarm coordination control cs.ro" data-themes="R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Multi-Agent Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2404.18221v1" target="_blank">Automatically designing robot swarms in environments populated by other robots: an experiment in robot shepherding</a>
                            </h3>
                            <p class="card-authors">David Garz√≥n Ramos, Mauro Birattari</p>
                            <div class="theme-tags"><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Swarm</span><span class="keyword-tag">Coordination</span><span class="keyword-tag">Control</span></div>

                            <div class="card-details" id="details-4445419312">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Automatic design is a promising approach to realizing robot swarms. Given a mission to be performed by the swarm, an automatic method produces the required control software for the individual robots. Automatic design has concentrated on missions that a swarm can execute independently, interacting only with a static environment and without the involvement of other active entities. In this paper, we investigate the design of robot swarms that perform their mission by interacting with other robots that populate their environment. We frame our research within robot shepherding: the problem of usin...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">We frame our research within robot shepherding: the problem of using a small group of robots, the shepherds, to coordinate a relatively larger group, the sheep</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Automatic design is a promising approach to realizing robot swarms</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2404.18221v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2404.18221v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. G. Ramos, and M. Birattari, &quot;Automatically designing robot swarms in environments populated by other robots: an experiment in robot shepherding,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2404.18221v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445419312')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="ukf-based sensor fusion for joint-torque sensorless humanoid robots" data-keywords="robot humanoid control sensor fusion cs.ro" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.18380v1" target="_blank">UKF-Based Sensor Fusion for Joint-Torque Sensorless Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Ines Sorrentino, Giulio Romualdi, Daniele Pucci</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Control</span><span class="keyword-tag">Sensor Fusion</span></div>

                            <div class="card-details" id="details-4444957440">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper proposes a novel sensor fusion based on Unscented Kalman Filtering for the online estimation of joint-torques of humanoid robots without joint-torque sensors. At the feature level, the proposed approach considers multimodal measurements (e.g. currents, accelerations, etc.) and non-directly measurable effects, such as external contacts, thus leading to joint torques readily usable in control architectures for human-robot interaction. The proposed sensor fusion can also integrate distributed, non-collocated force/torque sensors, thus being a flexible framework with respect to the unde...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Results demonstrate that our method achieves low root mean square errors in torque tracking, ranging from 0.05 Nm to 2.5 Nm, even in the presence of external contacts.</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">At the feature level, the proposed approach considers multimodal measurements (e.g</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.18380v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.18380v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'I. Sorrentino, G. Romualdi, and D. Pucci, &quot;UKF-Based Sensor Fusion for Joint-Torque Sensorless Humanoid Robots,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.18380v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444957440')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="bi-level motion imitation for humanoid robots" data-keywords="robot humanoid optimization simulation imu imitation learning cs.ro" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.01968v1" target="_blank">Bi-Level Motion Imitation for Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Wenshuai Zhao, Yi Zhao, Joni Pajarinen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">Simulation</span></div>

                            <div class="card-details" id="details-4444947168">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Imitation learning from human motion capture (MoCap) data provides a promising way to train humanoid robots. However, due to differences in morphology, such as varying degrees of joint freedom and force limits, exact replication of human behaviors may not be feasible for humanoid robots. Consequently, incorporating physically infeasible MoCap data in training datasets can adversely affect the performance of the robot policy. To address this issue, we propose a bi-level optimization-based imitation learning framework that alternates between optimizing both the robot policy and the target MoCap ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address this issue, we propose a bi-level optimization-based imitation learning framework that alternates between optimizing both the robot policy and the target MoCap data</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To address this issue, we propose a bi-level optimization-based imitation learning framework that alternates between optimizing both the robot policy and the target MoCap data</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">To address this issue, we propose a bi-level optimization-based imitation learning framework that alternates between optimizing both the robot policy and the target MoCap data</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.01968v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.01968v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. Zhao, Y. Zhao, J. Pajarinen, and M. Muehlebach, &quot;Bi-Level Motion Imitation for Humanoid Robots,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.01968v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444947168')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section year-section" data-year="2023">
                <h2 class="section-header">üìÖ 2023 <span class="section-count">(56 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.hc" data-title="exploring large language models to facilitate variable autonomy for human-robot teaming" data-keywords="transformer gpt robot multi-robot language model cs.hc cs.ai cs.ro" data-themes="E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Human-Computer Interaction</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.07214v3" target="_blank">Exploring Large Language Models to Facilitate Variable Autonomy for Human-Robot Teaming</a>
                            </h3>
                            <p class="card-authors">Younes Lakhnati, Max Pascher, Jens Gerken</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Transformer</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Robot</span></div>

                            <div class="card-details" id="details-4441927072">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In a rapidly evolving digital landscape autonomous tools and robots are becoming commonplace. Recognizing the significance of this development, this paper explores the integration of Large Language Models (LLMs) like Generative pre-trained transformer (GPT) into human-robot teaming environments to facilitate variable autonomy through the means of verbal human-robot communication. In this paper, we introduce a novel framework for such a GPT-powered multi-robot testbed environment, based on a Unity Virtual Reality (VR) setting. This system allows users to interact with robot agents through natur...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we introduce a novel framework for such a GPT-powered multi-robot testbed environment, based on a Unity Virtual Reality (VR) setting</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">By means of OpenAI&#x27;s function calling, we bridge the gap between unstructured natural language input and structure robot actions</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Recognizing the significance of this development, this paper explores the integration of Large Language Models (LLMs) like Generative pre-trained transformer (GPT) into human-robot teaming environments to facilitate variable autonomy through the means of verbal human-robot communication</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.07214v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.07214v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Lakhnati, M. Pascher, and J. Gerken, &quot;Exploring Large Language Models to Facilitate Variable Autonomy for Human-Robot Teaming,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.07214v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441927072')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="making large language models better reasoners with alignment" data-keywords="language model cs.cl cs.ai cs.lg" data-themes="E I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.02144v1" target="_blank">Making Large Language Models Better Reasoners with Alignment</a>
                            </h3>
                            <p class="card-authors">Peiyi Wang, Lei Li, Liang Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.LG</span></div>

                            <div class="card-details" id="details-4441921120">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Reasoning is a cognitive process of using evidence to reach a sound conclusion. The reasoning capability is essential for large language models (LLMs) to serve as the brain of the artificial general intelligence agent. Recent studies reveal that fine-tuning LLMs on data with the chain of thought (COT) reasoning process can significantly enhance their reasoning capabilities. However, we find that the fine-tuned LLMs suffer from an \textit{Assessment Misalignment} problem, i.e., they frequently assign higher scores to subpar COTs, leading to potential limitations in their reasoning abilities. To...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address this problem, we introduce an \textit{Alignment Fine-Tuning (AFT)} paradigm, which involves three steps: 1) fine-tuning LLMs with COT training data; 2) generating multiple COT responses for each question, and categorizing them into positive and negative ones based on whether they achieve the correct answer; 3) calibrating the scores of positive and negative responses given by LLMs with a novel constraint alignment loss</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, we find that the fine-tuned LLMs suffer from an \textit{Assessment Misalignment} problem, i.e., they frequently assign higher scores to subpar COTs, leading to potential limitations in their reasoning abilities. To address this problem, we introduce an \textit{Alignment Fine-Tuning (AFT)} paradigm, which involves three steps: 1) fine-tuning LLMs with COT training data; 2) generating multiple COT responses for each question, and categorizing them into positive and negative ones based on whether they achieve the correct answer; 3) calibrating the scores of positive and negative responses given by LLMs with a novel constraint alignment loss</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The reasoning capability is essential for large language models (LLMs) to serve as the brain of the artificial general intelligence agent</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.02144v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.02144v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Wang et al., &quot;Making Large Language Models Better Reasoners with Alignment,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.02144v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441921120')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="beneath the surface: unveiling harmful memes with multimodal reasoning distilled from large language models" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.05434v1" target="_blank">Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models</a>
                            </h3>
                            <p class="card-authors">Hongzhan Lin, Ziyang Luo, Jing Ma et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4441698992">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The age of social media is rife with memes. Understanding and detecting harmful memes pose a significant challenge due to their implicit meaning that is not explicitly conveyed through the surface text and image. However, existing harmful meme detection approaches only recognize superficial harm-indicative signals in an end-to-end classification manner but ignore in-depth cognition of the meme text and image. In this paper, we attempt to detect harmful memes based on advanced reasoning over the interplay of multimodal information in memes. Inspired by the success of Large Language Models (LLMs...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Then we propose a novel generative framework to learn reasonable thoughts from LLMs for better multimodal fusion and lightweight fine-tuning, which consists of two training stages: 1) Distill multimodal reasoning knowledge from LLMs; and 2) Fine-tune the generative framework to infer harmfulness</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Understanding and detecting harmful memes pose a significant challenge due to their implicit meaning that is not explicitly conveyed through the surface text and image</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">However, existing harmful meme detection approaches only recognize superficial harm-indicative signals in an end-to-end classification manner but ignore in-depth cognition of the meme text and image</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.05434v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.05434v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Lin, Z. Luo, J. Ma, and L. Chen, &quot;Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.05434v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441698992')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="all languages matter: on the multilingual safety of large language models" data-keywords="gpt ros language model cs.cl cs.ai" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2310.00905v2" target="_blank">All Languages Matter: On the Multilingual Safety of Large Language Models</a>
                            </h3>
                            <p class="card-authors">Wenxuan Wang, Zhaopeng Tu, Chang Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4441919536">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Safety lies at the core of developing and deploying large language models (LLMs). However, previous safety benchmarks only concern the safety in one language, e.g. the majority language in the pretraining data such as English. In this work, we build the first multilingual safety benchmark for LLMs, XSafety, in response to the global deployment of LLMs in practice. XSafety covers 14 kinds of commonly used safety issues across 10 languages that span several language families. We utilize XSafety to empirically study the multilingual safety for 4 widely-used LLMs, including both close-API and open...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In addition, we propose several simple and effective prompting methods to improve the multilingual safety of ChatGPT by evoking safety knowledge and improving cross-lingual generalization of safety alignment</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">XSafety covers 14 kinds of commonly used safety issues across 10 languages that span several language families</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Safety lies at the core of developing and deploying large language models (LLMs)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2310.00905v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2310.00905v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. Wang et al., &quot;All Languages Matter: On the Multilingual Safety of Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2310.00905v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441919536')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cv" data-title="a survey on multimodal large language models" data-keywords="gpt language model cs.cv cs.ai cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computer Vision</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2306.13549v4" target="_blank">A Survey on Multimodal Large Language Models</a>
                            </h3>
                            <p class="card-authors">Shukang Yin, Chaoyou Fu, Sirui Zhao et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4441921744">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. I...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">To conclude the paper, we discuss existing challenges and point out promising research directions</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2306.13549v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2306.13549v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Yin et al., &quot;A Survey on Multimodal Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2306.13549v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441921744')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="teal: tokenize and embed all for multi-modal large language models" data-keywords="language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2311.04589v3" target="_blank">TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models</a>
                            </h3>
                            <p class="card-authors">Zhen Yang, Yingxue Zhang, Fandong Meng et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4441927648">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Despite Multi-modal Large Language Models (MM-LLMs) have made exciting strides recently, they are still struggling to efficiently model the interactions among multi-modal inputs and the generation in non-textual modalities. In this work, we propose TEAL (Tokenize and Embed ALl)}, an approach to treat the input from any modality as a token sequence and learn a joint embedding space for all modalities. Specifically, for the input from any modality, TEAL first discretizes it into a token sequence with the off-the-shelf tokenizer and embeds the token sequence into a joint embedding space with a le...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose TEAL (Tokenize and Embed ALl)}, an approach to treat the input from any modality as a token sequence and learn a joint embedding space for all modalities</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Despite Multi-modal Large Language Models (MM-LLMs) have made exciting strides recently, they are still struggling to efficiently model the interactions among multi-modal inputs and the generation in non-textual modalities</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2311.04589v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2311.04589v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Yang, Y. Zhang, F. Meng, and J. Zhou, &quot;TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2311.04589v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441927648')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="large language models in ambulatory devices for home health diagnostics: a case study of sickle cell anemia management" data-keywords="language model cs.cl" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2305.03715v1" target="_blank">Large Language Models in Ambulatory Devices for Home Health Diagnostics: A case study of Sickle Cell Anemia Management</a>
                            </h3>
                            <p class="card-authors">Oluwatosin Ogundare, Subuola Sofolahan</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4441920880">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This study investigates the potential of an ambulatory device that incorporates Large Language Models (LLMs) in cadence with other specialized ML models to assess anemia severity in sickle cell patients in real time. The device would rely on sensor data that measures angiogenic material levels to assess anemia severity, providing real-time information to patients and clinicians to reduce the frequency of vaso-occlusive crises because of the early detection of anemia severity, allowing for timely interventions and potentially reducing the likelihood of serious complications. The main challenges...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">The main challenges in developing such a device are the creation of a reliable non-invasive tool for angiogenic level assessment, a biophysics model and the practical consideration of an LLM communicating with emergency personnel on behalf of an incapacitated patient. A possible system is proposed, and the limitations of this approach are discussed.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This study investigates the potential of an ambulatory device that incorporates Large Language Models (LLMs) in cadence with other specialized ML models to assess anemia severity in sickle cell patients in real time</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2305.03715v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2305.03715v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'O. Ogundare, and S. Sofolahan, &quot;Large Language Models in Ambulatory Devices for Home Health Diagnostics: A case study of Sickle Cell Anemia Management,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2305.03715v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441920880')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="larabench: benchmarking arabic ai with large language models" data-keywords="gpt ros nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2305.14982v2" target="_blank">LAraBench: Benchmarking Arabic AI with Large Language Models</a>
                            </h3>
                            <p class="card-authors">Ahmed Abdelali, Hamdy Mubarak, Shammur Absar Chowdhury et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4441920736">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recent advancements in Large Language Models (LLMs) have significantly influenced the landscape of language and speech research. Despite this progress, these models lack specific benchmarking against state-of-the-art (SOTA) models tailored to particular languages and tasks. LAraBench addresses this gap for Arabic Natural Language Processing (NLP) and Speech Processing tasks, including sequence tagging and content classification across different domains. We utilized models such as GPT-3.5-turbo, GPT-4, BLOOMZ, Jais-13b-chat, Whisper, and USM, employing zero and few-shot learning techniques to t...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">LAraBench addresses this gap for Arabic Natural Language Processing (NLP) and Speech Processing tasks, including sequence tagging and content classification across different domains. Our analysis focused on measuring the performance gap between SOTA models and LLMs</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Recent advancements in Large Language Models (LLMs) have significantly influenced the landscape of language and speech research</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2305.14982v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2305.14982v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Abdelali et al., &quot;LAraBench: Benchmarking Arabic AI with Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2305.14982v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441920736')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="neuron-level knowledge attribution in large language models" data-keywords="attention ros language model cs.cl cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.12141v4" target="_blank">Neuron-Level Knowledge Attribution in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Zeping Yu, Sophia Ananiadou</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4441925056">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Identifying important neurons for final predictions is essential for understanding the mechanisms of large language models. Due to computational constraints, current attribution techniques struggle to operate at neuron level. In this paper, we propose a static method for pinpointing significant neurons. Compared to seven other methods, our approach demonstrates superior performance across three metrics. Additionally, since most static methods typically only identify &quot;value neurons&quot; directly contributing to the final prediction, we propose a method for identifying &quot;query neurons&quot; which activate...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose a static method for pinpointing significant neurons</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Identifying important neurons for final predictions is essential for understanding the mechanisms of large language models</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.12141v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.12141v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Yu, and S. Ananiadou, &quot;Neuron-Level Knowledge Attribution in Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.12141v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441925056')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="factuality challenges in the era of large language models" data-keywords="attention gpt ros language model cs.cl cs.ai cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2310.05189v2" target="_blank">Factuality Challenges in the Era of Large Language Models</a>
                            </h3>
                            <p class="card-authors">Isabelle Augenstein, Timothy Baldwin, Meeyoung Cha et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4441922080">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The emergence of tools based on Large Language Models (LLMs), such as OpenAI&#x27;s ChatGPT, Microsoft&#x27;s Bing Chat, and Google&#x27;s Bard, has garnered immense public attention. These incredibly useful, natural-sounding tools mark significant advances in natural language generation, yet they exhibit a propensity to generate false, erroneous, or misleading content -- commonly referred to as &quot;hallucinations.&quot; Moreover, LLMs can be exploited for malicious applications, such as generating false but credible-sounding content and profiles at scale. This poses a significant challenge to society in terms of th...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">This poses a significant challenge to society in terms of the potential deception of users and the increasing dissemination of inaccurate information</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The emergence of tools based on Large Language Models (LLMs), such as OpenAI&#x27;s ChatGPT, Microsoft&#x27;s Bing Chat, and Google&#x27;s Bard, has garnered immense public attention</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2310.05189v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2310.05189v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'I. Augenstein et al., &quot;Factuality Challenges in the Era of Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2310.05189v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441922080')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="can chatgpt be your personal medical assistant?" data-keywords="gpt language model cs.cl cs.si" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.12006v1" target="_blank">Can ChatGPT be Your Personal Medical Assistant?</a>
                            </h3>
                            <p class="card-authors">Md. Rafiul Biswas, Ashhadul Islam, Zubair Shah et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.SI</span></div>

                            <div class="card-details" id="details-4441920688">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The advanced large language model (LLM) ChatGPT has shown its potential in different domains and remains unbeaten due to its characteristics compared to other LLMs. This study aims to evaluate the potential of using a fine-tuned ChatGPT model as a personal medical assistant in the Arabic language. To do so, this study uses publicly available online questions and answering datasets in Arabic language. There are almost 430K questions and answers for 20 disease-specific categories. GPT-3.5-turbo model was fine-tuned with a portion of this dataset. The performance of this fine-tuned model was eval...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The advanced large language model (LLM) ChatGPT has shown its potential in different domains and remains unbeaten due to its characteristics compared to other LLMs</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.12006v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.12006v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. R. Biswas, A. Islam, Z. Shah, W. Zaghouani, and S. B. Belhaouari, &quot;Can ChatGPT be Your Personal Medical Assistant?,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.12006v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441920688')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="large language models and multimodal retrieval for visual word sense disambiguation" data-keywords="transformer language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2310.14025v1" target="_blank">Large Language Models and Multimodal Retrieval for Visual Word Sense Disambiguation</a>
                            </h3>
                            <p class="card-authors">Anastasia Kritharoula, Maria Lymperaiou, Giorgos Stamou</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Transformer</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4441929040">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Visual Word Sense Disambiguation (VWSD) is a novel challenging task with the goal of retrieving an image among a set of candidates, which better represents the meaning of an ambiguous word within a given context. In this paper, we make a substantial step towards unveiling this interesting task by applying a varying set of approaches. Since VWSD is primarily a text-image retrieval task, we explore the latest transformer-based methods for multimodal retrieval. Additionally, we utilize Large Language Models (LLMs) as knowledge bases to enhance the given phrases and resolve ambiguity related to th...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">We also study VWSD as a unimodal problem by converting to text-to-text and image-to-image retrieval, as well as question-answering (QA), to fully explore the capabilities of relevant models</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In this paper, we make a substantial step towards unveiling this interesting task by applying a varying set of approaches</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2310.14025v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2310.14025v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Kritharoula, M. Lymperaiou, and G. Stamou, &quot;Large Language Models and Multimodal Retrieval for Visual Word Sense Disambiguation,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2310.14025v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441929040')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="open sesame! universal black box jailbreaking of large language models" data-keywords="language model cs.cl cs.cv cs.ne" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.01446v4" target="_blank">Open Sesame! Universal Black Box Jailbreaking of Large Language Models</a>
                            </h3>
                            <p class="card-authors">Raz Lapid, Ron Langberg, Moshe Sipper</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.CV</span><span class="keyword-tag">CS.NE</span></div>

                            <div class="card-details" id="details-4441919008">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs), designed to provide helpful and safe responses, often rely on alignment techniques to align with user intent and social guidelines. Unfortunately, this alignment can be exploited by malicious actors seeking to manipulate an LLM&#x27;s outputs for unintended purposes. In this paper we introduce a novel approach that employs a genetic algorithm (GA) to manipulate LLMs when model architecture and parameters are inaccessible. The GA attack works by optimizing a universal adversarial prompt that -- when combined with a user&#x27;s query -- disrupts the attacked model&#x27;s alignment...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper we introduce a novel approach that employs a genetic algorithm (GA) to manipulate LLMs when model architecture and parameters are inaccessible</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Our novel approach systematically reveals a model&#x27;s limitations and vulnerabilities by uncovering instances where its responses deviate from expected behavior</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large language models (LLMs), designed to provide helpful and safe responses, often rely on alignment techniques to align with user intent and social guidelines</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.01446v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.01446v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Lapid, R. Langberg, and M. Sipper, &quot;Open Sesame! Universal Black Box Jailbreaking of Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.01446v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441919008')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="demystifying instruction mixing for fine-tuning large language models" data-keywords="ros nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.10793v3" target="_blank">Demystifying Instruction Mixing for Fine-tuning Large Language Models</a>
                            </h3>
                            <p class="card-authors">Renxi Wang, Haonan Li, Minghao Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4441918960">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Instruction tuning significantly enhances the performance of large language models (LLMs) across various tasks. However, the procedure to optimizing the mixing of instruction datasets for LLM fine-tuning is still poorly understood. This study categorizes instructions into three primary types: NLP downstream tasks, coding, and general chat. We explore the effects of instruction tuning on different combinations of datasets on LLM performance, and find that certain instruction types are more advantageous for specific applications but can negatively impact other areas. This work provides insights ...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Instruction tuning significantly enhances the performance of large language models (LLMs) across various tasks</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.10793v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.10793v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Wang et al., &quot;Demystifying Instruction Mixing for Fine-tuning Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.10793v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441918960')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="response: emergent analogical reasoning in large language models" data-keywords="gpt ros language model cs.cl cs.ai" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2308.16118v2" target="_blank">Response: Emergent analogical reasoning in large language models</a>
                            </h3>
                            <p class="card-authors">Damian Hodel, Jevin West</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4441918672">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In their recent Nature Human Behaviour paper, &quot;Emergent analogical reasoning in large language models,&quot; (Webb, Holyoak, and Lu, 2023) the authors argue that &quot;large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems.&quot; In this response, we provide counterexamples of the letter string analogies. In our tests, GPT-3 fails to solve simplest variations of the original tasks, whereas human performance remains consistently high across all modified versions. Zero-shot reasoning is an extraordinary claim that requires extraord...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">In their recent Nature Human Behaviour paper, &quot;Emergent analogical reasoning in large language models,&quot; (Webb, Holyoak, and Lu, 2023) the authors argue that &quot;large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems.&quot; In this response, we provide counterexamples of the letter string analogies</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In their recent Nature Human Behaviour paper, &quot;Emergent analogical reasoning in large language models,&quot; (Webb, Holyoak, and Lu, 2023) the authors argue that &quot;large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems.&quot; In this response, we provide counterexamples of the letter string analogies</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2308.16118v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2308.16118v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Hodel, and J. West, &quot;Response: Emergent analogical reasoning in large language models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2308.16118v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441918672')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="using large language models to provide explanatory feedback to human tutors" data-keywords="language model cs.cl cs.ai cs.hc" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2306.15498v1" target="_blank">Using Large Language Models to Provide Explanatory Feedback to Human Tutors</a>
                            </h3>
                            <p class="card-authors">Jionghao Lin, Danielle R. Thomas, Feifei Han et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.HC</span></div>

                            <div class="card-details" id="details-4441920976">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Research demonstrates learners engaging in the process of producing explanations to support their reasoning, can have a positive impact on learning. However, providing learners real-time explanatory feedback often presents challenges related to classification accuracy, particularly in domain-specific environments, containing situationally complex and nuanced responses. We present two approaches for supplying tutors real-time feedback within an online lesson on how to give students effective praise. This work-in-progress demonstrates considerable accuracy in binary classification for corrective...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present two approaches for supplying tutors real-time feedback within an online lesson on how to give students effective praise</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, providing learners real-time explanatory feedback often presents challenges related to classification accuracy, particularly in domain-specific environments, containing situationally complex and nuanced responses</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We present two approaches for supplying tutors real-time feedback within an online lesson on how to give students effective praise</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2306.15498v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2306.15498v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Lin et al., &quot;Using Large Language Models to Provide Explanatory Feedback to Human Tutors,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2306.15498v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441920976')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="wizardlm: empowering large pre-trained language models to follow complex instructions" data-keywords="gpt nlp language model cs.cl cs.ai" data-themes="E L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2304.12244v3" target="_blank">WizardLM: Empowering large pre-trained language models to follow complex instructions</a>
                            </h3>
                            <p class="card-authors">Can Xu, Qingfeng Sun, Kai Zheng et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444261632">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data t...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Training large language models (LLMs) with open-domain instruction following data brings colossal success</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2304.12244v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2304.12244v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Xu et al., &quot;WizardLM: Empowering large pre-trained language models to follow complex instructions,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2304.12244v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444261632')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.lg" data-title="pb-llm: partially binarized large language models" data-keywords="gpt language model cs.lg cs.ai cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Machine Learning</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2310.00034v2" target="_blank">PB-LLM: Partially Binarized Large Language Models</a>
                            </h3>
                            <p class="card-authors">Yuzhang Shang, Zhihang Yuan, Qiang Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4444260768">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper explores network binarization, a radical form of quantization, compressing model weights to a single bit, specifically for Large Language Models (LLMs) compression. Due to previous binarization methods collapsing LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of quantized LLMs. Specifically, our exploration first uncovers the ineffectiveness of naive applications of existing binarization algorithms and highlights the imperative role of salient weights in achieving...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Due to previous binarization methods collapsing LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of quantized LLMs</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper explores network binarization, a radical form of quantization, compressing model weights to a single bit, specifically for Large Language Models (LLMs) compression</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2310.00034v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2310.00034v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Shang, Z. Yuan, Q. Wu, and Z. Dong, &quot;PB-LLM: Partially Binarized Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2310.00034v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444260768')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="prompting and fine-tuning open-sourced large language models for stance classification" data-keywords="ros language model cs.cl cs.ai" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.13734v2" target="_blank">Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification</a>
                            </h3>
                            <p class="card-authors">Iain J. Cruickshank, Lynnette Hui Xian Ng</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4444262640">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Stance classification, the task of predicting the viewpoint of an author on a subject of interest, has long been a focal point of research in domains ranging from social science to machine learning. Current stance detection methods rely predominantly on manual annotation of sentences, followed by training a supervised machine learning model. However, this manual annotation process requires laborious annotation effort, and thus hampers its potential to generalize across different contexts. In this work, we investigate the use of Large Language Models (LLMs) as a stance detection methodology tha...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Current stance detection methods rely predominantly on manual annotation of sentences, followed by training a supervised machine learning model</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.13734v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.13734v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'I. J. Cruickshank, and L. H. X. Ng, &quot;Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.13734v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444262640')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="tidybot: personalized robot assistance with large language models" data-keywords="robot perception planning language model cs.ro cs.ai cs.cl" data-themes="S E I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2305.05658v2" target="_blank">TidyBot: Personalized Robot Assistance with Large Language Models</a>
                            </h3>
                            <p class="card-authors">Jimmy Wu, Rika Antonova, Adam Kan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Perception</span><span class="keyword-tag">Planning</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4444262400">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people&#x27;s preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We also demonstrate our approach on a real-world mobile manipulator called TidyBot, which successfully puts away 85.0% of objects in real-world test scenarios.</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">A key challenge is determining the proper place to put each object, as people&#x27;s preferences can vary greatly depending on personal taste or cultural background</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2305.05658v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2305.05658v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Wu et al., &quot;TidyBot: Personalized Robot Assistance with Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2305.05658v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444262400')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ai" data-title="kglens: towards efficient and effective knowledge probing of large language models with knowledge graphs" data-keywords="simulation ros imu language model cs.ai cs.cl cs.lg" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Artificial Intelligence</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.11539v3" target="_blank">KGLens: Towards Efficient and Effective Knowledge Probing of Large Language Models with Knowledge Graphs</a>
                            </h3>
                            <p class="card-authors">Shangshang Zheng, He Bai, Yizhe Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Simulation</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Imu</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4444262784">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Language Models (LLMs) might hallucinate facts, while curated Knowledge Graph (KGs) are typically factually reliable especially with domain-specific knowledge. Measuring the alignment between KGs and LLMs can effectively probe the factualness and identify the knowledge blind spots of LLMs. However, verifying the LLMs over extensive KGs can be expensive. In this paper, we present KGLens, a Thompson-sampling-inspired framework aimed at effectively and efficiently measuring the alignment between KGs and LLMs. KGLens features a graph-guided question generator for converting KGs into natural ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we present KGLens, a Thompson-sampling-inspired framework aimed at effectively and efficiently measuring the alignment between KGs and LLMs</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large Language Models (LLMs) might hallucinate facts, while curated Knowledge Graph (KGs) are typically factually reliable especially with domain-specific knowledge</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.11539v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.11539v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Zheng, H. Bai, Y. Zhang, Y. Su, X. Niu, and N. Jaitly, &quot;KGLens: Towards Efficient and Effective Knowledge Probing of Large Language Models with Knowledge Graphs,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.11539v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444262784')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.se" data-title="metal: metamorphic testing framework for analyzing large-language model qualities" data-keywords="language model cs.se cs.ai cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ CS.SE</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.06056v1" target="_blank">METAL: Metamorphic Testing Framework for Analyzing Large-Language Model Qualities</a>
                            </h3>
                            <p class="card-authors">Sangwon Hyun, Mingyu Guo, M. Ali Babar</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.SE</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444263072">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large-Language Models (LLMs) have shifted the paradigm of natural language data processing. However, their black-boxed and probabilistic characteristics can lead to potential risks in the quality of outputs in diverse LLM applications. Recent studies have tested Quality Attributes (QAs), such as robustness or fairness, of LLMs by generating adversarial input texts. However, existing studies have limited their coverage of QAs and tasks in LLMs and are difficult to extend. Additionally, these studies have only used one evaluation metric, Attack Success Rate (ASR), to assess the effectiveness of ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a MEtamorphic Testing for Analyzing LLMs (METAL) framework to address these issues by applying Metamorphic Testing (MT) techniques</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, existing studies have limited their coverage of QAs and tasks in LLMs and are difficult to extend. We propose a MEtamorphic Testing for Analyzing LLMs (METAL) framework to address these issues by applying Metamorphic Testing (MT) techniques</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large-Language Models (LLMs) have shifted the paradigm of natural language data processing</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.06056v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.06056v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Hyun, M. Guo, and M. A. Babar, &quot;METAL: Metamorphic Testing Framework for Analyzing Large-Language Model Qualities,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.06056v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444263072')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="speaker attribution in german parliamentary debates with qlora-adapted large language models" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.09902v2" target="_blank">Speaker attribution in German parliamentary debates with QLoRA-adapted large language models</a>
                            </h3>
                            <p class="card-authors">Tobias Bornheim, Niklas Grieger, Patrick Gustav Blaneck et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444262880">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The growing body of political texts opens up new opportunities for rich insights into political dynamics and ideologies but also increases the workload for manual analysis. Automated speaker attribution, which detects who said what to whom in a speech event and is closely related to semantic role labeling, is an important processing step for computational text analysis. We study the potential of the large language model family Llama 2 to automate speaker attribution in German parliamentary debates from 2017-2021. We fine-tune Llama 2 with QLoRA, an efficient training strategy, and observe our ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We fine-tune Llama 2 with QLoRA, an efficient training strategy, and observe our approach to achieve competitive performance in the GermEval 2023 Shared Task On Speaker Attribution in German News Articles and Parliamentary Debates</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We study the potential of the large language model family Llama 2 to automate speaker attribution in German parliamentary debates from 2017-2021</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.09902v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.09902v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. Bornheim, N. Grieger, P. G. Blaneck, and S. Bialonski, &quot;Speaker attribution in German parliamentary debates with QLoRA-adapted large language models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.09902v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444262880')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="eess.as" data-title="acoustic prompt tuning: empowering large language models with audition capabilities" data-keywords="ros language model eess.as" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ EESS.AS</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.00249v2" target="_blank">Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities</a>
                            </h3>
                            <p class="card-authors">Jinhua Liang, Xubo Liu, Wenwu Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">EESS.AS</span></div>

                            <div class="card-details" id="details-4444262448">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The auditory system plays a substantial role in shaping the overall human perceptual experience. While prevailing large language models (LLMs) and visual language models (VLMs) have shown their promise in solving a wide variety of language and vision understanding tasks, only a few of them can be generalised to the audio domain without compromising their domain-specific capability. In this work, we introduce Acoustic Prompt Tuning (APT), a new adapter extending LLMs and VLMs to the audio domain by injecting audio embeddings to the input of LLMs, namely soft prompting. Specifically, APT applies...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we introduce Acoustic Prompt Tuning (APT), a new adapter extending LLMs and VLMs to the audio domain by injecting audio embeddings to the input of LLMs, namely soft prompting</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">While prevailing large language models (LLMs) and visual language models (VLMs) have shown their promise in solving a wide variety of language and vision understanding tasks, only a few of them can be generalised to the audio domain without compromising their domain-specific capability</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.00249v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.00249v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Liang, X. Liu, W. Wang, M. D. Plumbley, H. Phan, and E. Benetos, &quot;Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.00249v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444262448')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="a zero-shot and few-shot study of instruction-finetuned large language models applied to clinical and biomedical tasks" data-keywords="bert gpt nlp language model cs.cl cs.ai cs.lg" data-themes="S E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2307.12114v3" target="_blank">A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks</a>
                            </h3>
                            <p class="card-authors">Yanis Labrak, Mickael Rouvier, Richard Dufour</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Bert</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4444261920">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We evaluate four state-of-the-art instruction-tuned large language models (LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13 real-world clinical and biomedical natural language processing (NLP) tasks in English, such as named-entity recognition (NER), question-answering (QA), relation extraction (RE), etc. Our overall results demonstrate that the evaluated LLMs begin to approach performance of state-of-the-art models in zero- and few-shot scenarios for most tasks, and particularly well for the QA task, even though they have never seen examples from these tasks before. Ho...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We evaluate four state-of-the-art instruction-tuned large language models (LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13 real-world clinical and biomedical natural language processing (NLP) tasks in English, such as named-entity recognition (NER), question-answering (QA), relation extraction (RE), etc</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2307.12114v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2307.12114v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Labrak, M. Rouvier, and R. Dufour, &quot;A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2307.12114v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444261920')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="a variable autonomy approach for an automated weeding platform" data-keywords="robot imu cs.ro" data-themes="S">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2303.05461v1" target="_blank">A Variable Autonomy approach for an Automated Weeding Platform</a>
                            </h3>
                            <p class="card-authors">Ionut Moraru, Tsvetan Zhivkov, Shaun Coutts et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Imu</span><span class="keyword-tag">CS.RO</span></div>

                            <div class="card-details" id="details-4444262208">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Climate change, increase in world population and the war in Ukraine have led nations such as the UK to put a larger focus on food security, while simultaneously trying to halt declines in biodiversity and reduce risks to human health posed by chemically-reliant farming practices. Achieving these goals simultaneously will require novel approaches and accelerating the deployment of Agri-Robotics from the lab and into the field. In this paper we describe the ARWAC robot platform for mechanical weeding. We explain why the mechanical weeding approach is beneficial compared to the use of pesticides ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Thereafter, we present the system design and processing pipeline for generating a course of action for the robot to follow, such that it removes as many weeds as possible</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Achieving these goals simultaneously will require novel approaches and accelerating the deployment of Agri-Robotics from the lab and into the field</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2303.05461v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2303.05461v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'I. Moraru, T. Zhivkov, S. Coutts, D. Li, and E. I. Sklar, &quot;A Variable Autonomy approach for an Automated Weeding Platform,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2303.05461v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444262208')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cv" data-title="evaluation and enhancement of semantic grounding in large vision-language models" data-keywords="ros language model cs.cv cs.cl" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computer Vision</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.04041v2" target="_blank">Evaluation and Enhancement of Semantic Grounding in Large Vision-Language Models</a>
                            </h3>
                            <p class="card-authors">Jiaying Lu, Jinmeng Rao, Kezhen Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444262256">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Vision-Language Models (LVLMs) offer remarkable benefits for a variety of vision-language tasks. However, a challenge hindering their application in real-world scenarios, particularly regarding safety, robustness, and reliability, is their constrained semantic grounding ability, which pertains to connecting language to the physical-world entities or concepts referenced in images. Therefore, a crucial need arises for a comprehensive study to assess the semantic grounding ability of widely used LVLMs. Despite the significance, sufficient investigation in this direction is currently lacking...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address this issue, we propose a data-centric enhancement method that aims to improve LVLMs&#x27; semantic grounding ability through multimodal instruction tuning on fine-grained conversations</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, a challenge hindering their application in real-world scenarios, particularly regarding safety, robustness, and reliability, is their constrained semantic grounding ability, which pertains to connecting language to the physical-world entities or concepts referenced in images. Our work bridges this gap by designing a pipeline for generating large-scale evaluation datasets covering fine-grained semantic information, such as color, number, material, etc., along with a thorough assessment of seven popular LVLMs&#x27; semantic grounding ability</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large Vision-Language Models (LVLMs) offer remarkable benefits for a variety of vision-language tasks</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.04041v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.04041v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Lu et al., &quot;Evaluation and Enhancement of Semantic Grounding in Large Vision-Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.04041v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444262256')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cv" data-title="pushing boundaries: exploring zero shot object classification with large multimodal models" data-keywords="ros language model cs.cv cs.si" data-themes="S E M">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computer Vision</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.00127v1" target="_blank">Pushing Boundaries: Exploring Zero Shot Object Classification with Large Multimodal Models</a>
                            </h3>
                            <p class="card-authors">Ashhadul Islam, Md. Rafiul Biswas, Wajdi Zaghouani et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span><span class="keyword-tag">CS.SI</span></div>

                            <div class="card-details" id="details-4444263840">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">$ $The synergy of language and vision models has given rise to Large Language and Vision Assistant models (LLVAs), designed to engage users in rich conversational experiences intertwined with image-based queries. These comprehensive multimodal models seamlessly integrate vision encoders with Large Language Models (LLMs), expanding their applications in general-purpose language and visual comprehension. The advent of Large Multimodal Models (LMMs) heralds a new era in Artificial Intelligence (AI) assistance, extending the horizons of AI utilization. This paper takes a unique perspective on LMMs...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">$ $The synergy of language and vision models has given rise to Large Language and Vision Assistant models (LLVAs), designed to engage users in rich conversational experiences intertwined with image-based queries</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.00127v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.00127v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Islam, M. R. Biswas, W. Zaghouani, S. B. Belhaouari, and Z. Shah, &quot;Pushing Boundaries: Exploring Zero Shot Object Classification with Large Multimodal Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2401.00127v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444263840')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.se" data-title="assurance for autonomy -- jpl&#x27;s past research, lessons learned, and future directions" data-keywords="robot control cs.se cs.ro" data-themes="M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ CS.SE</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2305.11902v1" target="_blank">Assurance for Autonomy -- JPL&#x27;s past research, lessons learned, and future directions</a>
                            </h3>
                            <p class="card-authors">Martin S. Feather, Alessandro Pinto</p>
                            <div class="theme-tags"><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Control</span><span class="keyword-tag">CS.SE</span><span class="keyword-tag">CS.RO</span></div>

                            <div class="card-details" id="details-4444264176">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Robotic space missions have long depended on automation, defined in the 2015 NASA Technology Roadmaps as &quot;the automatically-controlled operation of an apparatus, process, or system using a pre-planned set of instructions (e.g., a command sequence),&quot; to react to events when a rapid response is required. Autonomy, defined there as &quot;the capacity of a system to achieve goals while operating independently from external control,&quot; is required when a wide variation in circumstances precludes responses being pre-planned, instead autonomy follows an on-board deliberative process to determine the situati...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">This paper summarizes over two decades of this research, and offers a vision of where further work is needed to address open issues.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">To remedy this situation, researchers in JPL&#x27;s software assurance group have been involved in the development of techniques specific to the assurance of autonomy</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2305.11902v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2305.11902v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. S. Feather, and A. Pinto, &quot;Assurance for Autonomy -- JPL&#x27;s past research, lessons learned, and future directions,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2305.11902v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444264176')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cv" data-title="lvlm-ehub: a comprehensive evaluation benchmark for large vision-language models" data-keywords="gpt language model cs.cv cs.ai" data-themes="E M">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computer Vision</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2306.09265v1" target="_blank">LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models</a>
                            </h3>
                            <p class="card-authors">Peng Xu, Wenqi Shao, Kaipeng Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4441928128">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Vision-Language Models (LVLMs) have recently played a dominant role in multimodal vision-language learning. Despite the great success, it lacks a holistic evaluation of their efficacy. This paper presents a comprehensive evaluation of publicly available large multimodal models by building a LVLM evaluation Hub (LVLM-eHub). Our LVLM-eHub consists of $8$ representative LVLMs such as InstructBLIP and MiniGPT-4, which are thoroughly evaluated by a quantitative capability evaluation and an online arena platform. The former evaluates $6$ categories of multimodal capabilities of LVLMs such as v...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents a comprehensive evaluation of publicly available large multimodal models by building a LVLM evaluation Hub (LVLM-eHub)</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Second, instruction-tuned LVLM with moderate instruction-following data may result in object hallucination issues (i.e., generate objects that are inconsistent with target images in the descriptions). Third, employing a multi-turn reasoning evaluation framework can mitigate the issue of object hallucination, shedding light on developing an effective pipeline for LVLM evaluation</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large Vision-Language Models (LVLMs) have recently played a dominant role in multimodal vision-language learning</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2306.09265v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2306.09265v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Xu et al., &quot;LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2306.09265v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4441928128')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="wizardcoder: empowering code large language models with evol-instruct" data-keywords="nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2306.08568v2" target="_blank">WizardCoder: Empowering Code Large Language Models with Evol-Instruct</a>
                            </h3>
                            <p class="card-authors">Ziyang Luo, Can Xu, Pu Zhao et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4444264224">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2306.08568v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2306.08568v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Luo et al., &quot;WizardCoder: Empowering Code Large Language Models with Evol-Instruct,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2306.08568v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444264224')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cv" data-title="mme: a comprehensive evaluation benchmark for multimodal large language models" data-keywords="perception optimization language model cs.cv" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computer Vision</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2306.13394v5" target="_blank">MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models</a>
                            </h3>
                            <p class="card-authors">Chaoyou Fu, Peixian Chen, Yunhang Shen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Perception</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span></div>

                            <div class="card-details" id="details-4444262832">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image. However, it is difficult for these case studies to fully reflect the performance of MLLM, lacking a comprehensive evaluation. In this paper, we fill in this blank, presenting the first comprehensive MLLM Evaluation benchmark MME. It measures both perception and cognition abilities on a total of 14 subtasks. In order to avoid data leakage that may arise from direct use of public datasets for evaluation, the ...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, it is difficult for these case studies to fully reflect the performance of MLLM, lacking a comprehensive evaluation</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2306.13394v5" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2306.13394v5" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Fu et al., &quot;MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2306.13394v5')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444262832')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="learning and autonomy for extraterrestrial terrain sampling: an experience report from owlat deployment" data-keywords="control cs.ro" data-themes="S I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2311.17405v2" target="_blank">Learning and Autonomy for Extraterrestrial Terrain Sampling: An Experience Report from OWLAT Deployment</a>
                            </h3>
                            <p class="card-authors">Pranay Thangeda, Ashish Goel, Erica Tevere et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Control</span><span class="keyword-tag">CS.RO</span></div>

                            <div class="card-details" id="details-4444266576">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Extraterrestrial autonomous lander missions increasingly demand adaptive capabilities to handle the unpredictable and diverse nature of the terrain. This paper discusses the deployment of a Deep Meta-Learning with Controlled Deployment Gaps (CoDeGa) trained model for terrain scooping tasks in Ocean Worlds Lander Autonomy Testbed (OWLAT) at NASA Jet Propulsion Laboratory. The CoDeGa-powered scooping strategy is designed to adapt to novel terrains, selecting scooping actions based on the available RGB-D image data and limited experience. The paper presents our experiences with transferring the s...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">This paper discusses the deployment of a Deep Meta-Learning with Controlled Deployment Gaps (CoDeGa) trained model for terrain scooping tasks in Ocean Worlds Lander Autonomy Testbed (OWLAT) at NASA Jet Propulsion Laboratory</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper discusses the deployment of a Deep Meta-Learning with Controlled Deployment Gaps (CoDeGa) trained model for terrain scooping tasks in Ocean Worlds Lander Autonomy Testbed (OWLAT) at NASA Jet Propulsion Laboratory</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2311.17405v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2311.17405v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Thangeda et al., &quot;Learning and Autonomy for Extraterrestrial Terrain Sampling: An Experience Report from OWLAT Deployment,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2311.17405v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444266576')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="turkish native language identification v2" data-keywords="cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2307.14850v6" target="_blank">Turkish Native Language Identification V2</a>
                            </h3>
                            <p class="card-authors">Ahmet Yavuz Uluslu, Gerold Schneider</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444264080">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents the first application of Native Language Identification (NLI) for the Turkish language. NLI is the task of automatically identifying an individual&#x27;s native language (L1) based on their writing or speech in a non-native language (L2). While most NLI research has focused on L2 English, our study extends this scope to L2 Turkish by analyzing a corpus of texts written by native speakers of Albanian, Arabic and Persian. We leverage a cleaned version of the Turkish Learner Corpus and demonstrate the effectiveness of syntactic features, comparing a structural Part-of-Speech n-gram...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents the first application of Native Language Identification (NLI) for the Turkish language</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We leverage a cleaned version of the Turkish Learner Corpus and demonstrate the effectiveness of syntactic features, comparing a structural Part-of-Speech n-gram model to a hybrid model that retains function words</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2307.14850v6" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2307.14850v6" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Y. Uluslu, and G. Schneider, &quot;Turkish Native Language Identification V2,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2307.14850v6')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444264080')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="a comprehensive review of state-of-the-art methods for java code generation from natural language text" data-keywords="deep learning transformer rnn nlp cs.cl" data-themes="E I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2306.06371v1" target="_blank">A Comprehensive Review of State-of-The-Art Methods for Java Code Generation from Natural Language Text</a>
                            </h3>
                            <p class="card-authors">Jessica L√≥pez Espejel, Mahaman Sanoussi Yahaya Alassan, El Mehdi Chouham et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Deep Learning</span><span class="keyword-tag">Transformer</span><span class="keyword-tag">Rnn</span><span class="keyword-tag">Nlp</span></div>

                            <div class="card-details" id="details-4444265040">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Java Code Generation consists in generating automatically Java code from a Natural Language Text. This NLP task helps in increasing programmers&#x27; productivity by providing them with immediate solutions to the simplest and most repetitive tasks. Code generation is a challenging task because of the hard syntactic rules and the necessity of a deep understanding of the semantic aspect of the programming language. Many works tried to tackle this task using either RNN-based, or Transformer-based models. The latter achieved remarkable advancement in the domain and they can be divided into three groups...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">We focus on the most important methods and present their merits and limitations, as well as the objective functions used by the community</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Many works tried to tackle this task using either RNN-based, or Transformer-based models</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2306.06371v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2306.06371v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. L. Espejel, M. S. Y. Alassan, E. M. Chouham, W. Dahhane, and E. H. Ettifouri, &quot;A Comprehensive Review of State-of-The-Art Methods for Java Code Generation from Natural Language Text,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2306.06371v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444265040')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="using large language models for knowledge engineering (llmke): a case study on wikidata" data-keywords="ros language model cs.cl cs.ai" data-themes="E M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.08491v1" target="_blank">Using Large Language Models for Knowledge Engineering (LLMKE): A Case Study on Wikidata</a>
                            </h3>
                            <p class="card-authors">Bohui Zhang, Ioannis Reklos, Nitisha Jain et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4444264272">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In this work, we explore the use of Large Language Models (LLMs) for knowledge engineering tasks in the context of the ISWC 2023 LM-KBC Challenge. For this task, given subject and relation pairs sourced from Wikidata, we utilize pre-trained LLMs to produce the relevant objects in string format and link them to their respective Wikidata QIDs. We developed a pipeline using LLMs for Knowledge Engineering (LLMKE), combining knowledge probing and Wikidata entity mapping. The method achieved a macro-averaged F1-score of 0.701 across the properties, with the scores varying from 1.00 to 0.328. These r...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We developed a pipeline using LLMs for Knowledge Engineering (LLMKE), combining knowledge probing and Wikidata entity mapping</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">In this work, we explore the use of Large Language Models (LLMs) for knowledge engineering tasks in the context of the ISWC 2023 LM-KBC Challenge. LLMKE won Track 2 of the challenge</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In this work, we explore the use of Large Language Models (LLMs) for knowledge engineering tasks in the context of the ISWC 2023 LM-KBC Challenge</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.08491v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.08491v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. Zhang, I. Reklos, N. Jain, A. M. Pe√±uela, and E. Simperl, &quot;Using Large Language Models for Knowledge Engineering (LLMKE): A Case Study on Wikidata,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.08491v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444264272')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="jais and jais-chat: arabic-centric foundation and instruction-tuned open generative large language models" data-keywords="gpt language model cs.cl cs.ai cs.lg" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2308.16149v2" target="_blank">Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models</a>
                            </h3>
                            <p class="card-authors">Neha Sengupta, Sunil Kumar Sahu, Bokang Jia et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4444264896">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-ce...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs)</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2308.16149v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2308.16149v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. Sengupta et al., &quot;Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2308.16149v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444264896')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="clipswarm: converting text into formations of robots" data-keywords="robot swarm multi-robot optimization cs.ro" data-themes="E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2311.11047v1" target="_blank">CLIPSwarm: Converting text into formations of robots</a>
                            </h3>
                            <p class="card-authors">Pablo Pueyo, Eduardo Montijano, Ana C. Murillo et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Swarm</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Optimization</span></div>

                            <div class="card-details" id="details-4445407264">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We present CLIPSwarm, an algorithm to generate robot swarm formations from natural language descriptions. CLIPSwarm receives an input text and finds the position of the robots to form a shape that corresponds to the given text. To do so, we implement a variation of the Montecarlo particle filter to obtain a matching formation iteratively. In every iteration, we generate a set of new formations and evaluate their Clip Similarity with the given text, selecting the best formations according to this metric. This metric is obtained using Clip, [1], an existing foundation model trained to encode ima...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present CLIPSwarm, an algorithm to generate robot swarm formations from natural language descriptions</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We present CLIPSwarm, an algorithm to generate robot swarm formations from natural language descriptions</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2311.11047v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2311.11047v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Pueyo, E. Montijano, A. C. Murillo, and M. Schwager, &quot;CLIPSwarm: Converting text into formations of robots,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2311.11047v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445407264')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="how to raise a robot -- a case for neuro-symbolic ai in constrained task planning for humanoid assistive robots" data-keywords="neural network robot humanoid planning control language model cs.ro cs.cr cs.lg" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.08820v3" target="_blank">How to Raise a Robot -- A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots</a>
                            </h3>
                            <p class="card-authors">Niklas Hemken, Florian Jacob, Fabian Peller-Konrad et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Planning</span></div>

                            <div class="card-details" id="details-4445408224">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots will be able to assist humans in their daily life, in particular due to their versatile action capabilities. However, while these robots need a certain degree of autonomy to learn and explore, they also should respect various constraints, for access control and beyond. We explore the novel field of incorporating privacy, security, and access control constraints with robot task planning approaches. We report preliminary results on the classical symbolic approach, deep-learned neural networks, and modern ideas using large language models as knowledge base. From analyzing their tr...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We explore the novel field of incorporating privacy, security, and access control constraints with robot task planning approaches</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.08820v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.08820v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. Hemken, F. Jacob, F. Peller-Konrad, R. Kartmann, T. Asfour, and H. Hartenstein, &quot;How to Raise a Robot -- A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.08820v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445408224')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="perception for humanoid robots" data-keywords="robot humanoid perception optimization imu sensor fusion cs.ro cs.ai" data-themes="S M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.15616v1" target="_blank">Perception for Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Arindam Roychoudhury, Shahram Khorshidi, Subham Agrawal et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Perception</span><span class="keyword-tag">Optimization</span></div>

                            <div class="card-details" id="details-4444265328">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Purpose of Review: The field of humanoid robotics, perception plays a fundamental role in enabling robots to interact seamlessly with humans and their surroundings, leading to improved safety, efficiency, and user experience. This scientific study investigates various perception modalities and techniques employed in humanoid robots, including visual, auditory, and tactile sensing by exploring recent state-of-the-art approaches for perceiving and understanding the internal state, the environment, objects, and human activities.
  Recent Findings: Internal state estimation makes extensive use of ...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This scientific study investigates various perception modalities and techniques employed in humanoid robots, including visual, auditory, and tactile sensing by exploring recent state-of-the-art approaches for perceiving and understanding the internal state, the environment, objects, and human activities.
  Recent Findings: Internal state estimation makes extensive use of Bayesian filtering methods and optimization techniques based on maximum a-posteriori formulation by utilizing proprioceptive sensing</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.15616v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.15616v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Roychoudhury, S. Khorshidi, S. Agrawal, and M. Bennewitz, &quot;Perception for Humanoid Robots,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.15616v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444265328')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="incremental learning of humanoid robot behavior from natural interaction and large language models" data-keywords="robot humanoid perception simulation imu language model cs.ro cs.ai" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.04316v3" target="_blank">Incremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models</a>
                            </h3>
                            <p class="card-authors">Leonard B√§rmann, Rainer Kartmann, Fabian Peller-Konrad et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Perception</span><span class="keyword-tag">Simulation</span></div>

                            <div class="card-details" id="details-4445407024">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Natural-language dialog is key for intuitive human-robot interaction. It can be used not only to express humans&#x27; intents, but also to communicate instructions for improvement if a robot does not understand a command correctly. Of great importance is to endow robots with the ability to learn from such interaction experience in an incremental way to allow them to improve their behaviors or avoid mistakes in the future. In this paper, we propose a system to achieve incremental learning of complex behavior from natural interaction, and demonstrate its implementation on a humanoid robot. Building o...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose a system to achieve incremental learning of complex behavior from natural interaction, and demonstrate its implementation on a humanoid robot</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Building on recent advances, we present a system that deploys Large Language Models (LLMs) for high-level orchestration of the robot&#x27;s behavior, based on the idea of enabling the LLM to generate Python statements in an interactive console to invoke both robot perception and action</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.04316v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.04316v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. B√§rmann, R. Kartmann, F. Peller-Konrad, J. Niehues, A. Waibel, and T. Asfour, &quot;Incremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.04316v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445407024')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="influence of team interactions on multi-robot cooperation: a relational network perspective" data-keywords="robot multi-robot cooperative coordination cs.ro cs.ma" data-themes="S R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Multi-Agent Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2310.12910v1" target="_blank">Influence of Team Interactions on Multi-Robot Cooperation: A Relational Network Perspective</a>
                            </h3>
                            <p class="card-authors">Yasin Findik, Hamid Osooli, Paul Robinette et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Cooperative</span><span class="keyword-tag">Coordination</span></div>

                            <div class="card-details" id="details-4445408800">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Relational networks within a team play a critical role in the performance of many real-world multi-robot systems. To successfully accomplish tasks that require cooperation and coordination, different agents (e.g., robots) necessitate different priorities based on their positioning within the team. Yet, many of the existing multi-robot cooperation algorithms regard agents as interchangeable and lack a mechanism to guide the type of cooperation strategy the agents should exhibit. To account for the team structure in cooperative tasks, we propose a novel algorithm that uses a relational network c...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To account for the team structure in cooperative tasks, we propose a novel algorithm that uses a relational network comprising inter-agent relationships to prioritize certain agents over others</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Yet, many of the existing multi-robot cooperation algorithms regard agents as interchangeable and lack a mechanism to guide the type of cooperation strategy the agents should exhibit</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2310.12910v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2310.12910v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Findik, H. Osooli, P. Robinette, K. Jerath, and S. R. Ahmadzadeh, &quot;Influence of Team Interactions on Multi-Robot Cooperation: A Relational Network Perspective,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2310.12910v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445408800')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="centroidal state estimation and control for hardware-constrained humanoid robots" data-keywords="robot humanoid control cs.ro" data-themes="L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.11019v1" target="_blank">Centroidal State Estimation and Control for Hardware-constrained Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Grzegorz Ficht, Sven Behnke</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Control</span><span class="keyword-tag">CS.RO</span></div>

                            <div class="card-details" id="details-4445408560">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We introduce novel methods for state estimation, feedforward and feedback control, which specifically target humanoid robots with hardware limitations. Our method combines a five-mass model with approximate dynamics of each mass. It enables acquiring an accurate assessment of the centroidal state and Center of Pressure, even when direct forms of force or contact sensing are unavailable. Upon this, we develop a feedforward scheme that operates on the centroidal state, accounting for insufficient joint tracking capabilities. Finally, we implement feedback mechanisms, which compensate for the lac...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce novel methods for state estimation, feedforward and feedback control, which specifically target humanoid robots with hardware limitations</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">We introduce novel methods for state estimation, feedforward and feedback control, which specifically target humanoid robots with hardware limitations. The whole approach allows for reactive stepping to maintain balance despite these limitations, which was verified on hardware during RoboCup 2023, in Bordeaux, France.</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We introduce novel methods for state estimation, feedforward and feedback control, which specifically target humanoid robots with hardware limitations</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.11019v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.11019v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Ficht, and S. Behnke, &quot;Centroidal State Estimation and Control for Hardware-constrained Humanoid Robots,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.11019v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445408560')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="can a single human supervise a swarm of 100 heterogeneous robots?" data-keywords="robot swarm control ros cs.ro cs.ai cs.hc" data-themes="S R M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2308.00102v1" target="_blank">Can A Single Human Supervise A Swarm of 100 Heterogeneous Robots?</a>
                            </h3>
                            <p class="card-authors">Julie A. Adams, Joshua Hamell, Phillip Walker</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-R">Multi-Robot</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Swarm</span><span class="keyword-tag">Control</span><span class="keyword-tag">Ros</span></div>

                            <div class="card-details" id="details-4445409280">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">An open research question has been whether a single human can supervise a true heterogeneous swarm of robots completing tasks in real world environments. A general concern is whether or not the human&#x27;s workload will be taxed to the breaking point. The Defense Advanced Research Projects Agency&#x27;s OFFsensive Swarm-Enabled Tactics program&#x27;s field exercises that occurred at U.S. Army urban training sites provided the opportunity to understand the impact of achieving such swarm deployments. The Command and Control of Aggregate Swarm Tactics integrator team&#x27;s swarm commander users the heterogeneous r...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">A multi-dimensional workload algorithm that estimates overall workload based on five components of workload was used to analyze the results</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2308.00102v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2308.00102v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. A. Adams, J. Hamell, and P. Walker, &quot;Can A Single Human Supervise A Swarm of 100 Heterogeneous Robots?,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2308.00102v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445409280')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="from rolling over to walking: enabling humanoid robots to develop complex motor skills" data-keywords="reinforcement learning robot humanoid simulation ros imu cs.ro" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2303.02581v2" target="_blank">From Rolling Over to Walking: Enabling Humanoid Robots to Develop Complex Motor Skills</a>
                            </h3>
                            <p class="card-authors">Fanxing Meng, Jing Xiao</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Simulation</span></div>

                            <div class="card-details" id="details-4445407360">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents an innovative method for humanoid robots to acquire a comprehensive set of motor skills through reinforcement learning. The approach utilizes an achievement-triggered multi-path reward function rooted in developmental robotics principles, facilitating the robot to learn gross motor skills typically mastered by human infants within a single training phase. The proposed method outperforms standard reinforcement learning techniques in success rates and learning speed within a simulation environment. By leveraging the principles of self-discovery and exploration integral to inf...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents an innovative method for humanoid robots to acquire a comprehensive set of motor skills through reinforcement learning</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This paper presents an innovative method for humanoid robots to acquire a comprehensive set of motor skills through reinforcement learning</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2303.02581v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2303.02581v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. Meng, and J. Xiao, &quot;From Rolling Over to Walking: Enabling Humanoid Robots to Develop Complex Motor Skills,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2303.02581v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445407360')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="advantages of multimodal versus verbal-only robot-to-human communication with an anthropomorphic robotic mock driver" data-keywords="attention robot humanoid cs.ro" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-skip">‚è≠Ô∏è Skip</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2307.00841v1" target="_blank">Advantages of Multimodal versus Verbal-Only Robot-to-Human Communication with an Anthropomorphic Robotic Mock Driver</a>
                            </h3>
                            <p class="card-authors">Tim Schreiter, Lucas Morillo-Mendez, Ravi T. Chadalavada et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">CS.RO</span></div>

                            <div class="card-details" id="details-4445408944">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Robots are increasingly used in shared environments with humans, making effective communication a necessity for successful human-robot interaction. In our work, we study a crucial component: active communication of robot intent. Here, we present an anthropomorphic solution where a humanoid robot communicates the intent of its host robot acting as an &quot;Anthropomorphic Robotic Mock Driver&quot; (ARMoD). We evaluate this approach in two experiments in which participants work alongside a mobile robot on various tasks, while the ARMoD communicates a need for human attention, when required, or gives instr...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Here, we present an anthropomorphic solution where a humanoid robot communicates the intent of its host robot acting as an &quot;Anthropomorphic Robotic Mock Driver&quot; (ARMoD)</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We evaluate this approach in two experiments in which participants work alongside a mobile robot on various tasks, while the ARMoD communicates a need for human attention, when required, or gives instructions to collaborate on a joint task</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2307.00841v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2307.00841v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. Schreiter et al., &quot;Advantages of Multimodal versus Verbal-Only Robot-to-Human Communication with an Anthropomorphic Robotic Mock Driver,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2307.00841v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445408944')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="a multi-modal table tennis robot system" data-keywords="neural network robot perception control camera cs.ro cs.ai" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2310.19062v2" target="_blank">A multi-modal table tennis robot system</a>
                            </h3>
                            <p class="card-authors">Andreas Ziegler, Thomas Gossard, Karl Vetter et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Perception</span><span class="keyword-tag">Control</span></div>

                            <div class="card-details" id="details-4445407936">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In recent years, robotic table tennis has become a popular research challenge for perception and robot control. Here, we present an improved table tennis robot system with high accuracy vision detection and fast robot reaction. Based on previous work, our system contains a KUKA robot arm with 6 DOF, with four frame-based cameras and two additional event-based cameras. We developed a novel calibration approach to calibrate this multimodal perception system. For table tennis, spin estimation is crucial. Therefore, we introduced a novel, and more accurate spin estimation approach. Finally, we sho...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Here, we present an improved table tennis robot system with high accuracy vision detection and fast robot reaction</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">In recent years, robotic table tennis has become a popular research challenge for perception and robot control</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We developed a novel calibration approach to calibrate this multimodal perception system</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2310.19062v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2310.19062v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Ziegler, T. Gossard, K. Vetter, J. Tebbe, and A. Zell, &quot;A multi-modal table tennis robot system,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2310.19062v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445407936')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="estimation of continuous environments by robot swarms: correlated networks and decision-making" data-keywords="robot swarm multi-robot control cs.ro cs.ai cs.ma" data-themes="S I R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2302.13629v2" target="_blank">Estimation of continuous environments by robot swarms: Correlated networks and decision-making</a>
                            </h3>
                            <p class="card-authors">Mohsen Raoufi, Pawel Romanczuk, Heiko Hamann</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Swarm</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Control</span></div>

                            <div class="card-details" id="details-4445419072">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Collective decision-making is an essential capability of large-scale multi-robot systems to establish autonomy on the swarm level. A large portion of literature on collective decision-making in swarm robotics focuses on discrete decisions selecting from a limited number of options. Here we assign a decentralized robot system with the task of exploring an unbounded environment, finding consensus on the mean of a measurable environmental feature, and aggregating at areas where that value is measured (e.g., a contour line). A unique quality of this task is a causal loop between the robots&#x27; dynami...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a control algorithm and study it in real-world robot swarm experiments in different environments</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We propose a control algorithm and study it in real-world robot swarm experiments in different environments</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2302.13629v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2302.13629v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Raoufi, P. Romanczuk, and H. Hamann, &quot;Estimation of continuous environments by robot swarms: Correlated networks and decision-making,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2302.13629v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445419072')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="social robots as companions for lonely hearts: the role of anthropomorphism and robot appearance" data-keywords="robot cs.ro cs.hc" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-skip">‚è≠Ô∏è Skip</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2306.02694v2" target="_blank">Social Robots As Companions for Lonely Hearts: The Role of Anthropomorphism and Robot Appearance</a>
                            </h3>
                            <p class="card-authors">Yoonwon Jung, Sowon Hahn</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">CS.RO</span><span class="keyword-tag">CS.HC</span></div>

                            <div class="card-details" id="details-4445421040">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Loneliness is a distressing personal experience and a growing social issue. Social robots could alleviate the pain of loneliness, particularly for those who lack in-person interaction. This paper investigated how the effect of loneliness on the anthropomorphism of social robots differs by robot appearance, and how it influences purchase intention. Participants viewed a video of one of the three robots (machine-like, animal-like, and human-like) moving and interacting with a human counterpart. Bootstrapped multiple regression results revealed that although the unique effect of animal-likeness o...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Loneliness is a distressing personal experience and a growing social issue</p></div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2306.02694v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2306.02694v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Jung, and S. Hahn, &quot;Social Robots As Companions for Lonely Hearts: The Role of Anthropomorphism and Robot Appearance,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2306.02694v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445421040')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="teleoperation of humanoid robots: a survey" data-keywords="robot humanoid cs.ro" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2301.04317v1" target="_blank">Teleoperation of Humanoid Robots: A Survey</a>
                            </h3>
                            <p class="card-authors">Kourosh Darvish, Luigi Penco, Joao Ramos et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">CS.RO</span></div>

                            <div class="card-details" id="details-4445421328">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Teleoperation of humanoid robots enables the integration of the cognitive skills and domain expertise of humans with the physical capabilities of humanoid robots. The operational versatility of humanoid robots makes them the ideal platform for a wide range of applications when teleoperating in a remote environment. However, the complexity of humanoid robots imposes challenges for teleoperation, particularly in unstructured dynamic environments with limited communication. Many advancements have been achieved in the last decades in this area, but a comprehensive overview is still missing. This s...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">However, the complexity of humanoid robots imposes challenges for teleoperation, particularly in unstructured dynamic environments with limited communication</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This survey paper gives an extensive overview of humanoid robot teleoperation, presenting the general architecture of a teleoperation system and analyzing the different components</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2301.04317v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2301.04317v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Darvish et al., &quot;Teleoperation of Humanoid Robots: A Survey,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2301.04317v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445421328')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="real-world humanoid locomotion with reinforcement learning" data-keywords="reinforcement learning transformer robot humanoid locomotion control simulation imu cs.ro cs.lg" data-themes="S L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2303.03381v2" target="_blank">Real-World Humanoid Locomotion with Reinforcement Learning</a>
                            </h3>
                            <p class="card-authors">Ilija Radosavovic, Tete Xiao, Bike Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Transformer</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span></div>

                            <div class="card-details" id="details-4445420656">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots that can autonomously operate in diverse environments have the potential to help address labour shortages in factories, assist elderly at homes, and colonize new planets. While classical controllers for humanoid robots have shown impressive results in a number of settings, they are challenging to generalize and adapt to new environments. Here, we present a fully learning-based approach for real-world humanoid locomotion. Our controller is a causal transformer that takes the history of proprioceptive observations and actions as input and predicts the next action. We hypothesize ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Here, we present a fully learning-based approach for real-world humanoid locomotion</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Here, we present a fully learning-based approach for real-world humanoid locomotion</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2303.03381v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2303.03381v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'I. Radosavovic, T. Xiao, B. Zhang, T. Darrell, J. Malik, and K. Sreenath, &quot;Real-World Humanoid Locomotion with Reinforcement Learning,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2303.03381v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445420656')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="multi-robot local motion planning using dynamic optimization fabrics" data-keywords="robot multi-robot planning imu cs.ro cs.ma" data-themes="I R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Multi-Agent Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2310.12816v1" target="_blank">Multi-Robot Local Motion Planning Using Dynamic Optimization Fabrics</a>
                            </h3>
                            <p class="card-authors">Saray Bakker, Luzia Knoedler, Max Spahn et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Planning</span><span class="keyword-tag">Imu</span></div>

                            <div class="card-details" id="details-4445408464">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In this paper, we address the problem of real-time motion planning for multiple robotic manipulators that operate in close proximity. We build upon the concept of dynamic fabrics and extend them to multi-robot systems, referred to as Multi-Robot Dynamic Fabrics (MRDF). This geometric method enables a very high planning frequency for high-dimensional systems at the expense of being reactive and prone to deadlocks. To detect and resolve deadlocks, we propose Rollout Fabrics where MRDF are forward simulated in a decentralized manner. We validate the methods in simulated close-proximity pick-and-p...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To detect and resolve deadlocks, we propose Rollout Fabrics where MRDF are forward simulated in a decentralized manner</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">In this paper, we address the problem of real-time motion planning for multiple robotic manipulators that operate in close proximity</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">This geometric method enables a very high planning frequency for high-dimensional systems at the expense of being reactive and prone to deadlocks</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2310.12816v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2310.12816v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Bakker, L. Knoedler, M. Spahn, W. B√∂hmer, and J. Alonso-Mora, &quot;Multi-Robot Local Motion Planning Using Dynamic Optimization Fabrics,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2310.12816v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445408464')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="mimicgen: a data generation system for scalable robot learning using human demonstrations" data-keywords="robot simulation ros imu imitation learning cs.ro cs.ai cs.cv" data-themes="I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Simulation</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2310.17596v1" target="_blank">MimicGen: A Data Generation System for Scalable Robot Learning using Human Demonstrations</a>
                            </h3>
                            <p class="card-authors">Ajay Mandlekar, Soroush Nasiriany, Bowen Wen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Simulation</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Imu</span></div>

                            <div class="card-details" id="details-4445421520">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Imitation learning from a large set of human demonstrations has proved to be an effective paradigm for building capable robot agents. However, the demonstrations can be extremely costly and time-consuming to collect. We introduce MimicGen, a system for automatically synthesizing large-scale, rich datasets from only a small number of human demonstrations by adapting them to new contexts. We use MimicGen to generate over 50K demonstrations across 18 tasks with diverse scene configurations, object instances, and robot arms from just ~200 human demonstrations. We show that robot agents can be effe...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce MimicGen, a system for automatically synthesizing large-scale, rich datasets from only a small number of human demonstrations by adapting them to new contexts</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We further demonstrate that the effectiveness and utility of MimicGen data compare favorably to collecting additional human demonstrations, making it a powerful and economical approach towards scaling up robot learning</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2310.17596v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2310.17596v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Mandlekar et al., &quot;MimicGen: A Data Generation System for Scalable Robot Learning using Human Demonstrations,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2310.17596v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445421520')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="rlss: real-time, decentralized, cooperative, networkless multi-robot trajectory planning using linear spatial separations" data-keywords="robot multi-robot cooperative planning optimization simulation imu cs.ro" data-themes="R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2302.12863v2" target="_blank">RLSS: Real-time, Decentralized, Cooperative, Networkless Multi-Robot Trajectory Planning using Linear Spatial Separations</a>
                            </h3>
                            <p class="card-authors">Baskƒ±n ≈ûenba≈ülar, Wolfgang H√∂nig, Nora Ayanian</p>
                            <div class="theme-tags"><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Cooperative</span><span class="keyword-tag">Planning</span></div>

                            <div class="card-details" id="details-4445421232">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Trajectory planning for multiple robots in shared environments is a challenging problem especially when there is limited communication available or no central entity. In this article, we present Real-time planning using Linear Spatial Separations, or RLSS: a real-time decentralized trajectory planning algorithm for cooperative multi-robot teams in static environments. The algorithm requires relatively few robot capabilities, namely sensing the positions of robots and obstacles without higher-order derivatives and the ability of distinguishing robots from obstacles. There is no communication re...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this article, we present Real-time planning using Linear Spatial Separations, or RLSS: a real-time decentralized trajectory planning algorithm for cooperative multi-robot teams in static environments</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">Trajectory planning for multiple robots in shared environments is a challenging problem especially when there is limited communication available or no central entity. RLSS generates and solves convex quadratic optimization problems that are kinematically feasible and guarantees collision avoidance if the resulting problems are feasible</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In this article, we present Real-time planning using Linear Spatial Separations, or RLSS: a real-time decentralized trajectory planning algorithm for cooperative multi-robot teams in static environments</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2302.12863v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2302.12863v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. ≈ûenba≈ülar, W. H√∂nig, and N. Ayanian, &quot;RLSS: Real-time, Decentralized, Cooperative, Networkless Multi-Robot Trajectory Planning using Linear Spatial Separations,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2302.12863v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445421232')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="proceedings of the dialogue robot competition 2023" data-keywords="robot humanoid cs.ro" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.14430v5" target="_blank">Proceedings of the Dialogue Robot Competition 2023</a>
                            </h3>
                            <p class="card-authors">Ryuichiro Higashinaka, Takashi Minato, Hiromitsu Nishizaki et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">CS.RO</span></div>

                            <div class="card-details" id="details-4445421376">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The Dialogic Robot Competition 2023 (DRC2023) is a competition for humanoid robots (android robots that closely resemble humans) to compete in interactive capabilities. This is the third year of the competition. The top four teams from the preliminary competition held in November 2023 will compete in the final competition on Saturday, December 23. The task for the interactive robots is to recommend a tourism plan for a specific region. The robots can employ multimodal behaviors, such as language and gestures, to engage the user in the sightseeing plan they recommend. In the preliminary round, ...</p>
                                </div>
                                
                                
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.14430v5" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.14430v5" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Higashinaka, T. Minato, H. Nishizaki, and T. Nagai, &quot;Proceedings of the Dialogue Robot Competition 2023,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.14430v5')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4445421376')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="failure detection and fault tolerant control of a jet-powered flying humanoid robot" data-keywords="robot humanoid control simulation gazebo imu cs.ro" data-themes="S">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ Robotics</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2305.16075v1" target="_blank">Failure Detection and Fault Tolerant Control of a Jet-Powered Flying Humanoid Robot</a>
                            </h3>
                            <p class="card-authors">Gabriele Nava, Daniele Pucci</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Control</span><span class="keyword-tag">Simulation</span></div>

                            <div class="card-details" id="details-4444956960">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Failure detection and fault tolerant control are fundamental safety features of any aerial vehicle. With the emergence of complex, multi-body flying systems such as jet-powered humanoid robots, it becomes of crucial importance to design fault detection and control strategies for these systems, too. In this paper we propose a fault detection and control framework for the flying humanoid robot iRonCub in case of loss of one turbine. The framework is composed of a failure detector based on turbines rotational speed, a momentum-based flight control for fault response, and an offline reference gene...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper we propose a fault detection and control framework for the flying humanoid robot iRonCub in case of loss of one turbine</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In this paper we propose a fault detection and control framework for the flying humanoid robot iRonCub in case of loss of one turbine</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2305.16075v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2305.16075v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Nava, and D. Pucci, &quot;Failure Detection and Fault Tolerant Control of a Jet-Powered Flying Humanoid Robot,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2305.16075v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444956960')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section year-section" data-year="2022">
                <h2 class="section-header">üìÖ 2022 <span class="section-count">(6 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-source="arXiv" data-year="2022" data-category="cs.cl" data-title="self-generated in-context learning: leveraging auto-regressive language models as a demonstration generator" data-keywords="language model cs.cl" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2206.08082v1" target="_blank">Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator</a>
                            </h3>
                            <p class="card-authors">Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444264032">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large-scale pre-trained language models (PLMs) are well-known for being capable of solving a task simply by conditioning a few input-label pairs dubbed demonstrations on a prompt without being explicitly tuned for the desired downstream task. Such a process (i.e., in-context learning), however, naturally leads to high reliance on the demonstrations which are usually selected from external datasets. In this paper, we propose self-generated in-context learning (SG-ICL), which generates demonstrations for in-context learning from PLM itself to minimize the reliance on the external demonstration. ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose self-generated in-context learning (SG-ICL), which generates demonstrations for in-context learning from PLM itself to minimize the reliance on the external demonstration</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large-scale pre-trained language models (PLMs) are well-known for being capable of solving a task simply by conditioning a few input-label pairs dubbed demonstrations on a prompt without being explicitly tuned for the desired downstream task</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2206.08082v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2206.08082v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. J. Kim, H. Cho, J. Kim, T. Kim, K. M. Yoo, and S. Lee, &quot;Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator,&quot; arXiv, 2022. [Online]. Available: http://arxiv.org/abs/2206.08082v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444264032')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2022" data-category="cs.cl" data-title="gpt-neox-20b: an open-source autoregressive language model" data-keywords="gpt language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2204.06745v1" target="_blank">GPT-NeoX-20B: An Open-Source Autoregressive Language Model</a>
                            </h3>
                            <p class="card-authors">Sid Black, Stella Biderman, Eric Hallahan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>

                            <div class="card-details" id="details-4444265664">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe \model{}&#x27;s architecture and training and evaluate its performance on a range of language-understanding, mathematics, and knowledge-based tasks. We find that GPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more i...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2204.06745v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2204.06745v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Black et al., &quot;GPT-NeoX-20B: An Open-Source Autoregressive Language Model,&quot; arXiv, 2022. [Online]. Available: http://arxiv.org/abs/2204.06745v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444265664')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2022" data-category="cs.cl" data-title="exploring the value of pre-trained language models for clinical named entity recognition" data-keywords="transformer bert nlp language model cs.cl cs.ai cs.lg" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2210.12770v4" target="_blank">Exploring the Value of Pre-trained Language Models for Clinical Named Entity Recognition</a>
                            </h3>
                            <p class="card-authors">Samuel Belkadi, Lifeng Han, Yuping Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Transformer</span><span class="keyword-tag">Bert</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4444265376">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The practice of fine-tuning Pre-trained Language Models (PLMs) from general or domain-specific data to a specific task with limited resources, has gained popularity within the field of natural language processing (NLP). In this work, we re-visit this assumption and carry out an investigation in clinical NLP, specifically Named Entity Recognition on drugs and their related attributes. We compare Transformer models that are trained from scratch to fine-tuned BERT-based LLMs namely BERT, BioBERT, and ClinicalBERT. Furthermore, we examine the impact of an additional CRF layer on such models to enc...</p>
                                </div>
                                
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">The practice of fine-tuning Pre-trained Language Models (PLMs) from general or domain-specific data to a specific task with limited resources, has gained popularity within the field of natural language processing (NLP)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2210.12770v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2210.12770v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Belkadi, L. Han, Y. Wu, and G. Nenadic, &quot;Exploring the Value of Pre-trained Language Models for Clinical Named Entity Recognition,&quot; arXiv, 2022. [Online]. Available: http://arxiv.org/abs/2210.12770v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444265376')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2022" data-category="cs.cl" data-title="prompting is programming: a query language for large language models" data-keywords="control language model cs.cl cs.ai" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2212.06094v3" target="_blank">Prompting Is Programming: A Query Language for Large Language Models</a>
                            </h3>
                            <p class="card-authors">Luca Beurer-Kellner, Marc Fischer, Martin Vechev</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Control</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>

                            <div class="card-details" id="details-4444264416">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models have demonstrated outstanding performance on a wide range of tasks such as question answering and code generation. On a high level, given an input, a language model can be used to automatically complete the sequence in a statistically-likely way. Based on this, users prompt these models with language instructions or examples, to implement a variety of downstream tasks. Advanced prompting methods can even imply interaction between the language model, a user, and external tools such as calculators. However, to obtain state-of-the-art performance or adapt language models for...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">However, to obtain state-of-the-art performance or adapt language models for specific tasks, complex task- and model-specific programs have to be implemented, which may still require ad-hoc interaction.
  Based on this, we present the novel idea of Language Model Programming (LMP)</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Large language models have demonstrated outstanding performance on a wide range of tasks such as question answering and code generation</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2212.06094v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2212.06094v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Beurer-Kellner, M. Fischer, and M. Vechev, &quot;Prompting Is Programming: A Query Language for Large Language Models,&quot; arXiv, 2022. [Online]. Available: http://arxiv.org/abs/2212.06094v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444264416')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2022" data-category="cs.cl" data-title="end-to-end spoken language understanding: performance analyses of a voice command task in a low resource setting" data-keywords="neural network optimization cs.cl cs.sd eess.as" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2207.08179v1" target="_blank">End-to-End Spoken Language Understanding: Performance analyses of a voice command task in a low resource setting</a>
                            </h3>
                            <p class="card-authors">Thierry Desot, Fran√ßois Portet, Michel Vacher</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.SD</span></div>

                            <div class="card-details" id="details-4444265520">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Spoken Language Understanding (SLU) is a core task in most human-machine interaction systems. With the emergence of smart homes, smart phones and smart speakers, SLU has become a key technology for the industry. In a classical SLU approach, an Automatic Speech Recognition (ASR) module transcribes the speech signal into a textual representation from which a Natural Language Understanding (NLU) module extracts semantic information. Recently End-to-End SLU (E2E SLU) based on Deep Neural Networks has gained momentum since it benefits from the joint optimization of the ASR and the NLU parts, hence ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we present a study identifying the signal features and other linguistic properties used by an E2E model to perform the SLU task</p></div>
                                
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">In a classical SLU approach, an Automatic Speech Recognition (ASR) module transcribes the speech signal into a textual representation from which a Natural Language Understanding (NLU) module extracts semantic information</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2207.08179v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2207.08179v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. Desot, F. Portet, and M. Vacher, &quot;End-to-End Spoken Language Understanding: Performance analyses of a voice command task in a low resource setting,&quot; arXiv, 2022. [Online]. Available: http://arxiv.org/abs/2207.08179v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444265520')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2022" data-category="cs.cl" data-title="an overview of indian spoken language recognition from machine learning perspective" data-keywords="neural network cs.cl cs.sd eess.as" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Computational Linguistics</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2212.03812v1" target="_blank">An Overview of Indian Spoken Language Recognition from Machine Learning Perspective</a>
                            </h3>
                            <p class="card-authors">Spandan Dey, Md Sahidullah, Goutam Saha</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.SD</span><span class="keyword-tag">EESS.AS</span></div>

                            <div class="card-details" id="details-4444260960">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Automatic spoken language identification (LID) is a very important research field in the era of multilingual voice-command-based human-computer interaction (HCI). A front-end LID module helps to improve the performance of many speech-based applications in the multilingual scenario. India is a populous country with diverse cultures and languages. The majority of the Indian population needs to use their respective native languages for verbal interaction with machines. Therefore, the development of efficient Indian spoken language recognition systems is useful for adapting smart technologies in e...</p>
                                </div>
                                
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">In-depth analysis has been presented to emphasize the unique challenges of low-resource and mutual influences for developing LID systems in the Indian contexts</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">Several essential aspects of the Indian LID research, such as the detailed description of the available speech corpora, the major research contributions, including the earlier attempts based on statistical modeling to the recent approaches based on different neural network architectures, and the future research trends are discussed</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2212.03812v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2212.03812v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Dey, M. Sahidullah, and G. Saha, &quot;An Overview of Indian Spoken Language Recognition from Machine Learning Perspective,&quot; arXiv, 2022. [Online]. Available: http://arxiv.org/abs/2212.03812v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444260960')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section year-section" data-year="2021">
                <h2 class="section-header">üìÖ 2021 <span class="section-count">(1 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-source="arXiv" data-year="2021" data-category="cs.lg" data-title="differentially private fine-tuning of language models" data-keywords="bert gpt nlp language model cs.lg cs.cl cs.cr" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ Machine Learning</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2110.06500v2" target="_blank">Differentially Private Fine-tuning of Language Models</a>
                            </h3>
                            <p class="card-authors">Da Yu, Saurabh Naik, Arturs Backurs et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Bert</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span></div>

                            <div class="card-details" id="details-4444265136">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We give simpler, sparser, and faster algorithms for differentially private fine-tuning of large-scale pre-trained language models, which achieve the state-of-the-art privacy versus utility tradeoffs on many standard NLP tasks. We propose a meta-framework for this problem, inspired by the recent success of highly parameter-efficient methods for fine-tuning. Our experiments show that differentially private adaptations of these approaches outperform previous private algorithms in three important dimensions: utility, privacy, and the computational and memory cost of private training. On many commo...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a meta-framework for this problem, inspired by the recent success of highly parameter-efficient methods for fine-tuning</p></div>
                                <div class="detail-block"><div class="detail-label">Problems Addressed</div><p class="detail-text">We propose a meta-framework for this problem, inspired by the recent success of highly parameter-efficient methods for fine-tuning</p></div>
                                <div class="detail-block"><div class="detail-label">Methodology</div><p class="detail-text">We give simpler, sparser, and faster algorithms for differentially private fine-tuning of large-scale pre-trained language models, which achieve the state-of-the-art privacy versus utility tradeoffs on many standard NLP tasks</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2110.06500v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2110.06500v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Yu et al., &quot;Differentially Private Fine-tuning of Language Models,&quot; arXiv, 2021. [Online]. Available: http://arxiv.org/abs/2110.06500v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'details-4444265136')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

        </div>

        <!-- Category View -->
        <div id="category-view" class="hidden">

            <div class="group-section cat-section" data-category="cs.ro">
                <h2 class="section-header">üè∑Ô∏è Robotics <span class="section-count">(104 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="a mini-review on mobile manipulators with variable autonomy" data-keywords="robot language model cs.ro cs.hc" data-themes="E L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.10887v1" target="_blank">A Mini-Review on Mobile Manipulators with Variable Autonomy</a>
                            </h3>
                            <p class="card-authors">Cesar Alan Contreras, Alireza Rastegarpanah, Rustam Stolkin et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.RO</span><span class="keyword-tag">CS.HC</span></div>
                            <div class="card-details" id="cat-details-4441928320">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents a mini-review of the current state of research in mobile manipulators with variable levels of autonomy, emphasizing their associated challenges and application environments. The need for mobile manipulators in different environments is evident due to the unique challenges and risks each presents. Many systems deployed in these environments are not fully autonomous, requiring human-robot teaming to ensure safe and reliable operations under uncertainties. Through this analysis, we identify gaps and challenges in the literature on Variable Autonomy, including cognitive workloa...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents a mini-review of the current state of research in mobile manipulators with variable levels of autonomy, emphasizing their associated challenges and application environments</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.10887v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.10887v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. A. Contreras, A. Rastegarpanah, R. Stolkin, and M. Chiou, &quot;A Mini-Review on Mobile Manipulators with Variable Autonomy,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.10887v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441928320')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="tidybot: personalized robot assistance with large language models" data-keywords="robot perception planning language model cs.ro cs.ai cs.cl" data-themes="S E I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2305.05658v2" target="_blank">TidyBot: Personalized Robot Assistance with Large Language Models</a>
                            </h3>
                            <p class="card-authors">Jimmy Wu, Rika Antonova, Adam Kan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Perception</span><span class="keyword-tag">Planning</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4444262400">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people&#x27;s preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We also demonstrate our approach on a real-world mobile manipulator called TidyBot, which successfully puts away 85.0% of objects in real-world test scenarios.</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2305.05658v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2305.05658v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Wu et al., &quot;TidyBot: Personalized Robot Assistance with Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2305.05658v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444262400')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="a variable autonomy approach for an automated weeding platform" data-keywords="robot imu cs.ro" data-themes="S">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2303.05461v1" target="_blank">A Variable Autonomy approach for an Automated Weeding Platform</a>
                            </h3>
                            <p class="card-authors">Ionut Moraru, Tsvetan Zhivkov, Shaun Coutts et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Imu</span><span class="keyword-tag">CS.RO</span></div>
                            <div class="card-details" id="cat-details-4444262208">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Climate change, increase in world population and the war in Ukraine have led nations such as the UK to put a larger focus on food security, while simultaneously trying to halt declines in biodiversity and reduce risks to human health posed by chemically-reliant farming practices. Achieving these goals simultaneously will require novel approaches and accelerating the deployment of Agri-Robotics from the lab and into the field. In this paper we describe the ARWAC robot platform for mechanical weeding. We explain why the mechanical weeding approach is beneficial compared to the use of pesticides ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Thereafter, we present the system design and processing pipeline for generating a course of action for the robot to follow, such that it removes as many weeds as possible</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2303.05461v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2303.05461v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'I. Moraru, T. Zhivkov, S. Coutts, D. Li, and E. I. Sklar, &quot;A Variable Autonomy approach for an Automated Weeding Platform,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2303.05461v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444262208')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="learning and autonomy for extraterrestrial terrain sampling: an experience report from owlat deployment" data-keywords="control cs.ro" data-themes="S I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2311.17405v2" target="_blank">Learning and Autonomy for Extraterrestrial Terrain Sampling: An Experience Report from OWLAT Deployment</a>
                            </h3>
                            <p class="card-authors">Pranay Thangeda, Ashish Goel, Erica Tevere et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Control</span><span class="keyword-tag">CS.RO</span></div>
                            <div class="card-details" id="cat-details-4444266576">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Extraterrestrial autonomous lander missions increasingly demand adaptive capabilities to handle the unpredictable and diverse nature of the terrain. This paper discusses the deployment of a Deep Meta-Learning with Controlled Deployment Gaps (CoDeGa) trained model for terrain scooping tasks in Ocean Worlds Lander Autonomy Testbed (OWLAT) at NASA Jet Propulsion Laboratory. The CoDeGa-powered scooping strategy is designed to adapt to novel terrains, selecting scooping actions based on the available RGB-D image data and limited experience. The paper presents our experiences with transferring the s...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2311.17405v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2311.17405v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Thangeda et al., &quot;Learning and Autonomy for Extraterrestrial Terrain Sampling: An Experience Report from OWLAT Deployment,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2311.17405v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444266576')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="clipswarm: converting text into formations of robots" data-keywords="robot swarm multi-robot optimization cs.ro" data-themes="E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2311.11047v1" target="_blank">CLIPSwarm: Converting text into formations of robots</a>
                            </h3>
                            <p class="card-authors">Pablo Pueyo, Eduardo Montijano, Ana C. Murillo et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Swarm</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Optimization</span></div>
                            <div class="card-details" id="cat-details-4445407264">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We present CLIPSwarm, an algorithm to generate robot swarm formations from natural language descriptions. CLIPSwarm receives an input text and finds the position of the robots to form a shape that corresponds to the given text. To do so, we implement a variation of the Montecarlo particle filter to obtain a matching formation iteratively. In every iteration, we generate a set of new formations and evaluate their Clip Similarity with the given text, selecting the best formations according to this metric. This metric is obtained using Clip, [1], an existing foundation model trained to encode ima...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present CLIPSwarm, an algorithm to generate robot swarm formations from natural language descriptions</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2311.11047v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2311.11047v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Pueyo, E. Montijano, A. C. Murillo, and M. Schwager, &quot;CLIPSwarm: Converting text into formations of robots,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2311.11047v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445407264')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="state-of-the-art in robot learning for multi-robot collaboration: a comprehensive survey" data-keywords="robot multi-robot cs.ro cs.ai" data-themes="S E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Multi-Agent Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.11822v1" target="_blank">State-of-the-art in Robot Learning for Multi-Robot Collaboration: A Comprehensive Survey</a>
                            </h3>
                            <p class="card-authors">Bin Wu, C Steve Suh</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">CS.RO</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4444264320">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">With the continuous breakthroughs in core technology, the dawn of large-scale integration of robotic systems into daily human life is on the horizon. Multi-robot systems (MRS) built on this foundation are undergoing drastic evolution. The fusion of artificial intelligence technology with robot hardware is seeing broad application possibilities for MRS. This article surveys the state-of-the-art of robot learning in the context of Multi-Robot Cooperation (MRC) of recent. Commonly adopted robot learning methods (or frameworks) that are inspired by humans and animals are reviewed and their advanta...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.11822v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.11822v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. Wu, and C. S. Suh, &quot;State-of-the-art in Robot Learning for Multi-Robot Collaboration: A Comprehensive Survey,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.11822v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444264320')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="learning getting-up policies for real-world humanoid robots" data-keywords="robot humanoid locomotion control cs.ro cs.lg" data-themes="S L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.12152v2" target="_blank">Learning Getting-Up Policies for Real-World Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Xialin He, Runpei Dong, Zixuan Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Control</span></div>
                            <div class="card-details" id="cat-details-4444264560">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Automatic fall recovery is a crucial prerequisite before humanoid robots can be reliably deployed. Hand-designing controllers for getting up is difficult because of the varied configurations a humanoid can end up in after a fall and the challenging terrains humanoid robots are expected to operate on. This paper develops a learning framework to produce controllers that enable humanoid robots to get up from varying configurations on varying terrains. Unlike previous successful applications of learning to humanoid locomotion, the getting-up task involves complex contact patterns (which necessitat...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.12152v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.12152v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. He, R. Dong, Z. Chen, and S. Gupta, &quot;Learning Getting-Up Policies for Real-World Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.12152v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444264560')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="unified multi-rate model predictive control for a jet-powered humanoid robot" data-keywords="robot humanoid control simulation mujoco imu cs.ro eess.sy" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2505.16478v2" target="_blank">Unified Multi-Rate Model Predictive Control for a Jet-Powered Humanoid Robot</a>
                            </h3>
                            <p class="card-authors">Davide Gorbani, Giuseppe L&#x27;Erario, Hosameldin Awadalla Omer Mohamed et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Control</span><span class="keyword-tag">Simulation</span></div>
                            <div class="card-details" id="cat-details-4445407840">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We propose a novel Model Predictive Control (MPC) framework for a jet-powered flying humanoid robot. The controller is based on a linearised centroidal momentum model to represent the flight dynamics, augmented with a second-order nonlinear model to explicitly account for the slow and nonlinear dynamics of jet propulsion. A key contribution is the introduction of a multi-rate MPC formulation that handles the different actuation rates of the robot&#x27;s joints and jet engines while embedding the jet dynamics directly into the predictive model. We validated the framework using the jet-powered humano...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a novel Model Predictive Control (MPC) framework for a jet-powered flying humanoid robot</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2505.16478v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2505.16478v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Gorbani, G. L&#x27;Erario, H. A. O. Mohamed, and D. Pucci, &quot;Unified Multi-Rate Model Predictive Control for a Jet-Powered Humanoid Robot,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2505.16478v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445407840')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="robot trains robot: automatic real-world policy adaptation and learning for humanoids" data-keywords="reinforcement learning robot humanoid locomotion simulation imu cs.ro" data-themes="S L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.12252v2" target="_blank">Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids</a>
                            </h3>
                            <p class="card-authors">Kaizhe Hu, Haochen Shi, Yao He et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4444264800">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Simulation-based reinforcement learning (RL) has significantly advanced humanoid locomotion tasks, yet direct real-world RL from scratch or adapting from pretrained policies remains rare, limiting the full potential of humanoid robots. Real-world learning, despite being crucial for overcoming the sim-to-real gap, faces substantial challenges related to safety, reward design, and learning efficiency. To address these limitations, we propose Robot-Trains-Robot (RTR), a novel framework where a robotic arm teacher actively supports and guides a humanoid robot student. The RTR system provides prote...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address these limitations, we propose Robot-Trains-Robot (RTR), a novel framework where a robotic arm teacher actively supports and guides a humanoid robot student</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.12252v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.12252v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Hu, H. Shi, Y. He, W. Wang, C. K. Liu, and S. Song, &quot;Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.12252v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444264800')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="how to raise a robot -- a case for neuro-symbolic ai in constrained task planning for humanoid assistive robots" data-keywords="neural network robot humanoid planning control language model cs.ro cs.cr cs.lg" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.08820v3" target="_blank">How to Raise a Robot -- A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots</a>
                            </h3>
                            <p class="card-authors">Niklas Hemken, Florian Jacob, Fabian Peller-Konrad et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Planning</span></div>
                            <div class="card-details" id="cat-details-4445408224">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots will be able to assist humans in their daily life, in particular due to their versatile action capabilities. However, while these robots need a certain degree of autonomy to learn and explore, they also should respect various constraints, for access control and beyond. We explore the novel field of incorporating privacy, security, and access control constraints with robot task planning approaches. We report preliminary results on the classical symbolic approach, deep-learned neural networks, and modern ideas using large language models as knowledge base. From analyzing their tr...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.08820v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.08820v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. Hemken, F. Jacob, F. Peller-Konrad, R. Kartmann, T. Asfour, and H. Hartenstein, &quot;How to Raise a Robot -- A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.08820v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445408224')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="a framework for optimal ankle design of humanoid robots" data-keywords="robot humanoid optimization ros cs.ro" data-themes="M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.16469v1" target="_blank">A Framework for Optimal Ankle Design of Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Guglielmo Cervettini, Roberto Mauceri, Alex Coppola et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">Ros</span></div>
                            <div class="card-details" id="cat-details-4444266480">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The design of the humanoid ankle is critical for safe and efficient ground interaction. Key factors such as mechanical compliance and motor mass distribution have driven the adoption of parallel mechanism architectures. However, selecting the optimal configuration depends on both actuator availability and task requirements. We propose a unified methodology for the design and evaluation of parallel ankle mechanisms. A multi-objective optimization synthesizes the mechanism geometry, the resulting solutions are evaluated using a scalar cost function that aggregates key performance metrics for cro...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a unified methodology for the design and evaluation of parallel ankle mechanisms</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.16469v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.16469v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Cervettini et al., &quot;A Framework for Optimal Ankle Design of Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.16469v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444266480')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="perception for humanoid robots" data-keywords="robot humanoid perception optimization imu sensor fusion cs.ro cs.ai" data-themes="S M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.15616v1" target="_blank">Perception for Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Arindam Roychoudhury, Shahram Khorshidi, Subham Agrawal et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Perception</span><span class="keyword-tag">Optimization</span></div>
                            <div class="card-details" id="cat-details-4444265328">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Purpose of Review: The field of humanoid robotics, perception plays a fundamental role in enabling robots to interact seamlessly with humans and their surroundings, leading to improved safety, efficiency, and user experience. This scientific study investigates various perception modalities and techniques employed in humanoid robots, including visual, auditory, and tactile sensing by exploring recent state-of-the-art approaches for perceiving and understanding the internal state, the environment, objects, and human activities.
  Recent Findings: Internal state estimation makes extensive use of ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.15616v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.15616v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Roychoudhury, S. Khorshidi, S. Agrawal, and M. Bennewitz, &quot;Perception for Humanoid Robots,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.15616v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444265328')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="incremental learning of humanoid robot behavior from natural interaction and large language models" data-keywords="robot humanoid perception simulation imu language model cs.ro cs.ai" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.04316v3" target="_blank">Incremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models</a>
                            </h3>
                            <p class="card-authors">Leonard B√§rmann, Rainer Kartmann, Fabian Peller-Konrad et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Perception</span><span class="keyword-tag">Simulation</span></div>
                            <div class="card-details" id="cat-details-4445407024">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Natural-language dialog is key for intuitive human-robot interaction. It can be used not only to express humans&#x27; intents, but also to communicate instructions for improvement if a robot does not understand a command correctly. Of great importance is to endow robots with the ability to learn from such interaction experience in an incremental way to allow them to improve their behaviors or avoid mistakes in the future. In this paper, we propose a system to achieve incremental learning of complex behavior from natural interaction, and demonstrate its implementation on a humanoid robot. Building o...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose a system to achieve incremental learning of complex behavior from natural interaction, and demonstrate its implementation on a humanoid robot</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.04316v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.04316v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. B√§rmann, R. Kartmann, F. Peller-Konrad, J. Niehues, A. Waibel, and T. Asfour, &quot;Incremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.04316v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445407024')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="compositional coordination for multi-robot teams with large language models" data-keywords="robot multi-robot coordination control simulation ros imu language model cs.ro cs.ai" data-themes="S E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.16068v3" target="_blank">Compositional Coordination for Multi-Robot Teams with Large Language Models</a>
                            </h3>
                            <p class="card-authors">Zhehui Huang, Guangyao Shi, Yuwei Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Coordination</span><span class="keyword-tag">Control</span></div>
                            <div class="card-details" id="cat-details-4444274640">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Multi-robot coordination has traditionally relied on a mission-specific and expert-driven pipeline, where natural language mission descriptions are manually translated by domain experts into mathematical formulation, algorithm design, and executable code. This conventional process is labor-intensive, inaccessible to non-experts, and inflexible to changes in mission requirements. Here, we propose LAN2CB (Language to Collective Behavior), a novel framework that leverages large language models (LLMs) to streamline and generalize the multi-robot coordination pipeline. LAN2CB transforms natural lan...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Here, we propose LAN2CB (Language to Collective Behavior), a novel framework that leverages large language models (LLMs) to streamline and generalize the multi-robot coordination pipeline</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.16068v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.16068v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Huang, G. Shi, Y. Wu, V. Kumar, and G. S. Sukhatme, &quot;Compositional Coordination for Multi-Robot Teams with Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.16068v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444274640')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="hierarchical reduced-order model predictive control for robust locomotion on humanoid robots" data-keywords="robot humanoid locomotion planning control simulation ros imu cs.ro" data-themes="S L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.04722v1" target="_blank">Hierarchical Reduced-Order Model Predictive Control for Robust Locomotion on Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Adrian B. Ghansah, Sergio A. Esteban, Aaron D. Ames</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Planning</span></div>
                            <div class="card-details" id="cat-details-4445408608">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">As humanoid robots enter real-world environments, ensuring robust locomotion across diverse environments is crucial. This paper presents a computationally efficient hierarchical control framework for humanoid robot locomotion based on reduced-order models -- enabling versatile step planning and incorporating arm and torso dynamics to better stabilize the walking. At the high level, we use the step-to-step dynamics of the ALIP model to simultaneously optimize over step periods, step lengths, and ankle torques via nonlinear MPC. The ALIP trajectories are used as references to a linear MPC framew...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents a computationally efficient hierarchical control framework for humanoid robot locomotion based on reduced-order models -- enabling versatile step planning and incorporating arm and torso dynamics to better stabilize the walking</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.04722v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.04722v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. B. Ghansah, S. A. Esteban, and A. D. Ames, &quot;Hierarchical Reduced-Order Model Predictive Control for Robust Locomotion on Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.04722v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445408608')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="learning vision-driven reactive soccer skills for humanoid robots" data-keywords="reinforcement learning robot humanoid coordination perception control ros cs.ro" data-themes="S I R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2511.03996v1" target="_blank">Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Yushi Wang, Changsheng Luo, Penghui Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Coordination</span></div>
                            <div class="card-details" id="cat-details-4445407792">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid soccer poses a representative challenge for embodied intelligence, requiring robots to operate within a tightly coupled perception-action loop. However, existing systems typically rely on decoupled modules, resulting in delayed responses and incoherent behaviors in dynamic environments, while real-world perceptual limitations further exacerbate these issues. In this work, we present a unified reinforcement learning-based controller that enables humanoid robots to acquire reactive soccer skills through the direct integration of visual perception and motion control. Our approach extends...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we present a unified reinforcement learning-based controller that enables humanoid robots to acquire reactive soccer skills through the direct integration of visual perception and motion control</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2511.03996v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2511.03996v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Wang et al., &quot;Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2511.03996v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445407792')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="influence of team interactions on multi-robot cooperation: a relational network perspective" data-keywords="robot multi-robot cooperative coordination cs.ro cs.ma" data-themes="S R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Multi-Agent Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2310.12910v1" target="_blank">Influence of Team Interactions on Multi-Robot Cooperation: A Relational Network Perspective</a>
                            </h3>
                            <p class="card-authors">Yasin Findik, Hamid Osooli, Paul Robinette et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Cooperative</span><span class="keyword-tag">Coordination</span></div>
                            <div class="card-details" id="cat-details-4445408800">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Relational networks within a team play a critical role in the performance of many real-world multi-robot systems. To successfully accomplish tasks that require cooperation and coordination, different agents (e.g., robots) necessitate different priorities based on their positioning within the team. Yet, many of the existing multi-robot cooperation algorithms regard agents as interchangeable and lack a mechanism to guide the type of cooperation strategy the agents should exhibit. To account for the team structure in cooperative tasks, we propose a novel algorithm that uses a relational network c...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To account for the team structure in cooperative tasks, we propose a novel algorithm that uses a relational network comprising inter-agent relationships to prioritize certain agents over others</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2310.12910v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2310.12910v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Findik, H. Osooli, P. Robinette, K. Jerath, and S. R. Ahmadzadeh, &quot;Influence of Team Interactions on Multi-Robot Cooperation: A Relational Network Perspective,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2310.12910v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445408800')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="deep reinforcement learning for decentralized multi-robot control: a dqn approach to robustness and information integration" data-keywords="reinforcement learning robot multi-robot control optimization imu cs.ro cs.ma" data-themes="S R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-skip">‚è≠Ô∏è Skip</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.11339v1" target="_blank">Deep Reinforcement Learning for Decentralized Multi-Robot Control: A DQN Approach to Robustness and Information Integration</a>
                            </h3>
                            <p class="card-authors">Bin Wu, C Steve Suh</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Control</span></div>
                            <div class="card-details" id="cat-details-4445407696">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The superiority of Multi-Robot Systems (MRS) in various complex environments is unquestionable. However, in complex situations such as search and rescue, environmental monitoring, and automated production, robots are often required to work collaboratively without a central control unit. This necessitates an efficient and robust decentralized control mechanism to process local information and guide the robots&#x27; behavior. In this work, we propose a new decentralized controller design method that utilizes the Deep Q-Network (DQN) algorithm from deep reinforcement learning, aimed at improving the i...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose a new decentralized controller design method that utilizes the Deep Q-Network (DQN) algorithm from deep reinforcement learning, aimed at improving the integration of local information and robustness of multi-robot systems</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.11339v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.11339v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. Wu, and C. S. Suh, &quot;Deep Reinforcement Learning for Decentralized Multi-Robot Control: A DQN Approach to Robustness and Information Integration,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.11339v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445407696')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="real-time polygonal semantic mapping for humanoid robot stair climbing" data-keywords="diffusion robot humanoid planning cs.ro cs.cv" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2411.01919v1" target="_blank">Real-Time Polygonal Semantic Mapping for Humanoid Robot Stair Climbing</a>
                            </h3>
                            <p class="card-authors">Teng Bin, Jianming Yao, Tin Lun Lam et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Diffusion</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Planning</span></div>
                            <div class="card-details" id="cat-details-4445407648">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We present a novel algorithm for real-time planar semantic mapping tailored for humanoid robots navigating complex terrains such as staircases. Our method is adaptable to any odometry input and leverages GPU-accelerated processes for planar extraction, enabling the rapid generation of globally consistent semantic maps. We utilize an anisotropic diffusion filter on depth images to effectively minimize noise from gradient jumps while preserving essential edge details, enhancing normal vector images&#x27; accuracy and smoothness. Both the anisotropic diffusion and the RANSAC-based plane extraction pro...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present a novel algorithm for real-time planar semantic mapping tailored for humanoid robots navigating complex terrains such as staircases</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2411.01919v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2411.01919v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. Bin, J. Yao, T. L. Lam, and T. Zhang, &quot;Real-Time Polygonal Semantic Mapping for Humanoid Robot Stair Climbing,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2411.01919v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445407648')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="centroidal state estimation and control for hardware-constrained humanoid robots" data-keywords="robot humanoid control cs.ro" data-themes="L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.11019v1" target="_blank">Centroidal State Estimation and Control for Hardware-constrained Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Grzegorz Ficht, Sven Behnke</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Control</span><span class="keyword-tag">CS.RO</span></div>
                            <div class="card-details" id="cat-details-4445408560">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We introduce novel methods for state estimation, feedforward and feedback control, which specifically target humanoid robots with hardware limitations. Our method combines a five-mass model with approximate dynamics of each mass. It enables acquiring an accurate assessment of the centroidal state and Center of Pressure, even when direct forms of force or contact sensing are unavailable. Upon this, we develop a feedforward scheme that operates on the centroidal state, accounting for insufficient joint tracking capabilities. Finally, we implement feedback mechanisms, which compensate for the lac...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce novel methods for state estimation, feedforward and feedback control, which specifically target humanoid robots with hardware limitations</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.11019v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.11019v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Ficht, and S. Behnke, &quot;Centroidal State Estimation and Control for Hardware-constrained Humanoid Robots,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.11019v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445408560')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="policies over poses: reinforcement learning based distributed pose-graph optimization for multi-robot slam" data-keywords="reinforcement learning neural network gnn robot multi-agent multi-robot slam optimization imu cs.ro" data-themes="S R M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2510.22740v1" target="_blank">Policies over Poses: Reinforcement Learning based Distributed Pose-Graph Optimization for Multi-Robot SLAM</a>
                            </h3>
                            <p class="card-authors">Sai Krishna Ghanta, Ramviyas Parasuraman</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-R">Multi-Robot</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Gnn</span><span class="keyword-tag">Robot</span></div>
                            <div class="card-details" id="cat-details-4445408176">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We consider the distributed pose-graph optimization (PGO) problem, which is fundamental in accurate trajectory estimation in multi-robot simultaneous localization and mapping (SLAM). Conventional iterative approaches linearize a highly non-convex optimization objective, requiring repeated solving of normal equations, which often converge to local minima and thus produce suboptimal estimates. We propose a scalable, outlier-robust distributed planar PGO framework using Multi-Agent Reinforcement Learning (MARL). We cast distributed PGO as a partially observable Markov game defined on local pose-g...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a scalable, outlier-robust distributed planar PGO framework using Multi-Agent Reinforcement Learning (MARL)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2510.22740v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2510.22740v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. K. Ghanta, and R. Parasuraman, &quot;Policies over Poses: Reinforcement Learning based Distributed Pose-Graph Optimization for Multi-Robot SLAM,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2510.22740v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445408176')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="destination-to-chutes task mapping optimization for multi-robot coordination in robotic sorting systems" data-keywords="robot planning optimization imu cs.ro cs.ai cs.ma" data-themes="S R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2510.03472v1" target="_blank">Destination-to-Chutes Task Mapping Optimization for Multi-Robot Coordination in Robotic Sorting Systems</a>
                            </h3>
                            <p class="card-authors">Yulun Zhang, Alexandre O. G. Barbosa, Federico Pecora et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Planning</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">Imu</span></div>
                            <div class="card-details" id="cat-details-4445407168">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We study optimizing a destination-to-chutes task mapping to improve throughput in Robotic Sorting Systems (RSS), where a team of robots sort packages on a sortation floor by transporting them from induct workstations to eject chutes based on their shipping destinations (e.g. Los Angeles or Pittsburgh). The destination-to-chutes task mapping is used to determine which chutes a robot can drop its package. Finding a high-quality task mapping is challenging because of the complexity of a real-world RSS. First, optimizing task mapping is interdependent with robot target assignment and path planning...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2510.03472v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2510.03472v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Zhang, A. O. G. Barbosa, F. Pecora, and J. Li, &quot;Destination-to-Chutes Task Mapping Optimization for Multi-Robot Coordination in Robotic Sorting Systems,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2510.03472v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445407168')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="whole-body multi-contact motion control for humanoid robots based on distributed tactile sensors" data-keywords="robot humanoid control simulation imu cs.ro" data-themes="L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2505.19580v1" target="_blank">Whole-body Multi-contact Motion Control for Humanoid Robots Based on Distributed Tactile Sensors</a>
                            </h3>
                            <p class="card-authors">Masaki Murooka, Kensuke Fukumitsu, Marwan Hamze et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Control</span><span class="keyword-tag">Simulation</span></div>
                            <div class="card-details" id="cat-details-4445406976">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">To enable humanoid robots to work robustly in confined environments, multi-contact motion that makes contacts not only at extremities, such as hands and feet, but also at intermediate areas of the limbs, such as knees and elbows, is essential. We develop a method to realize such whole-body multi-contact motion involving contacts at intermediate areas by a humanoid robot. Deformable sheet-shaped distributed tactile sensors are mounted on the surface of the robot&#x27;s limbs to measure the contact force without significantly changing the robot body shape. The multi-contact motion controller develope...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We develop a method to realize such whole-body multi-contact motion involving contacts at intermediate areas by a humanoid robot</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2505.19580v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2505.19580v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Murooka et al., &quot;Whole-body Multi-contact Motion Control for Humanoid Robots Based on Distributed Tactile Sensors,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2505.19580v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445406976')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="design, calibration, and control of compliant force-sensing gripping pads for humanoid robots" data-keywords="robot humanoid control cs.ro eess.sy" data-themes="M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.20969v1" target="_blank">Design, Calibration, and Control of Compliant Force-sensing Gripping Pads for Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Yuanfeng Han, Boren Jiang, Gregory S. Chirikjian</p>
                            <div class="theme-tags"><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Control</span><span class="keyword-tag">CS.RO</span></div>
                            <div class="card-details" id="cat-details-4445408704">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper introduces a pair of low-cost, light-weight and compliant force-sensing gripping pads used for manipulating box-like objects with smaller-sized humanoid robots. These pads measure normal gripping forces and center of pressure (CoP). A calibration method is developed to improve the CoP measurement accuracy. A hybrid force-alignment-position control framework is proposed to regulate the gripping forces and to ensure the surface alignment between the grippers and the object. Limit surface theory is incorporated as a contact friction modeling approach to determine the magnitude of gripp...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.20969v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.20969v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Han, B. Jiang, and G. S. Chirikjian, &quot;Design, Calibration, and Control of Compliant Force-sensing Gripping Pads for Humanoid Robots,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.20969v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445408704')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="can a single human supervise a swarm of 100 heterogeneous robots?" data-keywords="robot swarm control ros cs.ro cs.ai cs.hc" data-themes="S R M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2308.00102v1" target="_blank">Can A Single Human Supervise A Swarm of 100 Heterogeneous Robots?</a>
                            </h3>
                            <p class="card-authors">Julie A. Adams, Joshua Hamell, Phillip Walker</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-R">Multi-Robot</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Swarm</span><span class="keyword-tag">Control</span><span class="keyword-tag">Ros</span></div>
                            <div class="card-details" id="cat-details-4445409280">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">An open research question has been whether a single human can supervise a true heterogeneous swarm of robots completing tasks in real world environments. A general concern is whether or not the human&#x27;s workload will be taxed to the breaking point. The Defense Advanced Research Projects Agency&#x27;s OFFsensive Swarm-Enabled Tactics program&#x27;s field exercises that occurred at U.S. Army urban training sites provided the opportunity to understand the impact of achieving such swarm deployments. The Command and Control of Aggregate Swarm Tactics integrator team&#x27;s swarm commander users the heterogeneous r...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2308.00102v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2308.00102v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. A. Adams, J. Hamell, and P. Walker, &quot;Can A Single Human Supervise A Swarm of 100 Heterogeneous Robots?,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2308.00102v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445409280')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="mash: cooperative-heterogeneous multi-agent reinforcement learning for single humanoid robot locomotion" data-keywords="reinforcement learning robot humanoid locomotion multi-agent multi-robot cooperative control cs.ro cs.ai" data-themes="L I R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.10423v1" target="_blank">MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion</a>
                            </h3>
                            <p class="card-authors">Qi Liu, Xiaopeng Zhang, Mingshan Tan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4445407312">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper proposes a novel method to enhance locomotion for a single humanoid robot through cooperative-heterogeneous multi-agent deep reinforcement learning (MARL). While most existing methods typically employ single-agent reinforcement learning algorithms for a single humanoid robot or MARL algorithms for multi-robot system tasks, we propose a distinct paradigm: applying cooperative-heterogeneous MARL to optimize locomotion for a single humanoid robot. The proposed method, multi-agent reinforcement learning for single humanoid locomotion (MASH), treats each limb (legs and arms) as an indepe...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">While most existing methods typically employ single-agent reinforcement learning algorithms for a single humanoid robot or MARL algorithms for multi-robot system tasks, we propose a distinct paradigm: applying cooperative-heterogeneous MARL to optimize locomotion for a single humanoid robot</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.10423v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.10423v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Q. Liu, X. Zhang, M. Tan, S. Ma, J. Ding, and Y. Li, &quot;MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.10423v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445407312')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="inekformer: a hybrid state estimator for humanoid robots" data-keywords="deep learning transformer robot humanoid bipedal locomotion control cs.ro" data-themes="L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2511.16306v1" target="_blank">InEKFormer: A Hybrid State Estimator for Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Lasse Hohmeyer, Mihaela Popescu, Ivan Bergonzani et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Deep Learning</span><span class="keyword-tag">Transformer</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span></div>
                            <div class="card-details" id="cat-details-4445419552">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots have great potential for a wide range of applications, including industrial and domestic use, healthcare, and search and rescue missions. However, bipedal locomotion in different environments is still a challenge when it comes to performing stable and dynamic movements. This is where state estimation plays a crucial role, providing fast and accurate feedback of the robot&#x27;s floating base state to the motion controller. Although classical state estimation methods such as Kalman filters are widely used in robotics, they require expert knowledge to fine-tune the noise parameters. D...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose the InEKFormer, a novel hybrid state estimation method that incorporates an invariant extended Kalman filter (InEKF) and a Transformer network</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2511.16306v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2511.16306v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Hohmeyer, M. Popescu, I. Bergonzani, D. Mronga, and F. Kirchner, &quot;InEKFormer: A Hybrid State Estimator for Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2511.16306v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445419552')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="generalization of heterogeneous multi-robot policies via awareness and communication of capabilities" data-keywords="reinforcement learning robot multi-agent multi-robot coordination cs.ro cs.ma" data-themes="S R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Multi-Agent Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.13127v1" target="_blank">Generalization of Heterogeneous Multi-Robot Policies via Awareness and Communication of Capabilities</a>
                            </h3>
                            <p class="card-authors">Pierce Howell, Max Rudolph, Reza Torbati et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Agent</span><span class="keyword-tag">Multi-Robot</span></div>
                            <div class="card-details" id="cat-details-4445407504">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recent advances in multi-agent reinforcement learning (MARL) are enabling impressive coordination in heterogeneous multi-robot teams. However, existing approaches often overlook the challenge of generalizing learned policies to teams of new compositions, sizes, and robots. While such generalization might not be important in teams of virtual agents that can retrain policies on-demand, it is pivotal in multi-robot systems that are deployed in the real-world and must readily adapt to inevitable changes. As such, multi-robot policies must remain robust to team changes -- an ability we call adaptiv...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We demonstrate that shared decentralized policies, that enable robots to be both aware of and communicate their capabilities, can achieve adaptive teaming by implicitly capturing the fundamental relationship between collective capabilities and effective coordination</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.13127v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.13127v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Howell, M. Rudolph, R. Torbati, K. Fu, and H. Ravichandar, &quot;Generalization of Heterogeneous Multi-Robot Policies via Awareness and Communication of Capabilities,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.13127v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445407504')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="multi-scenario reasoning: unlocking cognitive autonomy in humanoid robots for multimodal understanding" data-keywords="robot humanoid planning simulation ros imu cs.ro cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.20429v4" target="_blank">Multi-Scenario Reasoning: Unlocking Cognitive Autonomy in Humanoid Robots for Multimodal Understanding</a>
                            </h3>
                            <p class="card-authors">Libo Wang</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Planning</span><span class="keyword-tag">Simulation</span></div>
                            <div class="card-details" id="cat-details-4445408896">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">To improve the cognitive autonomy of humanoid robots, this research proposes a multi-scenario reasoning architecture to solve the technical shortcomings of multi-modal understanding in this field. It draws on simulation based experimental design that adopts multi-modal synthesis (visual, auditory, tactile) and builds a simulator &quot;Maha&quot; to perform the experiment. The findings demonstrate the feasibility of this architecture in multimodal data. It provides reference experience for the exploration of cross-modal interaction strategies for humanoid robots in dynamic environments. In addition, mult...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.20429v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.20429v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Wang, &quot;Multi-Scenario Reasoning: Unlocking Cognitive Autonomy in Humanoid Robots for Multimodal Understanding,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.20429v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445408896')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="multi-robot rendezvous in unknown environment with limited communication" data-keywords="robot multi-robot simulation imu cs.ro" data-themes="S R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Multi-Agent Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.08345v2" target="_blank">Multi-Robot Rendezvous in Unknown Environment with Limited Communication</a>
                            </h3>
                            <p class="card-authors">Kun Song, Gaoming Chen, Wenhang Liu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Simulation</span><span class="keyword-tag">Imu</span></div>
                            <div class="card-details" id="cat-details-4445408656">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Rendezvous aims at gathering all robots at a specific location, which is an important collaborative behavior for multi-robot systems. However, in an unknown environment, it is challenging to achieve rendezvous. Previous researches mainly focus on special scenarios where communication is not allowed and each robot executes a random searching strategy, which is highly time-consuming, especially in large-scale environments. In this work, we focus on rendezvous in unknown environments where communication is available. We divide this task into two steps: rendezvous based environment exploration wit...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.08345v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.08345v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Song, G. Chen, W. Liu, and Z. Xiong, &quot;Multi-Robot Rendezvous in Unknown Environment with Limited Communication,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.08345v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445408656')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="robot see robot do: imitating articulated object manipulation with monocular 4d reconstruction" data-keywords="robot manipulation planning optimization ros cs.ro cs.cv" data-themes="I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.18121v1" target="_blank">Robot See Robot Do: Imitating Articulated Object Manipulation with Monocular 4D Reconstruction</a>
                            </h3>
                            <p class="card-authors">Justin Kerr, Chung Min Kim, Mingxuan Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Manipulation</span><span class="keyword-tag">Planning</span><span class="keyword-tag">Optimization</span></div>
                            <div class="card-details" id="cat-details-4445409088">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humans can learn to manipulate new objects by simply watching others; providing robots with the ability to learn from such demonstrations would enable a natural interface specifying new behaviors. This work develops Robot See Robot Do (RSRD), a method for imitating articulated object manipulation from a single monocular RGB human demonstration given a single static multi-view object scan. We first propose 4D Differentiable Part Models (4D-DPM), a method for recovering 3D part motion from a monocular video with differentiable rendering. This analysis-by-synthesis approach uses part-centric feat...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.18121v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.18121v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Kerr et al., &quot;Robot See Robot Do: Imitating Articulated Object Manipulation with Monocular 4D Reconstruction,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.18121v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445409088')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="generalizable humanoid manipulation with 3d diffusion policies" data-keywords="diffusion robot humanoid manipulation lidar cs.ro cs.cv cs.lg" data-themes="S M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.10803v3" target="_blank">Generalizable Humanoid Manipulation with 3D Diffusion Policies</a>
                            </h3>
                            <p class="card-authors">Yanjie Ze, Zixuan Chen, Wenhao Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Diffusion</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Manipulation</span></div>
                            <div class="card-details" id="cat-details-4445407216">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots capable of autonomous operation in diverse environments have long been a goal for roboticists. However, autonomous manipulation by humanoid robots has largely been restricted to one specific scene, primarily due to the difficulty of acquiring generalizable skills and the expensiveness of in-the-wild humanoid robot data. In this work, we build a real-world robotic system to address this challenging problem. Our system is mainly an integration of 1) a whole-upper-body robotic teleoperation system to acquire human-like robot data, 2) a 25-DoF humanoid robot platform with a height-...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.10803v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.10803v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Ze et al., &quot;Generalizable Humanoid Manipulation with 3D Diffusion Policies,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.10803v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445407216')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="humanplus: humanoid shadowing and imitation from humans" data-keywords="reinforcement learning robot humanoid perception control simulation camera imu cs.ro cs.ai" data-themes="L I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2406.10454v1" target="_blank">HumanPlus: Humanoid Shadowing and Imitation from Humans</a>
                            </h3>
                            <p class="card-authors">Zipeng Fu, Qingqing Zhao, Qi Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Perception</span></div>
                            <div class="card-details" id="cat-details-4445419408">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">One of the key arguments for building robots that have similar form factors to human beings is that we can leverage the massive human data for training. Yet, doing so has remained challenging in practice due to the complexities in humanoid perception and control, lingering physical gaps between humanoids and humans in morphologies and actuation, and lack of a data pipeline for humanoids to learn autonomous skills from egocentric vision. In this paper, we introduce a full-stack system for humanoids to learn motion and autonomous skills from human data. We first train a low-level policy in simul...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we introduce a full-stack system for humanoids to learn motion and autonomous skills from human data</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2406.10454v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2406.10454v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Fu, Q. Zhao, Q. Wu, G. Wetzstein, and C. Finn, &quot;HumanPlus: Humanoid Shadowing and Imitation from Humans,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2406.10454v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445419408')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="harmon: whole-body motion generation of humanoid robots from language descriptions" data-keywords="robot humanoid imu language model cs.ro cs.ai" data-themes="S E L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.12773v1" target="_blank">Harmon: Whole-Body Motion Generation of Humanoid Robots from Language Descriptions</a>
                            </h3>
                            <p class="card-authors">Zhenyu Jiang, Yuqi Xie, Jinhan Li et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Imu</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4445407984">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots, with their human-like embodiment, have the potential to integrate seamlessly into human environments. Critical to their coexistence and cooperation with humans is the ability to understand natural language communications and exhibit human-like behaviors. This work focuses on generating diverse whole-body motions for humanoid robots from language descriptions. We leverage human motion priors from extensive human motion datasets to initialize humanoid motions and employ the commonsense reasoning capabilities of Vision Language Models (VLMs) to edit and refine these motions. Our ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Our approach demonstrates the capability to produce natural, expressive, and text-aligned humanoid motions, validated through both simulated and real-world experiments</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.12773v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.12773v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Jiang, Y. Xie, J. Li, Y. Yuan, Y. Zhu, and Y. Zhu, &quot;Harmon: Whole-Body Motion Generation of Humanoid Robots from Language Descriptions,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.12773v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445407984')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="from rolling over to walking: enabling humanoid robots to develop complex motor skills" data-keywords="reinforcement learning robot humanoid simulation ros imu cs.ro" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2303.02581v2" target="_blank">From Rolling Over to Walking: Enabling Humanoid Robots to Develop Complex Motor Skills</a>
                            </h3>
                            <p class="card-authors">Fanxing Meng, Jing Xiao</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Simulation</span></div>
                            <div class="card-details" id="cat-details-4445407360">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents an innovative method for humanoid robots to acquire a comprehensive set of motor skills through reinforcement learning. The approach utilizes an achievement-triggered multi-path reward function rooted in developmental robotics principles, facilitating the robot to learn gross motor skills typically mastered by human infants within a single training phase. The proposed method outperforms standard reinforcement learning techniques in success rates and learning speed within a simulation environment. By leveraging the principles of self-discovery and exploration integral to inf...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents an innovative method for humanoid robots to acquire a comprehensive set of motor skills through reinforcement learning</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2303.02581v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2303.02581v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. Meng, and J. Xiao, &quot;From Rolling Over to Walking: Enabling Humanoid Robots to Develop Complex Motor Skills,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2303.02581v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445407360')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="advantages of multimodal versus verbal-only robot-to-human communication with an anthropomorphic robotic mock driver" data-keywords="attention robot humanoid cs.ro" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-skip">‚è≠Ô∏è Skip</span></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2307.00841v1" target="_blank">Advantages of Multimodal versus Verbal-Only Robot-to-Human Communication with an Anthropomorphic Robotic Mock Driver</a>
                            </h3>
                            <p class="card-authors">Tim Schreiter, Lucas Morillo-Mendez, Ravi T. Chadalavada et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">CS.RO</span></div>
                            <div class="card-details" id="cat-details-4445408944">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Robots are increasingly used in shared environments with humans, making effective communication a necessity for successful human-robot interaction. In our work, we study a crucial component: active communication of robot intent. Here, we present an anthropomorphic solution where a humanoid robot communicates the intent of its host robot acting as an &quot;Anthropomorphic Robotic Mock Driver&quot; (ARMoD). We evaluate this approach in two experiments in which participants work alongside a mobile robot on various tasks, while the ARMoD communicates a need for human attention, when required, or gives instr...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Here, we present an anthropomorphic solution where a humanoid robot communicates the intent of its host robot acting as an &quot;Anthropomorphic Robotic Mock Driver&quot; (ARMoD)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2307.00841v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2307.00841v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. Schreiter et al., &quot;Advantages of Multimodal versus Verbal-Only Robot-to-Human Communication with an Anthropomorphic Robotic Mock Driver,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2307.00841v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445408944')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2026" data-category="cs.ro" data-title="fauna sprout: a lightweight, approachable, developer-ready humanoid robot" data-keywords="robot humanoid manipulation control simulation imu cs.ro cs.ai" data-themes="S L I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-new">New</span></div>
                            <span class="card-source">arXiv ¬∑ 2026</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2601.18963v1" target="_blank">Fauna Sprout: A lightweight, approachable, developer-ready humanoid robot</a>
                            </h3>
                            <p class="card-authors">Fauna Robotics, :, Diego Aldarondo et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Manipulation</span><span class="keyword-tag">Control</span></div>
                            <div class="card-details" id="cat-details-4445407456">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recent advances in learned control, large-scale simulation, and generative models have accelerated progress toward general-purpose robotic controllers, yet the field still lacks platforms suitable for safe, expressive, long-term deployment in human environments. Most existing humanoids are either closed industrial systems or academic prototypes that are difficult to deploy and operate around people, limiting progress in robotics. We introduce Sprout, a developer platform designed to address these limitations through an emphasis on safety, expressivity, and developer accessibility. Sprout adopt...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce Sprout, a developer platform designed to address these limitations through an emphasis on safety, expressivity, and developer accessibility</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2601.18963v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2601.18963v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. Robotics et al., &quot;Fauna Sprout: A lightweight, approachable, developer-ready humanoid robot,&quot; arXiv, 2026. [Online]. Available: http://arxiv.org/abs/2601.18963v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445407456')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="unitracker: learning universal whole-body motion tracker for humanoid robots" data-keywords="robot humanoid control simulation ros imu cs.ro" data-themes="S L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.07356v3" target="_blank">UniTracker: Learning Universal Whole-Body Motion Tracker for Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Kangning Yin, Weishuai Zeng, Ke Fan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Control</span><span class="keyword-tag">Simulation</span></div>
                            <div class="card-details" id="cat-details-4445419648">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Achieving expressive and generalizable whole-body motion control is essential for deploying humanoid robots in real-world environments. In this work, we propose UniTracker, a three-stage training framework that enables robust and scalable motion tracking across a wide range of human behaviors. In the first stage, we train a teacher policy with privileged observations to generate high-quality actions. In the second stage, we introduce a Conditional Variational Autoencoder (CVAE) to model a universal student policy that can be deployed directly on real hardware. The CVAE structure allows the pol...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose UniTracker, a three-stage training framework that enables robust and scalable motion tracking across a wide range of human behaviors</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.07356v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.07356v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Yin et al., &quot;UniTracker: Learning Universal Whole-Body Motion Tracker for Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.07356v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445419648')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="demonstrating berkeley humanoid lite: an open-source, accessible, and customizable 3d-printed humanoid robot" data-keywords="reinforcement learning robot humanoid locomotion control simulation imu cs.ro" data-themes="S L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.17249v1" target="_blank">Demonstrating Berkeley Humanoid Lite: An Open-source, Accessible, and Customizable 3D-printed Humanoid Robot</a>
                            </h3>
                            <p class="card-authors">Yufeng Chi, Qiayuan Liao, Junfeng Long et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4445419840">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Despite significant interest and advancements in humanoid robotics, most existing commercially available hardware remains high-cost, closed-source, and non-transparent within the robotics community. This lack of accessibility and customization hinders the growth of the field and the broader development of humanoid technologies. To address these challenges and promote democratization in humanoid robotics, we demonstrate Berkeley Humanoid Lite, an open-source humanoid robot designed to be accessible, customizable, and beneficial for the entire community. The core of this design is a modular 3D-p...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address these challenges and promote democratization in humanoid robotics, we demonstrate Berkeley Humanoid Lite, an open-source humanoid robot designed to be accessible, customizable, and beneficial for the entire community</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.17249v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.17249v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Chi et al., &quot;Demonstrating Berkeley Humanoid Lite: An Open-source, Accessible, and Customizable 3D-printed Humanoid Robot,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.17249v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445419840')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="humanoid-gym: reinforcement learning for humanoid robot with zero-shot sim2real transfer" data-keywords="reinforcement learning robot humanoid locomotion simulation mujoco isaac imu cs.ro cs.ai" data-themes="S L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2404.05695v2" target="_blank">Humanoid-Gym: Reinforcement Learning for Humanoid Robot with Zero-Shot Sim2Real Transfer</a>
                            </h3>
                            <p class="card-authors">Xinyang Gu, Yen-Jen Wang, Jianyu Chen</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4445419264">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid-Gym is an easy-to-use reinforcement learning (RL) framework based on Nvidia Isaac Gym, designed to train locomotion skills for humanoid robots, emphasizing zero-shot transfer from simulation to the real-world environment. Humanoid-Gym also integrates a sim-to-sim framework from Isaac Gym to Mujoco that allows users to verify the trained policies in different physical simulations to ensure the robustness and generalization of the policies. This framework is verified by RobotEra&#x27;s XBot-S (1.2-meter tall humanoid robot) and XBot-L (1.65-meter tall humanoid robot) in a real-world environm...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2404.05695v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2404.05695v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. Gu, Y. Wang, and J. Chen, &quot;Humanoid-Gym: Reinforcement Learning for Humanoid Robot with Zero-Shot Sim2Real Transfer,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2404.05695v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445419264')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="cohrt: a collaboration system for human-robot teamwork" data-keywords="robot manipulation coordination simulation ros imu cs.ro cs.hc" data-themes="S R M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.08504v1" target="_blank">CoHRT: A Collaboration System for Human-Robot Teamwork</a>
                            </h3>
                            <p class="card-authors">Sujan Sarker, Haley N. Green, Mohammad Samin Yasar et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-R">Multi-Robot</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Manipulation</span><span class="keyword-tag">Coordination</span><span class="keyword-tag">Simulation</span></div>
                            <div class="card-details" id="cat-details-4445420032">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Collaborative robots are increasingly deployed alongside humans in factories, hospitals, schools, and other domains to enhance teamwork and efficiency. Systems that seamlessly integrate humans and robots into cohesive teams for coordinated and efficient task execution are needed, enabling studies on how robot collaboration policies affect team performance and teammates&#x27; perceived fairness, trust, and safety. Such a system can also be utilized to study the impact of a robot&#x27;s normative behavior on team collaboration. Additionally, it allows for investigation into how the legibility and predicta...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.08504v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.08504v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Sarker, H. N. Green, M. S. Yasar, and T. Iqbal, &quot;CoHRT: A Collaboration System for Human-Robot Teamwork,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.08504v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445420032')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="child (controller for humanoid imitation and live demonstration): a whole-body humanoid teleoperation system" data-keywords="robot humanoid manipulation control cs.ro" data-themes="L I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.00162v2" target="_blank">CHILD (Controller for Humanoid Imitation and Live Demonstration): a Whole-Body Humanoid Teleoperation System</a>
                            </h3>
                            <p class="card-authors">Noboru Myers, Obin Kwon, Sankalp Yamsani et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Manipulation</span><span class="keyword-tag">Control</span></div>
                            <div class="card-details" id="cat-details-4445407600">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recent advances in teleoperation have demonstrated robots performing complex manipulation tasks. However, existing works rarely support whole-body joint-level teleoperation for humanoid robots, limiting the diversity of tasks that can be accomplished. This work presents Controller for Humanoid Imitation and Live Demonstration (CHILD), a compact reconfigurable teleoperation system that enables joint level control over humanoid robots. CHILD fits within a standard baby carrier, allowing the operator control over all four limbs, and supports both direct joint mapping for full-body control and loc...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.00162v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.00162v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. Myers, O. Kwon, S. Yamsani, and J. Kim, &quot;CHILD (Controller for Humanoid Imitation and Live Demonstration): a Whole-Body Humanoid Teleoperation System,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.00162v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445407600')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="rhino: learning real-time humanoid-human-object interaction from human demonstrations" data-keywords="robot humanoid locomotion manipulation control cs.ro cs.hc cs.lg" data-themes="S L M E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.13134v1" target="_blank">RHINO: Learning Real-Time Humanoid-Human-Object Interaction from Human Demonstrations</a>
                            </h3>
                            <p class="card-authors">Jingxiao Chen, Xinyao Li, Jiahang Cao et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Manipulation</span></div>
                            <div class="card-details" id="cat-details-4445408080">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots have shown success in locomotion and manipulation. Despite these basic abilities, humanoids are still required to quickly understand human instructions and react based on human interaction signals to become valuable assistants in human daily life. Unfortunately, most existing works only focus on multi-stage interactions, treating each task separately, and neglecting real-time feedback. In this work, we aim to empower humanoid robots with real-time reaction abilities to achieve various tasks, allowing human to interrupt robots at any time, and making robots respond to humans imm...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To support such abilities, we propose a general humanoid-human-object interaction framework, named RHINO, i.e., Real-time Humanoid-human Interaction and Object manipulation</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.13134v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.13134v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Chen et al., &quot;RHINO: Learning Real-Time Humanoid-Human-Object Interaction from Human Demonstrations,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.13134v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445408080')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="learning humanoid locomotion over challenging terrain" data-keywords="reinforcement learning transformer robot humanoid locomotion control ros cs.ro cs.lg" data-themes="S L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.03654v1" target="_blank">Learning Humanoid Locomotion over Challenging Terrain</a>
                            </h3>
                            <p class="card-authors">Ilija Radosavovic, Sarthak Kamat, Trevor Darrell et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Transformer</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span></div>
                            <div class="card-details" id="cat-details-4445419984">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots can, in principle, use their legs to go almost anywhere. Developing controllers capable of traversing diverse terrains, however, remains a considerable challenge. Classical controllers are hard to generalize broadly while the learning-based methods have primarily focused on gentle terrains. Here, we present a learning-based approach for blind humanoid locomotion capable of traversing challenging natural and man-made terrain. Our method uses a transformer model to predict the next action based on the history of proprioceptive observations and actions. The model is first pre-trai...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Here, we present a learning-based approach for blind humanoid locomotion capable of traversing challenging natural and man-made terrain</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.03654v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.03654v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'I. Radosavovic, S. Kamat, T. Darrell, and J. Malik, &quot;Learning Humanoid Locomotion over Challenging Terrain,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.03654v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445419984')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="robot vulnerability and the elicitation of user empathy" data-keywords="robot cs.ro cs.hc" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-skip">‚è≠Ô∏è Skip</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.02684v1" target="_blank">Robot Vulnerability and the Elicitation of User Empathy</a>
                            </h3>
                            <p class="card-authors">Morten Roed Frederiksen, Katrin Fischer, Maja Matariƒá</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">CS.RO</span><span class="keyword-tag">CS.HC</span></div>
                            <div class="card-details" id="cat-details-4445419744">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper describes a between-subjects Amazon Mechanical Turk study (n = 220) that investigated how a robot&#x27;s affective narrative influences its ability to elicit empathy in human observers. We first conducted a pilot study to develop and validate the robot&#x27;s affective narratives. Then, in the full study, the robot used one of three different affective narrative strategies (funny, sad, neutral) while becoming less functional at its shopping task over the course of the interaction. As the functionality of the robot degraded, participants were repeatedly asked if they were willing to help the r...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.02684v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.02684v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. R. Frederiksen, K. Fischer, and M. Matariƒá, &quot;Robot Vulnerability and the Elicitation of User Empathy,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.02684v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445419744')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="roboballet: planning for multi-robot reaching with graph neural networks and reinforcement learning" data-keywords="reinforcement learning neural network gnn robot coordination perception planning optimization simulation imu" data-themes="S I R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.05397v1" target="_blank">RoboBallet: Planning for Multi-Robot Reaching with Graph Neural Networks and Reinforcement Learning</a>
                            </h3>
                            <p class="card-authors">Matthew Lai, Keegan Go, Zhibin Li et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Gnn</span><span class="keyword-tag">Robot</span></div>
                            <div class="card-details" id="cat-details-4445408368">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Modern robotic manufacturing requires collision-free coordination of multiple robots to complete numerous tasks in shared, obstacle-rich workspaces. Although individual tasks may be simple in isolation, automated joint task allocation, scheduling, and motion planning under spatio-temporal constraints remain computationally intractable for classical methods at real-world scales. Existing multi-arm systems deployed in the industry rely on human intuition and experience to design feasible trajectories manually in a labor-intensive process. To address this challenge, we propose a reinforcement lea...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address this challenge, we propose a reinforcement learning (RL) framework to achieve automated task and motion planning, tested in an obstacle-rich environment with eight robots performing 40 reaching tasks in a shared workspace, where any robot can perform any task in any order</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.05397v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.05397v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Lai et al., &quot;RoboBallet: Planning for Multi-Robot Reaching with Graph Neural Networks and Reinforcement Learning,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.05397v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445408368')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="a multi-modal table tennis robot system" data-keywords="neural network robot perception control camera cs.ro cs.ai" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2310.19062v2" target="_blank">A multi-modal table tennis robot system</a>
                            </h3>
                            <p class="card-authors">Andreas Ziegler, Thomas Gossard, Karl Vetter et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Perception</span><span class="keyword-tag">Control</span></div>
                            <div class="card-details" id="cat-details-4445407936">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In recent years, robotic table tennis has become a popular research challenge for perception and robot control. Here, we present an improved table tennis robot system with high accuracy vision detection and fast robot reaction. Based on previous work, our system contains a KUKA robot arm with 6 DOF, with four frame-based cameras and two additional event-based cameras. We developed a novel calibration approach to calibrate this multimodal perception system. For table tennis, spin estimation is crucial. Therefore, we introduced a novel, and more accurate spin estimation approach. Finally, we sho...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Here, we present an improved table tennis robot system with high accuracy vision detection and fast robot reaction</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2310.19062v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2310.19062v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Ziegler, T. Gossard, K. Vetter, J. Tebbe, and A. Zell, &quot;A multi-modal table tennis robot system,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2310.19062v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445407936')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="estimation of continuous environments by robot swarms: correlated networks and decision-making" data-keywords="robot swarm multi-robot control cs.ro cs.ai cs.ma" data-themes="S I R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2302.13629v2" target="_blank">Estimation of continuous environments by robot swarms: Correlated networks and decision-making</a>
                            </h3>
                            <p class="card-authors">Mohsen Raoufi, Pawel Romanczuk, Heiko Hamann</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Swarm</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Control</span></div>
                            <div class="card-details" id="cat-details-4445419072">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Collective decision-making is an essential capability of large-scale multi-robot systems to establish autonomy on the swarm level. A large portion of literature on collective decision-making in swarm robotics focuses on discrete decisions selecting from a limited number of options. Here we assign a decentralized robot system with the task of exploring an unbounded environment, finding consensus on the mean of a measurable environmental feature, and aggregating at areas where that value is measured (e.g., a contour line). A unique quality of this task is a causal loop between the robots&#x27; dynami...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a control algorithm and study it in real-world robot swarm experiments in different environments</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2302.13629v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2302.13629v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Raoufi, P. Romanczuk, and H. Hamann, &quot;Estimation of continuous environments by robot swarms: Correlated networks and decision-making,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2302.13629v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445419072')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="agiloped: agile open-source humanoid robot for research" data-keywords="robot humanoid cs.ro" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.09364v1" target="_blank">AGILOped: Agile Open-Source Humanoid Robot for Research</a>
                            </h3>
                            <p class="card-authors">Grzegorz Ficht, Luis Denninger, Sven Behnke</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">CS.RO</span></div>
                            <div class="card-details" id="cat-details-4445420512">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">With academic and commercial interest for humanoid robots peaking, multiple platforms are being developed. Through a high level of customization, they showcase impressive performance. Most of these systems remain closed-source or have high acquisition and maintenance costs, however. In this work, we present AGILOped - an open-source humanoid robot that closes the gap between high performance and accessibility. Our robot is driven by off-the-shelf backdrivable actuators with high power density and uses standard electronic components. With a height of 110 cm and weighing only 14.5 kg, AGILOped c...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we present AGILOped - an open-source humanoid robot that closes the gap between high performance and accessibility</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.09364v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.09364v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Ficht, L. Denninger, and S. Behnke, &quot;AGILOped: Agile Open-Source Humanoid Robot for Research,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.09364v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445420512')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="learning bipedal walking for humanoid robots in challenging environments with obstacle avoidance" data-keywords="reinforcement learning robot humanoid bipedal locomotion cs.ro cs.lg" data-themes="L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-skip">‚è≠Ô∏è Skip</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.08212v1" target="_blank">Learning Bipedal Walking for Humanoid Robots in Challenging Environments with Obstacle Avoidance</a>
                            </h3>
                            <p class="card-authors">Marwan Hamze, Mitsuharu Morisawa, Eiichi Yoshida</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span></div>
                            <div class="card-details" id="cat-details-4445409040">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Deep reinforcement learning has seen successful implementations on humanoid robots to achieve dynamic walking. However, these implementations have been so far successful in simple environments void of obstacles. In this paper, we aim to achieve bipedal locomotion in an environment where obstacles are present using a policy-based reinforcement learning. By adding simple distance reward terms to a state of art reward function that can achieve basic bipedal locomotion, the trained policy succeeds in navigating the robot towards the desired destination without colliding with the obstacles along th...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.08212v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.08212v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Hamze, M. Morisawa, and E. Yoshida, &quot;Learning Bipedal Walking for Humanoid Robots in Challenging Environments with Obstacle Avoidance,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.08212v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445409040')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="robust humanoid walking on compliant and uneven terrain with deep reinforcement learning" data-keywords="reinforcement learning robot humanoid bipedal locomotion control simulation imu cs.ro" data-themes="S L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.13619v1" target="_blank">Robust Humanoid Walking on Compliant and Uneven Terrain with Deep Reinforcement Learning</a>
                            </h3>
                            <p class="card-authors">Rohan P. Singh, Mitsuharu Morisawa, Mehdi Benallegue et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span></div>
                            <div class="card-details" id="cat-details-4445420944">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">For the deployment of legged robots in real-world environments, it is essential to develop robust locomotion control methods for challenging terrains that may exhibit unexpected deformability and irregularity. In this paper, we explore the application of sim-to-real deep reinforcement learning (RL) for the design of bipedal locomotion controllers for humanoid robots on compliant and uneven terrains. Our key contribution is to show that a simple training curriculum for exposing the RL agent to randomized terrains in simulation can achieve robust walking on a real humanoid robot using only propr...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a new control policy to enable modification of the observed clock signal, leading to adaptive gait frequencies depending on the terrain and command velocity</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.13619v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.13619v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. P. Singh, M. Morisawa, M. Benallegue, Z. Xie, and F. Kanehiro, &quot;Robust Humanoid Walking on Compliant and Uneven Terrain with Deep Reinforcement Learning,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.13619v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445420944')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="lips: large-scale humanoid robot reinforcement learning with parallel-series structures" data-keywords="reinforcement learning attention robot humanoid control simulation imu cs.ro" data-themes="S I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.08349v1" target="_blank">LiPS: Large-Scale Humanoid Robot Reinforcement Learning with Parallel-Series Structures</a>
                            </h3>
                            <p class="card-authors">Qiang Zhang, Gang Han, Jingkai Sun et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Attention</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span></div>
                            <div class="card-details" id="cat-details-4445408272">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In recent years, research on humanoid robots has garnered significant attention, particularly in reinforcement learning based control algorithms, which have achieved major breakthroughs. Compared to traditional model-based control algorithms, reinforcement learning based algorithms demonstrate substantial advantages in handling complex tasks. Leveraging the large-scale parallel computing capabilities of GPUs, contemporary humanoid robots can undergo extensive parallel training in simulated environments. A physical simulation platform capable of large-scale parallel training is crucial for the ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">For enabling reinforcement learning-based humanoid robot control algorithms to train in large-scale parallel environments, we propose a novel training method LiPS</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.08349v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.08349v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Q. Zhang et al., &quot;LiPS: Large-Scale Humanoid Robot Reinforcement Learning with Parallel-Series Structures,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.08349v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445408272')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="jaxrobotarium: training and deploying multi-robot policies in 10 minutes" data-keywords="reinforcement learning robot multi-agent multi-robot coordination simulation imu cs.ro cs.lg cs.ma" data-themes="S R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2505.06771v3" target="_blank">JaxRobotarium: Training and Deploying Multi-Robot Policies in 10 Minutes</a>
                            </h3>
                            <p class="card-authors">Shalin Anand Jain, Jiazhen Liu, Siva Kailas et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Agent</span><span class="keyword-tag">Multi-Robot</span></div>
                            <div class="card-details" id="cat-details-4445419936">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Multi-agent reinforcement learning (MARL) has emerged as a promising solution for learning complex and scalable coordination behaviors in multi-robot systems. However, established MARL platforms (e.g., SMAC and MPE) lack robotics relevance and hardware deployment, leaving multi-robot learning researchers to develop bespoke environments and hardware testbeds dedicated to the development and evaluation of their individual contributions. The Multi-Agent RL Benchmark and Learning Environment for the Robotarium (MARBLER) is an exciting recent step in providing a standardized robotics-relevant platf...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We demonstrate that JaxRobotarium retains high simulation fidelity while achieving dramatic speedups over baseline (20x in training and 150x in simulation), and provides an open-access sim-to-real evaluation pipeline through the Robotarium testbed, accelerating and democratizing access to multi-robot learning research and evaluation</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2505.06771v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2505.06771v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. A. Jain, J. Liu, S. Kailas, and H. Ravichandar, &quot;JaxRobotarium: Training and Deploying Multi-Robot Policies in 10 Minutes,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2505.06771v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445419936')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="social robots as companions for lonely hearts: the role of anthropomorphism and robot appearance" data-keywords="robot cs.ro cs.hc" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-skip">‚è≠Ô∏è Skip</span></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2306.02694v2" target="_blank">Social Robots As Companions for Lonely Hearts: The Role of Anthropomorphism and Robot Appearance</a>
                            </h3>
                            <p class="card-authors">Yoonwon Jung, Sowon Hahn</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">CS.RO</span><span class="keyword-tag">CS.HC</span></div>
                            <div class="card-details" id="cat-details-4445421040">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Loneliness is a distressing personal experience and a growing social issue. Social robots could alleviate the pain of loneliness, particularly for those who lack in-person interaction. This paper investigated how the effect of loneliness on the anthropomorphism of social robots differs by robot appearance, and how it influences purchase intention. Participants viewed a video of one of the three robots (machine-like, animal-like, and human-like) moving and interacting with a human counterpart. Bootstrapped multiple regression results revealed that although the unique effect of animal-likeness o...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2306.02694v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2306.02694v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Jung, and S. Hahn, &quot;Social Robots As Companions for Lonely Hearts: The Role of Anthropomorphism and Robot Appearance,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2306.02694v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445421040')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="mr.cap: multi-robot joint control and planning for object transport" data-keywords="robot multi-robot planning control optimization imu cs.ro" data-themes="R M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.11634v1" target="_blank">MR.CAP: Multi-Robot Joint Control and Planning for Object Transport</a>
                            </h3>
                            <p class="card-authors">Hussein Ali Jaafar, Cheng-Hao Kao, Sajad Saeedi</p>
                            <div class="theme-tags"><span class="theme-badge theme-R">Multi-Robot</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Planning</span><span class="keyword-tag">Control</span></div>
                            <div class="card-details" id="cat-details-4445407744">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">With the recent influx in demand for multi-robot systems throughout industry and academia, there is an increasing need for faster, robust, and generalizable path planning algorithms. Similarly, given the inherent connection between control algorithms and multi-robot path planners, there is in turn an increased demand for fast, efficient, and robust controllers. We propose a scalable joint path planning and control algorithm for multi-robot systems with constrained behaviours based on factor graph optimization. We demonstrate our algorithm on a series of hardware and simulated experiments. Our ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a scalable joint path planning and control algorithm for multi-robot systems with constrained behaviours based on factor graph optimization</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.11634v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.11634v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. A. Jaafar, C. Kao, and S. Saeedi, &quot;MR.CAP: Multi-Robot Joint Control and Planning for Object Transport,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.11634v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445407744')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="human impression of humanoid robots mirroring social cues" data-keywords="robot humanoid perception control ros imu cs.ro cs.hc" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-skip">‚è≠Ô∏è Skip</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.12076v1" target="_blank">Human Impression of Humanoid Robots Mirroring Social Cues</a>
                            </h3>
                            <p class="card-authors">Di Fu, Fares Abawi, Philipp Allgeuer et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Perception</span><span class="keyword-tag">Control</span></div>
                            <div class="card-details" id="cat-details-4444265472">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Mirroring non-verbal social cues such as affect or movement can enhance human-human and human-robot interactions in the real world. The robotic platforms and control methods also impact people&#x27;s perception of human-robot interaction. However, limited studies have compared robot imitation across different platforms and control methods. Our research addresses this gap by conducting two experiments comparing people&#x27;s perception of affective mirroring between the iCub and Pepper robots and movement mirroring between vision-based iCub control and Inertial Measurement Unit (IMU)-based iCub control. ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.12076v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.12076v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Fu, F. Abawi, P. Allgeuer, and S. Wermter, &quot;Human Impression of Humanoid Robots Mirroring Social Cues,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.12076v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444265472')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="learning aerodynamics for the control of flying humanoid robots" data-keywords="neural network robot humanoid locomotion control simulation imu cs.ro cs.lg" data-themes="L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.00305v2" target="_blank">Learning Aerodynamics for the Control of Flying Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Antonello Paolino, Gabriele Nava, Fabio Di Natale et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4445419600">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Robots with multi-modal locomotion are an active research field due to their versatility in diverse environments. In this context, additional actuation can provide humanoid robots with aerial capabilities. Flying humanoid robots face challenges in modeling and control, particularly with aerodynamic forces. This paper addresses these challenges from a technological and scientific standpoint. The technological contribution includes the mechanical design of iRonCub-Mk1, a jet-powered humanoid robot, optimized for jet engine integration, and hardware modifications for wind tunnel experiments on hu...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.00305v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.00305v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Paolino et al., &quot;Learning Aerodynamics for the Control of Flying Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.00305v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445419600')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="a collaborative robot-assisted manufacturing assembly process" data-keywords="robot cs.ro" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.05306v1" target="_blank">A Collaborative Robot-Assisted Manufacturing Assembly Process</a>
                            </h3>
                            <p class="card-authors">Miguel Neves, Laura Duarte, Pedro Neto</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">CS.RO</span></div>
                            <div class="card-details" id="cat-details-4445420992">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">An effective human-robot collaborative process results in the reduction of the operator&#x27;s workload, promoting a more efficient, productive, safer and less error-prone working environment. However, the implementation of collaborative robots in industry is still challenging. In this work, we compare manual and robot-assisted assembly processes to evaluate the effectiveness of collaborative robots while featuring different modes of operation (coexistence, cooperation and collaboration). Results indicate an improvement in ergonomic conditions and ease of execution without substantially compromisin...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.05306v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.05306v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Neves, L. Duarte, and P. Neto, &quot;A Collaborative Robot-Assisted Manufacturing Assembly Process,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.05306v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445420992')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="humanoid robot rhp friends: seamless combination of autonomous and teleoperated tasks in a nursing context" data-keywords="robot humanoid manipulation cs.ro cs.hc" data-themes="S I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.20770v2" target="_blank">Humanoid Robot RHP Friends: Seamless Combination of Autonomous and Teleoperated Tasks in a Nursing Context</a>
                            </h3>
                            <p class="card-authors">Mehdi Benallegue, Guillaume Lorthioir, Antonin Dallard et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Manipulation</span><span class="keyword-tag">CS.RO</span></div>
                            <div class="card-details" id="cat-details-4445419792">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper describes RHP Friends, a social humanoid robot developed to enable assistive robotic deployments in human-coexisting environments. As a use-case application, we present its potential use in nursing by extending its capabilities to operate human devices and tools according to the task and by enabling remote assistance operations. To meet a wide variety of tasks and situations in environments designed by and for humans, we developed a system that seamlessly integrates the slim and lightweight robot and several technologies: locomanipulation, multi-contact motion, teleoperation, and ob...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">As a use-case application, we present its potential use in nursing by extending its capabilities to operate human devices and tools according to the task and by enabling remote assistance operations</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.20770v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.20770v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Benallegue et al., &quot;Humanoid Robot RHP Friends: Seamless Combination of Autonomous and Teleoperated Tasks in a Nursing Context,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.20770v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445419792')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="play everywhere: a temporal logic based game environment independent approach for playing soccer with robots" data-keywords="robot cs.ro cs.ai" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.12628v1" target="_blank">Play Everywhere: A Temporal Logic based Game Environment Independent Approach for Playing Soccer with Robots</a>
                            </h3>
                            <p class="card-authors">Vincenzo Suriani, Emanuele Musumeci, Daniele Nardi et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">CS.RO</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4445420464">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Robots playing soccer often rely on hard-coded behaviors that struggle to generalize when the game environment change. In this paper, we propose a temporal logic based approach that allows robots&#x27; behaviors and goals to adapt to the semantics of the environment. In particular, we present a hierarchical representation of soccer in which the robot selects the level of operation based on the perceived semantic characteristics of the environment, thus modifying dynamically the set of rules and goals to apply. The proposed approach enables the robot to operate in unstructured environments, just as ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose a temporal logic based approach that allows robots&#x27; behaviors and goals to adapt to the semantics of the environment</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.12628v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.12628v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'V. Suriani, E. Musumeci, D. Nardi, and D. D. Bloisi, &quot;Play Everywhere: A Temporal Logic based Game Environment Independent Approach for Playing Soccer with Robots,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.12628v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445420464')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="gengrid: a generalised distributed experimental environmental grid for swarm robotics" data-keywords="robot swarm multi-robot control cs.ro cs.ma" data-themes="R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Multi-Agent Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.20071v1" target="_blank">GenGrid: A Generalised Distributed Experimental Environmental Grid for Swarm Robotics</a>
                            </h3>
                            <p class="card-authors">Pranav Kedia, Madhav Rao</p>
                            <div class="theme-tags"><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Swarm</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Control</span></div>
                            <div class="card-details" id="cat-details-4445419504">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">GenGrid is a novel comprehensive open-source, distributed platform intended for conducting extensive swarm robotic experiments. The modular platform is designed to run swarm robotics experiments that are compatible with different types of mobile robots ranging from Colias, Kilobot, and E puck. The platform offers programmable control over the experimental setup and its parameters and acts as a tool to collect swarm robot data, including localization, sensory feedback, messaging, and interaction. GenGrid is designed as a modular grid of attachable computing nodes that offers bidirectional commu...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.20071v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.20071v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Kedia, and M. Rao, &quot;GenGrid: A Generalised Distributed Experimental Environmental Grid for Swarm Robotics,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.20071v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445419504')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="what can you say to a robot? capability communication leads to more natural conversations" data-keywords="robot cs.ro cs.hc" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-skip">‚è≠Ô∏è Skip</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.01448v1" target="_blank">What Can You Say to a Robot? Capability Communication Leads to More Natural Conversations</a>
                            </h3>
                            <p class="card-authors">Merle M. Reimann, Koen V. Hindriks, Florian A. Kunneman et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">CS.RO</span><span class="keyword-tag">CS.HC</span></div>
                            <div class="card-details" id="cat-details-4445418928">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">When encountering a robot in the wild, it is not inherently clear to human users what the robot&#x27;s capabilities are. When encountering misunderstandings or problems in spoken interaction, robots often just apologize and move on, without additional effort to make sure the user understands what happened. We set out to compare the effect of two speech based capability communication strategies (proactive, reactive) to a robot without such a strategy, in regard to the user&#x27;s rating of and their behavior during the interaction. For this, we conducted an in-person user study with 120 participants who ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.01448v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.01448v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. M. Reimann, K. V. Hindriks, F. A. Kunneman, C. Oertel, G. Skantze, and I. Leite, &quot;What Can You Say to a Robot? Capability Communication Leads to More Natural Conversations,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.01448v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445418928')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="humanoid occupancy: enabling a generalized multimodal occupancy perception system on humanoid robots" data-keywords="robot humanoid navigation perception planning cs.ro cs.ai cs.cv" data-themes="S E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.20217v2" target="_blank">Humanoid Occupancy: Enabling A Generalized Multimodal Occupancy Perception System on Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Wei Cui, Haoyu Wang, Wenkang Qin et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Navigation</span><span class="keyword-tag">Perception</span></div>
                            <div class="card-details" id="cat-details-4445419888">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robot technology is advancing rapidly, with manufacturers introducing diverse heterogeneous visual perception modules tailored to specific scenarios. Among various perception paradigms, occupancy-based representation has become widely recognized as particularly suitable for humanoid robots, as it provides both rich semantic and 3D geometric information essential for comprehensive environmental understanding. In this work, we present Humanoid Occupancy, a generalized multimodal occupancy perception system that integrates hardware and software components, data acquisition devices, and a...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we present Humanoid Occupancy, a generalized multimodal occupancy perception system that integrates hardware and software components, data acquisition devices, and a dedicated annotation pipeline</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.20217v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.20217v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. Cui et al., &quot;Humanoid Occupancy: Enabling A Generalized Multimodal Occupancy Perception System on Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.20217v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445419888')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="llm-based ambiguity detection in natural language instructions for collaborative surgical robots" data-keywords="robot language model cs.ro cs.hc" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.11525v1" target="_blank">LLM-based ambiguity detection in natural language instructions for collaborative surgical robots</a>
                            </h3>
                            <p class="card-authors">Ana Davila, Jacinto Colan, Yasuhisa Hasegawa</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.RO</span><span class="keyword-tag">CS.HC</span></div>
                            <div class="card-details" id="cat-details-4445419120">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Ambiguity in natural language instructions poses significant risks in safety-critical human-robot interaction, particularly in domains such as surgery. To address this, we propose a framework that uses Large Language Models (LLMs) for ambiguity detection specifically designed for collaborative surgical scenarios. Our method employs an ensemble of LLM evaluators, each configured with distinct prompting techniques to identify linguistic, contextual, procedural, and critical ambiguities. A chain-of-thought evaluator is included to systematically analyze instruction structure for potential issues....</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address this, we propose a framework that uses Large Language Models (LLMs) for ambiguity detection specifically designed for collaborative surgical scenarios</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.11525v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.11525v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Davila, J. Colan, and Y. Hasegawa, &quot;LLM-based ambiguity detection in natural language instructions for collaborative surgical robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.11525v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445419120')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="ros2swarm - a ros 2 package for swarm robot behaviors" data-keywords="robot swarm multi-robot control ros cs.ro cs.ma" data-themes="R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Multi-Agent Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.02438v1" target="_blank">ROS2swarm - A ROS 2 Package for Swarm Robot Behaviors</a>
                            </h3>
                            <p class="card-authors">Tanja Katharina Kaiser, Marian Johannes Begemann, Tavia Plattenteich et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Swarm</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Control</span></div>
                            <div class="card-details" id="cat-details-4445418352">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Developing reusable software for mobile robots is still challenging. Even more so for swarm robots, despite the desired simplicity of the robot controllers. Prototyping and experimenting are difficult due to the multi-robot setting and often require robot-robot communication. Also, the diversity of swarm robot hardware platforms increases the need for hardware-independent software concepts. The main advantages of the commonly used robot software architecture ROS 2 are modularity and platform independence. We propose a new ROS 2 package, ROS2swarm, for applications of swarm robotics that provid...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a new ROS 2 package, ROS2swarm, for applications of swarm robotics that provides a library of ready-to-use swarm behavioral primitives</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.02438v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.02438v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. K. Kaiser, M. J. Begemann, T. Plattenteich, L. Schilling, G. Schildbach, and H. Hamann, &quot;ROS2swarm - A ROS 2 Package for Swarm Robot Behaviors,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.02438v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445418352')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="what am i? evaluating the effect of language fluency and task competency on the perception of a social robot" data-keywords="robot perception cs.ro" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-skip">‚è≠Ô∏è Skip</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.11085v1" target="_blank">What Am I? Evaluating the Effect of Language Fluency and Task Competency on the Perception of a Social Robot</a>
                            </h3>
                            <p class="card-authors">Shahira Ali, Haley N. Green, Tariq Iqbal</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Perception</span><span class="keyword-tag">CS.RO</span></div>
                            <div class="card-details" id="cat-details-4445408992">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recent advancements in robot capabilities have enabled them to interact with people in various human-social environments (HSEs). In many of these environments, the perception of the robot often depends on its capabilities, e.g., task competency, language fluency, etc. To enable fluent human-robot interaction (HRI) in HSEs, it is crucial to understand the impact of these capabilities on the perception of the robot. Although many works have investigated the effects of various robot capabilities on the robot&#x27;s perception separately, in this paper, we present a large-scale HRI study (n = 60) to in...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Although many works have investigated the effects of various robot capabilities on the robot&#x27;s perception separately, in this paper, we present a large-scale HRI study (n = 60) to investigate the combined impact of both language fluency and task competency on the perception of a robot</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.11085v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.11085v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Ali, H. N. Green, and T. Iqbal, &quot;What Am I? Evaluating the Effect of Language Fluency and Task Competency on the Perception of a Social Robot,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.11085v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445408992')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="the beatbots: a musician-informed multi-robot percussion quartet" data-keywords="robot multi-robot control ros cs.ro cs.hc" data-themes="R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-skip">‚è≠Ô∏è Skip</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.00966v1" target="_blank">The Beatbots: A Musician-Informed Multi-Robot Percussion Quartet</a>
                            </h3>
                            <p class="card-authors">Isabella Pu, Jeff Snyder, Naomi Ehrich Leonard</p>
                            <div class="theme-tags"><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Control</span><span class="keyword-tag">Ros</span></div>
                            <div class="card-details" id="cat-details-4445420368">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Artistic creation is often seen as a uniquely human endeavor, yet robots bring distinct advantages to music-making, such as precise tempo control, unpredictable rhythmic complexities, and the ability to coordinate intricate human and robot performances. While many robotic music systems aim to mimic human musicianship, our work emphasizes the unique strengths of robots, resulting in a novel multi-robot performance instrument called the Beatbots, capable of producing music that is challenging for humans to replicate using current methods. The Beatbots were designed using an ``informed prototypin...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose design principles to guide the development of future robotic music systems and identify key robotic music affordances that our musician consultants considered particularly important for robotic music performance.</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.00966v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.00966v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'I. Pu, J. Snyder, and N. E. Leonard, &quot;The Beatbots: A Musician-Informed Multi-Robot Percussion Quartet,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.00966v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445420368')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="teleoperation of humanoid robots: a survey" data-keywords="robot humanoid cs.ro" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2301.04317v1" target="_blank">Teleoperation of Humanoid Robots: A Survey</a>
                            </h3>
                            <p class="card-authors">Kourosh Darvish, Luigi Penco, Joao Ramos et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">CS.RO</span></div>
                            <div class="card-details" id="cat-details-4445421328">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Teleoperation of humanoid robots enables the integration of the cognitive skills and domain expertise of humans with the physical capabilities of humanoid robots. The operational versatility of humanoid robots makes them the ideal platform for a wide range of applications when teleoperating in a remote environment. However, the complexity of humanoid robots imposes challenges for teleoperation, particularly in unstructured dynamic environments with limited communication. Many advancements have been achieved in the last decades in this area, but a comprehensive overview is still missing. This s...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2301.04317v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2301.04317v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Darvish et al., &quot;Teleoperation of Humanoid Robots: A Survey,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2301.04317v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445421328')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="learning sim-to-real humanoid locomotion in 15 minutes" data-keywords="reinforcement learning robot humanoid locomotion control simulation imu cs.ro cs.ai cs.lg" data-themes="L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2512.01996v1" target="_blank">Learning Sim-to-Real Humanoid Locomotion in 15 Minutes</a>
                            </h3>
                            <p class="card-authors">Younggyo Seo, Carmelo Sferrazza, Juyue Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4445419168">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Massively parallel simulation has reduced reinforcement learning (RL) training time for robots from days to minutes. However, achieving fast and reliable sim-to-real RL for humanoid control remains difficult due to the challenges introduced by factors such as high dimensionality and domain randomization. In this work, we introduce a simple and practical recipe based on off-policy RL algorithms, i.e., FastSAC and FastTD3, that enables rapid training of humanoid locomotion policies in just 15 minutes with a single RTX 4090 GPU. Our simple recipe stabilizes off-policy RL algorithms at massive sca...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we introduce a simple and practical recipe based on off-policy RL algorithms, i.e., FastSAC and FastTD3, that enables rapid training of humanoid locomotion policies in just 15 minutes with a single RTX 4090 GPU</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2512.01996v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2512.01996v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Seo, C. Sferrazza, J. Chen, G. Shi, R. Duan, and P. Abbeel, &quot;Learning Sim-to-Real Humanoid Locomotion in 15 Minutes,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2512.01996v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445419168')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="sadcher: scheduling using attention-based dynamic coalitions of heterogeneous robots in real-time" data-keywords="transformer attention robot multi-robot ros imitation learning cs.ro cs.ma" data-themes="E I R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2510.14851v1" target="_blank">SADCHER: Scheduling using Attention-based Dynamic Coalitions of Heterogeneous Robots in Real-Time</a>
                            </h3>
                            <p class="card-authors">Jakob Bichler, Andreu Matoses Gimenez, Javier Alonso-Mora</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Transformer</span><span class="keyword-tag">Attention</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Robot</span></div>
                            <div class="card-details" id="cat-details-4445407552">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We present Sadcher, a real-time task assignment framework for heterogeneous multi-robot teams that incorporates dynamic coalition formation and task precedence constraints. Sadcher is trained through Imitation Learning and combines graph attention and transformers to predict assignment rewards between robots and tasks. Based on the predicted rewards, a relaxed bipartite matching step generates high-quality schedules with feasibility guarantees. We explicitly model robot and task positions, task durations, and robots&#x27; remaining processing times, enabling advanced temporal and spatial reasoning ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present Sadcher, a real-time task assignment framework for heterogeneous multi-robot teams that incorporates dynamic coalition formation and task precedence constraints</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2510.14851v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2510.14851v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Bichler, A. M. Gimenez, and J. Alonso-Mora, &quot;SADCHER: Scheduling using Attention-based Dynamic Coalitions of Heterogeneous Robots in Real-Time,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2510.14851v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445407552')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="human-humanoid robots cross-embodiment behavior-skill transfer using decomposed adversarial learning from demonstration" data-keywords="robot humanoid manipulation ros cs.ro cs.ai" data-themes="I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.15166v1" target="_blank">Human-Humanoid Robots Cross-Embodiment Behavior-Skill Transfer Using Decomposed Adversarial Learning from Demonstration</a>
                            </h3>
                            <p class="card-authors">Junjia Liu, Zhuo Li, Minghao Yu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Manipulation</span><span class="keyword-tag">Ros</span></div>
                            <div class="card-details" id="cat-details-4445419360">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots are envisioned as embodied intelligent agents capable of performing a wide range of human-level loco-manipulation tasks, particularly in scenarios requiring strenuous and repetitive labor. However, learning these skills is challenging due to the high degrees of freedom of humanoid robots, and collecting sufficient training data for humanoid is a laborious process. Given the rapid introduction of new humanoid platforms, a cross-embodiment framework that allows generalizable skill transfer is becoming increasingly critical. To address this, we propose a transferable framework tha...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address this, we propose a transferable framework that reduces the data bottleneck by using a unified digital human model as a common prototype and bypassing the need for re-training on every new robot platform</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.15166v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.15166v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Liu et al., &quot;Human-Humanoid Robots Cross-Embodiment Behavior-Skill Transfer Using Decomposed Adversarial Learning from Demonstration,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.15166v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445419360')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="real-world humanoid locomotion with reinforcement learning" data-keywords="reinforcement learning transformer robot humanoid locomotion control simulation imu cs.ro cs.lg" data-themes="S L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2303.03381v2" target="_blank">Real-World Humanoid Locomotion with Reinforcement Learning</a>
                            </h3>
                            <p class="card-authors">Ilija Radosavovic, Tete Xiao, Bike Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Transformer</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span></div>
                            <div class="card-details" id="cat-details-4445420656">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots that can autonomously operate in diverse environments have the potential to help address labour shortages in factories, assist elderly at homes, and colonize new planets. While classical controllers for humanoid robots have shown impressive results in a number of settings, they are challenging to generalize and adapt to new environments. Here, we present a fully learning-based approach for real-world humanoid locomotion. Our controller is a causal transformer that takes the history of proprioceptive observations and actions as input and predicts the next action. We hypothesize ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Here, we present a fully learning-based approach for real-world humanoid locomotion</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2303.03381v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2303.03381v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'I. Radosavovic, T. Xiao, B. Zhang, T. Darrell, J. Malik, and K. Sreenath, &quot;Real-World Humanoid Locomotion with Reinforcement Learning,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2303.03381v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445420656')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="polygmap: a perceptive locomotion framework for humanoid robot stair climbing" data-keywords="robot humanoid locomotion perception planning lidar camera imu sensor fusion cs.ro" data-themes="L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2510.12346v1" target="_blank">PolygMap: A Perceptive Locomotion Framework for Humanoid Robot Stair Climbing</a>
                            </h3>
                            <p class="card-authors">Bingquan Li, Ning Wang, Tianwei Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Perception</span></div>
                            <div class="card-details" id="cat-details-4445419456">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recently, biped robot walking technology has been significantly developed, mainly in the context of a bland walking scheme. To emulate human walking, robots need to step on the positions they see in unknown spaces accurately. In this paper, we present PolyMap, a perception-based locomotion planning framework for humanoid robots to climb stairs. Our core idea is to build a real-time polygonal staircase plane semantic map, followed by a footstep planar using these polygonal plane segments. These plane segmentation and visual odometry are done by multi-sensor fusion(LiDAR, RGB-D camera and IMUs)....</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we present PolyMap, a perception-based locomotion planning framework for humanoid robots to climb stairs</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2510.12346v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2510.12346v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. Li, N. Wang, T. Zhang, Z. He, and Y. Wu, &quot;PolygMap: A Perceptive Locomotion Framework for Humanoid Robot Stair Climbing,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2510.12346v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445419456')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="let&#x27;s move on: topic change in robot-facilitated group discussions" data-keywords="robot cs.ro cs.hc" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.02123v1" target="_blank">Let&#x27;s move on: Topic Change in Robot-Facilitated Group Discussions</a>
                            </h3>
                            <p class="card-authors">Georgios Hadjiantonis, Sarah Gillet, Marynel V√°zquez et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">CS.RO</span><span class="keyword-tag">CS.HC</span></div>
                            <div class="card-details" id="cat-details-4445406928">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Robot-moderated group discussions have the potential to facilitate engaging and productive interactions among human participants. Previous work on topic management in conversational agents has predominantly focused on human engagement and topic personalization, with the agent having an active role in the discussion. Also, studies have shown the usefulness of including robots in groups, yet further exploration is still needed for robots to learn when to change the topic while facilitating discussions. Accordingly, our work investigates the suitability of machine-learning models and audiovisual ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.02123v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.02123v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Hadjiantonis, S. Gillet, M. V√°zquez, I. Leite, and F. I. Dogan, &quot;Let&#x27;s move on: Topic Change in Robot-Facilitated Group Discussions,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.02123v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445406928')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="learning humanoid standing-up control across diverse postures" data-keywords="reinforcement learning robot humanoid locomotion manipulation control simulation ros imu cs.ro" data-themes="S L I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.08378v2" target="_blank">Learning Humanoid Standing-up Control across Diverse Postures</a>
                            </h3>
                            <p class="card-authors">Tao Huang, Junli Ren, Huayi Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4445420224">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Standing-up control is crucial for humanoid robots, with the potential for integration into current locomotion and loco-manipulation systems, such as fall recovery. Existing approaches are either limited to simulations that overlook hardware constraints or rely on predefined ground-specific motion trajectories, failing to enable standing up across postures in real-world scenes. To bridge this gap, we present HoST (Humanoid Standing-up Control), a reinforcement learning framework that learns standing-up control from scratch, enabling robust sim-to-real transfer across diverse postures. HoST eff...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To bridge this gap, we present HoST (Humanoid Standing-up Control), a reinforcement learning framework that learns standing-up control from scratch, enabling robust sim-to-real transfer across diverse postures</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.08378v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.08378v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. Huang et al., &quot;Learning Humanoid Standing-up Control across Diverse Postures,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.08378v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445420224')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="humanoid robots and humanoid ai: review, perspectives and directions" data-keywords="robot humanoid language model cs.ro" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.15775v3" target="_blank">Humanoid Robots and Humanoid AI: Review, Perspectives and Directions</a>
                            </h3>
                            <p class="card-authors">Longbing Cao</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.RO</span></div>
                            <div class="card-details" id="cat-details-4445421472">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In the approximately century-long journey of robotics, humanoid robots made their debut around six decades ago. While current humanoids bear human-like appearances, none have embodied true humaneness, remaining distant from achieving human-like to human-level intelligence. The rapid recent advancements in generative AI and (multimodal) large language models have further reignited and escalated interest in humanoids towards real-time, interactive, and multimodal designs and applications, such as fostering humanoid workers, advisers, educators, medical professionals, caregivers, and receptionist...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.15775v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.15775v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Cao, &quot;Humanoid Robots and Humanoid AI: Review, Perspectives and Directions,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.15775v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445421472')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="multi-robot local motion planning using dynamic optimization fabrics" data-keywords="robot multi-robot planning imu cs.ro cs.ma" data-themes="I R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Multi-Agent Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2310.12816v1" target="_blank">Multi-Robot Local Motion Planning Using Dynamic Optimization Fabrics</a>
                            </h3>
                            <p class="card-authors">Saray Bakker, Luzia Knoedler, Max Spahn et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Planning</span><span class="keyword-tag">Imu</span></div>
                            <div class="card-details" id="cat-details-4445408464">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In this paper, we address the problem of real-time motion planning for multiple robotic manipulators that operate in close proximity. We build upon the concept of dynamic fabrics and extend them to multi-robot systems, referred to as Multi-Robot Dynamic Fabrics (MRDF). This geometric method enables a very high planning frequency for high-dimensional systems at the expense of being reactive and prone to deadlocks. To detect and resolve deadlocks, we propose Rollout Fabrics where MRDF are forward simulated in a decentralized manner. We validate the methods in simulated close-proximity pick-and-p...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To detect and resolve deadlocks, we propose Rollout Fabrics where MRDF are forward simulated in a decentralized manner</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2310.12816v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2310.12816v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Bakker, L. Knoedler, M. Spahn, W. B√∂hmer, and J. Alonso-Mora, &quot;Multi-Robot Local Motion Planning Using Dynamic Optimization Fabrics,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2310.12816v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445408464')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="robot drummer: learning rhythmic skills for humanoid drumming" data-keywords="reinforcement learning robot humanoid locomotion coordination ros cs.ro" data-themes="E L I R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-skip">‚è≠Ô∏è Skip</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.11498v2" target="_blank">Robot Drummer: Learning Rhythmic Skills for Humanoid Drumming</a>
                            </h3>
                            <p class="card-authors">Asad Ali Shahid, Francesco Braghin, Loris Roveda</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4445408032">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots have seen remarkable advances in dexterity, balance, and locomotion, yet their role in expressive domains such as music performance remains largely unexplored. Musical tasks, like drumming, present unique challenges, including split-second timing, rapid contacts, and multi-limb coordination over performances lasting minutes. In this paper, we introduce Robot Drummer, a humanoid capable of expressive, high-precision drumming across a diverse repertoire of songs. We formulate humanoid drumming as sequential fulfillment of timed contacts and transform drum scores into a Rhythmic C...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we introduce Robot Drummer, a humanoid capable of expressive, high-precision drumming across a diverse repertoire of songs</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.11498v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.11498v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. A. Shahid, F. Braghin, and L. Roveda, &quot;Robot Drummer: Learning Rhythmic Skills for Humanoid Drumming,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.11498v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445408032')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="okami: teaching humanoid robots manipulation skills through single video imitation" data-keywords="robot humanoid manipulation ros cs.ro cs.ai cs.cv" data-themes="S I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.11792v1" target="_blank">OKAMI: Teaching Humanoid Robots Manipulation Skills through Single Video Imitation</a>
                            </h3>
                            <p class="card-authors">Jinhan Li, Yifeng Zhu, Yuqi Xie et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Manipulation</span><span class="keyword-tag">Ros</span></div>
                            <div class="card-details" id="cat-details-4445406880">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We study the problem of teaching humanoid robots manipulation skills by imitating from single video demonstrations. We introduce OKAMI, a method that generates a manipulation plan from a single RGB-D video and derives a policy for execution. At the heart of our approach is object-aware retargeting, which enables the humanoid robot to mimic the human motions in an RGB-D video while adjusting to different object locations during deployment. OKAMI uses open-world vision models to identify task-relevant objects and retarget the body motions and hand poses separately. Our experiments show that OKAM...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce OKAMI, a method that generates a manipulation plan from a single RGB-D video and derives a policy for execution</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.11792v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.11792v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Li et al., &quot;OKAMI: Teaching Humanoid Robots Manipulation Skills through Single Video Imitation,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.11792v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445406880')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="human-robot collaboration in surgery: advances and challenges towards autonomous surgical assistants" data-keywords="robot manipulation ros cs.ro cs.hc" data-themes="S I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.11460v1" target="_blank">Human-Robot collaboration in surgery: Advances and challenges towards autonomous surgical assistants</a>
                            </h3>
                            <p class="card-authors">Jacinto Colan, Ana Davila, Yutaro Yamada et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Manipulation</span><span class="keyword-tag">Ros</span><span class="keyword-tag">CS.RO</span></div>
                            <div class="card-details" id="cat-details-4445420704">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Human-robot collaboration in surgery represents a significant area of research, driven by the increasing capability of autonomous robotic systems to assist surgeons in complex procedures. This systematic review examines the advancements and persistent challenges in the development of autonomous surgical robotic assistants (ASARs), focusing specifically on scenarios where robots provide meaningful and active support to human surgeons. Adhering to the PRISMA guidelines, a comprehensive literature search was conducted across the IEEE Xplore, Scopus, and Web of Science databases, resulting in the ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.11460v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.11460v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Colan, A. Davila, Y. Yamada, and Y. Hasegawa, &quot;Human-Robot collaboration in surgery: Advances and challenges towards autonomous surgical assistants,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.11460v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445420704')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="humanoid agent via embodied chain-of-action reasoning with multimodal foundation models for zero-shot loco-manipulation" data-keywords="robot humanoid locomotion manipulation coordination perception ros cs.ro cs.ai" data-themes="L M E I R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.09532v3" target="_blank">Humanoid Agent via Embodied Chain-of-Action Reasoning with Multimodal Foundation Models for Zero-Shot Loco-Manipulation</a>
                            </h3>
                            <p class="card-authors">Congcong Wen, Geeta Chandra Raju Bethala, Yu Hao et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-M">Manipulation</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span><span class="keyword-tag">Manipulation</span></div>
                            <div class="card-details" id="cat-details-4445419696">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid loco-manipulation, which integrates whole-body locomotion with dexterous manipulation, remains a fundamental challenge in robotics. Beyond whole-body coordination and balance, a central difficulty lies in understanding human instructions and translating them into coherent sequences of embodied actions. Recent advances in foundation models provide transferable multimodal representations and reasoning capabilities, yet existing efforts remain largely restricted to either locomotion or manipulation in isolation, with limited applicability to humanoid settings. In this paper, we propose H...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose Humanoid-COA, the first humanoid agent framework that integrates foundation model reasoning with an Embodied Chain-of-Action (CoA) mechanism for zero-shot loco-manipulation</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.09532v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.09532v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Wen et al., &quot;Humanoid Agent via Embodied Chain-of-Action Reasoning with Multimodal Foundation Models for Zero-Shot Loco-Manipulation,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.09532v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445419696')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="why robots are bad at detecting their mistakes: limitations of miscommunication detection in human-robot dialogue" data-keywords="robot computer vision cs.ro cs.cl cs.hc" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.20268v1" target="_blank">Why Robots Are Bad at Detecting Their Mistakes: Limitations of Miscommunication Detection in Human-Robot Dialogue</a>
                            </h3>
                            <p class="card-authors">Ruben Janssens, Jens De Bock, Sofie Labat et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Computer Vision</span><span class="keyword-tag">CS.RO</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4445421088">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Detecting miscommunication in human-robot interaction is a critical function for maintaining user engagement and trust. While humans effortlessly detect communication errors in conversations through both verbal and non-verbal cues, robots face significant challenges in interpreting non-verbal feedback, despite advances in computer vision for recognizing affective expressions. This research evaluates the effectiveness of machine learning models in detecting miscommunications in robot dialogue. Using a multi-modal dataset of 240 human-robot conversations, where four distinct types of conversatio...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.20268v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.20268v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Janssens, J. D. Bock, S. Labat, E. Verhelst, V. Hoste, and T. Belpaeme, &quot;Why Robots Are Bad at Detecting Their Mistakes: Limitations of Miscommunication Detection in Human-Robot Dialogue,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.20268v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445421088')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="ce-mrs: contrastive explanations for multi-robot systems" data-keywords="robot multi-robot planning cs.ro cs.hc cs.ma" data-themes="E I R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Multi-Agent Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.08408v1" target="_blank">CE-MRS: Contrastive Explanations for Multi-Robot Systems</a>
                            </h3>
                            <p class="card-authors">Ethan Schneider, Daniel Wu, Devleena Das et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Planning</span><span class="keyword-tag">CS.RO</span></div>
                            <div class="card-details" id="cat-details-4445421184">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">As the complexity of multi-robot systems grows to incorporate a greater number of robots, more complex tasks, and longer time horizons, the solutions to such problems often become too complex to be fully intelligible to human users. In this work, we introduce an approach for generating natural language explanations that justify the validity of the system&#x27;s solution to the user, or else aid the user in correcting any errors that led to a suboptimal system solution. Toward this goal, we first contribute a generalizable formalism of contrastive explanations for multi-robot systems, and then intro...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we introduce an approach for generating natural language explanations that justify the validity of the system&#x27;s solution to the user, or else aid the user in correcting any errors that led to a suboptimal system solution</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.08408v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.08408v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'E. Schneider, D. Wu, D. Das, and S. Chernova, &quot;CE-MRS: Contrastive Explanations for Multi-Robot Systems,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.08408v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445421184')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="learning multi-modal whole-body control for real-world humanoid robots" data-keywords="robot humanoid control simulation imu cs.ro cs.ai" data-themes="S E L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.07295v3" target="_blank">Learning Multi-Modal Whole-Body Control for Real-World Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Pranay Dugar, Aayam Shrestha, Fangzhou Yu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Control</span><span class="keyword-tag">Simulation</span></div>
                            <div class="card-details" id="cat-details-4445420176">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The foundational capabilities of humanoid robots should include robustly standing, walking, and mimicry of whole and partial-body motions. This work introduces the Masked Humanoid Controller (MHC), which supports all of these capabilities by tracking target trajectories over selected subsets of humanoid state variables while ensuring balance and robustness against disturbances. The MHC is trained in simulation using a carefully designed curriculum that imitates partially masked motions from a library of behaviors spanning standing, walking, optimized reference trajectories, re-targeted video c...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This work introduces the Masked Humanoid Controller (MHC), which supports all of these capabilities by tracking target trajectories over selected subsets of humanoid state variables while ensuring balance and robustness against disturbances</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.07295v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.07295v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Dugar, A. Shrestha, F. Yu, B. v. Marum, and A. Fern, &quot;Learning Multi-Modal Whole-Body Control for Real-World Humanoid Robots,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.07295v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445420176')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="romoco: robotic motion control toolbox for reduced-order model-based locomotion on bipedal and humanoid robots" data-keywords="robot humanoid bipedal locomotion control simulation ros imu cs.ro" data-themes="S L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.19545v1" target="_blank">RoMoCo: Robotic Motion Control Toolbox for Reduced-Order Model-Based Locomotion on Bipedal and Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Min Dai, Aaron D. Ames</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4445420896">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We present RoMoCo, an open-source C++ toolbox for the synthesis and evaluation of reduced-order model-based planners and whole-body controllers for bipedal and humanoid robots. RoMoCo&#x27;s modular architecture unifies state-of-the-art planners and whole-body locomotion controllers under a consistent API, enabling rapid prototyping and reproducible benchmarking. By leveraging reduced-order models for platform-agnostic gait generation, RoMoCo enables flexible controller design across diverse robots. We demonstrate its versatility and performance through extensive simulations on the Cassie, Unitree ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present RoMoCo, an open-source C++ toolbox for the synthesis and evaluation of reduced-order model-based planners and whole-body controllers for bipedal and humanoid robots</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.19545v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.19545v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Dai, and A. D. Ames, &quot;RoMoCo: Robotic Motion Control Toolbox for Reduced-Order Model-Based Locomotion on Bipedal and Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.19545v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445420896')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="online dnn-driven nonlinear mpc for stylistic humanoid robot walking with step adjustment" data-keywords="neural network robot humanoid locomotion control optimization cs.ro" data-themes="L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.07849v1" target="_blank">Online DNN-driven Nonlinear MPC for Stylistic Humanoid Robot Walking with Step Adjustment</a>
                            </h3>
                            <p class="card-authors">Giulio Romualdi, Paolo Maria Viceconte, Lorenzo Moretti et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4445408128">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents a three-layered architecture that enables stylistic locomotion with online contact location adjustment. Our method combines an autoregressive Deep Neural Network (DNN) acting as a trajectory generation layer with a model-based trajectory adjustment and trajectory control layers. The DNN produces centroidal and postural references serving as an initial guess and regularizer for the other layers. Being the DNN trained on human motion capture data, the resulting robot motion exhibits locomotion patterns, resembling a human walking style. The trajectory adjustment layer utilize...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents a three-layered architecture that enables stylistic locomotion with online contact location adjustment</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.07849v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.07849v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Romualdi et al., &quot;Online DNN-driven Nonlinear MPC for Stylistic Humanoid Robot Walking with Step Adjustment,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.07849v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445408128')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="mimicgen: a data generation system for scalable robot learning using human demonstrations" data-keywords="robot simulation ros imu imitation learning cs.ro cs.ai cs.cv" data-themes="I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Simulation</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2310.17596v1" target="_blank">MimicGen: A Data Generation System for Scalable Robot Learning using Human Demonstrations</a>
                            </h3>
                            <p class="card-authors">Ajay Mandlekar, Soroush Nasiriany, Bowen Wen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Simulation</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Imu</span></div>
                            <div class="card-details" id="cat-details-4445421520">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Imitation learning from a large set of human demonstrations has proved to be an effective paradigm for building capable robot agents. However, the demonstrations can be extremely costly and time-consuming to collect. We introduce MimicGen, a system for automatically synthesizing large-scale, rich datasets from only a small number of human demonstrations by adapting them to new contexts. We use MimicGen to generate over 50K demonstrations across 18 tasks with diverse scene configurations, object instances, and robot arms from just ~200 human demonstrations. We show that robot agents can be effe...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce MimicGen, a system for automatically synthesizing large-scale, rich datasets from only a small number of human demonstrations by adapting them to new contexts</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2310.17596v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2310.17596v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Mandlekar et al., &quot;MimicGen: A Data Generation System for Scalable Robot Learning using Human Demonstrations,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2310.17596v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445421520')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="rlss: real-time, decentralized, cooperative, networkless multi-robot trajectory planning using linear spatial separations" data-keywords="robot multi-robot cooperative planning optimization simulation imu cs.ro" data-themes="R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2302.12863v2" target="_blank">RLSS: Real-time, Decentralized, Cooperative, Networkless Multi-Robot Trajectory Planning using Linear Spatial Separations</a>
                            </h3>
                            <p class="card-authors">Baskƒ±n ≈ûenba≈ülar, Wolfgang H√∂nig, Nora Ayanian</p>
                            <div class="theme-tags"><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Cooperative</span><span class="keyword-tag">Planning</span></div>
                            <div class="card-details" id="cat-details-4445421232">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Trajectory planning for multiple robots in shared environments is a challenging problem especially when there is limited communication available or no central entity. In this article, we present Real-time planning using Linear Spatial Separations, or RLSS: a real-time decentralized trajectory planning algorithm for cooperative multi-robot teams in static environments. The algorithm requires relatively few robot capabilities, namely sensing the positions of robots and obstacles without higher-order derivatives and the ability of distinguishing robots from obstacles. There is no communication re...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this article, we present Real-time planning using Linear Spatial Separations, or RLSS: a real-time decentralized trajectory planning algorithm for cooperative multi-robot teams in static environments</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2302.12863v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2302.12863v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. ≈ûenba≈ülar, W. H√∂nig, and N. Ayanian, &quot;RLSS: Real-time, Decentralized, Cooperative, Networkless Multi-Robot Trajectory Planning using Linear Spatial Separations,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2302.12863v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445421232')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="mechanical intelligence-aware curriculum reinforcement learning for humanoids with parallel actuation" data-keywords="reinforcement learning robot humanoid locomotion control simulation mujoco imu cs.ro" data-themes="S L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.00273v3" target="_blank">Mechanical Intelligence-Aware Curriculum Reinforcement Learning for Humanoids with Parallel Actuation</a>
                            </h3>
                            <p class="card-authors">Yusuke Tanaka, Alvin Zhu, Quanyou Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Locomotion</span></div>
                            <div class="card-details" id="cat-details-4445420848">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Reinforcement learning (RL) has enabled advances in humanoid robot locomotion, yet most learning frameworks do not account for mechanical intelligence embedded in parallel actuation mechanisms due to limitations in simulator support for closed kinematic chains. This omission can lead to inaccurate motion modeling and suboptimal policies, particularly for robots with high actuation complexity. This paper presents general formulations and simulation methods for three types of parallel mechanisms: a differential pulley, a five-bar linkage, and a four-bar linkage, and trains a parallel-mechanism a...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents general formulations and simulation methods for three types of parallel mechanisms: a differential pulley, a five-bar linkage, and a four-bar linkage, and trains a parallel-mechanism aware policy through an end-to-end curriculum RL framework for BRUCE, a kid-sized humanoid robot</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.00273v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.00273v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Tanaka, A. Zhu, Q. Wang, Y. Liu, and D. Hong, &quot;Mechanical Intelligence-Aware Curriculum Reinforcement Learning for Humanoids with Parallel Actuation,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.00273v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445420848')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="does chatgpt and whisper make humanoid robots more relatable?" data-keywords="gpt robot humanoid language model cs.ro cs.hc" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.07095v1" target="_blank">Does ChatGPT and Whisper Make Humanoid Robots More Relatable?</a>
                            </h3>
                            <p class="card-authors">Xiaohui Chen, Katherine Luo, Trevor Gee et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4445420320">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots are designed to be relatable to humans for applications such as customer support and helpdesk services. However, many such systems, including Softbank&#x27;s Pepper, fall short because they fail to communicate effectively with humans. The advent of Large Language Models (LLMs) shows the potential to solve the communication barrier for humanoid robotics. This paper outlines the comparison of different Automatic Speech Recognition (ASR) APIs, the integration of Whisper ASR and ChatGPT with the Pepper robot and the evaluation of the system (Pepper-GPT) tested by 15 human users. The com...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.07095v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.07095v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. Chen, K. Luo, T. Gee, and M. Nejati, &quot;Does ChatGPT and Whisper Make Humanoid Robots More Relatable?,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.07095v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445420320')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="guiding collision-free humanoid multi-contact locomotion using convex kinematic relaxations and dynamic optimization" data-keywords="robot humanoid navigation planning optimization simulation imu cs.ro" data-themes="L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.08335v1" target="_blank">Guiding Collision-Free Humanoid Multi-Contact Locomotion using Convex Kinematic Relaxations and Dynamic Optimization</a>
                            </h3>
                            <p class="card-authors">Carlos Gonzalez, Luis Sentis</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Navigation</span><span class="keyword-tag">Planning</span></div>
                            <div class="card-details" id="cat-details-4445420752">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humanoid robots rely on multi-contact planners to navigate a diverse set of environments, including those that are unstructured and highly constrained. To synthesize stable multi-contact plans within a reasonable time frame, most planners assume statically stable motions or rely on reduced order models. However, these approaches can also render the problem infeasible in the presence of large obstacles or when operating near kinematic and dynamic limits. To that end, we propose a new multi-contact framework that leverages recent advancements in relaxing collision-free path planning into a conve...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To that end, we propose a new multi-contact framework that leverages recent advancements in relaxing collision-free path planning into a convex optimization problem, extending it to be applicable to humanoid multi-contact navigation</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.08335v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.08335v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Gonzalez, and L. Sentis, &quot;Guiding Collision-Free Humanoid Multi-Contact Locomotion using Convex Kinematic Relaxations and Dynamic Optimization,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.08335v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445420752')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="ironcub 3: the jet-powered flying humanoid robot" data-keywords="robot humanoid control simulation imu cs.ro" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.01125v1" target="_blank">iRonCub 3: The Jet-Powered Flying Humanoid Robot</a>
                            </h3>
                            <p class="card-authors">Davide Gorbani, Hosameldin Awadalla Omer Mohamed, Giuseppe L&#x27;Erario et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Control</span><span class="keyword-tag">Simulation</span></div>
                            <div class="card-details" id="cat-details-4445420416">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This article presents iRonCub 3, a jet-powered humanoid robot, and its first flight experiments. Unlike traditional aerial vehicles, iRonCub 3 aims to achieve flight using a full-body humanoid form, which poses unique challenges in control, estimation, and system integration. We highlight the robot&#x27;s current mechanical and software architecture, including its propulsion system, control framework, and experimental infrastructure. The control and estimation framework is first validated in simulation by performing a takeoff and tracking a reference trajectory. Then, we demonstrate, for the first ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Then, we demonstrate, for the first time, a liftoff of a jet-powered humanoid robot - an initial but significant step toward aerial humanoid mobility</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.01125v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.01125v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Gorbani et al., &quot;iRonCub 3: The Jet-Powered Flying Humanoid Robot,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.01125v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445420416')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="trinity: a modular humanoid robot ai system" data-keywords="reinforcement learning attention robot humanoid planning control imu language model cs.ro" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.08338v1" target="_blank">Trinity: A Modular Humanoid Robot AI System</a>
                            </h3>
                            <p class="card-authors">Jingkai Sun, Qiang Zhang, Gang Han et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Attention</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span></div>
                            <div class="card-details" id="cat-details-4445421136">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In recent years, research on humanoid robots has garnered increasing attention. With breakthroughs in various types of artificial intelligence algorithms, embodied intelligence, exemplified by humanoid robots, has been highly anticipated. The advancements in reinforcement learning (RL) algorithms have significantly improved the motion control and generalization capabilities of humanoid robots. Simultaneously, the groundbreaking progress in large language models (LLM) and visual language models (VLM) has brought more possibilities and imagination to humanoid robots. LLM enables humanoid robots ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.08338v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.08338v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Sun et al., &quot;Trinity: A Modular Humanoid Robot AI System,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.08338v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445421136')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="advancing behavior generation in mobile robotics through high-fidelity procedural simulations" data-keywords="reinforcement learning robot navigation multi-agent simulation imu nlp cs.ro cs.hc" data-themes="S E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-skip">‚è≠Ô∏è Skip</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.16818v1" target="_blank">Advancing Behavior Generation in Mobile Robotics through High-Fidelity Procedural Simulations</a>
                            </h3>
                            <p class="card-authors">Victor A. Kich, Jair A. Bottega, Raul Steinmetz et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Navigation</span><span class="keyword-tag">Multi-Agent</span></div>
                            <div class="card-details" id="cat-details-4445420560">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper introduces YamaS, a simulator integrating Unity3D Engine with Robotic Operating System for robot navigation research and aims to facilitate the development of both Deep Reinforcement Learning (Deep-RL) and Natural Language Processing (NLP). It supports single and multi-agent configurations with features like procedural environment generation, RGB vision, and dynamic obstacle navigation. Unique to YamaS is its ability to construct single and multi-agent environments, as well as generating agent&#x27;s behaviour through textual descriptions. The simulator&#x27;s fidelity is underscored by compa...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.16818v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.16818v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'V. A. Kich, J. A. Bottega, R. Steinmetz, R. B. Grando, A. Yorozu, and A. Ohya, &quot;Advancing Behavior Generation in Mobile Robotics through High-Fidelity Procedural Simulations,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.16818v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445420560')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="periodic bipedal gait learning using reward composition based on a novel gait planner for humanoid robots" data-keywords="reinforcement learning robot humanoid bipedal locomotion planning cs.ro" data-themes="L I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.08416v1" target="_blank">Periodic Bipedal Gait Learning Using Reward Composition Based on a Novel Gait Planner for Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Bolin Li, Linwei Sun, Xuecong Huang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Bipedal</span></div>
                            <div class="card-details" id="cat-details-4445408512">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents a periodic bipedal gait learning method using reward composition, integrated with a real-time gait planner for humanoid robots. First, we introduce a novel gait planner that incorporates dynamics to design the desired joint trajectory. In the gait design process, the 3D robot model is decoupled into two 2D models, which are then approximated as hybrid inverted pendulums (H-LIP) for trajectory planning. The gait planner operates in parallel in real time within the robot&#x27;s learning environment. Second, based on this gait planner, we design three effective reward functions wit...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents a periodic bipedal gait learning method using reward composition, integrated with a real-time gait planner for humanoid robots</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.08416v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.08416v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. Li, L. Sun, X. Huang, Y. Jiang, and L. Zhu, &quot;Periodic Bipedal Gait Learning Using Reward Composition Based on a Novel Gait Planner for Humanoid Robots,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.08416v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445408512')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="human robot pacing mismatch" data-keywords="robot navigation planning cs.ro cs.hc" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.01542v1" target="_blank">Human Robot Pacing Mismatch</a>
                            </h3>
                            <p class="card-authors">Muchen Sun, Peter Trautman, Todd Murphey</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Navigation</span><span class="keyword-tag">Planning</span><span class="keyword-tag">CS.RO</span></div>
                            <div class="card-details" id="cat-details-4445407888">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">A widely accepted explanation for robots planning overcautious or overaggressive trajectories alongside human is that the crowd density exceeds a threshold such that all feasible trajectories are considered unsafe -- the freezing robot problem. However, even with low crowd density, the robot&#x27;s navigation performance could still drop drastically when in close proximity to human. In this work, we argue that a broader cause of suboptimal navigation performance near human is due to the robot&#x27;s misjudgement for the human&#x27;s willingness (flexibility) to share space with others, particularly when the ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We demonstrate the advantage of distribution space coupling through an anecdotal case study and discuss the future directions of solving human robot pacing mismatch.</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.01542v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.01542v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Sun, P. Trautman, and T. Murphey, &quot;Human Robot Pacing Mismatch,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.01542v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445407888')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="innate motivation for robot swarms by minimizing surprise: from simple simulations to real-world experiments" data-keywords="robot swarm multi-robot control simulation imu cs.ro cs.ma cs.ne" data-themes="S R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.02579v1" target="_blank">Innate Motivation for Robot Swarms by Minimizing Surprise: From Simple Simulations to Real-World Experiments</a>
                            </h3>
                            <p class="card-authors">Tanja Katharina Kaiser, Heiko Hamann</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Swarm</span><span class="keyword-tag">Multi-Robot</span><span class="keyword-tag">Control</span></div>
                            <div class="card-details" id="cat-details-4445421424">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Applications of large-scale mobile multi-robot systems can be beneficial over monolithic robots because of higher potential for robustness and scalability. Developing controllers for multi-robot systems is challenging because the multitude of interactions is hard to anticipate and difficult to model. Automatic design using machine learning or evolutionary robotics seem to be options to avoid that challenge, but bring the challenge of designing reward or fitness functions. Generic reward and fitness functions seem unlikely to exist and task-specific rewards often have undesired side effects. Ap...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Our approach to innate motivation is to minimize surprise, which we implement by maximizing the accuracy of the swarm robot&#x27;s sensor predictions using neuroevolution</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.02579v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.02579v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. K. Kaiser, and H. Hamann, &quot;Innate Motivation for Robot Swarms by Minimizing Surprise: From Simple Simulations to Real-World Experiments,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.02579v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445421424')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="online automatic code generation for robot swarms: llms and self-organizing hierarchy" data-keywords="robot swarm simulation imu cs.ro cs.ai cs.ma" data-themes="E I R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Multi-Agent Systems</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2510.04774v2" target="_blank">Online automatic code generation for robot swarms: LLMs and self-organizing hierarchy</a>
                            </h3>
                            <p class="card-authors">Weixu Zhu, Marco Dorigo, Mary Katherine Heinrich</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Swarm</span><span class="keyword-tag">Simulation</span><span class="keyword-tag">Imu</span></div>
                            <div class="card-details" id="cat-details-4445420128">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Our recently introduced self-organizing nervous system (SoNS) provides robot swarms with 1) ease of behavior design and 2) global estimation of the swarm configuration and its collective environment, facilitating the implementation of online automatic code generation for robot swarms. In a demonstration with 6 real robots and simulation trials with &gt;30 robots, we show that when a SoNS-enhanced robot swarm gets stuck, it can automatically solicit and run code generated by an external LLM on the fly, completing its mission with an 85% success rate.</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2510.04774v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2510.04774v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. Zhu, M. Dorigo, and M. K. Heinrich, &quot;Online automatic code generation for robot swarms: LLMs and self-organizing hierarchy,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2510.04774v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445420128')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="proceedings of the dialogue robot competition 2023" data-keywords="robot humanoid cs.ro" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.14430v5" target="_blank">Proceedings of the Dialogue Robot Competition 2023</a>
                            </h3>
                            <p class="card-authors">Ryuichiro Higashinaka, Takashi Minato, Hiromitsu Nishizaki et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">CS.RO</span></div>
                            <div class="card-details" id="cat-details-4445421376">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The Dialogic Robot Competition 2023 (DRC2023) is a competition for humanoid robots (android robots that closely resemble humans) to compete in interactive capabilities. This is the third year of the competition. The top four teams from the preliminary competition held in November 2023 will compete in the final competition on Saturday, December 23. The task for the interactive robots is to recommend a tourism plan for a specific region. The robots can employ multimodal behaviors, such as language and gestures, to engage the user in the sightseeing plan they recommend. In the preliminary round, ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.14430v5" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.14430v5" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Higashinaka, T. Minato, H. Nishizaki, and T. Nagai, &quot;Proceedings of the Dialogue Robot Competition 2023,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.14430v5')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445421376')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ro" data-title="learning differentiable reachability maps for optimization-based humanoid motion generation" data-keywords="neural network robot humanoid manipulation planning optimization cs.ro" data-themes="I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.11275v1" target="_blank">Learning Differentiable Reachability Maps for Optimization-based Humanoid Motion Generation</a>
                            </h3>
                            <p class="card-authors">Masaki Murooka, Iori Kumagai, Mitsuharu Morisawa et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Manipulation</span></div>
                            <div class="card-details" id="cat-details-4445420080">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">To reduce the computational cost of humanoid motion generation, we introduce a new approach to representing robot kinematic reachability: the differentiable reachability map. This map is a scalar-valued function defined in the task space that takes positive values only in regions reachable by the robot&#x27;s end-effector. A key feature of this representation is that it is continuous and differentiable with respect to task-space coordinates, enabling its direct use as constraints in continuous optimization for humanoid motion planning. We describe a method to learn such differentiable reachability ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To reduce the computational cost of humanoid motion generation, we introduce a new approach to representing robot kinematic reachability: the differentiable reachability map</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.11275v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.11275v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Murooka, I. Kumagai, M. Morisawa, and F. Kanehiro, &quot;Learning Differentiable Reachability Maps for Optimization-based Humanoid Motion Generation,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.11275v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445420080')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="automatically designing robot swarms in environments populated by other robots: an experiment in robot shepherding" data-keywords="robot swarm coordination control cs.ro" data-themes="R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Multi-Agent Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2404.18221v1" target="_blank">Automatically designing robot swarms in environments populated by other robots: an experiment in robot shepherding</a>
                            </h3>
                            <p class="card-authors">David Garz√≥n Ramos, Mauro Birattari</p>
                            <div class="theme-tags"><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Swarm</span><span class="keyword-tag">Coordination</span><span class="keyword-tag">Control</span></div>
                            <div class="card-details" id="cat-details-4445419312">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Automatic design is a promising approach to realizing robot swarms. Given a mission to be performed by the swarm, an automatic method produces the required control software for the individual robots. Automatic design has concentrated on missions that a swarm can execute independently, interacting only with a static environment and without the involvement of other active entities. In this paper, we investigate the design of robot swarms that perform their mission by interacting with other robots that populate their environment. We frame our research within robot shepherding: the problem of usin...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2404.18221v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2404.18221v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. G. Ramos, and M. Birattari, &quot;Automatically designing robot swarms in environments populated by other robots: an experiment in robot shepherding,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2404.18221v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445419312')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="ukf-based sensor fusion for joint-torque sensorless humanoid robots" data-keywords="robot humanoid control sensor fusion cs.ro" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.18380v1" target="_blank">UKF-Based Sensor Fusion for Joint-Torque Sensorless Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Ines Sorrentino, Giulio Romualdi, Daniele Pucci</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Control</span><span class="keyword-tag">Sensor Fusion</span></div>
                            <div class="card-details" id="cat-details-4444957440">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper proposes a novel sensor fusion based on Unscented Kalman Filtering for the online estimation of joint-torques of humanoid robots without joint-torque sensors. At the feature level, the proposed approach considers multimodal measurements (e.g. currents, accelerations, etc.) and non-directly measurable effects, such as external contacts, thus leading to joint torques readily usable in control architectures for human-robot interaction. The proposed sensor fusion can also integrate distributed, non-collocated force/torque sensors, thus being a flexible framework with respect to the unde...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Results demonstrate that our method achieves low root mean square errors in torque tracking, ranging from 0.05 Nm to 2.5 Nm, even in the presence of external contacts.</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.18380v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.18380v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'I. Sorrentino, G. Romualdi, and D. Pucci, &quot;UKF-Based Sensor Fusion for Joint-Torque Sensorless Humanoid Robots,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.18380v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444957440')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ro" data-title="bi-level motion imitation for humanoid robots" data-keywords="robot humanoid optimization simulation imu imitation learning cs.ro" data-themes="I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.01968v1" target="_blank">Bi-Level Motion Imitation for Humanoid Robots</a>
                            </h3>
                            <p class="card-authors">Wenshuai Zhao, Yi Zhao, Joni Pajarinen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">Simulation</span></div>
                            <div class="card-details" id="cat-details-4444947168">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Imitation learning from human motion capture (MoCap) data provides a promising way to train humanoid robots. However, due to differences in morphology, such as varying degrees of joint freedom and force limits, exact replication of human behaviors may not be feasible for humanoid robots. Consequently, incorporating physically infeasible MoCap data in training datasets can adversely affect the performance of the robot policy. To address this issue, we propose a bi-level optimization-based imitation learning framework that alternates between optimizing both the robot policy and the target MoCap ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address this issue, we propose a bi-level optimization-based imitation learning framework that alternates between optimizing both the robot policy and the target MoCap data</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.01968v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.01968v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. Zhao, Y. Zhao, J. Pajarinen, and M. Muehlebach, &quot;Bi-Level Motion Imitation for Humanoid Robots,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.01968v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444947168')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ro" data-title="failure detection and fault tolerant control of a jet-powered flying humanoid robot" data-keywords="robot humanoid control simulation gazebo imu cs.ro" data-themes="S">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2305.16075v1" target="_blank">Failure Detection and Fault Tolerant Control of a Jet-Powered Flying Humanoid Robot</a>
                            </h3>
                            <p class="card-authors">Gabriele Nava, Daniele Pucci</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Humanoid</span><span class="keyword-tag">Control</span><span class="keyword-tag">Simulation</span></div>
                            <div class="card-details" id="cat-details-4444956960">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Failure detection and fault tolerant control are fundamental safety features of any aerial vehicle. With the emergence of complex, multi-body flying systems such as jet-powered humanoid robots, it becomes of crucial importance to design fault detection and control strategies for these systems, too. In this paper we propose a fault detection and control framework for the flying humanoid robot iRonCub in case of loss of one turbine. The framework is composed of a failure detector based on turbines rotational speed, a momentum-based flight control for fault response, and an offline reference gene...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper we propose a fault detection and control framework for the flying humanoid robot iRonCub in case of loss of one turbine</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2305.16075v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2305.16075v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Nava, and D. Pucci, &quot;Failure Detection and Fault Tolerant Control of a Jet-Powered Flying Humanoid Robot,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2305.16075v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444956960')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="cs.cl">
                <h2 class="section-header">üè∑Ô∏è Computational Linguistics <span class="section-count">(95 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="is self-knowledge and action consistent or not: investigating large language model&#x27;s personality" data-keywords="language model cs.cl cs.cy" data-themes="S E M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.14679v2" target="_blank">Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model&#x27;s Personality</a>
                            </h3>
                            <p class="card-authors">Yiming Ai, Zhiwei He, Ziyin Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.CY</span></div>
                            <div class="card-details" id="cat-details-4441927216">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In this study, we delve into the validity of conventional personality questionnaires in capturing the human-like personality traits of Large Language Models (LLMs). Our objective is to assess the congruence between the personality traits LLMs claim to possess and their demonstrated tendencies in real-world scenarios. By conducting an extensive examination of LLM outputs against observed human response patterns, we aim to understand the disjunction between self-knowledge and action in LLMs.</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.14679v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.14679v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Ai et al., &quot;Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model&#x27;s Personality,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.14679v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441927216')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="large language models lack understanding of character composition of words" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.11357v3" target="_blank">Large Language Models Lack Understanding of Character Composition of Words</a>
                            </h3>
                            <p class="card-authors">Andrew Shin, Kunitake Kaneko</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4441922464">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) have demonstrated remarkable performances on a wide range of natural language tasks. Yet, LLMs&#x27; successes have been largely restricted to tasks concerning words, sentences, or documents, and it remains questionable how much they understand the minimal units of text, namely characters. In this paper, we examine contemporary LLMs regarding their ability to understand character composition of words, and show that most of them fail to reliably carry out even the simple tasks that can be handled by humans with perfection. We analyze their behaviors with comparison to to...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.11357v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.11357v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Shin, and K. Kaneko, &quot;Large Language Models Lack Understanding of Character Composition of Words,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.11357v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441922464')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="unmasking the shadows of ai: investigating deceptive capabilities in large language models" data-keywords="language model cs.cl cs.ai" data-themes="S E I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.09676v1" target="_blank">Unmasking the Shadows of AI: Investigating Deceptive Capabilities in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Linge Guo</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4441918816">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This research critically navigates the intricate landscape of AI deception, concentrating on deceptive behaviours of Large Language Models (LLMs). My objective is to elucidate this issue, examine the discourse surrounding it, and subsequently delve into its categorization and ramifications. The essay initiates with an evaluation of the AI Safety Summit 2023 (ASS) and introduction of LLMs, emphasising multidimensional biases that underlie their deceptive behaviours.The literature review covers four types of deception categorised: Strategic deception, Imitation, Sycophancy, and Unfaithful Reason...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.09676v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.09676v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Guo, &quot;Unmasking the Shadows of AI: Investigating Deceptive Capabilities in Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.09676v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441918816')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="self-cognition in large language models: an exploratory study" data-keywords="ros language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.01505v1" target="_blank">Self-Cognition in Large Language Models: An Exploratory Study</a>
                            </h3>
                            <p class="card-authors">Dongping Chen, Jiawen Shi, Yao Wan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4441918432">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">While Large Language Models (LLMs) have achieved remarkable success across various applications, they also raise concerns regarding self-cognition. In this paper, we perform a pioneering study to explore self-cognition in LLMs. Specifically, we first construct a pool of self-cognition instruction prompts to evaluate where an LLM exhibits self-cognition and four well-designed principles to quantify LLMs&#x27; self-cognition. Our study reveals that 4 of the 48 models on Chatbot Arena--specifically Command R, Claude3-Opus, Llama-3-70b-Instruct, and Reka-core--demonstrate some level of detectable self-...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.01505v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.01505v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Chen, J. Shi, Y. Wan, P. Zhou, N. Z. Gong, and L. Sun, &quot;Self-Cognition in Large Language Models: An Exploratory Study,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.01505v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441918432')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="making large language models better reasoners with alignment" data-keywords="language model cs.cl cs.ai cs.lg" data-themes="E I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.02144v1" target="_blank">Making Large Language Models Better Reasoners with Alignment</a>
                            </h3>
                            <p class="card-authors">Peiyi Wang, Lei Li, Liang Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.LG</span></div>
                            <div class="card-details" id="cat-details-4441921120">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Reasoning is a cognitive process of using evidence to reach a sound conclusion. The reasoning capability is essential for large language models (LLMs) to serve as the brain of the artificial general intelligence agent. Recent studies reveal that fine-tuning LLMs on data with the chain of thought (COT) reasoning process can significantly enhance their reasoning capabilities. However, we find that the fine-tuned LLMs suffer from an \textit{Assessment Misalignment} problem, i.e., they frequently assign higher scores to subpar COTs, leading to potential limitations in their reasoning abilities. To...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address this problem, we introduce an \textit{Alignment Fine-Tuning (AFT)} paradigm, which involves three steps: 1) fine-tuning LLMs with COT training data; 2) generating multiple COT responses for each question, and categorizing them into positive and negative ones based on whether they achieve the correct answer; 3) calibrating the scores of positive and negative responses given by LLMs with a novel constraint alignment loss</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.02144v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.02144v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Wang et al., &quot;Making Large Language Models Better Reasoners with Alignment,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.02144v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441921120')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="beneath the surface: unveiling harmful memes with multimodal reasoning distilled from large language models" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.05434v1" target="_blank">Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models</a>
                            </h3>
                            <p class="card-authors">Hongzhan Lin, Ziyang Luo, Jing Ma et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4441698992">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The age of social media is rife with memes. Understanding and detecting harmful memes pose a significant challenge due to their implicit meaning that is not explicitly conveyed through the surface text and image. However, existing harmful meme detection approaches only recognize superficial harm-indicative signals in an end-to-end classification manner but ignore in-depth cognition of the meme text and image. In this paper, we attempt to detect harmful memes based on advanced reasoning over the interplay of multimodal information in memes. Inspired by the success of Large Language Models (LLMs...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Then we propose a novel generative framework to learn reasonable thoughts from LLMs for better multimodal fusion and lightweight fine-tuning, which consists of two training stages: 1) Distill multimodal reasoning knowledge from LLMs; and 2) Fine-tune the generative framework to infer harmfulness</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.05434v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.05434v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Lin, Z. Luo, J. Ma, and L. Chen, &quot;Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.05434v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441698992')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="all languages matter: on the multilingual safety of large language models" data-keywords="gpt ros language model cs.cl cs.ai" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2310.00905v2" target="_blank">All Languages Matter: On the Multilingual Safety of Large Language Models</a>
                            </h3>
                            <p class="card-authors">Wenxuan Wang, Zhaopeng Tu, Chang Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4441919536">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Safety lies at the core of developing and deploying large language models (LLMs). However, previous safety benchmarks only concern the safety in one language, e.g. the majority language in the pretraining data such as English. In this work, we build the first multilingual safety benchmark for LLMs, XSafety, in response to the global deployment of LLMs in practice. XSafety covers 14 kinds of commonly used safety issues across 10 languages that span several language families. We utilize XSafety to empirically study the multilingual safety for 4 widely-used LLMs, including both close-API and open...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In addition, we propose several simple and effective prompting methods to improve the multilingual safety of ChatGPT by evoking safety knowledge and improving cross-lingual generalization of safety alignment</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2310.00905v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2310.00905v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. Wang et al., &quot;All Languages Matter: On the Multilingual Safety of Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2310.00905v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441919536')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="classifying german language proficiency levels using large language models" data-keywords="language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2512.06483v1" target="_blank">Classifying German Language Proficiency Levels Using Large Language Models</a>
                            </h3>
                            <p class="card-authors">Elias-Leander Ahlers, Witold Brunsmann, Malte Schilling</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4441927456">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Assessing language proficiency is essential for education, as it enables instruction tailored to learners needs. This paper investigates the use of Large Language Models (LLMs) for automatically classifying German texts according to the Common European Framework of Reference for Languages (CEFR) into different proficiency levels. To support robust training and evaluation, we construct a diverse dataset by combining multiple existing CEFR-annotated corpora with synthetic data. We then evaluate prompt-engineering strategies, fine-tuning of a LLaMA-3-8B-Instruct model and a probing-based approach...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2512.06483v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2512.06483v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'E. Ahlers, W. Brunsmann, and M. Schilling, &quot;Classifying German Language Proficiency Levels Using Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2512.06483v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441927456')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="not all experts are equal: efficient expert pruning and skipping for mixture-of-experts large language models" data-keywords="ros imu language model cs.cl cs.ai cs.lg" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.14800v2" target="_blank">Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models</a>
                            </h3>
                            <p class="card-authors">Xudong Lu, Qi Liu, Yuhui Xu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Imu</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4441921792">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">A pivotal advancement in the progress of large language models (LLMs) is the emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer parameters, but it is still hard to deploy them due to their immense parameter sizes. Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques. Specifically, we propose, for the first time to our best knowledge, post-tr...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.14800v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.14800v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. Lu et al., &quot;Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.14800v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441921792')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="teal: tokenize and embed all for multi-modal large language models" data-keywords="language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2311.04589v3" target="_blank">TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models</a>
                            </h3>
                            <p class="card-authors">Zhen Yang, Yingxue Zhang, Fandong Meng et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4441927648">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Despite Multi-modal Large Language Models (MM-LLMs) have made exciting strides recently, they are still struggling to efficiently model the interactions among multi-modal inputs and the generation in non-textual modalities. In this work, we propose TEAL (Tokenize and Embed ALl)}, an approach to treat the input from any modality as a token sequence and learn a joint embedding space for all modalities. Specifically, for the input from any modality, TEAL first discretizes it into a token sequence with the off-the-shelf tokenizer and embeds the token sequence into a joint embedding space with a le...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose TEAL (Tokenize and Embed ALl)}, an approach to treat the input from any modality as a token sequence and learn a joint embedding space for all modalities</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2311.04589v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2311.04589v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Yang, Y. Zhang, F. Meng, and J. Zhou, &quot;TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2311.04589v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441927648')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="exploring large language models to generate easy to read content" data-keywords="ros nlp language model cs.cl cs.hc" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.20046v1" target="_blank">Exploring Large Language Models to generate Easy to Read content</a>
                            </h3>
                            <p class="card-authors">Paloma Mart√≠nez, Lourdes Moreno, Alberto Ramos</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4441918384">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Ensuring text accessibility and understandability are essential goals, particularly for individuals with cognitive impairments and intellectual disabilities, who encounter challenges in accessing information across various mediums such as web pages, newspapers, administrative tasks, or health documents. Initiatives like Easy to Read and Plain Language guidelines aim to simplify complex texts; however, standardizing these guidelines remains challenging and often involves manual processes. This work presents an exploratory investigation into leveraging Artificial Intelligence (AI) and Natural La...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.20046v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.20046v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Mart√≠nez, L. Moreno, and A. Ramos, &quot;Exploring Large Language Models to generate Easy to Read content,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.20046v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441918384')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="large language models in ambulatory devices for home health diagnostics: a case study of sickle cell anemia management" data-keywords="language model cs.cl" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2305.03715v1" target="_blank">Large Language Models in Ambulatory Devices for Home Health Diagnostics: A case study of Sickle Cell Anemia Management</a>
                            </h3>
                            <p class="card-authors">Oluwatosin Ogundare, Subuola Sofolahan</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4441920880">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This study investigates the potential of an ambulatory device that incorporates Large Language Models (LLMs) in cadence with other specialized ML models to assess anemia severity in sickle cell patients in real time. The device would rely on sensor data that measures angiogenic material levels to assess anemia severity, providing real-time information to patients and clinicians to reduce the frequency of vaso-occlusive crises because of the early detection of anemia severity, allowing for timely interventions and potentially reducing the likelihood of serious complications. The main challenges...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2305.03715v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2305.03715v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'O. Ogundare, and S. Sofolahan, &quot;Large Language Models in Ambulatory Devices for Home Health Diagnostics: A case study of Sickle Cell Anemia Management,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2305.03715v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441920880')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="reinforcement learning meets large language models: a survey of advancements and applications across the llm lifecycle" data-keywords="reinforcement learning ros language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.16679v1" target="_blank">Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle</a>
                            </h3>
                            <p class="card-authors">Keliang Liu, Dingkang Yang, Ziyun Qian et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4441921072">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In recent years, training methods centered on Reinforcement Learning (RL) have markedly enhanced the reasoning and alignment performance of Large Language Models (LLMs), particularly in understanding human intents, following user instructions, and bolstering inferential strength. Although existing surveys offer overviews of RL augmented LLMs, their scope is often limited, failing to provide a comprehensive summary of how RL operates across the full lifecycle of LLMs. We systematically review the theoretical and practical advancements whereby RL empowers LLMs, especially Reinforcement Learning ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.16679v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.16679v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Liu et al., &quot;Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.16679v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441921072')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="larabench: benchmarking arabic ai with large language models" data-keywords="gpt ros nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2305.14982v2" target="_blank">LAraBench: Benchmarking Arabic AI with Large Language Models</a>
                            </h3>
                            <p class="card-authors">Ahmed Abdelali, Hamdy Mubarak, Shammur Absar Chowdhury et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4441920736">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recent advancements in Large Language Models (LLMs) have significantly influenced the landscape of language and speech research. Despite this progress, these models lack specific benchmarking against state-of-the-art (SOTA) models tailored to particular languages and tasks. LAraBench addresses this gap for Arabic Natural Language Processing (NLP) and Speech Processing tasks, including sequence tagging and content classification across different domains. We utilized models such as GPT-3.5-turbo, GPT-4, BLOOMZ, Jais-13b-chat, Whisper, and USM, employing zero and few-shot learning techniques to t...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2305.14982v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2305.14982v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Abdelali et al., &quot;LAraBench: Benchmarking Arabic AI with Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2305.14982v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441920736')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="scaling behavior of machine translation with large language models under prompt injection attacks" data-keywords="language model cs.cl" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.09832v1" target="_blank">Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks</a>
                            </h3>
                            <p class="card-authors">Zhifan Sun, Antonio Valerio Miceli-Barone</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4441918720">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Language Models (LLMs) are increasingly becoming the preferred foundation platforms for many Natural Language Processing tasks such as Machine Translation, owing to their quality often comparable to or better than task-specific models, and the simplicity of specifying the task through natural language instructions or in-context examples. Their generality, however, opens them up to subversion by end users who may embed into their requests instructions that cause the model to behave in unauthorized and possibly unsafe ways. In this work we study these Prompt Injection Attacks (PIAs) on mul...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce a new benchmark data set and we discover that on multiple language pairs and injected prompts written in English, larger models under certain conditions may become more susceptible to successful attacks, an instance of the Inverse Scaling phenomenon (McKenzie et al., 2023)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.09832v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.09832v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Sun, and A. V. Miceli-Barone, &quot;Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.09832v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441918720')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="emissions and performance trade-off between small and large language models" data-keywords="ros language model cs.cl cs.ai cs.cy" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2601.08844v1" target="_blank">Emissions and Performance Trade-off Between Small and Large Language Models</a>
                            </h3>
                            <p class="card-authors">Anandita Garg, Uma Gaba, Deepan Muthirayan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4441918768">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The advent of Large Language Models (LLMs) has raised concerns about their enormous carbon footprint, starting with energy-intensive training and continuing through repeated inference. This study investigates the potential of using fine-tuned Small Language Models (SLMs) as a sustainable alternative for predefined tasks. Here, we present a comparative analysis of the performance-emissions trade-off between LLMs and fine-tuned SLMs across selected tasks under Natural Language Processing, Reasoning and Programming. Our results show that in four out of the six selected tasks, SLMs maintained comp...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Here, we present a comparative analysis of the performance-emissions trade-off between LLMs and fine-tuned SLMs across selected tasks under Natural Language Processing, Reasoning and Programming</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2601.08844v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2601.08844v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Garg, U. Gaba, D. Muthirayan, and A. R. Chowdhury, &quot;Emissions and Performance Trade-off Between Small and Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2601.08844v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441918768')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="babysit a language model from scratch: interactive language learning by trials and demonstrations" data-keywords="control language model cs.cl cs.ai" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.13828v2" target="_blank">Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations</a>
                            </h3>
                            <p class="card-authors">Ziqiao Ma, Zekun Wang, Joyce Chai</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Control</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4441928176">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Humans are efficient language learners and inherently social creatures. Our language development is largely shaped by our social interactions, for example, the demonstration and feedback from caregivers. Contrary to human language learning, recent advancements in large language models have primarily adopted a non-interactive training paradigm, and refined pre-trained models through feedback afterward. In this work, we explore how corrective feedback from interactions influences neural language acquisition from scratch through systematically controlled experiments, assessing whether it contribu...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce a trial-and-demonstration (TnD) learning framework that incorporates three distinct components: student trials, teacher demonstrations, and a reward conditioned on language competence at various developmental stages</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.13828v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.13828v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Ma, Z. Wang, and J. Chai, &quot;Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.13828v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441928176')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="neuron-level knowledge attribution in large language models" data-keywords="attention ros language model cs.cl cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.12141v4" target="_blank">Neuron-Level Knowledge Attribution in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Zeping Yu, Sophia Ananiadou</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4441925056">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Identifying important neurons for final predictions is essential for understanding the mechanisms of large language models. Due to computational constraints, current attribution techniques struggle to operate at neuron level. In this paper, we propose a static method for pinpointing significant neurons. Compared to seven other methods, our approach demonstrates superior performance across three metrics. Additionally, since most static methods typically only identify &quot;value neurons&quot; directly contributing to the final prediction, we propose a method for identifying &quot;query neurons&quot; which activate...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose a static method for pinpointing significant neurons</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.12141v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.12141v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Yu, and S. Ananiadou, &quot;Neuron-Level Knowledge Attribution in Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.12141v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441925056')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="how do language models learn facts? dynamics, curricula and hallucinations" data-keywords="neural network attention imu language model cs.cl cs.lg" data-themes="E L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.21676v2" target="_blank">How do language models learn facts? Dynamics, curricula and hallucinations</a>
                            </h3>
                            <p class="card-authors">Nicolas Zucchet, J√∂rg Bornschein, Stephanie Chan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Attention</span><span class="keyword-tag">Imu</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4441927168">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models accumulate vast knowledge during pre-training, yet the dynamics governing this acquisition remain poorly understood. This work investigates the learning dynamics of language models on a synthetic factual recall task, uncovering three key findings: First, language models learn in three phases, exhibiting a performance plateau before acquiring precise factual knowledge. Mechanistically, this plateau coincides with the formation of attention-based circuits that support recall. Second, the training data distribution significantly impacts learning dynamics, as imbalanced distr...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.21676v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.21676v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. Zucchet, J. Bornschein, S. Chan, A. Lampinen, R. Pascanu, and S. De, &quot;How do language models learn facts? Dynamics, curricula and hallucinations,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.21676v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441927168')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="behavioral bias of vision-language models: a behavioral finance view" data-keywords="gpt language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.15256v1" target="_blank">Behavioral Bias of Vision-Language Models: A Behavioral Finance View</a>
                            </h3>
                            <p class="card-authors">Yuhang Xiao, Yudi Lin, Ming-Chang Chiu</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4441918336">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Vision-Language Models (LVLMs) evolve rapidly as Large Language Models (LLMs) was equipped with vision modules to create more human-like models. However, we should carefully evaluate their applications in different domains, as they may possess undesired biases. Our work studies the potential behavioral biases of LVLMs from a behavioral finance perspective, an interdisciplinary subject that jointly considers finance and psychology. We propose an end-to-end framework, from data collection to new evaluation metrics, to assess LVLMs&#x27; reasoning capabilities and the dynamic behaviors manifeste...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose an end-to-end framework, from data collection to new evaluation metrics, to assess LVLMs&#x27; reasoning capabilities and the dynamic behaviors manifested in two established human financial behavioral biases: recency bias and authority bias</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.15256v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.15256v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Xiao, Y. Lin, and M. Chiu, &quot;Behavioral Bias of Vision-Language Models: A Behavioral Finance View,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.15256v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441918336')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="factuality challenges in the era of large language models" data-keywords="attention gpt ros language model cs.cl cs.ai cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2310.05189v2" target="_blank">Factuality Challenges in the Era of Large Language Models</a>
                            </h3>
                            <p class="card-authors">Isabelle Augenstein, Timothy Baldwin, Meeyoung Cha et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4441922080">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The emergence of tools based on Large Language Models (LLMs), such as OpenAI&#x27;s ChatGPT, Microsoft&#x27;s Bing Chat, and Google&#x27;s Bard, has garnered immense public attention. These incredibly useful, natural-sounding tools mark significant advances in natural language generation, yet they exhibit a propensity to generate false, erroneous, or misleading content -- commonly referred to as &quot;hallucinations.&quot; Moreover, LLMs can be exploited for malicious applications, such as generating false but credible-sounding content and profiles at scale. This poses a significant challenge to society in terms of th...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2310.05189v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2310.05189v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'I. Augenstein et al., &quot;Factuality Challenges in the Era of Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2310.05189v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441922080')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="can chatgpt be your personal medical assistant?" data-keywords="gpt language model cs.cl cs.si" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.12006v1" target="_blank">Can ChatGPT be Your Personal Medical Assistant?</a>
                            </h3>
                            <p class="card-authors">Md. Rafiul Biswas, Ashhadul Islam, Zubair Shah et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.SI</span></div>
                            <div class="card-details" id="cat-details-4441920688">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The advanced large language model (LLM) ChatGPT has shown its potential in different domains and remains unbeaten due to its characteristics compared to other LLMs. This study aims to evaluate the potential of using a fine-tuned ChatGPT model as a personal medical assistant in the Arabic language. To do so, this study uses publicly available online questions and answering datasets in Arabic language. There are almost 430K questions and answers for 20 disease-specific categories. GPT-3.5-turbo model was fine-tuned with a portion of this dataset. The performance of this fine-tuned model was eval...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.12006v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.12006v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. R. Biswas, A. Islam, Z. Shah, W. Zaghouani, and S. B. Belhaouari, &quot;Can ChatGPT be Your Personal Medical Assistant?,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.12006v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441920688')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="exploring advanced large language models with llmsuite" data-keywords="reinforcement learning transformer gpt language model cs.cl cs.cv" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-must_read">‚≠ê Must Read</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.12036v2" target="_blank">Exploring Advanced Large Language Models with LLMsuite</a>
                            </h3>
                            <p class="card-authors">Giorgio Roffo</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Transformer</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4441916992">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This tutorial explores the advancements and challenges in the development of Large Language Models (LLMs) such as ChatGPT and Gemini. It addresses inherent limitations like temporal knowledge cutoffs, mathematical inaccuracies, and the generation of incorrect information, proposing solutions like Retrieval Augmented Generation (RAG), Program-Aided Language Models (PAL), and frameworks such as ReAct and LangChain. The integration of these techniques enhances LLM performance and reliability, especially in multi-step reasoning and complex task execution. The paper also covers fine-tuning strategi...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.12036v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.12036v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Roffo, &quot;Exploring Advanced Large Language Models with LLMsuite,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.12036v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441916992')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="large language models and multimodal retrieval for visual word sense disambiguation" data-keywords="transformer language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2310.14025v1" target="_blank">Large Language Models and Multimodal Retrieval for Visual Word Sense Disambiguation</a>
                            </h3>
                            <p class="card-authors">Anastasia Kritharoula, Maria Lymperaiou, Giorgos Stamou</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Transformer</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4441929040">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Visual Word Sense Disambiguation (VWSD) is a novel challenging task with the goal of retrieving an image among a set of candidates, which better represents the meaning of an ambiguous word within a given context. In this paper, we make a substantial step towards unveiling this interesting task by applying a varying set of approaches. Since VWSD is primarily a text-image retrieval task, we explore the latest transformer-based methods for multimodal retrieval. Additionally, we utilize Large Language Models (LLMs) as knowledge bases to enhance the given phrases and resolve ambiguity related to th...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2310.14025v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2310.14025v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Kritharoula, M. Lymperaiou, and G. Stamou, &quot;Large Language Models and Multimodal Retrieval for Visual Word Sense Disambiguation,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2310.14025v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441929040')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="open sesame! universal black box jailbreaking of large language models" data-keywords="language model cs.cl cs.cv cs.ne" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.01446v4" target="_blank">Open Sesame! Universal Black Box Jailbreaking of Large Language Models</a>
                            </h3>
                            <p class="card-authors">Raz Lapid, Ron Langberg, Moshe Sipper</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.CV</span><span class="keyword-tag">CS.NE</span></div>
                            <div class="card-details" id="cat-details-4441919008">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs), designed to provide helpful and safe responses, often rely on alignment techniques to align with user intent and social guidelines. Unfortunately, this alignment can be exploited by malicious actors seeking to manipulate an LLM&#x27;s outputs for unintended purposes. In this paper we introduce a novel approach that employs a genetic algorithm (GA) to manipulate LLMs when model architecture and parameters are inaccessible. The GA attack works by optimizing a universal adversarial prompt that -- when combined with a user&#x27;s query -- disrupts the attacked model&#x27;s alignment...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper we introduce a novel approach that employs a genetic algorithm (GA) to manipulate LLMs when model architecture and parameters are inaccessible</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.01446v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.01446v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Lapid, R. Langberg, and M. Sipper, &quot;Open Sesame! Universal Black Box Jailbreaking of Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.01446v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441919008')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="fs-rag: a frame semantics based approach for improved factual accuracy in large language models" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2406.16167v1" target="_blank">FS-RAG: A Frame Semantics Based Approach for Improved Factual Accuracy in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Harish Tayyar Madabushi</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4441918576">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We present a novel extension to Retrieval Augmented Generation with the goal of mitigating factual inaccuracies in the output of large language models. Specifically, our method draws on the cognitive linguistic theory of frame semantics for the indexing and retrieval of factual information relevant to helping large language models answer queries. We conduct experiments to demonstrate the effectiveness of this method both in terms of retrieval effectiveness and in terms of the relevance of the frames and frame relations automatically generated. Our results show that this novel mechanism of Fram...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present a novel extension to Retrieval Augmented Generation with the goal of mitigating factual inaccuracies in the output of large language models</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2406.16167v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2406.16167v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. T. Madabushi, &quot;FS-RAG: A Frame Semantics Based Approach for Improved Factual Accuracy in Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2406.16167v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441918576')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="large language models have learned to use language" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2512.12447v1" target="_blank">Large language models have learned to use language</a>
                            </h3>
                            <p class="card-authors">Gary Lupyan</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4441927264">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Acknowledging that large language models have learned to use language can open doors to breakthrough language science. Achieving these breakthroughs may require abandoning some long-held ideas about how language knowledge is evaluated and reckoning with the difficult fact that we have entered a post-Turing test era.</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2512.12447v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2512.12447v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Lupyan, &quot;Large language models have learned to use language,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2512.12447v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441927264')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="demystifying instruction mixing for fine-tuning large language models" data-keywords="ros nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.10793v3" target="_blank">Demystifying Instruction Mixing for Fine-tuning Large Language Models</a>
                            </h3>
                            <p class="card-authors">Renxi Wang, Haonan Li, Minghao Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4441918960">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Instruction tuning significantly enhances the performance of large language models (LLMs) across various tasks. However, the procedure to optimizing the mixing of instruction datasets for LLM fine-tuning is still poorly understood. This study categorizes instructions into three primary types: NLP downstream tasks, coding, and general chat. We explore the effects of instruction tuning on different combinations of datasets on LLM performance, and find that certain instruction types are more advantageous for specific applications but can negatively impact other areas. This work provides insights ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.10793v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.10793v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Wang et al., &quot;Demystifying Instruction Mixing for Fine-tuning Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.10793v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441918960')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="precise length control in large language models" data-keywords="control language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.11937v1" target="_blank">Precise Length Control in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Bradley Butcher, Michael O&#x27;Keefe, James Titchener</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Control</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4441928224">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Language Models (LLMs) are increasingly used in production systems, powering applications such as chatbots, summarization, and question answering. Despite their success, controlling the length of their response remains a significant challenge, particularly for tasks requiring structured outputs or specific levels of detail. In this work, we propose a method to adapt pre-trained decoder-only LLMs for precise control of response length. Our approach incorporates a secondary length-difference positional encoding (LDPE) into the input embeddings, which counts down to a user-set response term...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose a method to adapt pre-trained decoder-only LLMs for precise control of response length</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.11937v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.11937v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. Butcher, M. O&#x27;Keefe, and J. Titchener, &quot;Precise Length Control in Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.11937v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441928224')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="response: emergent analogical reasoning in large language models" data-keywords="gpt ros language model cs.cl cs.ai" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2308.16118v2" target="_blank">Response: Emergent analogical reasoning in large language models</a>
                            </h3>
                            <p class="card-authors">Damian Hodel, Jevin West</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4441918672">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In their recent Nature Human Behaviour paper, &quot;Emergent analogical reasoning in large language models,&quot; (Webb, Holyoak, and Lu, 2023) the authors argue that &quot;large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems.&quot; In this response, we provide counterexamples of the letter string analogies. In our tests, GPT-3 fails to solve simplest variations of the original tasks, whereas human performance remains consistently high across all modified versions. Zero-shot reasoning is an extraordinary claim that requires extraord...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2308.16118v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2308.16118v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Hodel, and J. West, &quot;Response: Emergent analogical reasoning in large language models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2308.16118v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441918672')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="using large language models to provide explanatory feedback to human tutors" data-keywords="language model cs.cl cs.ai cs.hc" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2306.15498v1" target="_blank">Using Large Language Models to Provide Explanatory Feedback to Human Tutors</a>
                            </h3>
                            <p class="card-authors">Jionghao Lin, Danielle R. Thomas, Feifei Han et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.HC</span></div>
                            <div class="card-details" id="cat-details-4441920976">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Research demonstrates learners engaging in the process of producing explanations to support their reasoning, can have a positive impact on learning. However, providing learners real-time explanatory feedback often presents challenges related to classification accuracy, particularly in domain-specific environments, containing situationally complex and nuanced responses. We present two approaches for supplying tutors real-time feedback within an online lesson on how to give students effective praise. This work-in-progress demonstrates considerable accuracy in binary classification for corrective...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present two approaches for supplying tutors real-time feedback within an online lesson on how to give students effective praise</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2306.15498v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2306.15498v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Lin et al., &quot;Using Large Language Models to Provide Explanatory Feedback to Human Tutors,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2306.15498v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441920976')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="instruction-tuned large language models for machine translation in the medical domain" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.16440v2" target="_blank">Instruction-tuned Large Language Models for Machine Translation in the Medical Domain</a>
                            </h3>
                            <p class="card-authors">Miguel Rios</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4441920016">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Language Models (LLMs) have shown promising results on machine translation for high resource language pairs and domains. However, in specialised domains (e.g. medical) LLMs have shown lower performance compared to standard neural machine translation models. The consistency in the machine translation of terminology is crucial for users, researchers, and translators in specialised domains. In this study, we compare the performance between baseline LLMs and instruction-tuned LLMs in the medical domain. In addition, we introduce terminology from specialised medical dictionaries into the inst...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In addition, we introduce terminology from specialised medical dictionaries into the instruction formatted datasets for fine-tuning LLMs</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.16440v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.16440v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Rios, &quot;Instruction-tuned Large Language Models for Machine Translation in the Medical Domain,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.16440v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441920016')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="learning from failure: integrating negative examples when fine-tuning large language models as agents" data-keywords="control optimization language model cs.cl" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.11651v2" target="_blank">Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents</a>
                            </h3>
                            <p class="card-authors">Renxi Wang, Haonan Li, Xudong Han et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Control</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4441922560">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools such as search engines. However, LLMs are optimized for language generation instead of tool use during training or alignment, limiting their effectiveness as agents. To resolve this problem, previous work has first collected interaction trajectories between LLMs and environments, using only trajectories that successfully finished the task to fine-tune smaller models, making fine-tuning data scarce and acquiring it both difficult and costly. Discarding failed trajectories also ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We further analyze the inference results and find that our method provides a better trade-off between valuable information and errors in unsuccessful trajectories</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.11651v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.11651v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Wang, H. Li, X. Han, Y. Zhang, and T. Baldwin, &quot;Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.11651v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441922560')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="soft inductive bias approach via explicit reasoning perspectives in inappropriate utterance detection using large language models" data-keywords="attention ros language model cs.cl" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2512.08480v1" target="_blank">Soft Inductive Bias Approach via Explicit Reasoning Perspectives in Inappropriate Utterance Detection Using Large Language Models</a>
                            </h3>
                            <p class="card-authors">Ju-Young Kim, Ji-Hong Park, Se-Yeon Lee et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444261584">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recent incidents in certain online games and communities, where anonymity is guaranteed, show that unchecked inappropriate remarks frequently escalate into verbal abuse and even criminal behavior, raising significant social concerns. Consequently, there is a growing need for research on techniques that can detect inappropriate utterances within conversational texts to help build a safer communication environment. Although large-scale language models trained on Korean corpora and chain-of-thought reasoning have recently gained attention, research applying these approaches to inappropriate utter...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this study, we propose a soft inductive bias approach that explicitly defines reasoning perspectives to guide the inference process, thereby promoting rational decision-making and preventing errors that may arise during reasoning</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2512.08480v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2512.08480v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Kim, J. Park, S. Lee, S. Park, and G. Kim, &quot;Soft Inductive Bias Approach via Explicit Reasoning Perspectives in Inappropriate Utterance Detection Using Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2512.08480v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444261584')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="detecting mode collapse in language models via narration" data-keywords="reinforcement learning gpt simulation imu language model cs.cl cs.ai" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.04477v1" target="_blank">Detecting Mode Collapse in Language Models via Narration</a>
                            </h3>
                            <p class="card-authors">Sil Hamilton</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Simulation</span><span class="keyword-tag">Imu</span></div>
                            <div class="card-details" id="cat-details-4444261680">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">No two authors write alike. Personal flourishes invoked in written narratives, from lexicon to rhetorical devices, imply a particular author--what literary theorists label the implied or virtual author; distinct from the real author or narrator of a text. Early large language models trained on unfiltered training sets drawn from a variety of discordant sources yielded incoherent personalities, problematic for conversational tasks but proving useful for sampling literature from multiple perspectives. Successes in alignment research in recent years have allowed researchers to impose subjectively...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Our method and results are significant for researchers seeking to employ language models in sociological simulations.</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.04477v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.04477v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Hamilton, &quot;Detecting Mode Collapse in Language Models via Narration,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.04477v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444261680')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="knowledge-driven agentic scientific corpus distillation framework for biomedical large language models training" data-keywords="gpt multi-agent language model cs.cl cs.ai q-bio.qm" data-themes="E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Multi-Agent Systems</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.19565v3" target="_blank">Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training</a>
                            </h3>
                            <p class="card-authors">Meng Xiao, Xunxin Cai, Qingqing Long et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Multi-Agent</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444261200">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Corpus distillation for biomedical large language models (LLMs) seeks to address the pressing challenge of insufficient quantity and quality in open-source annotated scientific corpora, which remains a bottleneck for effective LLM training in biomedical research. This paper proposes a knowledge-driven, agentic framework for scientific corpus distillation, tailored explicitly for LLM training in the biomedical domain, addressing the challenge posed by the complex hierarchy of biomedical knowledge. Central to our approach is a collaborative multi-agent architecture, where specialized agents, eac...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Central to our approach is a collaborative multi-agent architecture, where specialized agents, each guided by the Medical Subject Headings (MeSH) hierarchy, work in concert to autonomously extract, synthesize, and self-evaluate high-quality textual data from vast scientific literature</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.19565v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.19565v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Xiao, X. Cai, Q. Long, C. Wang, Y. Zhou, and H. Zhu, &quot;Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.19565v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444261200')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="investigating retrieval-augmented generation in quranic studies: a study of 13 open-source large language models" data-keywords="language model cs.cl cs.ai cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.16581v1" target="_blank">Investigating Retrieval-Augmented Generation in Quranic Studies: A Study of 13 Open-Source Large Language Models</a>
                            </h3>
                            <p class="card-authors">Zahra Khalila, Arbi Haza Nasution, Winda Monika et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.LG</span></div>
                            <div class="card-details" id="cat-details-4444260624">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Accurate and contextually faithful responses are critical when applying large language models (LLMs) to sensitive and domain-specific tasks, such as answering queries related to quranic studies. General-purpose LLMs often struggle with hallucinations, where generated responses deviate from authoritative sources, raising concerns about their reliability in religious contexts. This challenge highlights the need for systems that can integrate domain-specific knowledge while maintaining response accuracy, relevance, and faithfulness. In this study, we investigate 13 open-source LLMs categorized in...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.16581v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.16581v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Khalila et al., &quot;Investigating Retrieval-Augmented Generation in Quranic Studies: A Study of 13 Open-Source Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.16581v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444260624')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="wizardlm: empowering large pre-trained language models to follow complex instructions" data-keywords="gpt nlp language model cs.cl cs.ai" data-themes="E L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2304.12244v3" target="_blank">WizardLM: Empowering large pre-trained language models to follow complex instructions</a>
                            </h3>
                            <p class="card-authors">Can Xu, Qingfeng Sun, Kai Zheng et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444261632">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data t...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2304.12244v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2304.12244v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Xu et al., &quot;WizardLM: Empowering large pre-trained language models to follow complex instructions,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2304.12244v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444261632')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="pallm: evaluating and enhancing palliative care conversations with large language models" data-keywords="gpt imu nlp language model cs.cl cs.hc" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.15188v2" target="_blank">PALLM: Evaluating and Enhancing PALLiative Care Conversations with Large Language Models</a>
                            </h3>
                            <p class="card-authors">Zhiyuan Wang, Fangxu Yuan, Virginia LeBaron et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Imu</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4444261872">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Effective patient-provider communication is crucial in clinical care, directly impacting patient outcomes and quality of life. Traditional evaluation methods, such as human ratings, patient feedback, and provider self-assessments, are often limited by high costs and scalability issues. Although existing natural language processing (NLP) techniques show promise, they struggle with the nuances of clinical communication and require sensitive clinical data for training, reducing their effectiveness in real-world applications. Emerging large language models (LLMs) offer a new approach to assessing ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.15188v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.15188v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Wang, F. Yuan, V. LeBaron, T. Flickinger, and L. E. Barnes, &quot;PALLM: Evaluating and Enhancing PALLiative Care Conversations with Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.15188v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444261872')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="efficacy of large language models in systematic reviews" data-keywords="gpt language model cs.cl cs.lg" data-themes="E M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.04646v2" target="_blank">Efficacy of Large Language Models in Systematic Reviews</a>
                            </h3>
                            <p class="card-authors">Aaditya Shah, Shridhar Mehendale, Siddha Kanthi</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.LG</span></div>
                            <div class="card-details" id="cat-details-4444262976">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This study investigates the effectiveness of Large Language Models (LLMs) in interpreting existing literature through a systematic review of the relationship between Environmental, Social, and Governance (ESG) factors and financial performance. The primary objective is to assess how LLMs can replicate a systematic review on a corpus of ESG-focused papers. We compiled and hand-coded a database of 88 relevant papers published from March 2020 to May 2024. Additionally, we used a set of 238 papers from a previous systematic review of ESG literature from January 2015 to February 2020. We evaluated ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.04646v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.04646v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Shah, S. Mehendale, and S. Kanthi, &quot;Efficacy of Large Language Models in Systematic Reviews,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.04646v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444262976')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="thames: an end-to-end tool for hallucination mitigation and evaluation in large language models" data-keywords="gpt ros language model cs.cl" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.11353v3" target="_blank">THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Mengfei Liang, Archish Arun, Zekun Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444260912">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Hallucination, the generation of factually incorrect content, is a growing challenge in Large Language Models (LLMs). Existing detection and mitigation methods are often isolated and insufficient for domain-specific needs, lacking a standardized pipeline. This paper introduces THaMES (Tool for Hallucination Mitigations and EvaluationS), an integrated framework and library addressing this gap. THaMES offers an end-to-end solution for evaluating and mitigating hallucinations in LLMs, featuring automated test set generation, multifaceted benchmarking, and adaptable mitigation strategies. It autom...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.11353v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.11353v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Liang et al., &quot;THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.11353v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444260912')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="head-specific intervention can induce misaligned ai coordination in large language models" data-keywords="attention coordination control language model cs.cl cs.ai" data-themes="S E I R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.05945v3" target="_blank">Head-Specific Intervention Can Induce Misaligned AI Coordination in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Paul Darm, Annalisa Riccardi</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Coordination</span><span class="keyword-tag">Control</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4444262544">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Robust alignment guardrails for large language models (LLMs) are becoming increasingly important with their widespread application. In contrast to previous studies, we demonstrate that inference-time activation interventions can bypass safety alignments and effectively steer model generations towards harmful AI coordination. Our method applies fine-grained interventions at specific attention heads, which we identify by probing each head in a simple binary choice task. We then show that interventions on these heads generalise to the open-ended generation setting, effectively circumventing safet...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In contrast to previous studies, we demonstrate that inference-time activation interventions can bypass safety alignments and effectively steer model generations towards harmful AI coordination</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.05945v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.05945v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Darm, and A. Riccardi, &quot;Head-Specific Intervention Can Induce Misaligned AI Coordination in Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.05945v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444262544')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="understanding network behaviors through natural language question-answering" data-keywords="ros language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2510.21894v1" target="_blank">Understanding Network Behaviors through Natural Language Question-Answering</a>
                            </h3>
                            <p class="card-authors">Mingzhe Xing, Chang Tian, Jianan Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4444261536">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Modern large-scale networks introduce significant complexity in understanding network behaviors, increasing the risk of misconfiguration. Prior work proposed to understand network behaviors by mining network configurations, typically relying on domain-specific languages interfaced with formal models. While effective, they suffer from a steep learning curve and limited flexibility. In contrast, natural language (NL) offers a more accessible and interpretable interface, motivating recent research on NL-guided network behavior understanding. Recent advances in large language models (LLMs) further...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To tackle the above challenges, we propose NetMind, a novel framework for querying networks using NL</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2510.21894v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2510.21894v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Xing et al., &quot;Understanding Network Behaviors through Natural Language Question-Answering,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2510.21894v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444261536')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="prompting and fine-tuning open-sourced large language models for stance classification" data-keywords="ros language model cs.cl cs.ai" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.13734v2" target="_blank">Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification</a>
                            </h3>
                            <p class="card-authors">Iain J. Cruickshank, Lynnette Hui Xian Ng</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4444262640">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Stance classification, the task of predicting the viewpoint of an author on a subject of interest, has long been a focal point of research in domains ranging from social science to machine learning. Current stance detection methods rely predominantly on manual annotation of sentences, followed by training a supervised machine learning model. However, this manual annotation process requires laborious annotation effort, and thus hampers its potential to generalize across different contexts. In this work, we investigate the use of Large Language Models (LLMs) as a stance detection methodology tha...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.13734v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.13734v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'I. J. Cruickshank, and L. H. X. Ng, &quot;Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.13734v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444262640')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="can large language models (or humans) disentangle text?" data-keywords="language model cs.cl" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.16584v2" target="_blank">Can Large Language Models (or Humans) Disentangle Text?</a>
                            </h3>
                            <p class="card-authors">Nicolas Audinet de Pieuchon, Adel Daoud, Connor Thomas Jerzak et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444261968">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We investigate the potential of large language models (LLMs) to disentangle text variables--to remove the textual traces of an undesired forbidden variable in a task sometimes known as text distillation and closely related to the fairness in AI and causal inference literature. We employ a range of various LLM approaches in an attempt to disentangle text by identifying and removing information about a target variable while preserving other relevant signals. We show that in the strong test of removing sentiment, the statistical association between the processed text and sentiment is still detect...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.16584v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.16584v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. A. d. Pieuchon, A. Daoud, C. T. Jerzak, M. Johansson, and R. Johansson, &quot;Can Large Language Models (or Humans) Disentangle Text?,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.16584v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444261968')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="speaker attribution in german parliamentary debates with qlora-adapted large language models" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.09902v2" target="_blank">Speaker attribution in German parliamentary debates with QLoRA-adapted large language models</a>
                            </h3>
                            <p class="card-authors">Tobias Bornheim, Niklas Grieger, Patrick Gustav Blaneck et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444262880">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The growing body of political texts opens up new opportunities for rich insights into political dynamics and ideologies but also increases the workload for manual analysis. Automated speaker attribution, which detects who said what to whom in a speech event and is closely related to semantic role labeling, is an important processing step for computational text analysis. We study the potential of the large language model family Llama 2 to automate speaker attribution in German parliamentary debates from 2017-2021. We fine-tune Llama 2 with QLoRA, an efficient training strategy, and observe our ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We fine-tune Llama 2 with QLoRA, an efficient training strategy, and observe our approach to achieve competitive performance in the GermEval 2023 Shared Task On Speaker Attribution in German News Articles and Parliamentary Debates</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.09902v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.09902v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. Bornheim, N. Grieger, P. G. Blaneck, and S. Bialonski, &quot;Speaker attribution in German parliamentary debates with QLoRA-adapted large language models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.09902v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444262880')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="fanal -- financial activity news alerting language modeling framework" data-keywords="bert gpt optimization language model cs.cl cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.03527v1" target="_blank">FANAL -- Financial Activity News Alerting Language Modeling Framework</a>
                            </h3>
                            <p class="card-authors">Urjitkumar Patel, Fang-Chun Yeh, Chinmay Gondhalekar et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Bert</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4444263120">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In the rapidly evolving financial sector, the accurate and timely interpretation of market news is essential for stakeholders needing to navigate unpredictable events. This paper introduces FANAL (Financial Activity News Alerting Language Modeling Framework), a specialized BERT-based framework engineered for real-time financial event detection and analysis, categorizing news into twelve distinct financial categories. FANAL leverages silver-labeled data processed through XGBoost and employs advanced fine-tuning techniques, alongside ORBERT (Odds Ratio BERT), a novel variant of BERT fine-tuned w...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.03527v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.03527v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'U. Patel, F. Yeh, C. Gondhalekar, and H. Nalluri, &quot;FANAL -- Financial Activity News Alerting Language Modeling Framework,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.03527v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444263120')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="a zero-shot and few-shot study of instruction-finetuned large language models applied to clinical and biomedical tasks" data-keywords="bert gpt nlp language model cs.cl cs.ai cs.lg" data-themes="S E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2307.12114v3" target="_blank">A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks</a>
                            </h3>
                            <p class="card-authors">Yanis Labrak, Mickael Rouvier, Richard Dufour</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Bert</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4444261920">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We evaluate four state-of-the-art instruction-tuned large language models (LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13 real-world clinical and biomedical natural language processing (NLP) tasks in English, such as named-entity recognition (NER), question-answering (QA), relation extraction (RE), etc. Our overall results demonstrate that the evaluated LLMs begin to approach performance of state-of-the-art models in zero- and few-shot scenarios for most tasks, and particularly well for the QA task, even though they have never seen examples from these tasks before. Ho...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2307.12114v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2307.12114v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Labrak, M. Rouvier, and R. Dufour, &quot;A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2307.12114v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444261920')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="measuring the inconsistency of large language models in preferential ranking" data-keywords="language model cs.cl" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.08851v1" target="_blank">Measuring the Inconsistency of Large Language Models in Preferential Ranking</a>
                            </h3>
                            <p class="card-authors">Xiutian Zhao, Ke Wang, Wei Peng</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444261152">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Despite large language models&#x27; (LLMs) recent advancements, their bias and hallucination issues persist, and their ability to offer consistent preferential rankings remains underexplored. This study investigates the capacity of LLMs to provide consistent ordinal preferences, a crucial aspect in scenarios with dense decision space or lacking absolute answers. We introduce a formalization of consistency based on order theory, outlining criteria such as transitivity, asymmetry, reversibility, and independence from irrelevant alternatives. Our diagnostic experiments on selected state-of-the-art LLM...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce a formalization of consistency based on order theory, outlining criteria such as transitivity, asymmetry, reversibility, and independence from irrelevant alternatives</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.08851v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.08851v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. Zhao, K. Wang, and W. Peng, &quot;Measuring the Inconsistency of Large Language Models in Preferential Ranking,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.08851v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444261152')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="overview of the first workshop on language models for low-resource languages (loreslm 2025)" data-keywords="nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.16365v1" target="_blank">Overview of the First Workshop on Language Models for Low-Resource Languages (LoResLM 2025)</a>
                            </h3>
                            <p class="card-authors">Hansi Hettiarachchi, Tharindu Ranasinghe, Paul Rayson et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4444263024">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The first Workshop on Language Models for Low-Resource Languages (LoResLM 2025) was held in conjunction with the 31st International Conference on Computational Linguistics (COLING 2025) in Abu Dhabi, United Arab Emirates. This workshop mainly aimed to provide a forum for researchers to share and discuss their ongoing work on language models (LMs) focusing on low-resource languages, following the recent advancements in neural language models and their linguistic biases towards high-resource languages. LoResLM 2025 attracted notable interest from the natural language processing (NLP) community, ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.16365v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.16365v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Hettiarachchi et al., &quot;Overview of the First Workshop on Language Models for Low-Resource Languages (LoResLM 2025),&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.16365v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444263024')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="commander-gpt: fully unleashing the sarcasm detection capability of multi-modal large language models" data-keywords="attention gpt nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.18681v3" target="_blank">Commander-GPT: Fully Unleashing the Sarcasm Detection Capability of Multi-Modal Large Language Models</a>
                            </h3>
                            <p class="card-authors">Yazhou Zhang, Chunwang Zou, Bo Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4444261728">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Sarcasm detection, as a crucial research direction in the field of Natural Language Processing (NLP), has attracted widespread attention. Traditional sarcasm detection tasks have typically focused on single-modal approaches (e.g., text), but due to the implicit and subtle nature of sarcasm, such methods often fail to yield satisfactory results. In recent years, researchers have shifted the focus of sarcasm detection to multi-modal approaches. However, effectively leveraging multi-modal information to accurately identify sarcastic content remains a challenge that warrants further exploration. L...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Leveraging the powerful integrated processing capabilities of Multi-Modal Large Language Models (MLLMs) for various information sources, we propose an innovative multi-modal Commander-GPT framework</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.18681v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.18681v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Zhang, C. Zou, B. Wang, and J. Qin, &quot;Commander-GPT: Fully Unleashing the Sarcasm Detection Capability of Multi-Modal Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.18681v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444261728')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="how important is tokenization in french medical masked language models?" data-keywords="ros nlp language model cs.cl cs.ai cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.15010v2" target="_blank">How Important Is Tokenization in French Medical Masked Language Models?</a>
                            </h3>
                            <p class="card-authors">Yanis Labrak, Adrien Bazoge, Beatrice Daille et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444263696">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Subword tokenization has become the prevailing standard in the field of natural language processing (NLP) over recent years, primarily due to the widespread utilization of pre-trained language models. This shift began with Byte-Pair Encoding (BPE) and was later followed by the adoption of SentencePiece and WordPiece. While subword tokenization consistently outperforms character and word-level tokenization, the precise factors contributing to its success remain unclear. Key aspects such as the optimal segmentation granularity for diverse tasks and languages, the influence of data sources on tok...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.15010v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.15010v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Labrak, A. Bazoge, B. Daille, M. Rouvier, and R. Dufour, &quot;How Important Is Tokenization in French Medical Masked Language Models?,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.15010v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444263696')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="evaluating class membership relations in knowledge graphs using large language models" data-keywords="gpt language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2404.17000v1" target="_blank">Evaluating Class Membership Relations in Knowledge Graphs using Large Language Models</a>
                            </h3>
                            <p class="card-authors">Bradley P. Allen, Paul T. Groth</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4444261392">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">A backbone of knowledge graphs are their class membership relations, which assign entities to a given class. As part of the knowledge engineering process, we propose a new method for evaluating the quality of these relations by processing descriptions of a given entity and class using a zero-shot chain-of-thought classifier that uses a natural language intensional definition of a class. We evaluate the method using two publicly available knowledge graphs, Wikidata and CaLiGraph, and 7 large language models. Using the gpt-4-0125-preview large language model, the method&#x27;s classification performa...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">As part of the knowledge engineering process, we propose a new method for evaluating the quality of these relations by processing descriptions of a given entity and class using a zero-shot chain-of-thought classifier that uses a natural language intensional definition of a class</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2404.17000v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2404.17000v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. P. Allen, and P. T. Groth, &quot;Evaluating Class Membership Relations in Knowledge Graphs using Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2404.17000v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444261392')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="social bias in large language models for bangla: an empirical study on gender and religious bias" data-keywords="nlp language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.03536v3" target="_blank">Social Bias in Large Language Models For Bangla: An Empirical Study on Gender and Religious Bias</a>
                            </h3>
                            <p class="card-authors">Jayanta Sadhu, Maneesha Rani Saha, Rifat Shahriyar</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444263360">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The rapid growth of Large Language Models (LLMs) has put forward the study of biases as a crucial field. It is important to assess the influence of different types of biases embedded in LLMs to ensure fair use in sensitive fields. Although there have been extensive works on bias assessment in English, such efforts are rare and scarce for a major language like Bangla. In this work, we examine two types of social biases in LLM generated outputs for Bangla language. Our main contributions in this work are: (1) bias studies on two different social biases for Bangla, (2) a curated dataset for bias ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.03536v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.03536v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Sadhu, M. R. Saha, and R. Shahriyar, &quot;Social Bias in Large Language Models For Bangla: An Empirical Study on Gender and Religious Bias,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.03536v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444263360')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="llm-as-a-judge: rapid evaluation of legal document recommendation for retrieval-augmented generation" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.12382v1" target="_blank">LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for Retrieval-Augmented Generation</a>
                            </h3>
                            <p class="card-authors">Anu Pradhan, Alexandra Ortan, Apurv Verma et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444260720">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The evaluation bottleneck in recommendation systems has become particularly acute with the rise of Generative AI, where traditional metrics fall short of capturing nuanced quality dimensions that matter in specialized domains like legal research. Can we trust Large Language Models to serve as reliable judges of their own kind? This paper investigates LLM-as-a-Judge as a principled approach to evaluating Retrieval-Augmented Generation systems in legal contexts, where the stakes of recommendation quality are exceptionally high.
  We tackle two fundamental questions that determine practical viabi...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.12382v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.12382v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Pradhan, A. Ortan, A. Verma, and M. Seshadri, &quot;LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for Retrieval-Augmented Generation,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.12382v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444260720')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2022" data-category="cs.cl" data-title="self-generated in-context learning: leveraging auto-regressive language models as a demonstration generator" data-keywords="language model cs.cl" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2022</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2206.08082v1" target="_blank">Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator</a>
                            </h3>
                            <p class="card-authors">Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444264032">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large-scale pre-trained language models (PLMs) are well-known for being capable of solving a task simply by conditioning a few input-label pairs dubbed demonstrations on a prompt without being explicitly tuned for the desired downstream task. Such a process (i.e., in-context learning), however, naturally leads to high reliance on the demonstrations which are usually selected from external datasets. In this paper, we propose self-generated in-context learning (SG-ICL), which generates demonstrations for in-context learning from PLM itself to minimize the reliance on the external demonstration. ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose self-generated in-context learning (SG-ICL), which generates demonstrations for in-context learning from PLM itself to minimize the reliance on the external demonstration</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2206.08082v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2206.08082v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. J. Kim, H. Cho, J. Kim, T. Kim, K. M. Yoo, and S. Lee, &quot;Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator,&quot; arXiv, 2022. [Online]. Available: http://arxiv.org/abs/2206.08082v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444264032')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="multi-model synthetic training for mission-critical small language models" data-keywords="gpt ros language model cs.cl cs.ai cs.lg" data-themes="S E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.13047v1" target="_blank">Multi-Model Synthetic Training for Mission-Critical Small Language Models</a>
                            </h3>
                            <p class="card-authors">Nolan Platt, Pragyansmita Nayak</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4441925248">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Language Models (LLMs) have demonstrated remarkable capabilities across many domains, yet their appli- cation to specialized fields remains constrained by the scarcity and complexity of domain-specific training data. We present a novel approach that achieves a 261x cost reduction for maritime intelligence by using LLMs as one-time teachers rather than using them directly for inference. Our method transforms 3.2 billion Automatic Identification System (AIS) vessel tracking records into 21,543 synthetic question and answer pairs through multi-model generation (GPT-4o and o3-mini), preventi...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present a novel approach that achieves a 261x cost reduction for maritime intelligence by using LLMs as one-time teachers rather than using them directly for inference</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.13047v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.13047v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. Platt, and P. Nayak, &quot;Multi-Model Synthetic Training for Mission-Critical Small Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.13047v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441925248')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="clinical information extraction for low-resource languages with few-shot learning using pre-trained language models and prompting" data-keywords="language model cs.cl cs.ai cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.13369v2" target="_blank">Clinical information extraction for Low-resource languages with Few-shot learning using Pre-trained language models and Prompting</a>
                            </h3>
                            <p class="card-authors">Phillip Richter-Pechanski, Philipp Wiesenbach, Dominic M. Schwab et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.LG</span></div>
                            <div class="card-details" id="cat-details-4444262112">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Automatic extraction of medical information from clinical documents poses several challenges: high costs of required clinical expertise, limited interpretability of model predictions, restricted computational resources and privacy regulations. Recent advances in domain-adaptation and prompting methods showed promising results with minimal training data using lightweight masked language models, which are suited for well-established interpretability methods. We are first to present a systematic evaluation of these methods in a low-resource setting, by performing multi-class section classificatio...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We demonstrate that a lightweight, domain-adapted pretrained model, prompted with just 20 shots, outperforms a traditional classification model by 30.5% accuracy</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.13369v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.13369v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Richter-Pechanski et al., &quot;Clinical information extraction for Low-resource languages with Few-shot learning using Pre-trained language models and Prompting,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.13369v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444262112')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="towards typologically aware rescoring to mitigate unfaithfulness in lower-resource languages" data-keywords="bert ros language model cs.cl" data-themes="E M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.17664v2" target="_blank">Towards Typologically Aware Rescoring to Mitigate Unfaithfulness in Lower-Resource Languages</a>
                            </h3>
                            <p class="card-authors">Tsan Tsai Chan, Xin Tong, Thi Thu Uyen Hoang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Bert</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444262352">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Multilingual large language models (LLMs) are known to more frequently generate non-faithful output in resource-constrained languages (Guerreiro et al., 2023 - arXiv:2303.16104), potentially because these typologically diverse languages are underrepresented in their training data. To mitigate unfaithfulness in such settings, we propose using computationally light auxiliary models to rescore the outputs of larger architectures. As proof of the feasibility of such an approach, we show that monolingual 4-layer BERT models pretrained from scratch on less than 700 MB of data without fine-tuning are...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To mitigate unfaithfulness in such settings, we propose using computationally light auxiliary models to rescore the outputs of larger architectures</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.17664v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.17664v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. T. Chan, X. Tong, T. T. U. Hoang, B. Tepnadze, and W. Stempniak, &quot;Towards Typologically Aware Rescoring to Mitigate Unfaithfulness in Lower-Resource Languages,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.17664v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444262352')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="wizardcoder: empowering code large language models with evol-instruct" data-keywords="nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2306.08568v2" target="_blank">WizardCoder: Empowering Code Large Language Models with Evol-Instruct</a>
                            </h3>
                            <p class="card-authors">Ziyang Luo, Can Xu, Pu Zhao et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4444264224">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2306.08568v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2306.08568v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Luo et al., &quot;WizardCoder: Empowering Code Large Language Models with Evol-Instruct,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2306.08568v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444264224')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2026" data-category="cs.cl" data-title="quantifying non deterministic drift in large language models" data-keywords="gpt control ros language model cs.cl cs.ai" data-themes="S E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-new">New</span></div>
                            <span class="card-source">arXiv ¬∑ 2026</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2601.19934v1" target="_blank">Quantifying non deterministic drift in large language models</a>
                            </h3>
                            <p class="card-authors">Claire Nicholson</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Control</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4444264752">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) are widely used for tasks ranging from summarisation to decision support. In practice, identical prompts do not always produce identical outputs, even when temperature and other decoding parameters are fixed. In this work, we conduct repeated-run experiments to empirically quantify baseline behavioural drift, defined as output variability observed when the same prompt is issued multiple times under operator-free conditions. We evaluate two publicly accessible models, gpt-4o-mini and llama3.1-8b, across five prompt categories using exact repeats, perturbed inputs, a...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2601.19934v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2601.19934v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Nicholson, &quot;Quantifying non deterministic drift in large language models,&quot; arXiv, 2026. [Online]. Available: http://arxiv.org/abs/2601.19934v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444264752')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="causal reasoning in large language models: a knowledge graph approach" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.11588v1" target="_blank">Causal Reasoning in Large Language Models: A Knowledge Graph Approach</a>
                            </h3>
                            <p class="card-authors">Yejin Kim, Eojin Kang, Juae Kim et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444264848">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) typically improve performance by either retrieving semantically similar information, or enhancing reasoning abilities through structured prompts like chain-of-thought. While both strategies are considered crucial, it remains unclear which has a greater impact on model performance or whether a combination of both is necessary. This paper answers this question by proposing a knowledge graph (KG)-based random-walk reasoning approach that leverages causal relationships. We conduct experiments on the commonsense question answering task that is based on a KG. The KG inhe...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.11588v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.11588v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Kim, E. Kang, J. Kim, and H. H. Huang, &quot;Causal Reasoning in Large Language Models: A Knowledge Graph Approach,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.11588v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444264848')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="pediatricsgpt: large language models as chinese medical assistants for pediatric applications" data-keywords="gpt optimization ros language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2405.19266v4" target="_blank">PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications</a>
                            </h3>
                            <p class="card-authors">Dingkang Yang, Jinjie Wei, Dongling Xiao et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4444265184">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Developing intelligent pediatric consultation systems offers promising prospects for improving diagnostic efficiency, especially in China, where healthcare resources are scarce. Despite recent advances in Large Language Models (LLMs) for Chinese medicine, their performance is sub-optimal in pediatric applications due to inadequate instruction data and vulnerable training procedures. To address the above issues, this paper builds PedCorpus, a high-quality dataset of over 300,000 multi-task instructions from pediatric textbooks, guidelines, and knowledge graph resources to fulfil diverse diagnos...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Upon well-designed PedCorpus, we propose PediatricsGPT, the first Chinese pediatric LLM assistant built on a systematic and robust training pipeline</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2405.19266v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2405.19266v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Yang et al., &quot;PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2405.19266v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444265184')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2026" data-category="cs.cl" data-title="sharp: social harm analysis via risk profiles for measuring inequities in large language models" data-keywords="ros language model cs.cl cs.ai" data-themes="E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge badge-new">New</span></div>
                            <span class="card-source">arXiv ¬∑ 2026</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2601.21235v1" target="_blank">SHARP: Social Harm Analysis via Risk Profiles for Measuring Inequities in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Alok Abhishek, Tushar Bandopadhyay, Lisa Erickson</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4444261056">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) are increasingly deployed in high-stakes domains, where rare but severe failures can result in irreversible harm. However, prevailing evaluation benchmarks often reduce complex social risk to mean-centered scalar scores, thereby obscuring distributional structure, cross-dimensional interactions, and worst-case behavior. This paper introduces Social Harm Analysis via Risk Profiles (SHARP), a framework for multidimensional, distribution-aware evaluation of social harm. SHARP models harm as a multivariate random variable and integrates explicit decomposition into bias...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2601.21235v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2601.21235v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Abhishek, T. Bandopadhyay, and L. Erickson, &quot;SHARP: Social Harm Analysis via Risk Profiles for Measuring Inequities in Large Language Models,&quot; arXiv, 2026. [Online]. Available: http://arxiv.org/abs/2601.21235v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444261056')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2022" data-category="cs.cl" data-title="gpt-neox-20b: an open-source autoregressive language model" data-keywords="gpt language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2022</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2204.06745v1" target="_blank">GPT-NeoX-20B: An Open-Source Autoregressive Language Model</a>
                            </h3>
                            <p class="card-authors">Sid Black, Stella Biderman, Eric Hallahan et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444265664">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe \model{}&#x27;s architecture and training and evaluate its performance on a range of language-understanding, mathematics, and knowledge-based tasks. We find that GPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more i...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2204.06745v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2204.06745v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Black et al., &quot;GPT-NeoX-20B: An Open-Source Autoregressive Language Model,&quot; arXiv, 2022. [Online]. Available: http://arxiv.org/abs/2204.06745v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444265664')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="detection of personal data in structured datasets using a large language model" data-keywords="gpt ros language model cs.cl" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.22305v1" target="_blank">Detection of Personal Data in Structured Datasets Using a Large Language Model</a>
                            </h3>
                            <p class="card-authors">Albert Agisha Ntwali, Luca R√ºck, Martin Heckmann</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444263600">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We propose a novel approach for detecting personal data in structured datasets, leveraging GPT-4o, a state-of-the-art Large Language Model. A key innovation of our method is the incorporation of contextual information: in addition to a feature&#x27;s name and values, we utilize information from other feature names within the dataset as well as the dataset description. We compare our approach to alternative methods, including Microsoft Presidio and CASSED, evaluating them on multiple datasets: DeSSI, a large synthetic dataset, datasets we collected from Kaggle and OpenML as well as MIMIC-Demo-Ext, a...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a novel approach for detecting personal data in structured datasets, leveraging GPT-4o, a state-of-the-art Large Language Model</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.22305v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.22305v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. A. Ntwali, L. R√ºck, and M. Heckmann, &quot;Detection of Personal Data in Structured Datasets Using a Large Language Model,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.22305v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444263600')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="beyond data quantity: key factors driving performance in multilingual language models" data-keywords="ros language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.12500v1" target="_blank">Beyond Data Quantity: Key Factors Driving Performance in Multilingual Language Models</a>
                            </h3>
                            <p class="card-authors">Sina Bagheri Nezhad, Ameeta Agrawal, Rhitabrat Pokharel</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4444265568">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Multilingual language models (MLLMs) are crucial for handling text across various languages, yet they often show performance disparities due to differences in resource availability and linguistic characteristics. While the impact of pre-train data percentage and model size on performance is well-known, our study reveals additional critical factors that significantly influence MLLM effectiveness. Analyzing a wide range of features, including geographical, linguistic, and resource-related aspects, we focus on the SIB-200 dataset for classification and the Flores-200 dataset for machine translati...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.12500v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.12500v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. B. Nezhad, A. Agrawal, and R. Pokharel, &quot;Beyond Data Quantity: Key Factors Driving Performance in Multilingual Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.12500v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444265568')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="a comprehensive survey of scientific large language models and their applications in scientific discovery" data-keywords="ros language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2406.10833v3" target="_blank">A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery</a>
                            </h3>
                            <p class="card-authors">Yu Zhang, Xiusi Chen, Bowen Jin et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444266000">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In many scientific fields, large language models (LLMs) have revolutionized the way text and other modalities of data (e.g., molecules and proteins) are handled, achieving superior performance in various applications and augmenting the scientific discovery process. Nevertheless, previous surveys on scientific LLMs often concentrate on one or two fields or a single modality. In this paper, we aim to provide a more holistic view of the research landscape by unveiling cross-field and cross-modal connections between scientific LLMs regarding their architectures and pre-training techniques. To this...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2406.10833v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2406.10833v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Zhang et al., &quot;A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2406.10833v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444266000')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2022" data-category="cs.cl" data-title="exploring the value of pre-trained language models for clinical named entity recognition" data-keywords="transformer bert nlp language model cs.cl cs.ai cs.lg" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2022</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2210.12770v4" target="_blank">Exploring the Value of Pre-trained Language Models for Clinical Named Entity Recognition</a>
                            </h3>
                            <p class="card-authors">Samuel Belkadi, Lifeng Han, Yuping Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Transformer</span><span class="keyword-tag">Bert</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4444265376">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The practice of fine-tuning Pre-trained Language Models (PLMs) from general or domain-specific data to a specific task with limited resources, has gained popularity within the field of natural language processing (NLP). In this work, we re-visit this assumption and carry out an investigation in clinical NLP, specifically Named Entity Recognition on drugs and their related attributes. We compare Transformer models that are trained from scratch to fine-tuned BERT-based LLMs namely BERT, BioBERT, and ClinicalBERT. Furthermore, we examine the impact of an additional CRF layer on such models to enc...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2210.12770v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2210.12770v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Belkadi, L. Han, Y. Wu, and G. Nenadic, &quot;Exploring the Value of Pre-trained Language Models for Clinical Named Entity Recognition,&quot; arXiv, 2022. [Online]. Available: http://arxiv.org/abs/2210.12770v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444265376')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="a survey of large language models for arabic language and its dialects" data-keywords="ros language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.20238v2" target="_blank">A Survey of Large Language Models for Arabic Language and its Dialects</a>
                            </h3>
                            <p class="card-authors">Malak Mashaabi, Shahad Al-Khalifa, Hend Al-Khalifa</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4444264368">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This survey offers a comprehensive overview of Large Language Models (LLMs) designed for Arabic language and its dialects. It covers key architectures, including encoder-only, decoder-only, and encoder-decoder models, along with the datasets used for pre-training, spanning Classical Arabic, Modern Standard Arabic, and Dialectal Arabic. The study also explores monolingual, bilingual, and multilingual LLMs, analyzing their architectures and performance across downstream tasks, such as sentiment analysis, named entity recognition, and question answering. Furthermore, it assesses the openness of A...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.20238v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.20238v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Mashaabi, S. Al-Khalifa, and H. Al-Khalifa, &quot;A Survey of Large Language Models for Arabic Language and its Dialects,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.20238v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444264368')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="turkish native language identification v2" data-keywords="cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2307.14850v6" target="_blank">Turkish Native Language Identification V2</a>
                            </h3>
                            <p class="card-authors">Ahmet Yavuz Uluslu, Gerold Schneider</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444264080">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents the first application of Native Language Identification (NLI) for the Turkish language. NLI is the task of automatically identifying an individual&#x27;s native language (L1) based on their writing or speech in a non-native language (L2). While most NLI research has focused on L2 English, our study extends this scope to L2 Turkish by analyzing a corpus of texts written by native speakers of Albanian, Arabic and Persian. We leverage a cleaned version of the Turkish Learner Corpus and demonstrate the effectiveness of syntactic features, comparing a structural Part-of-Speech n-gram...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents the first application of Native Language Identification (NLI) for the Turkish language</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2307.14850v6" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2307.14850v6" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Y. Uluslu, and G. Schneider, &quot;Turkish Native Language Identification V2,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2307.14850v6')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444264080')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="a few-shot approach for relation extraction domain adaptation using large language models" data-keywords="deep learning transformer ros language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.02377v1" target="_blank">A Few-Shot Approach for Relation Extraction Domain Adaptation using Large Language Models</a>
                            </h3>
                            <p class="card-authors">Vanni Zavarella, Juan Carlos Gamero-Salinas, Sergio Consoli</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Deep Learning</span><span class="keyword-tag">Transformer</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4444263552">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Knowledge graphs (KGs) have been successfully applied to the analysis of complex scientific and technological domains, with automatic KG generation methods typically building upon relation extraction models capturing fine-grained relations between domain entities in text. While these relations are fully applicable across scientific areas, existing models are trained on few domain-specific datasets such as SciERC and do not perform well on new target domains. In this paper, we experiment with leveraging in-context learning capabilities of Large Language Models to perform schema-constrained data...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.02377v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.02377v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'V. Zavarella, J. C. Gamero-Salinas, and S. Consoli, &quot;A Few-Shot Approach for Relation Extraction Domain Adaptation using Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.02377v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444263552')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="fast vocabulary transfer for language model compression" data-keywords="language model cs.cl cs.ai cs.lg" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.09977v1" target="_blank">Fast Vocabulary Transfer for Language Model Compression</a>
                            </h3>
                            <p class="card-authors">Leonidas Gee, Andrea Zugarini, Leonardo Rigutini et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.LG</span></div>
                            <div class="card-details" id="cat-details-4444265904">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Real-world business applications require a trade-off between language model performance and size. We propose a new method for model compression that relies on vocabulary transfer. We evaluate the method on various vertical domains and downstream tasks. Our results indicate that vocabulary transfer can be effectively used in combination with other compression techniques, yielding a significant reduction in model size and inference time while marginally compromising on performance.</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a new method for model compression that relies on vocabulary transfer</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.09977v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.09977v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Gee, A. Zugarini, L. Rigutini, and P. Torroni, &quot;Fast Vocabulary Transfer for Language Model Compression,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.09977v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444265904')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="language-conditioned world model improves policy generalization by reading environmental descriptions" data-keywords="reinforcement learning attention planning cs.cl cs.lg" data-themes="S E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2511.22904v1" target="_blank">Language-conditioned world model improves policy generalization by reading environmental descriptions</a>
                            </h3>
                            <p class="card-authors">Anh Nguyen, Stefan Lee</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Attention</span><span class="keyword-tag">Planning</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444274592">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">To interact effectively with humans in the real world, it is important for agents to understand language that describes the dynamics of the environment--that is, how the environment behaves--rather than just task instructions specifying &quot;what to do&quot;. Understanding this dynamics-descriptive language is important for human-agent interaction and agent behavior. Recent work address this problem using a model-based approach: language is incorporated into a world model, which is then used to learn a behavior policy. However, these existing methods either do not demonstrate policy generalization to u...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a model-based reinforcement learning approach, where a language-conditioned world model is trained through interaction with the environment, and a policy is learned from this model--without planning or expert demonstrations</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2511.22904v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2511.22904v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Nguyen, and S. Lee, &quot;Language-conditioned world model improves policy generalization by reading environmental descriptions,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2511.22904v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444274592')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="exploring gender bias in large language models: an in-depth dive into the german language" data-keywords="perception ros language model cs.cl cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.16557v1" target="_blank">Exploring Gender Bias in Large Language Models: An In-depth Dive into the German Language</a>
                            </h3>
                            <p class="card-authors">Kristin Gnadt, David Thulke, Simone Kopeinik et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Perception</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444261344">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In recent years, various methods have been proposed to evaluate gender bias in large language models (LLMs). A key challenge lies in the transferability of bias measurement methods initially developed for the English language when applied to other languages. This work aims to contribute to this research strand by presenting five German datasets for gender bias evaluation in LLMs. The datasets are grounded in well-established concepts of gender bias and are accessible through multiple methodologies. Our findings, reported for eight multilingual LLM models, reveal unique challenges associated wi...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.16557v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.16557v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Gnadt, D. Thulke, S. Kopeinik, and R. Schl√ºter, &quot;Exploring Gender Bias in Large Language Models: An In-depth Dive into the German Language,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.16557v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444261344')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="native vs non-native language prompting: a comparative analysis" data-keywords="nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.07054v2" target="_blank">Native vs Non-Native Language Prompting: A Comparative Analysis</a>
                            </h3>
                            <p class="card-authors">Mohamed Bayan Kmainasi, Rakif Khan, Ali Ezzat Shahroor et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4444265280">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) have shown remarkable abilities in different fields, including standard Natural Language Processing (NLP) tasks. To elicit knowledge from LLMs, prompts play a key role, consisting of natural language instructions. Most open and closed source LLMs are trained on available labeled and unlabeled resources--digital content such as text, images, audio, and videos. Hence, these models have better knowledge for high-resourced languages but struggle with low-resourced languages. Since prompts play a crucial role in understanding their capabilities, the language used for pr...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.07054v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.07054v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. B. Kmainasi, R. Khan, A. E. Shahroor, B. Bendou, M. Hasanain, and F. Alam, &quot;Native vs Non-Native Language Prompting: A Comparative Analysis,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.07054v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444265280')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="pragmatic competence evaluation of large language models for the korean language" data-keywords="gpt language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.12675v2" target="_blank">Pragmatic Competence Evaluation of Large Language Models for the Korean Language</a>
                            </h3>
                            <p class="card-authors">Dojun Park, Jiwoo Lee, Hyeyun Jeong et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444264512">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Benchmarks play a significant role in the current evaluation of Large Language Models (LLMs), yet they often overlook the models&#x27; abilities to capture the nuances of human language, primarily focusing on evaluating embedded knowledge and technical skills. To address this gap, our study evaluates how well LLMs understand context-dependent expressions from a pragmatic standpoint, specifically in Korean. We use both Multiple-Choice Questions (MCQs) for automatic evaluation and Open-Ended Questions (OEQs) assessed by human experts. Our results show that GPT-4 leads with scores of 81.11 in MCQs and...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.12675v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.12675v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Park, J. Lee, H. Jeong, S. Park, and S. Lee, &quot;Pragmatic Competence Evaluation of Large Language Models for the Korean Language,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.12675v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444264512')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="metasc: test-time safety specification optimization for language models" data-keywords="optimization ros language model cs.cl cs.ai" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.07985v2" target="_blank">MetaSC: Test-Time Safety Specification Optimization for Language Models</a>
                            </h3>
                            <p class="card-authors">V√≠ctor Gallego</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Optimization</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444264944">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We propose a novel dynamic safety framework that optimizes language model (LM) safety reasoning at inference time without modifying model weights. Building on recent advances in self-critique methods, our approach leverages a meta-critique mechanism that iteratively updates safety prompts-termed specifications-to drive the critique and revision process adaptively. This test-time optimization not only improves performance against adversarial jailbreak requests but also in diverse general safety-related tasks, such as avoiding moral harm or pursuing honest responses. Our empirical evaluations ac...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a novel dynamic safety framework that optimizes language model (LM) safety reasoning at inference time without modifying model weights</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.07985v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.07985v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'V. Gallego, &quot;MetaSC: Test-Time Safety Specification Optimization for Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.07985v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444264944')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2022" data-category="cs.cl" data-title="prompting is programming: a query language for large language models" data-keywords="control language model cs.cl cs.ai" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2022</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2212.06094v3" target="_blank">Prompting Is Programming: A Query Language for Large Language Models</a>
                            </h3>
                            <p class="card-authors">Luca Beurer-Kellner, Marc Fischer, Martin Vechev</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Control</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4444264416">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models have demonstrated outstanding performance on a wide range of tasks such as question answering and code generation. On a high level, given an input, a language model can be used to automatically complete the sequence in a statistically-likely way. Based on this, users prompt these models with language instructions or examples, to implement a variety of downstream tasks. Advanced prompting methods can even imply interaction between the language model, a user, and external tools such as calculators. However, to obtain state-of-the-art performance or adapt language models for...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">However, to obtain state-of-the-art performance or adapt language models for specific tasks, complex task- and model-specific programs have to be implemented, which may still require ad-hoc interaction.
  Based on this, we present the novel idea of Language Model Programming (LMP)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2212.06094v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2212.06094v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Beurer-Kellner, M. Fischer, and M. Vechev, &quot;Prompting Is Programming: A Query Language for Large Language Models,&quot; arXiv, 2022. [Online]. Available: http://arxiv.org/abs/2212.06094v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444264416')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="untangling the influence of typology, data and model architecture on ranking transfer languages for cross-lingual pos tagging" data-keywords="lstm ros cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.19979v1" target="_blank">Untangling the Influence of Typology, Data and Model Architecture on Ranking Transfer Languages for Cross-Lingual POS Tagging</a>
                            </h3>
                            <p class="card-authors">Enora Rice, Ali Marashian, Hannah Haynie et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Lstm</span><span class="keyword-tag">Ros</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444263264">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Cross-lingual transfer learning is an invaluable tool for overcoming data scarcity, yet selecting a suitable transfer language remains a challenge. The precise roles of linguistic typology, training data, and model architecture in transfer language choice are not fully understood. We take a holistic approach, examining how both dataset-specific and fine-grained typological features influence transfer language selection for part-of-speech tagging, considering two different sources for morphosyntactic features. While previous work examines these dynamics in the context of bilingual biLSTMS, we e...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.19979v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.19979v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'E. Rice, A. Marashian, H. Haynie, K. v. d. Wense, and A. Palmer, &quot;Untangling the Influence of Typology, Data and Model Architecture on Ranking Transfer Languages for Cross-Lingual POS Tagging,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.19979v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444263264')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="a comprehensive review of state-of-the-art methods for java code generation from natural language text" data-keywords="deep learning transformer rnn nlp cs.cl" data-themes="E I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2306.06371v1" target="_blank">A Comprehensive Review of State-of-The-Art Methods for Java Code Generation from Natural Language Text</a>
                            </h3>
                            <p class="card-authors">Jessica L√≥pez Espejel, Mahaman Sanoussi Yahaya Alassan, El Mehdi Chouham et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Deep Learning</span><span class="keyword-tag">Transformer</span><span class="keyword-tag">Rnn</span><span class="keyword-tag">Nlp</span></div>
                            <div class="card-details" id="cat-details-4444265040">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Java Code Generation consists in generating automatically Java code from a Natural Language Text. This NLP task helps in increasing programmers&#x27; productivity by providing them with immediate solutions to the simplest and most repetitive tasks. Code generation is a challenging task because of the hard syntactic rules and the necessity of a deep understanding of the semantic aspect of the programming language. Many works tried to tackle this task using either RNN-based, or Transformer-based models. The latter achieved remarkable advancement in the domain and they can be divided into three groups...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2306.06371v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2306.06371v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. L. Espejel, M. S. Y. Alassan, E. M. Chouham, W. Dahhane, and E. H. Ettifouri, &quot;A Comprehensive Review of State-of-The-Art Methods for Java Code Generation from Natural Language Text,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2306.06371v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444265040')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="the generation gap: exploring age bias in the value systems of large language models" data-keywords="ros nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2404.08760v4" target="_blank">The Generation Gap: Exploring Age Bias in the Value Systems of Large Language Models</a>
                            </h3>
                            <p class="card-authors">Siyang Liu, Trish Maturi, Bowen Yi et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444261440">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We explore the alignment of values in Large Language Models (LLMs) with specific age groups, leveraging data from the World Value Survey across thirteen categories. Through a diverse set of prompts tailored to ensure response robustness, we find a general inclination of LLM values towards younger demographics, especially when compared to the US population. Although a general inclination can be observed, we also found that this inclination toward younger groups can be different across different value categories. Additionally, we explore the impact of incorporating age identity information in pr...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2404.08760v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2404.08760v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Liu, T. Maturi, B. Yi, S. Shen, and R. Mihalcea, &quot;The Generation Gap: Exploring Age Bias in the Value Systems of Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2404.08760v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444261440')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="automatic generation of question hints for mathematics problems using large language models in educational technology" data-keywords="gpt imu language model cs.cl cs.ai" data-themes="E M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2411.03495v1" target="_blank">Automatic Generation of Question Hints for Mathematics Problems using Large Language Models in Educational Technology</a>
                            </h3>
                            <p class="card-authors">Junior Cedric Tonga, Benjamin Clement, Pierre-Yves Oudeyer</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Imu</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444266048">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The automatic generation of hints by Large Language Models (LLMs) within Intelligent Tutoring Systems (ITSs) has shown potential to enhance student learning. However, generating pedagogically sound hints that address student misconceptions and adhere to specific educational objectives remains challenging. This work explores using LLMs (GPT-4o and Llama-3-8B-instruct) as teachers to generate effective hints for students simulated through LLMs (GPT-3.5-turbo, Llama-3-8B-Instruct, or Mistral-7B-instruct-v0.3) tackling math exercises designed for human high-school students, and designed using cogn...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We present here the study of several dimensions: 1) identifying error patterns made by simulated students on secondary-level math exercises; 2) developing various prompts for GPT-4o as a teacher and evaluating their effectiveness in generating hints that enable simulated students to self-correct; and 3) testing the best-performing prompts, based on their ability to produce relevant hints and facilitate error correction, with Llama-3-8B-Instruct as the teacher, allowing for a performance comparison with GPT-4o</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2411.03495v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2411.03495v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. C. Tonga, B. Clement, and P. Oudeyer, &quot;Automatic Generation of Question Hints for Mathematics Problems using Large Language Models in Educational Technology,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2411.03495v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444266048')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="understanding survey paper taxonomy about large language models via graph representation learning" data-keywords="language model cs.cl cs.ai cs.ir" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.10409v1" target="_blank">Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning</a>
                            </h3>
                            <p class="card-authors">Jun Zhuang, Casey Kennington</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.IR</span></div>
                            <div class="card-details" id="cat-details-4444266432">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">As new research on Large Language Models (LLMs) continues, it is difficult to keep up with new research and models. To help researchers synthesize the new research many have written survey papers, but even those have become numerous. In this paper, we develop a method to automatically assign survey papers to a taxonomy. We collect the metadata of 144 LLM survey papers and explore three paradigms to classify papers within the taxonomy. Our work indicates that leveraging graph structure information on co-category graphs can significantly outperform the language models in two paradigms; pre-train...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we develop a method to automatically assign survey papers to a taxonomy</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.10409v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.10409v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Zhuang, and C. Kennington, &quot;Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.10409v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444266432')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="using large language models for knowledge engineering (llmke): a case study on wikidata" data-keywords="ros language model cs.cl cs.ai" data-themes="E M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.08491v1" target="_blank">Using Large Language Models for Knowledge Engineering (LLMKE): A Case Study on Wikidata</a>
                            </h3>
                            <p class="card-authors">Bohui Zhang, Ioannis Reklos, Nitisha Jain et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4444264272">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In this work, we explore the use of Large Language Models (LLMs) for knowledge engineering tasks in the context of the ISWC 2023 LM-KBC Challenge. For this task, given subject and relation pairs sourced from Wikidata, we utilize pre-trained LLMs to produce the relevant objects in string format and link them to their respective Wikidata QIDs. We developed a pipeline using LLMs for Knowledge Engineering (LLMKE), combining knowledge probing and Wikidata entity mapping. The method achieved a macro-averaged F1-score of 0.701 across the properties, with the scores varying from 1.00 to 0.328. These r...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We developed a pipeline using LLMs for Knowledge Engineering (LLMKE), combining knowledge probing and Wikidata entity mapping</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.08491v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.08491v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. Zhang, I. Reklos, N. Jain, A. M. Pe√±uela, and E. Simperl, &quot;Using Large Language Models for Knowledge Engineering (LLMKE): A Case Study on Wikidata,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.08491v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444264272')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="conditional and modal reasoning in large language models" data-keywords="ros language model cs.cl cs.ai cs.lo" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.17169v4" target="_blank">Conditional and Modal Reasoning in Large Language Models</a>
                            </h3>
                            <p class="card-authors">Wesley H. Holliday, Matthew Mandelkern, Cedegao E. Zhang</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4444265808">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The reasoning abilities of large language models (LLMs) are the topic of a growing body of research in AI and cognitive science. In this paper, we probe the extent to which twenty-nine LLMs are able to distinguish logically correct inferences from logically fallacious ones. We focus on inference patterns involving conditionals (e.g., &#x27;If Ann has a queen, then Bob has a jack&#x27;) and epistemic modals (e.g., &#x27;Ann might have an ace&#x27;, &#x27;Bob must have a king&#x27;). These inferences have been of special interest to logicians, philosophers, and linguists, since they play a central role in the fundamental hum...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.17169v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.17169v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. H. Holliday, M. Mandelkern, and C. E. Zhang, &quot;Conditional and Modal Reasoning in Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.17169v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444265808')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2022" data-category="cs.cl" data-title="end-to-end spoken language understanding: performance analyses of a voice command task in a low resource setting" data-keywords="neural network optimization cs.cl cs.sd eess.as" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2022</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2207.08179v1" target="_blank">End-to-End Spoken Language Understanding: Performance analyses of a voice command task in a low resource setting</a>
                            </h3>
                            <p class="card-authors">Thierry Desot, Fran√ßois Portet, Michel Vacher</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.SD</span></div>
                            <div class="card-details" id="cat-details-4444265520">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Spoken Language Understanding (SLU) is a core task in most human-machine interaction systems. With the emergence of smart homes, smart phones and smart speakers, SLU has become a key technology for the industry. In a classical SLU approach, an Automatic Speech Recognition (ASR) module transcribes the speech signal into a textual representation from which a Natural Language Understanding (NLU) module extracts semantic information. Recently End-to-End SLU (E2E SLU) based on Deep Neural Networks has gained momentum since it benefits from the joint optimization of the ASR and the NLU parts, hence ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we present a study identifying the signal features and other linguistic properties used by an E2E model to perform the SLU task</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2207.08179v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2207.08179v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. Desot, F. Portet, and M. Vacher, &quot;End-to-End Spoken Language Understanding: Performance analyses of a voice command task in a low resource setting,&quot; arXiv, 2022. [Online]. Available: http://arxiv.org/abs/2207.08179v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444265520')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="enhancing small language models for cross-lingual generalized zero-shot classification with soft prompt tuning" data-keywords="ros nlp language model cs.cl cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.19469v2" target="_blank">Enhancing Small Language Models for Cross-Lingual Generalized Zero-Shot Classification with Soft Prompt Tuning</a>
                            </h3>
                            <p class="card-authors">Fred Philippy, Siwen Guo, Cedric Lothritz et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444266144">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In NLP, Zero-Shot Classification (ZSC) has become essential for enabling models to classify text into categories unseen during training, particularly in low-resource languages and domains where labeled data is scarce. While pretrained language models (PLMs) have shown promise in ZSC, they often rely on large training datasets or external knowledge, limiting their applicability in multilingual and low-resource scenarios. Recent approaches leveraging natural language prompts reduce the dependence on large training datasets but struggle to effectively incorporate available labeled data from relat...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address these challenges, we introduce RoSPrompt, a lightweight and data-efficient approach for training soft prompts that enhance cross-lingual ZSC while ensuring robust generalization across data distribution shifts</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.19469v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.19469v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. Philippy, S. Guo, C. Lothritz, J. Klein, and T. F. Bissyand√©, &quot;Enhancing Small Language Models for Cross-Lingual Generalized Zero-Shot Classification with Soft Prompt Tuning,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.19469v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444266144')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="studies with impossible languages falsify lms as models of human language" data-keywords="language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2511.11389v1" target="_blank">Studies with impossible languages falsify LMs as models of human language</a>
                            </h3>
                            <p class="card-authors">Jeffrey S. Bowers, Jeff Mitchell</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444266096">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">According to Futrell and Mahowald [arXiv:2501.17047], both infants and language models (LMs) find attested languages easier to learn than impossible languages that have unnatural structures. We review the literature and show that LMs often learn attested and many impossible languages equally well. Difficult to learn impossible languages are simply more complex (or random). LMs are missing human inductive biases that support language acquisition.</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2511.11389v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2511.11389v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. S. Bowers, and J. Mitchell, &quot;Studies with impossible languages falsify LMs as models of human language,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2511.11389v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444266096')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="benchmarking adversarial robustness to bias elicitation in large language models: scalable automated assessment with llm-as-a-judge" data-keywords="manipulation ros language model cs.cl cs.ai" data-themes="S E L M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.07887v2" target="_blank">Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge</a>
                            </h3>
                            <p class="card-authors">Riccardo Cantini, Alessio Orsino, Massimo Ruggiero et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Manipulation</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444262736">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The growing integration of Large Language Models (LLMs) into critical societal domains has raised concerns about embedded biases that can perpetuate stereotypes and undermine fairness. Such biases may stem from historical inequalities in training data, linguistic imbalances, or adversarial manipulation. Despite mitigation efforts, recent studies show that LLMs remain vulnerable to adversarial attacks that elicit biased outputs. This work proposes a scalable benchmarking framework to assess LLM robustness to adversarial bias elicitation. Our methodology involves: (i) systematically probing mode...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Our methodology involves: (i) systematically probing models across multiple tasks targeting diverse sociocultural biases, (ii) quantifying robustness through safety scores using an LLM-as-a-Judge approach, and (iii) employing jailbreak techniques to reveal safety vulnerabilities</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.07887v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.07887v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Cantini, A. Orsino, M. Ruggiero, and D. Talia, &quot;Benchmarking Adversarial Robustness to Bias Elicitation in Large Language Models: Scalable Automated Assessment with LLM-as-a-Judge,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.07887v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444262736')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cl" data-title="facilitating large language model russian adaptation with learned embedding propagation" data-keywords="gpt language model cs.cl cs.ai" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.21140v1" target="_blank">Facilitating large language model Russian adaptation with Learned Embedding Propagation</a>
                            </h3>
                            <p class="card-authors">Mikhail Tikhomirov, Daniil Chernyshev</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4444266240">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Rapid advancements of large language model (LLM) technologies led to the introduction of powerful open-source instruction-tuned LLMs that have the same text generation quality as the state-of-the-art counterparts such as GPT-4. While the emergence of such models accelerates the adoption of LLM technologies in sensitive-information environments the authors of such models don not disclose the training data necessary for replication of the results thus making the achievements model-exclusive. Since those open-source models are also multilingual this in turn reduces the benefits of training a lang...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address the limitations and cut the costs of the language adaptation pipeline we propose Learned Embedding Propagation (LEP)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.21140v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.21140v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Tikhomirov, and D. Chernyshev, &quot;Facilitating large language model Russian adaptation with Learned Embedding Propagation,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.21140v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444266240')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="distinct social-linguistic processing between humans and large audio-language models: evidence from model-brain alignment" data-keywords="ros language model cs.cl q-bio.nc" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2503.19586v2" target="_blank">Distinct social-linguistic processing between humans and large audio-language models: Evidence from model-brain alignment</a>
                            </h3>
                            <p class="card-authors">Hanlin Wu, Xufeng Duan, Zhenguang Cai</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">Q-BIO.NC</span></div>
                            <div class="card-details" id="cat-details-4444265760">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Voice-based AI development faces unique challenges in processing both linguistic and paralinguistic information. This study compares how large audio-language models (LALMs) and humans integrate speaker characteristics during speech comprehension, asking whether LALMs process speaker-contextualized language in ways that parallel human cognitive mechanisms. We compared two LALMs&#x27; (Qwen2-Audio and Ultravox 0.5) processing patterns with human EEG responses. Using surprisal and entropy metrics from the models, we analyzed their sensitivity to speaker-content incongruency across social stereotype vi...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2503.19586v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2503.19586v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Wu, X. Duan, and Z. Cai, &quot;Distinct social-linguistic processing between humans and large audio-language models: Evidence from model-brain alignment,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2503.19586v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444265760')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cl" data-title="tiny language models" data-keywords="transformer bert ros nlp language model cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.14871v2" target="_blank">Tiny language models</a>
                            </h3>
                            <p class="card-authors">Ronit D. Gross, Yarden Tzach, Tal Halevi et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Transformer</span><span class="keyword-tag">Bert</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Nlp</span></div>
                            <div class="card-details" id="cat-details-4444265424">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">A prominent achievement of natural language processing (NLP) is its ability to understand and generate meaningful human language. This capability relies on complex feedforward transformer block architectures pre-trained on large language models (LLMs). However, LLM pre-training is currently feasible only for a few dominant companies due to the immense computational resources required, limiting broader research participation. This creates a critical need for more accessible alternatives. In this study, we explore whether tiny language models (TLMs) exhibit the same key qualitative features of L...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We demonstrate that TLMs exhibit a clear performance gap between pre-trained and non-pre-trained models across classification tasks, indicating the effectiveness of pre-training, even at a tiny scale</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.14871v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.14871v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. D. Gross, Y. Tzach, T. Halevi, E. Koresh, and I. Kanter, &quot;Tiny language models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.14871v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444265424')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2022" data-category="cs.cl" data-title="an overview of indian spoken language recognition from machine learning perspective" data-keywords="neural network cs.cl cs.sd eess.as" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2022</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2212.03812v1" target="_blank">An Overview of Indian Spoken Language Recognition from Machine Learning Perspective</a>
                            </h3>
                            <p class="card-authors">Spandan Dey, Md Sahidullah, Goutam Saha</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.SD</span><span class="keyword-tag">EESS.AS</span></div>
                            <div class="card-details" id="cat-details-4444260960">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Automatic spoken language identification (LID) is a very important research field in the era of multilingual voice-command-based human-computer interaction (HCI). A front-end LID module helps to improve the performance of many speech-based applications in the multilingual scenario. India is a populous country with diverse cultures and languages. The majority of the Indian population needs to use their respective native languages for verbal interaction with machines. Therefore, the development of efficient Indian spoken language recognition systems is useful for adapting smart technologies in e...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2212.03812v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2212.03812v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Dey, M. Sahidullah, and G. Saha, &quot;An Overview of Indian Spoken Language Recognition from Machine Learning Perspective,&quot; arXiv, 2022. [Online]. Available: http://arxiv.org/abs/2212.03812v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444260960')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cl" data-title="jais and jais-chat: arabic-centric foundation and instruction-tuned open generative large language models" data-keywords="gpt language model cs.cl cs.ai cs.lg" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2308.16149v2" target="_blank">Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models</a>
                            </h3>
                            <p class="card-authors">Neha Sengupta, Sunil Kumar Sahu, Bokang Jia et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CL</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4444264896">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-ce...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2308.16149v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2308.16149v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. Sengupta et al., &quot;Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2308.16149v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444264896')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="cs.lg">
                <h2 class="section-header">üè∑Ô∏è Machine Learning <span class="section-count">(12 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="a critical review of causal reasoning benchmarks for large language models" data-keywords="language model cs.lg cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.08029v1" target="_blank">A Critical Review of Causal Reasoning Benchmarks for Large Language Models</a>
                            </h3>
                            <p class="card-authors">Linying Yang, Vik Shirvaikar, Oscar Clivio et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4441928272">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Numerous benchmarks aim to evaluate the capabilities of Large Language Models (LLMs) for causal inference and reasoning. However, many of them can likely be solved through the retrieval of domain knowledge, questioning whether they achieve their purpose. In this review, we present a comprehensive overview of LLM benchmarks for causality. We highlight how recent benchmarks move towards a more thorough definition of causal reasoning by incorporating interventional or counterfactual reasoning. We derive a set of criteria that a useful benchmark or set of benchmarks should aim to satisfy. We hope ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this review, we present a comprehensive overview of LLM benchmarks for causality</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.08029v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.08029v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Yang, V. Shirvaikar, O. Clivio, and F. Falck, &quot;A Critical Review of Causal Reasoning Benchmarks for Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.08029v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441928272')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="wkvquant: quantizing weight and key/value cache for large language models gains more" data-keywords="attention optimization ros language model cs.lg cs.ai cs.cl" data-themes="S E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.12065v2" target="_blank">WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More</a>
                            </h3>
                            <p class="card-authors">Yuxuan Yue, Zhihang Yuan, Haojie Duanmu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4441926256">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process. This paper addresses these challenges by focusing on the quantization of LLMs, a technique that reduces memory consumption by converting model parameters and activations into low-bit integers. We critically analyze the existing quantization approaches, identifying their limitations in balancing the accuracy and efficiency of the quantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ fr...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To advance beyond these limitations, we propose WKVQuant, a PTQ framework especially designed for quantizing weights and the key/value (KV) cache of LLMs</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.12065v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.12065v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Yue, Z. Yuan, H. Duanmu, S. Zhou, J. Wu, and L. Nie, &quot;WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.12065v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441926256')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="unraveling arithmetic in large language models: the role of algebraic structures" data-keywords="transformer attention language model cs.lg cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2411.16260v3" target="_blank">Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures</a>
                            </h3>
                            <p class="card-authors">Fu-Chieh Chang, You-Chen Lin, Pei-Yuan Wu</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Transformer</span><span class="keyword-tag">Attention</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span></div>
                            <div class="card-details" id="cat-details-4441918288">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) have demonstrated remarkable mathematical capabilities, largely driven by chain-of-thought (CoT) prompting, which decomposes complex reasoning into step-by-step solutions. This approach has enabled significant advancements, as evidenced by performance on benchmarks like GSM8K and MATH. However, the mechanisms underlying LLMs&#x27; ability to perform arithmetic in a single step of CoT remain poorly understood. Existing studies debate whether LLMs encode numerical values or rely on symbolic reasoning, while others explore attention and multi-layered processing in arithmet...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we propose that LLMs learn arithmetic by capturing algebraic structures, such as commutativity and identity properties</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2411.16260v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2411.16260v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. Chang, Y. Lin, and P. Wu, &quot;Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2411.16260v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441918288')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="bridging large language models and graph structure learning models for robust representation learning" data-keywords="ros language model cs.lg cs.ai" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.12096v1" target="_blank">Bridging Large Language Models and Graph Structure Learning Models for Robust Representation Learning</a>
                            </h3>
                            <p class="card-authors">Guangxin Su, Yifan Zhu, Wenjie Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4441922320">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Graph representation learning, involving both node features and graph structures, is crucial for real-world applications but often encounters pervasive noise. State-of-the-art methods typically address noise by focusing separately on node features with large language models (LLMs) and on graph structures with graph structure learning models (GSLMs). In this paper, we introduce LangGSL, a robust framework that integrates the complementary strengths of pre-trained language models and GSLMs to jointly enhance both node feature and graph structure learning. In LangGSL, we first leverage LLMs to fi...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we introduce LangGSL, a robust framework that integrates the complementary strengths of pre-trained language models and GSLMs to jointly enhance both node feature and graph structure learning</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.12096v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.12096v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'G. Su, Y. Zhu, W. Zhang, H. Wang, and Y. Zhang, &quot;Bridging Large Language Models and Graph Structure Learning Models for Robust Representation Learning,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.12096v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441922320')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="unforgettable generalization in language models" data-keywords="transformer ros language model cs.lg cs.cl" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2409.02228v1" target="_blank">Unforgettable Generalization in Language Models</a>
                            </h3>
                            <p class="card-authors">Eric Zhang, Leshem Chosen, Jacob Andreas</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Transformer</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span></div>
                            <div class="card-details" id="cat-details-4441927504">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">When language models (LMs) are trained to forget (or &quot;unlearn&#x27;&#x27;) a skill, how precisely does their behavior change? We study the behavior of transformer LMs in which tasks have been forgotten via fine-tuning on randomized labels. Such LMs learn to generate near-random predictions for individual examples in the &quot;training&#x27;&#x27; set used for forgetting. Across tasks, however, LMs exhibit extreme variability in whether LM predictions change on examples outside the training set. In some tasks (like entailment classification), forgetting generalizes robustly, and causes models to produce uninformative p...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2409.02228v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2409.02228v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'E. Zhang, L. Chosen, and J. Andreas, &quot;Unforgettable Generalization in Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2409.02228v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441927504')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.lg" data-title="two-stage representation learning for analyzing movement behavior dynamics in people living with dementia" data-keywords="language model cs.lg cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.09173v1" target="_blank">Two-Stage Representation Learning for Analyzing Movement Behavior Dynamics in People Living with Dementia</a>
                            </h3>
                            <p class="card-authors">Jin Cui, Alexander Capstick, Payam Barnaghi et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4444261296">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In remote healthcare monitoring, time series representation learning reveals critical patient behavior patterns from high-frequency data. This study analyzes home activity data from individuals living with dementia by proposing a two-stage, self-supervised learning approach tailored to uncover low-rank structures. The first stage converts time-series activities into text sequences encoded by a pre-trained language model, providing a rich, high-dimensional latent state space using a PageRank-based method. This PageRank vector captures latent state transitions, effectively compressing complex be...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.09173v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.09173v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Cui, A. Capstick, P. Barnaghi, and G. Scott, &quot;Two-Stage Representation Learning for Analyzing Movement Behavior Dynamics in People Living with Dementia,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.09173v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444261296')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.lg" data-title="pb-llm: partially binarized large language models" data-keywords="gpt language model cs.lg cs.ai cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2310.00034v2" target="_blank">PB-LLM: Partially Binarized Large Language Models</a>
                            </h3>
                            <p class="card-authors">Yuzhang Shang, Zhihang Yuan, Qiang Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4444260768">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper explores network binarization, a radical form of quantization, compressing model weights to a single bit, specifically for Large Language Models (LLMs) compression. Due to previous binarization methods collapsing LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of quantized LLMs. Specifically, our exploration first uncovers the ineffectiveness of naive applications of existing binarization algorithms and highlights the imperative role of salient weights in achieving...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Due to previous binarization methods collapsing LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of quantized LLMs</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2310.00034v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2310.00034v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Shang, Z. Yuan, Q. Wu, and Z. Dong, &quot;PB-LLM: Partially Binarized Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2310.00034v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444260768')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="are compressed language models less subgroup robust?" data-keywords="bert language model cs.lg cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.17811v1" target="_blank">Are Compressed Language Models Less Subgroup Robust?</a>
                            </h3>
                            <p class="card-authors">Leonidas Gee, Andrea Zugarini, Novi Quadrianto</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Bert</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444263168">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">To reduce the inference cost of large language models, model compression is increasingly used to create smaller scalable models. However, little is known about their robustness to minority subgroups defined by the labels and attributes of a dataset. In this paper, we investigate the effects of 18 different compression methods and settings on the subgroup robustness of BERT language models. We show that worst-group performance does not depend on model size alone, but also on the compression method used. Additionally, we find that model compression does not always worsen the performance on minor...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.17811v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.17811v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Gee, A. Zugarini, and N. Quadrianto, &quot;Are Compressed Language Models Less Subgroup Robust?,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.17811v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444263168')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="activation sparsity opportunities for compressing general large language models" data-keywords="optimization language model cs.lg cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.12178v2" target="_blank">Activation Sparsity Opportunities for Compressing General Large Language Models</a>
                            </h3>
                            <p class="card-authors">Nobel Dhar, Bobin Deng, Md Romyull Islam et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Optimization</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4444261008">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Deploying local AI models, such as Large Language Models (LLMs), to edge devices can substantially enhance devices&#x27; independent capabilities, alleviate the server&#x27;s burden, and lower the response time. Owing to these tremendous potentials, many big tech companies have released several lightweight Small Language Models (SLMs) to bridge this gap. However, we still have huge motivations to deploy more powerful (LLMs) AI models on edge devices and enhance their smartness level. Unlike the conventional approaches for AI model compression, we investigate activation sparsity. The activation sparsity ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.12178v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.12178v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'N. Dhar, B. Deng, M. R. Islam, K. F. A. Nasif, L. Zhao, and K. Suo, &quot;Activation Sparsity Opportunities for Compressing General Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.12178v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444261008')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.lg" data-title="understanding reasoning in thinking language models via steering vectors" data-keywords="control ros language model cs.lg cs.ai" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.18167v4" target="_blank">Understanding Reasoning in Thinking Language Models via Steering Vectors</a>
                            </h3>
                            <p class="card-authors">Constantin Venhoff, Iv√°n Arcuschin, Philip Torr et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Control</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span></div>
                            <div class="card-details" id="cat-details-4444264464">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recent advances in large language models (LLMs) have led to the development of thinking language models that generate extensive internal reasoning chains before producing responses. While these models achieve improved performance, controlling their reasoning processes remains challenging. This work presents a steering approach for thinking LLMs by analyzing and manipulating specific reasoning behaviors in DeepSeek-R1-Distill models. Through a systematic experiment on 500 tasks across 10 diverse categories, we identify several reasoning behaviors exhibited by thinking models, including expressi...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We demonstrate that these behaviors are mediated by linear directions in the model&#x27;s activation space and can be controlled using steering vectors</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.18167v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.18167v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Venhoff, I. Arcuschin, P. Torr, A. Conmy, and N. Nanda, &quot;Understanding Reasoning in Thinking Language Models via Steering Vectors,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.18167v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444264464')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2021" data-category="cs.lg" data-title="differentially private fine-tuning of language models" data-keywords="bert gpt nlp language model cs.lg cs.cl cs.cr" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2021</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2110.06500v2" target="_blank">Differentially Private Fine-tuning of Language Models</a>
                            </h3>
                            <p class="card-authors">Da Yu, Saurabh Naik, Arturs Backurs et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Bert</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Nlp</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4444265136">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We give simpler, sparser, and faster algorithms for differentially private fine-tuning of large-scale pre-trained language models, which achieve the state-of-the-art privacy versus utility tradeoffs on many standard NLP tasks. We propose a meta-framework for this problem, inspired by the recent success of highly parameter-efficient methods for fine-tuning. Our experiments show that differentially private adaptations of these approaches outperform previous private algorithms in three important dimensions: utility, privacy, and the computational and memory cost of private training. On many commo...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a meta-framework for this problem, inspired by the recent success of highly parameter-efficient methods for fine-tuning</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2110.06500v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2110.06500v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Yu et al., &quot;Differentially Private Fine-tuning of Language Models,&quot; arXiv, 2021. [Online]. Available: http://arxiv.org/abs/2110.06500v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444265136')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.lg" data-title="emergent world models and latent variable estimation in chess-playing language models" data-keywords="gpt language model cs.lg cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üß†</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.15498v2" target="_blank">Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models</a>
                            </h3>
                            <p class="card-authors">Adam Karvonen</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.LG</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444262064">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Language models have shown unprecedented capabilities, sparking debate over the source of their performance. Is it merely the outcome of learning syntactic patterns and surface level statistics, or do they extract semantics and a world model from the text? Prior work by Li et al. investigated this by training a GPT model on synthetic, randomly generated Othello games and found that the model learned an internal representation of the board state. We extend this work into the more complex domain of chess, training on real games and investigating our model&#x27;s internal representations using linear ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.15498v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.15498v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Karvonen, &quot;Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.15498v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444262064')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="cs.cv">
                <h2 class="section-header">üè∑Ô∏è Computer Vision <span class="section-count">(10 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cv" data-title="a survey on multimodal large language models" data-keywords="gpt language model cs.cv cs.ai cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2306.13549v4" target="_blank">A Survey on Multimodal Large Language Models</a>
                            </h3>
                            <p class="card-authors">Shukang Yin, Chaoyou Fu, Sirui Zhao et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4441921744">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. I...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2306.13549v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2306.13549v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Yin et al., &quot;A Survey on Multimodal Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2306.13549v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441921744')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cv" data-title="prunevid: visual token pruning for efficient video large language models" data-keywords="ros language model cs.cv" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.16117v1" target="_blank">PruneVid: Visual Token Pruning for Efficient Video Large Language Models</a>
                            </h3>
                            <p class="card-authors">Xiaohu Huang, Hao Zhou, Kai Han</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span></div>
                            <div class="card-details" id="cat-details-4441920256">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In this paper, we introduce PruneVid, a visual token pruning method designed to enhance the efficiency of multi-modal video understanding. Large Language Models (LLMs) have shown promising performance in video tasks due to their extended capabilities in comprehending visual modalities. However, the substantial redundancy in video data presents significant computational challenges for LLMs. To address this issue, we introduce a training-free method that 1) minimizes video redundancy by merging spatial-temporal tokens, and 2) leverages LLMs&#x27; reasoning capabilities to selectively prune visual fea...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we introduce PruneVid, a visual token pruning method designed to enhance the efficiency of multi-modal video understanding</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.16117v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.16117v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. Huang, H. Zhou, and K. Han, &quot;PruneVid: Visual Token Pruning for Efficient Video Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.16117v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441920256')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cv" data-title="exploring the frontier of vision-language models: a survey of current methodologies and future directions" data-keywords="language model cs.cv cs.ai cs.cl" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2404.07214v4" target="_blank">Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions</a>
                            </h3>
                            <p class="card-authors">Akash Ghosh, Arkadeep Acharya, Sriparna Saha et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444262304">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The advent of Large Language Models (LLMs) has significantly reshaped the trajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable limitation, as they are primarily adept at processing textual information. To address this constraint, researchers have endeavored to integrate visual capabilities with LLMs, resulting in the emergence of Vision-Language Models (VLMs). These advanced models are instrumental in tackling more intricate tasks such as image captioning and visual question answering. In our comprehensive survey paper, we delve into the key advancements within the realm...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2404.07214v4" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2404.07214v4" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Ghosh, A. Acharya, S. Saha, V. Jain, and A. Chadha, &quot;Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2404.07214v4')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444262304')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cv" data-title="object detection with multimodal large vision-language models: an in-depth review" data-keywords="deep learning robot computer vision nlp language model cs.cv cs.ai cs.cl" data-themes="E I M">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.19294v2" target="_blank">Object Detection with Multimodal Large Vision-Language Models: An In-depth Review</a>
                            </h3>
                            <p class="card-authors">Ranjan Sapkota, Manoj Karkee</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Deep Learning</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Computer Vision</span><span class="keyword-tag">Nlp</span></div>
                            <div class="card-details" id="cat-details-4444261488">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The fusion of language and vision in large vision-language models (LVLMs) has revolutionized deep learning-based object detection by enhancing adaptability, contextual reasoning, and generalization beyond traditional architectures. This in-depth review presents a structured exploration of the state-of-the-art in LVLMs, systematically organized through a three-step research review process. First, we discuss the functioning of vision language models (VLMs) for object detection, describing how these models harness natural language processing (NLP) and computer vision (CV) techniques to revolution...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.19294v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.19294v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Sapkota, and M. Karkee, &quot;Object Detection with Multimodal Large Vision-Language Models: An In-depth Review,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.19294v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444261488')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cv" data-title="evaluation and enhancement of semantic grounding in large vision-language models" data-keywords="ros language model cs.cv cs.cl" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2309.04041v2" target="_blank">Evaluation and Enhancement of Semantic Grounding in Large Vision-Language Models</a>
                            </h3>
                            <p class="card-authors">Jiaying Lu, Jinmeng Rao, Kezhen Chen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444262256">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Vision-Language Models (LVLMs) offer remarkable benefits for a variety of vision-language tasks. However, a challenge hindering their application in real-world scenarios, particularly regarding safety, robustness, and reliability, is their constrained semantic grounding ability, which pertains to connecting language to the physical-world entities or concepts referenced in images. Therefore, a crucial need arises for a comprehensive study to assess the semantic grounding ability of widely used LVLMs. Despite the significance, sufficient investigation in this direction is currently lacking...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To address this issue, we propose a data-centric enhancement method that aims to improve LVLMs&#x27; semantic grounding ability through multimodal instruction tuning on fine-grained conversations</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2309.04041v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2309.04041v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Lu et al., &quot;Evaluation and Enhancement of Semantic Grounding in Large Vision-Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2309.04041v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444262256')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cv" data-title="pushing boundaries: exploring zero shot object classification with large multimodal models" data-keywords="ros language model cs.cv cs.si" data-themes="S E M">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.00127v1" target="_blank">Pushing Boundaries: Exploring Zero Shot Object Classification with Large Multimodal Models</a>
                            </h3>
                            <p class="card-authors">Ashhadul Islam, Md. Rafiul Biswas, Wajdi Zaghouani et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span><span class="keyword-tag">CS.SI</span></div>
                            <div class="card-details" id="cat-details-4444263840">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">$ $The synergy of language and vision models has given rise to Large Language and Vision Assistant models (LLVAs), designed to engage users in rich conversational experiences intertwined with image-based queries. These comprehensive multimodal models seamlessly integrate vision encoders with Large Language Models (LLMs), expanding their applications in general-purpose language and visual comprehension. The advent of Large Multimodal Models (LMMs) heralds a new era in Artificial Intelligence (AI) assistance, extending the horizons of AI utilization. This paper takes a unique perspective on LMMs...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.00127v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.00127v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Islam, M. R. Biswas, W. Zaghouani, S. B. Belhaouari, and Z. Shah, &quot;Pushing Boundaries: Exploring Zero Shot Object Classification with Large Multimodal Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2401.00127v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444263840')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cv" data-title="mquant: unleashing the inference potential of multimodal large language models via full static quantization" data-keywords="attention language model cs.cv cs.ai" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.00425v2" target="_blank">MQuant: Unleashing the Inference Potential of Multimodal Large Language Models via Full Static Quantization</a>
                            </h3>
                            <p class="card-authors">JiangYong Yu, Sifan Zhou, Dawei Yang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4444263408">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Multimodal large language models (MLLMs) have garnered widespread attention due to their ability to understand multimodal input. However, their large parameter sizes and substantial computational demands severely hinder their practical deployment and application.While quantization is an effective way to reduce model size and inference latency, its application to MLLMs remains underexplored. In this paper, we propose MQuant, a post-training quantization (PTQ) framework designed to tackle the unique challenges of multimodal large language models (MLLMs). Conventional quantization often struggles...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose MQuant, a post-training quantization (PTQ) framework designed to tackle the unique challenges of multimodal large language models (MLLMs)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.00425v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.00425v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Yu et al., &quot;MQuant: Unleashing the Inference Potential of Multimodal Large Language Models via Full Static Quantization,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.00425v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444263408')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cv" data-title="lvlm-ehub: a comprehensive evaluation benchmark for large vision-language models" data-keywords="gpt language model cs.cv cs.ai" data-themes="E M">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2306.09265v1" target="_blank">LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models</a>
                            </h3>
                            <p class="card-authors">Peng Xu, Wenqi Shao, Kaipeng Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4441928128">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Vision-Language Models (LVLMs) have recently played a dominant role in multimodal vision-language learning. Despite the great success, it lacks a holistic evaluation of their efficacy. This paper presents a comprehensive evaluation of publicly available large multimodal models by building a LVLM evaluation Hub (LVLM-eHub). Our LVLM-eHub consists of $8$ representative LVLMs such as InstructBLIP and MiniGPT-4, which are thoroughly evaluated by a quantitative capability evaluation and an online arena platform. The former evaluates $6$ categories of multimodal capabilities of LVLMs such as v...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents a comprehensive evaluation of publicly available large multimodal models by building a LVLM evaluation Hub (LVLM-eHub)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2306.09265v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2306.09265v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'P. Xu et al., &quot;LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2306.09265v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441928128')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.cv" data-title="mme: a comprehensive evaluation benchmark for multimodal large language models" data-keywords="perception optimization language model cs.cv" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2306.13394v5" target="_blank">MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models</a>
                            </h3>
                            <p class="card-authors">Chaoyou Fu, Peixian Chen, Yunhang Shen et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Perception</span><span class="keyword-tag">Optimization</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CV</span></div>
                            <div class="card-details" id="cat-details-4444262832">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform multimodal tasks, showing amazing emergent abilities in recent studies, such as writing poems based on an image. However, it is difficult for these case studies to fully reflect the performance of MLLM, lacking a comprehensive evaluation. In this paper, we fill in this blank, presenting the first comprehensive MLLM Evaluation benchmark MME. It measures both perception and cognition abilities on a total of 14 subtasks. In order to avoid data leakage that may arise from direct use of public datasets for evaluation, the ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2306.13394v5" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2306.13394v5" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Fu et al., &quot;MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2306.13394v5')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444262832')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cv" data-title="integrating large language models into a tri-modal architecture for automated depression classification on the daic-woz" data-keywords="lstm gpt ros language model cs.cv cs.ai cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üëÅ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.19340v5" target="_blank">Integrating Large Language Models into a Tri-Modal Architecture for Automated Depression Classification on the DAIC-WOZ</a>
                            </h3>
                            <p class="card-authors">Santosh V. Patapati</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Lstm</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4444261104">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Major Depressive Disorder (MDD) is a pervasive mental health condition that affects 300 million people worldwide. This work presents a novel, BiLSTM-based tri-modal model-level fusion architecture for the binary classification of depression from clinical interview recordings. The proposed architecture incorporates Mel Frequency Cepstral Coefficients, Facial Action Units, and uses a two-shot learning based GPT-4 model to process text data. This is the first work to incorporate large language models into a multi-modal architecture for this task. It achieves impressive results on the DAIC-WOZ AVE...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.19340v5" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.19340v5" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. V. Patapati, &quot;Integrating Large Language Models into a Tri-Modal Architecture for Automated Depression Classification on the DAIC-WOZ,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.19340v5')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444261104')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="cs.ai">
                <h2 class="section-header">üè∑Ô∏è Artificial Intelligence <span class="section-count">(10 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ai" data-title="large language models reasoning abilities under non-ideal conditions after rl-fine-tuning" data-keywords="reinforcement learning ros language model cs.ai" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2508.04848v1" target="_blank">Large Language Models Reasoning Abilities Under Non-Ideal Conditions After RL-Fine-Tuning</a>
                            </h3>
                            <p class="card-authors">Chang Tian, Matthew B. Blaschko, Mingzhe Xing et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Reinforcement Learning</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4441918096">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Reinforcement learning (RL) has become a key technique for enhancing the reasoning abilities of large language models (LLMs), with policy-gradient algorithms dominating the post-training stage because of their efficiency and effectiveness. However, most existing benchmarks evaluate large-language-model reasoning under idealized settings, overlooking performance in realistic, non-ideal scenarios. We identify three representative non-ideal scenarios with practical relevance: summary inference, fine-grained noise suppression, and contextual filtering. We introduce a new research direction guided ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce a new research direction guided by brain-science findings that human reasoning remains reliable under imperfect inputs</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2508.04848v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2508.04848v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'C. Tian, M. B. Blaschko, M. Xing, X. Li, Y. Yue, and M. Moens, &quot;Large Language Models Reasoning Abilities Under Non-Ideal Conditions After RL-Fine-Tuning,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2508.04848v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441918096')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.ai" data-title="kglens: towards efficient and effective knowledge probing of large language models with knowledge graphs" data-keywords="simulation ros imu language model cs.ai cs.cl cs.lg" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.11539v3" target="_blank">KGLens: Towards Efficient and Effective Knowledge Probing of Large Language Models with Knowledge Graphs</a>
                            </h3>
                            <p class="card-authors">Shangshang Zheng, He Bai, Yizhe Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Simulation</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Imu</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4444262784">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Language Models (LLMs) might hallucinate facts, while curated Knowledge Graph (KGs) are typically factually reliable especially with domain-specific knowledge. Measuring the alignment between KGs and LLMs can effectively probe the factualness and identify the knowledge blind spots of LLMs. However, verifying the LLMs over extensive KGs can be expensive. In this paper, we present KGLens, a Thompson-sampling-inspired framework aimed at effectively and efficiently measuring the alignment between KGs and LLMs. KGLens features a graph-guided question generator for converting KGs into natural ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we present KGLens, a Thompson-sampling-inspired framework aimed at effectively and efficiently measuring the alignment between KGs and LLMs</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.11539v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.11539v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Zheng, H. Bai, Y. Zhang, Y. Su, X. Niu, and N. Jaitly, &quot;KGLens: Towards Efficient and Effective Knowledge Probing of Large Language Models with Knowledge Graphs,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.11539v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444262784')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ai" data-title="large language models for interpretable mental health diagnosis" data-keywords="language model cs.ai cs.lo" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2501.07653v2" target="_blank">Large Language Models for Interpretable Mental Health Diagnosis</a>
                            </h3>
                            <p class="card-authors">Brian Hyeongseok Kim, Chao Wang</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.LO</span></div>
                            <div class="card-details" id="cat-details-4444261248">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">We propose a clinical decision support system (CDSS) for mental health diagnosis that combines the strengths of large language models (LLMs) and constraint logic programming (CLP). Having a CDSS is important because of the high complexity of diagnostic manuals used by mental health professionals and the danger of diagnostic errors. Our CDSS is a software tool that uses an LLM to translate diagnostic manuals to a logic program and solves the program using an off-the-shelf CLP engine to query a patient&#x27;s diagnosis based on the encoded rules and provided data. By giving domain experts the opportu...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a clinical decision support system (CDSS) for mental health diagnosis that combines the strengths of large language models (LLMs) and constraint logic programming (CLP)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2501.07653v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2501.07653v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'B. H. Kim, and C. Wang, &quot;Large Language Models for Interpretable Mental Health Diagnosis,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2501.07653v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444261248')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ai" data-title="ars: adaptive reasoning suppression for efficient large reasoning language models" data-keywords="ros language model cs.ai cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2510.00071v2" target="_blank">ARS: Adaptive Reasoning Suppression for Efficient Large Reasoning Language Models</a>
                            </h3>
                            <p class="card-authors">Dongqi Zheng</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444263936">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Reasoning Language Models (LRLMs or LRMs) demonstrate remarkable capabilities in complex reasoning tasks, but suffer from significant computational inefficiencies due to overthinking phenomena. Existing efficient reasoning methods face the challenge of balancing reasoning quality with inference cost reduction. We propose \textbf{Adaptive Reasoning Suppression (ARS)}, a novel training-free approach that dynamically suppresses redundant reasoning steps while preserving accuracy through adaptive certainty monitoring. ARS introduces a multi-checkpoint certainty estimation mechanism with prog...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose \textbf{Adaptive Reasoning Suppression (ARS)}, a novel training-free approach that dynamically suppresses redundant reasoning steps while preserving accuracy through adaptive certainty monitoring</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2510.00071v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2510.00071v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Zheng, &quot;ARS: Adaptive Reasoning Suppression for Efficient Large Reasoning Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2510.00071v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444263936')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ai" data-title="from code to play: benchmarking program search for games using large language models" data-keywords="control ros language model cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.04057v2" target="_blank">From Code to Play: Benchmarking Program Search for Games Using Large Language Models</a>
                            </h3>
                            <p class="card-authors">Manuel Eberhardinger, James Goodman, Alexander Dockhorn et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Control</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4444261776">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) have shown impressive capabilities in generating program code, opening exciting opportunities for applying program synthesis to games. In this work, we explore the potential of LLMs to directly synthesize usable code for a wide range of gaming applications, focusing on two programming languages, Python and Java. We use an evolutionary hill-climbing algorithm, where the mutations and seeds of the initial programs are controlled by LLMs. For Python, the framework covers various game-related tasks, including five miniature versions of Atari games, ten levels of Baba i...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.04057v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.04057v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. Eberhardinger et al., &quot;From Code to Play: Benchmarking Program Search for Games Using Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.04057v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444261776')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ai" data-title="graph-of-thought: utilizing large language models to solve complex and dynamic business problems" data-keywords="ros language model cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Motion Planning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.06801v2" target="_blank">Graph-of-Thought: Utilizing Large Language Models to Solve Complex and Dynamic Business Problems</a>
                            </h3>
                            <p class="card-authors">Ye Li</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4444264656">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents Graph-of-Thought (GoT), a new model for workflow automation that enhances the flexibility and efficiency of Large Language Models (LLMs) in complex task execution. GoT advances beyond traditional linear and tree-like cognitive models with a graph structure that enables dynamic path selection. The open-source engine GoTFlow demonstrates the practical application of GoT, facilitating automated, data-driven decision-making across various domains. Despite challenges in complexity and transparency, GoTFlow&#x27;s potential for improving business processes is significant, promising ad...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents Graph-of-Thought (GoT), a new model for workflow automation that enhances the flexibility and efficiency of Large Language Models (LLMs) in complex task execution</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.06801v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.06801v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Li, &quot;Graph-of-Thought: Utilizing Large Language Models to Solve Complex and Dynamic Business Problems,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.06801v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444264656')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ai" data-title="from system 1 to system 2: a survey of reasoning large language models" data-keywords="language model cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.17419v6" target="_blank">From System 1 to System 2: A Survey of Reasoning Large Language Models</a>
                            </h3>
                            <p class="card-authors">Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4444263984">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more accurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently, reasoning LLMs like OpenAI&#x27;s o1/o3 and DeepSeek&#x27;s R1 have demonstrated expert-lev...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.17419v6" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.17419v6" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Z. Li et al., &quot;From System 1 to System 2: A Survey of Reasoning Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.17419v6')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444263984')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ai" data-title="game theory meets large language models: a systematic survey with taxonomy and new frontiers" data-keywords="language model cs.ai cs.gt cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Control Systems</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2502.09053v2" target="_blank">Game Theory Meets Large Language Models: A Systematic Survey with Taxonomy and New Frontiers</a>
                            </h3>
                            <p class="card-authors">Haoran Sun, Yusen Wu, Peng Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.GT</span><span class="keyword-tag">CS.LG</span></div>
                            <div class="card-details" id="cat-details-4444263456">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Game theory is a foundational framework for analyzing strategic interactions, and its intersection with large language models (LLMs) is a rapidly growing field. However, existing surveys mainly focus narrowly on using game theory to evaluate LLM behavior. This paper provides the first comprehensive survey of the bidirectional relationship between Game Theory and LLMs. We propose a novel taxonomy that categorizes the research in this intersection into four distinct perspectives: (1) evaluating LLMs in game-based scenarios; (2) improving LLMs using game-theoretic concepts for better interpretabi...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a novel taxonomy that categorizes the research in this intersection into four distinct perspectives: (1) evaluating LLMs in game-based scenarios; (2) improving LLMs using game-theoretic concepts for better interpretability and alignment; (3) modeling the competitive landscape of LLM development and its societal impact; and (4) leveraging LLMs to advance game models and to solve corresponding game theory problems</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2502.09053v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2502.09053v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'H. Sun et al., &quot;Game Theory Meets Large Language Models: A Systematic Survey with Taxonomy and New Frontiers,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2502.09053v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444263456')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.ai" data-title="language games as the pathway to artificial superhuman intelligence" data-keywords="multi-agent ros language model cs.ai cs.cl cs.ma" data-themes="E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Motion Planning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2501.18924v1" target="_blank">Language Games as the Pathway to Artificial Superhuman Intelligence</a>
                            </h3>
                            <p class="card-authors">Ying Wen, Ziyu Wan, Shao Zhang</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Multi-Agent</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4444263504">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The evolution of large language models (LLMs) toward artificial superhuman intelligence (ASI) hinges on data reproduction, a cyclical process in which models generate, curate and retrain on novel data to refine capabilities. Current methods, however, risk getting stuck in a data reproduction trap: optimizing outputs within fixed human-generated distributions in a closed loop leads to stagnation, as models merely recombine existing knowledge rather than explore new frontiers. In this paper, we propose language games as a pathway to expanded data reproduction, breaking this cycle through three m...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose language games as a pathway to expanded data reproduction, breaking this cycle through three mechanisms: (1) \textit{role fluidity}, which enhances data diversity and coverage by enabling multi-agent systems to dynamically shift roles across tasks; (2) \textit{reward variety}, embedding multiple feedback criteria that can drive complex intelligent behaviors; and (3) \textit{rule plasticity}, iteratively evolving interaction constraints to foster learnability, thereby injecting continual novelty</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2501.18924v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2501.18924v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Wen, Z. Wan, and S. Zhang, &quot;Language Games as the Pathway to Artificial Superhuman Intelligence,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2501.18924v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444263504')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ai" data-title="revealing hidden bias in ai: lessons from large language models" data-keywords="gpt ros language model cs.ai cs.cy" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">üîÆ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.16927v1" target="_blank">Revealing Hidden Bias in AI: Lessons from Large Language Models</a>
                            </h3>
                            <p class="card-authors">Django Beatty, Kritsada Masanthia, Teepakorn Kaphol et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4444262928">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">As large language models (LLMs) become integral to recruitment processes, concerns about AI-induced bias have intensified. This study examines biases in candidate interview reports generated by Claude 3.5 Sonnet, GPT-4o, Gemini 1.5, and Llama 3.1 405B, focusing on characteristics such as gender, race, and age. We evaluate the effectiveness of LLM-based anonymization in reducing these biases. Findings indicate that while anonymization reduces certain biases, particularly gender bias, the degree of effectiveness varies across models and bias types. Notably, Llama 3.1 405B exhibited the lowest ov...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Moreover, our methodology of comparing anonymized and non-anonymized data reveals a novel approach to assessing inherent biases in LLMs beyond recruitment applications</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.16927v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.16927v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'D. Beatty, K. Masanthia, T. Kaphol, and N. Sethi, &quot;Revealing Hidden Bias in AI: Lessons from Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.16927v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444262928')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="cs.cr">
                <h2 class="section-header">üè∑Ô∏è CS.CR <span class="section-count">(6 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cr" data-title="attacks on third-party apis of large language models" data-keywords="ros language model cs.cr cs.ai cs.cl" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2404.16891v1" target="_blank">Attacks on Third-Party APIs of Large Language Models</a>
                            </h3>
                            <p class="card-authors">Wanru Zhao, Vidit Khazanchi, Haodi Xing et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CR</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4441918480">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language model (LLM) services have recently begun offering a plugin ecosystem to interact with third-party API services. This innovation enhances the capabilities of LLMs, but it also introduces risks, as these plugins developed by various third parties cannot be easily trusted. This paper proposes a new attacking framework to examine security and safety vulnerabilities within LLM platforms that incorporate third-party services. Applying our framework specifically to widely used LLMs, we identify real-world malicious attacks across various domains on third-party APIs that can imperceptib...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2404.16891v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2404.16891v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'W. Zhao, V. Khazanchi, H. Xing, X. He, Q. Xu, and N. D. Lane, &quot;Attacks on Third-Party APIs of Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2404.16891v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441918480')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cr" data-title="large language models merging for enhancing the link stealing attack on graph neural networks" data-keywords="neural network gnn ros language model cs.cr cs.ai" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2412.05830v1" target="_blank">Large Language Models Merging for Enhancing the Link Stealing Attack on Graph Neural Networks</a>
                            </h3>
                            <p class="card-authors">Faqian Guan, Tianqing Zhu, Wenhan Chang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Gnn</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span></div>
                            <div class="card-details" id="cat-details-4444262688">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Graph Neural Networks (GNNs), specifically designed to process the graph data, have achieved remarkable success in various applications. Link stealing attacks on graph data pose a significant privacy threat, as attackers aim to extract sensitive relationships between nodes (entities), potentially leading to academic misconduct, fraudulent transactions, or other malicious activities. Previous studies have primarily focused on single datasets and did not explore cross-dataset attacks, let alone attacks that leverage the combined knowledge of multiple attackers. However, we find that an attacker ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we propose a novel link stealing attack method that takes advantage of cross-dataset and Large Language Models (LLMs)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2412.05830v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2412.05830v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'F. Guan, T. Zhu, W. Chang, W. Ren, and W. Zhou, &quot;Large Language Models Merging for Enhancing the Link Stealing Attack on Graph Neural Networks,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2412.05830v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444262688')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cr" data-title="adashield: safeguarding multimodal large language models from structure-based attack via adaptive shield prompting" data-keywords="language model cs.cr cs.ai" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2403.09513v1" target="_blank">AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting</a>
                            </h3>
                            <p class="card-authors">Yu Wang, Xiaogeng Liu, Yu Li et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CR</span><span class="keyword-tag">CS.AI</span></div>
                            <div class="card-details" id="cat-details-4444262160">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">With the advent and widespread deployment of Multimodal Large Language Models (MLLMs), the imperative to ensure their safety has become increasingly pronounced. However, with the integration of additional modalities, MLLMs are exposed to new vulnerabilities, rendering them prone to structured-based jailbreak attacks, where semantic content (e.g., &quot;harmful text&quot;) has been injected into the images to mislead MLLMs. In this work, we aim to defend against such threats. Specifically, we propose \textbf{Ada}ptive \textbf{Shield} Prompting (\textbf{AdaShield}), which prepends inputs with defense prom...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">Specifically, we propose \textbf{Ada}ptive \textbf{Shield} Prompting (\textbf{AdaShield}), which prepends inputs with defense prompts to defend MLLMs against structure-based jailbreak attacks without fine-tuning MLLMs or training additional modules (e.g., post-stage content detector)</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2403.09513v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2403.09513v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Wang, X. Liu, Y. Li, M. Chen, and C. Xiao, &quot;AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2403.09513v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444262160')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cr" data-title="sbfa: single sneaky bit flip attack to break large language models" data-keywords="neural network ros language model cs.cr cs.cl cs.lg" data-themes="S E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2509.21843v1" target="_blank">SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models</a>
                            </h3>
                            <p class="card-authors">Jingkai Guo, Chaitali Chakrabarti, Deliang Fan</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Neural Network</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CR</span></div>
                            <div class="card-details" id="cat-details-4444262016">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Model integrity of Large language models (LLMs) has become a pressing security concern with their massive online deployment. Prior Bit-Flip Attacks (BFAs) -- a class of popular AI weight memory fault-injection techniques -- can severely compromise Deep Neural Networks (DNNs): as few as tens of bit flips can degrade accuracy toward random guessing. Recent studies extend BFAs to LLMs and reveal that, despite the intuition of better robustness from modularity and redundancy, only a handful of adversarial bit flips can also cause LLMs&#x27; catastrophic accuracy degradation. However, existing BFA metho...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, for the first time, we propose SBFA (Sneaky Bit-Flip Attack), which collapses LLM performance with only one single bit flip while keeping perturbed values within benign layer-wise weight distribution</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2509.21843v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2509.21843v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Guo, C. Chakrabarti, and D. Fan, &quot;SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2509.21843v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444262016')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.cr" data-title="on trojan signatures in large language models of code" data-keywords="computer vision language model cs.cr cs.lg cs.se" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2402.16896v2" target="_blank">On Trojan Signatures in Large Language Models of Code</a>
                            </h3>
                            <p class="card-authors">Aftab Hussain, Md Rafiqul Islam Rabin, Mohammad Amin Alipour</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Computer Vision</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CR</span><span class="keyword-tag">CS.LG</span></div>
                            <div class="card-details" id="cat-details-4444263792">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Trojan signatures, as described by Fields et al. (2021), are noticeable differences in the distribution of the trojaned class parameters (weights) and the non-trojaned class parameters of the trojaned model, that can be used to detect the trojaned model. Fields et al. (2021) found trojan signatures in computer vision classification tasks with image models, such as, Resnet, WideResnet, Densenet, and VGG. In this paper, we investigate such signatures in the classifier layer parameters of large language models of source code.
  Our results suggest that trojan signatures could not generalize to LL...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2402.16896v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2402.16896v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. Hussain, M. R. I. Rabin, and M. A. Alipour, &quot;On Trojan Signatures in Large Language Models of Code,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2402.16896v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444263792')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.cr" data-title="augmenting anonymized data with ai: exploring the feasibility and limitations of large language models in data enrichment" data-keywords="language model cs.cr cs.et" data-themes="S E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.03778v1" target="_blank">Augmenting Anonymized Data with AI: Exploring the Feasibility and Limitations of Large Language Models in Data Enrichment</a>
                            </h3>
                            <p class="card-authors">Stefano Cirillo, Domenico Desiato, Giuseppe Polese et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-S">Safety</span><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CR</span><span class="keyword-tag">CS.ET</span></div>
                            <div class="card-details" id="cat-details-4444264608">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large Language Models (LLMs) have demonstrated advanced capabilities in both text generation and comprehension, and their application to data archives might facilitate the privatization of sensitive information about the data subjects. In fact, the information contained in data often includes sensitive and personally identifiable details. This data, if not safeguarded, may bring privacy risks in terms of both disclosure and identification. Furthermore, the application of anonymisation techniques, such as k-anonymity, can lead to a significant reduction in the amount of data within data sources...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To this end, we designed new ad-hoc prompt template engineering strategies to perform anonymized Data Augmentation and assess the effectiveness of LLM-based approaches in providing anonymized data</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.03778v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.03778v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Cirillo, D. Desiato, G. Polese, M. M. L. Sebillo, and G. Solimando, &quot;Augmenting Anonymized Data with AI: Exploring the Feasibility and Limitations of Large Language Models in Data Enrichment,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.03778v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444264608')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="cs.hc">
                <h2 class="section-header">üè∑Ô∏è Human-Computer Interaction <span class="section-count">(5 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.hc" data-title="exploring large language models to facilitate variable autonomy for human-robot teaming" data-keywords="transformer gpt robot multi-robot language model cs.hc cs.ai cs.ro" data-themes="E R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Deep Learning</span><span class="tech-chip">Reinforcement Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.07214v3" target="_blank">Exploring Large Language Models to Facilitate Variable Autonomy for Human-Robot Teaming</a>
                            </h3>
                            <p class="card-authors">Younes Lakhnati, Max Pascher, Jens Gerken</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Transformer</span><span class="keyword-tag">Gpt</span><span class="keyword-tag">Robot</span><span class="keyword-tag">Multi-Robot</span></div>
                            <div class="card-details" id="cat-details-4441927072">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">In a rapidly evolving digital landscape autonomous tools and robots are becoming commonplace. Recognizing the significance of this development, this paper explores the integration of Large Language Models (LLMs) like Generative pre-trained transformer (GPT) into human-robot teaming environments to facilitate variable autonomy through the means of verbal human-robot communication. In this paper, we introduce a novel framework for such a GPT-powered multi-robot testbed environment, based on a Unity Virtual Reality (VR) setting. This system allows users to interact with robot agents through natur...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this paper, we introduce a novel framework for such a GPT-powered multi-robot testbed environment, based on a Unity Virtual Reality (VR) setting</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.07214v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.07214v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Lakhnati, M. Pascher, and J. Gerken, &quot;Exploring Large Language Models to Facilitate Variable Autonomy for Human-Robot Teaming,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.07214v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441927072')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.hc" data-title="exploring bengali religious dialect biases in large language models with evaluation perspectives" data-keywords="gpt ros language model cs.hc cs.cl cs.cy" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2407.18376v1" target="_blank">Exploring Bengali Religious Dialect Biases in Large Language Models with Evaluation Perspectives</a>
                            </h3>
                            <p class="card-authors">Azmine Toushik Wasi, Raima Islam, Mst Rafia Islam et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Gpt</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.HC</span></div>
                            <div class="card-details" id="cat-details-4441921696">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">While Large Language Models (LLM) have created a massive technological impact in the past decade, allowing for human-enabled applications, they can produce output that contains stereotypes and biases, especially when using low-resource languages. This can be of great ethical concern when dealing with sensitive topics such as religion. As a means toward making LLMS more fair, we explore bias from a religious perspective in Bengali, focusing specifically on two main religious dialects: Hindu and Muslim-majority dialects. Here, we perform different experiments and audit showing the comparative an...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2407.18376v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2407.18376v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. T. Wasi, R. Islam, M. R. Islam, T. H. Rafi, and D. Chae, &quot;Exploring Bengali Religious Dialect Biases in Large Language Models with Evaluation Perspectives,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2407.18376v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441921696')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.hc" data-title="large language models will change the way children think about technology and impact every interaction paradigm" data-keywords="language model cs.hc cs.ai cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2504.13667v1" target="_blank">Large Language Models Will Change The Way Children Think About Technology And Impact Every Interaction Paradigm</a>
                            </h3>
                            <p class="card-authors">Russell Beale</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.HC</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444260816">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">This paper presents a hopeful perspective on the potentially dramatic impacts of Large Language Models on how we children learn and how they will expect to interact with technology. We review the effects of LLMs on education so far, and make the case that these effects are minor compared to the upcoming changes that are occurring. We present a small scenario and self-ethnographic study demonstrating the effects of these changes, and define five significant considerations that interactive systems designers will have to accommodate in the future.</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">This paper presents a hopeful perspective on the potentially dramatic impacts of Large Language Models on how we children learn and how they will expect to interact with technology</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2504.13667v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2504.13667v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'R. Beale, &quot;Large Language Models Will Change The Way Children Think About Technology And Impact Every Interaction Paradigm,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2504.13667v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444260816')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.hc" data-title="users&#x27; perception on appropriateness of robotic coaching assistant&#x27;s disclosure behaviors" data-keywords="robot perception cs.hc" data-themes="L">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-skip">‚è≠Ô∏è Skip</span></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2410.10550v1" target="_blank">Users&#x27; Perception on Appropriateness of Robotic Coaching Assistant&#x27;s Disclosure Behaviors</a>
                            </h3>
                            <p class="card-authors">Atikkhan Faridkhan Nilgar, Manuel Dietrich, Kristof Van Laerhoven</p>
                            <div class="theme-tags"><span class="theme-badge theme-L">Locomotion</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Perception</span><span class="keyword-tag">CS.HC</span></div>
                            <div class="card-details" id="cat-details-4445408320">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Social robots have emerged as valuable contributors to individuals&#x27; well-being coaching. Notably, their integration into long-term human coaching trials shows particular promise, emphasizing a complementary role alongside human coaches rather than outright replacement. In this context, robots serve as supportive entities during coaching sessions, offering insights based on their knowledge about users&#x27; well-being and activity. Traditionally, such insights have been gathered through methods like written self-reports or wearable data visualizations. However, the disclosure of people&#x27;s information...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2410.10550v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2410.10550v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'A. F. Nilgar, M. Dietrich, and K. V. Laerhoven, &quot;Users&#x27; Perception on Appropriateness of Robotic Coaching Assistant&#x27;s Disclosure Behaviors,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2410.10550v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445408320')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.hc" data-title="help or hindrance: understanding the impact of robot communication in action teams" data-keywords="robot coordination perception cs.hc cs.ro" data-themes="R M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"><span class="badge rating-optional">üìñ Optional</span></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Computer Vision</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.08892v3" target="_blank">Help or Hindrance: Understanding the Impact of Robot Communication in Action Teams</a>
                            </h3>
                            <p class="card-authors">Tauhid Tanjim, Jonathan St. George, Kevin Ching et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-R">Multi-Robot</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Coordination</span><span class="keyword-tag">Perception</span><span class="keyword-tag">CS.HC</span></div>
                            <div class="card-details" id="cat-details-4445418976">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The human-robot interaction (HRI) field has recognized the importance of enabling robots to interact with teams. Human teams rely on effective communication for successful collaboration in time-sensitive environments. Robots can play a role in enhancing team coordination through real-time assistance. Despite significant progress in human-robot teaming research, there remains an essential gap in how robots can effectively communicate with action teams using multimodal interaction cues in time-sensitive environments. This study addresses this knowledge gap in an experimental in-lab study to inve...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.08892v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.08892v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'T. Tanjim, J. S. George, K. Ching, and A. Taylor, &quot;Help or Hindrance: Understanding the Impact of Robot Communication in Action Teams,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.08892v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4445418976')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="cs.se">
                <h2 class="section-header">üè∑Ô∏è CS.SE <span class="section-count">(3 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cs.se" data-title="a survey of aiops in the era of large language models" data-keywords="attention ros language model cs.se cs.cl" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span><span class="tech-chip">Imitation Learning</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2507.12472v1" target="_blank">A Survey of AIOps in the Era of Large Language Models</a>
                            </h3>
                            <p class="card-authors">Lingzhe Zhang, Tong Jia, Mengxi Jia et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Attention</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.SE</span></div>
                            <div class="card-details" id="cat-details-4441927552">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">As large language models (LLMs) grow increasingly sophisticated and pervasive, their application to various Artificial Intelligence for IT Operations (AIOps) tasks has garnered significant attention. However, a comprehensive understanding of the impact, potential, and limitations of LLMs in AIOps remains in its infancy. To address this gap, we conducted a detailed survey of LLM4AIOps, focusing on how LLMs can optimize processes and improve outcomes in this domain. We analyzed 183 research papers published between January 2020 and December 2024 to answer four key research questions (RQs). In RQ...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2507.12472v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2507.12472v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'L. Zhang et al., &quot;A Survey of AIOps in the Era of Large Language Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2507.12472v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441927552')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.se" data-title="metal: metamorphic testing framework for analyzing large-language model qualities" data-keywords="language model cs.se cs.ai cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.06056v1" target="_blank">METAL: Metamorphic Testing Framework for Analyzing Large-Language Model Qualities</a>
                            </h3>
                            <p class="card-authors">Sangwon Hyun, Mingyu Guo, M. Ali Babar</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.SE</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444263072">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large-Language Models (LLMs) have shifted the paradigm of natural language data processing. However, their black-boxed and probabilistic characteristics can lead to potential risks in the quality of outputs in diverse LLM applications. Recent studies have tested Quality Attributes (QAs), such as robustness or fairness, of LLMs by generating adversarial input texts. However, existing studies have limited their coverage of QAs and tasks in LLMs and are difficult to extend. Additionally, these studies have only used one evaluation metric, Attack Success Rate (ASR), to assess the effectiveness of ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose a MEtamorphic Testing for Analyzing LLMs (METAL) framework to address these issues by applying Metamorphic Testing (MT) techniques</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.06056v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.06056v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Hyun, M. Guo, and M. A. Babar, &quot;METAL: Metamorphic Testing Framework for Analyzing Large-Language Model Qualities,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.06056v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444263072')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="cs.se" data-title="assurance for autonomy -- jpl&#x27;s past research, lessons learned, and future directions" data-keywords="robot control cs.se cs.ro" data-themes="M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2305.11902v1" target="_blank">Assurance for Autonomy -- JPL&#x27;s past research, lessons learned, and future directions</a>
                            </h3>
                            <p class="card-authors">Martin S. Feather, Alessandro Pinto</p>
                            <div class="theme-tags"><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Robot</span><span class="keyword-tag">Control</span><span class="keyword-tag">CS.SE</span><span class="keyword-tag">CS.RO</span></div>
                            <div class="card-details" id="cat-details-4444264176">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Robotic space missions have long depended on automation, defined in the 2015 NASA Technology Roadmaps as &quot;the automatically-controlled operation of an apparatus, process, or system using a pre-planned set of instructions (e.g., a command sequence),&quot; to react to events when a rapid response is required. Autonomy, defined there as &quot;the capacity of a system to achieve goals while operating independently from external control,&quot; is required when a wide variation in circumstances precludes responses being pre-planned, instead autonomy follows an on-board deliberative process to determine the situati...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2305.11902v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2305.11902v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'M. S. Feather, and A. Pinto, &quot;Assurance for Autonomy -- JPL&#x27;s past research, lessons learned, and future directions,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2305.11902v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444264176')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="eess.as">
                <h2 class="section-header">üè∑Ô∏è EESS.AS <span class="section-count">(2 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-source="arXiv" data-year="2023" data-category="eess.as" data-title="acoustic prompt tuning: empowering large language models with audition capabilities" data-keywords="ros language model eess.as" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2023</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Computer Vision</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2312.00249v2" target="_blank">Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities</a>
                            </h3>
                            <p class="card-authors">Jinhua Liang, Xubo Liu, Wenwu Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">EESS.AS</span></div>
                            <div class="card-details" id="cat-details-4444262448">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The auditory system plays a substantial role in shaping the overall human perceptual experience. While prevailing large language models (LLMs) and visual language models (VLMs) have shown their promise in solving a wide variety of language and vision understanding tasks, only a few of them can be generalised to the audio domain without compromising their domain-specific capability. In this work, we introduce Acoustic Prompt Tuning (APT), a new adapter extending LLMs and VLMs to the audio domain by injecting audio embeddings to the input of LLMs, namely soft prompting. Specifically, APT applies...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">In this work, we introduce Acoustic Prompt Tuning (APT), a new adapter extending LLMs and VLMs to the audio domain by injecting audio embeddings to the input of LLMs, namely soft prompting</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2312.00249v2" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2312.00249v2" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Liang, X. Liu, W. Wang, M. D. Plumbley, H. Phan, and E. Benetos, &quot;Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities,&quot; arXiv, 2023. [Online]. Available: http://arxiv.org/abs/2312.00249v2')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444262448')">Show More</button>
                        </div>
                    </div>

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="eess.as" data-title="speechprompt: prompting speech language models for speech processing tasks" data-keywords="language model eess.as cs.ai cs.cl" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2408.13040v1" target="_blank">SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks</a>
                            </h3>
                            <p class="card-authors">Kai-Wei Chang, Haibin Wu, Yu-Kai Wang et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">EESS.AS</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.CL</span></div>
                            <div class="card-details" id="cat-details-4444263648">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Prompting has become a practical method for utilizing pre-trained language models (LMs). This approach offers several advantages. It allows an LM to adapt to new tasks with minimal training and parameter updates, thus achieving efficiency in both storage and computation. Additionally, prompting modifies only the LM&#x27;s inputs and harnesses the generative capabilities of language models to address various downstream tasks in a unified manner. This significantly reduces the need for human labor in designing task-specific models. These advantages become even more evident as the number of tasks serv...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2408.13040v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2408.13040v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'K. Chang et al., &quot;SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2408.13040v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444263648')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="cond-mat.mtrl-sci">
                <h2 class="section-header">üè∑Ô∏è COND-MAT.MTRL-SCI <span class="section-count">(1 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="cond-mat.mtrl-sci" data-title="hierarchical multi-agent large language model reasoning for autonomous functional materials discovery" data-keywords="multi-agent simulation ros imu language model cond-mat.mtrl-sci cs.ai cs.cl" data-themes="E I R">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Multi-Agent Systems</span><span class="tech-chip">Simulation</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2512.13930v1" target="_blank">Hierarchical Multi-agent Large Language Model Reasoning for Autonomous Functional Materials Discovery</a>
                            </h3>
                            <p class="card-authors">Samuel Rothfarb, Megan C. Davis, Ivana Matanovic et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span><span class="theme-badge theme-R">Multi-Robot</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Multi-Agent</span><span class="keyword-tag">Simulation</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Imu</span></div>
                            <div class="card-details" id="cat-details-4441922224">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Artificial intelligence is reshaping scientific exploration, but most methods automate procedural tasks without engaging in scientific reasoning, limiting autonomy in discovery. We introduce Materials Agents for Simulation and Theory in Electronic-structure Reasoning (MASTER), an active learning framework where large language models autonomously design, execute, and interpret atomistic simulations. In MASTER, a multimodal system translates natural language into density functional theory workflows, while higher-level reasoning agents guide discovery through a hierarchy of strategies, including ...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We introduce Materials Agents for Simulation and Theory in Electronic-structure Reasoning (MASTER), an active learning framework where large language models autonomously design, execute, and interpret atomistic simulations</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2512.13930v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2512.13930v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. Rothfarb, M. C. Davis, I. Matanovic, B. Li, E. F. Holby, and W. J. M. Kort-Kamp, &quot;Hierarchical Multi-agent Large Language Model Reasoning for Autonomous Functional Materials Discovery,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2512.13930v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4441922224')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="cs.ce">
                <h2 class="section-header">üè∑Ô∏è CS.CE <span class="section-count">(1 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ce" data-title="leveraging large language models for institutional portfolio management: persona-based ensembles" data-keywords="ros language model cs.ce cs.ma" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2411.19515v1" target="_blank">Leveraging Large Language Models for Institutional Portfolio Management: Persona-Based Ensembles</a>
                            </h3>
                            <p class="card-authors">Yoshia Abe, Shuhei Matsuo, Ryoma Kondo et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.CE</span><span class="keyword-tag">CS.MA</span></div>
                            <div class="card-details" id="cat-details-4444261824">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) have demonstrated promising performance in various financial applications, though their potential in complex investment strategies remains underexplored. To address this gap, we investigate how LLMs can predict price movements in stock and bond portfolios using economic indicators, enabling portfolio adjustments akin to those employed by institutional investors. Additionally, we explore the impact of incorporating different personas within LLMs, using an ensemble approach to leverage their diverse predictions. Our findings show that LLM-based strategies, especially...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2411.19515v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2411.19515v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'Y. Abe, S. Matsuo, R. Kondo, and R. Hisano, &quot;Leveraging Large Language Models for Institutional Portfolio Management: Persona-Based Ensembles,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2411.19515v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444261824')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="q-bio.qm">
                <h2 class="section-header">üè∑Ô∏è Q-BIO.QM <span class="section-count">(1 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="q-bio.qm" data-title="gene-associated disease discovery powered by large language models" data-keywords="language model q-bio.qm cs.ir" data-themes="E I">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.09490v1" target="_blank">Gene-associated Disease Discovery Powered by Large Language Models</a>
                            </h3>
                            <p class="card-authors">Jiayu Chang, Shiyu Wang, Chen Ling et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-I">Imitation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Language Model</span><span class="keyword-tag">Q-BIO.QM</span><span class="keyword-tag">CS.IR</span></div>
                            <div class="card-details" id="cat-details-4444263744">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">The intricate relationship between genetic variation and human diseases has been a focal point of medical research, evidenced by the identification of risk genes regarding specific diseases. The advent of advanced genome sequencing techniques has significantly improved the efficiency and cost-effectiveness of detecting these genetic markers, playing a crucial role in disease diagnosis and forming the basis for clinical decision-making and early risk assessment. To overcome the limitations of existing databases that record disease-gene associations from existing literature, which often lack rea...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">To overcome the limitations of existing databases that record disease-gene associations from existing literature, which often lack real-time updates, we propose a novel framework employing Large Language Models (LLMs) for the discovery of diseases associated with specific genes</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.09490v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.09490v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'J. Chang, S. Wang, C. Ling, Z. Qin, and L. Zhao, &quot;Gene-associated Disease Discovery Powered by Large Language Models,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.09490v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444263744')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="cs.ne">
                <h2 class="section-header">üè∑Ô∏è Neural & Evolutionary <span class="section-count">(1 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-source="arXiv" data-year="2024" data-category="cs.ne" data-title="evolutionary computation in the era of large language model: survey and roadmap" data-keywords="optimization ros language model cs.ne cs.ai cs.cl" data-themes="E M">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2024</span>
                            <div class="card-tech"><span class="tech-chip">Natural Language</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2401.10034v3" target="_blank">Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap</a>
                            </h3>
                            <p class="card-authors">Xingyu Wu, Sheng-hao Wu, Jibin Wu et al.</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span><span class="theme-badge theme-M">Manipulation</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Optimization</span><span class="keyword-tag">Ros</span><span class="keyword-tag">Language Model</span><span class="keyword-tag">CS.NE</span></div>
                            <div class="card-details" id="cat-details-4444263888">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Large language models (LLMs) have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride towards artificial general intelligence. The interplay between LLMs and evolutionary algorithms (EAs), despite differing in objectives and methodologies, share a common pursuit of applicability in complex problems. Meanwhile, EA can provide an optimization framework for LLM&#x27;s further enhancement under black-box settings, empowering LLM with flexible global search capacities. On the other hand, the abundant domain knowledge inhere...</p>
                                </div>
                                
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2401.10034v3" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2401.10034v3" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'X. Wu, S. Wu, J. Wu, L. Feng, and K. C. Tan, &quot;Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap,&quot; arXiv, 2024. [Online]. Available: http://arxiv.org/abs/2401.10034v3')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444263888')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

            <div class="group-section cat-section" data-category="q-bio.gn">
                <h2 class="section-header">üè∑Ô∏è Q-BIO.GN <span class="section-count">(1 papers)</span></h2>
                <div class="papers-grid">

                    <div class="paper-card" data-source="arXiv" data-year="2025" data-category="q-bio.gn" data-title="deepseq: high-throughput single-cell rna sequencing data labeling via web search-augmented agentic generative ai foundation models" data-keywords="q-bio.gn cs.ai cs.lg" data-themes="E">
                        <div class="card-visual">
                            <span class="card-visual-icon">ü§ñ</span>
                            <div class="card-badges"></div>
                            <span class="card-source">arXiv ¬∑ 2025</span>
                            <div class="card-tech"><span class="tech-chip">Reinforcement Learning</span><span class="tech-chip">Control Systems</span></div>
                        </div>
                        <div class="card-content">
                            <h3 class="card-title">
                                <a href="http://arxiv.org/abs/2506.13817v1" target="_blank">DeepSeq: High-Throughput Single-Cell RNA Sequencing Data Labeling via Web Search-Augmented Agentic Generative AI Foundation Models</a>
                            </h3>
                            <p class="card-authors">Saleem A. Al Dajani, Abel Sanchez, John R. Williams</p>
                            <div class="theme-tags"><span class="theme-badge theme-E">Embodied AI</span></div>
                            <div class="card-keywords"><span class="keyword-tag">Q-BIO.GN</span><span class="keyword-tag">CS.AI</span><span class="keyword-tag">CS.LG</span></div>
                            <div class="card-details" id="cat-details-4444263312">
                                
                                <div class="detail-block">
                                    <div class="detail-label">Abstract</div>
                                    <p class="detail-text abstract-text">Generative AI foundation models offer transformative potential for processing structured biological data, particularly in single-cell RNA sequencing, where datasets are rapidly scaling toward billions of cells. We propose the use of agentic foundation models with real-time web search to automate the labeling of experimental data, achieving up to 82.5% accuracy. This addresses a key bottleneck in supervised learning for structured omics data by increasing annotation throughput without manual curation and human error. Our approach enables the development of virtual cell foundation models capable...</p>
                                </div>
                                <div class="detail-block"><div class="detail-label">Core Contribution</div><p class="detail-text">We propose the use of agentic foundation models with real-time web search to automate the labeling of experimental data, achieving up to 82.5% accuracy</p></div>
                            </div>
                        </div>
                        <div class="card-footer">
                            <a href="http://arxiv.org/abs/2506.13817v1" class="card-link link-paper" target="_blank">üìÑ Paper</a><a href="https://arxiv.org/pdf/2506.13817v1" class="card-link link-pdf" target="_blank">‚¨á PDF</a>
                            <button class="cite-btn" onclick="copyCitation(this, 'S. A. A. Dajani, A. Sanchez, and J. R. Williams, &quot;DeepSeq: High-Throughput Single-Cell RNA Sequencing Data Labeling via Web Search-Augmented Agentic Generative AI Foundation Models,&quot; arXiv, 2025. [Online]. Available: http://arxiv.org/abs/2506.13817v1')">üìã Cite</button>
                            <button class="expand-toggle" onclick="toggleCard(this, 'cat-details-4444263312')">Show More</button>
                        </div>
                    </div>

                </div>
            </div>

        </div>
    </div>

    <!-- Citation Tooltip -->
    <div class="cite-tooltip" id="cite-tooltip">‚úì IEEE Citation Copied!</div>

    <script>
        let currentSource = 'all';
        let currentYear = 'all';
        let currentCategory = 'all';
        let currentSearch = '';
        let currentView = 'year';

        function setView(view) {
            currentView = view;
            document.getElementById('btn-year').classList.toggle('active', view === 'year');
            document.getElementById('btn-category').classList.toggle('active', view === 'category');
            document.getElementById('year-view').classList.toggle('hidden', view !== 'year');
            document.getElementById('category-view').classList.toggle('hidden', view !== 'category');

            // Reset filters when switching views
            currentYear = 'all';
            currentCategory = 'all';
            document.querySelectorAll('.year-btn, .cat-btn').forEach(btn => btn.classList.remove('active'));
            document.querySelector('.year-btn').classList.add('active');
            document.querySelector('.cat-btn').classList.add('active');

            applyFilters();
        }

        function applyFilters() {
            const activeView = currentView === 'year' ? '#year-view' : '#category-view';

            document.querySelectorAll(activeView + ' .paper-card').forEach(card => {
                const matchSource = currentSource === 'all' || card.dataset.source === currentSource;
                const matchYear = currentYear === 'all' || card.dataset.year === String(currentYear);
                const matchCategory = currentCategory === 'all' || card.dataset.category === currentCategory;
                const matchSearch = currentSearch === '' ||
                    card.dataset.title.includes(currentSearch) ||
                    card.dataset.keywords.includes(currentSearch);

                if (matchSource && matchYear && matchCategory && matchSearch) {
                    card.classList.remove('hidden');
                } else {
                    card.classList.add('hidden');
                }
            });

            // Hide empty sections
            document.querySelectorAll(activeView + ' .group-section').forEach(section => {
                const visibleCards = section.querySelectorAll('.paper-card:not(.hidden)');
                section.style.display = visibleCards.length === 0 ? 'none' : 'block';
            });
        }

        function filterBySource(source) {
            currentSource = source;
            document.querySelectorAll('.source-btn').forEach(btn => btn.classList.remove('active'));
            event.target.classList.add('active');
            applyFilters();
        }

        function filterByYear(year) {
            currentYear = year;
            document.querySelectorAll('.year-btn').forEach(btn => btn.classList.remove('active'));
            event.target.classList.add('active');
            applyFilters();
        }

        function filterByCategory(category) {
            currentCategory = category;
            document.querySelectorAll('.cat-btn').forEach(btn => btn.classList.remove('active'));
            event.target.classList.add('active');
            applyFilters();
        }

        function searchPapers(query) {
            currentSearch = query.toLowerCase();
            applyFilters();
        }

        function toggleCard(btn, detailsId) {
            const details = document.getElementById(detailsId);
            details.classList.toggle('show');
            btn.textContent = details.classList.contains('show') ? 'Show Less' : 'Show More';
        }

        function copyCitation(btn, citation) {
            // Decode HTML entities
            const textarea = document.createElement('textarea');
            textarea.innerHTML = citation;
            const decodedCitation = textarea.value;

            // Copy to clipboard
            navigator.clipboard.writeText(decodedCitation).then(() => {
                // Visual feedback on button
                const originalText = btn.innerHTML;
                btn.innerHTML = '‚úì Copied!';
                btn.classList.add('copied');

                // Show tooltip
                const tooltip = document.getElementById('cite-tooltip');
                tooltip.classList.add('show');

                // Reset after 2 seconds
                setTimeout(() => {
                    btn.innerHTML = originalText;
                    btn.classList.remove('copied');
                    tooltip.classList.remove('show');
                }, 2000);
            }).catch(err => {
                // Fallback for older browsers
                const textArea = document.createElement('textarea');
                textArea.value = decodedCitation;
                textArea.style.position = 'fixed';
                textArea.style.left = '-999999px';
                document.body.appendChild(textArea);
                textArea.select();
                try {
                    document.execCommand('copy');
                    btn.innerHTML = '‚úì Copied!';
                    btn.classList.add('copied');
                    setTimeout(() => {
                        btn.innerHTML = 'üìã Cite';
                        btn.classList.remove('copied');
                    }, 2000);
                } catch (e) {
                    alert('Failed to copy citation. Please copy manually:\n\n' + decodedCitation);
                }
                document.body.removeChild(textArea);
            });
        }
    </script>
</body>
</html>
